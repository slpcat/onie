diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/common.mk linux-5.4.86.new/drivers/net/ethernet/intel/i40e/common.mk
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/common.mk	1969-12-31 19:00:00.000000000 -0500
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/common.mk	2024-05-13 03:58:25.368491972 -0400
@@ -0,0 +1,392 @@
+# SPDX-License-Identifier: GPL-2.0
+# Copyright(c) 2013 - 2021 Intel Corporation.
+
+#
+# common Makefile rules useful for out-of-tree Linux driver builds
+#
+# Usage: include common.mk
+#
+# After including, you probably want to add a minimum_kver_check call
+#
+# Required Variables:
+# DRIVER
+#   -- Set to the lowercase driver name
+
+#####################
+# Helpful functions #
+#####################
+
+readlink = $(shell readlink -f ${1})
+
+# helper functions for converting kernel version to version codes
+get_kver = $(or $(word ${2},$(subst ., ,${1})),0)
+get_kvercode = $(shell [ "${1}" -ge 0 -a "${1}" -le 255 2>/dev/null ] && \
+                       [ "${2}" -ge 0 -a "${2}" -le 255 2>/dev/null ] && \
+                       [ "${3}" -ge 0 -a "${3}" -le 255 2>/dev/null ] && \
+                       printf %d $$(( ( ${1} << 16 ) + ( ${2} << 8 ) + ( ${3} ) )) )
+
+################
+# depmod Macro #
+################
+
+cmd_depmod = /sbin/depmod $(if ${SYSTEM_MAP_FILE},-e -F ${SYSTEM_MAP_FILE}) \
+                          $(if $(strip ${INSTALL_MOD_PATH}),-b ${INSTALL_MOD_PATH}) \
+                          -a ${KVER}
+
+################
+# dracut Macro #
+################
+
+cmd_initrd := $(shell \
+                if which dracut > /dev/null 2>&1 ; then \
+                    echo "dracut --force"; \
+                elif which update-initramfs > /dev/null 2>&1 ; then \
+                    echo "update-initramfs -u"; \
+                fi )
+
+#####################
+# Environment tests #
+#####################
+
+DRIVER_UPPERCASE := $(shell echo ${DRIVER} | tr "[:lower:]" "[:upper:]")
+
+ifeq (,${BUILD_KERNEL})
+BUILD_KERNEL=$(shell uname -r)
+endif
+
+# Kernel Search Path
+# All the places we look for kernel source
+KSP :=  /lib/modules/${BUILD_KERNEL}/source \
+        /lib/modules/${BUILD_KERNEL}/build \
+        /usr/src/linux-${BUILD_KERNEL} \
+        /usr/src/linux-$(${BUILD_KERNEL} | sed 's/-.*//') \
+        /usr/src/kernel-headers-${BUILD_KERNEL} \
+        /usr/src/kernel-source-${BUILD_KERNEL} \
+        /usr/src/linux-$(${BUILD_KERNEL} | sed 's/\([0-9]*\.[0-9]*\)\..*/\1/') \
+        /usr/src/linux \
+        /usr/src/kernels/${BUILD_KERNEL} \
+        /usr/src/kernels
+
+# prune the list down to only values that exist and have an include/linux
+# sub-directory. We can't use include/config because some older kernels don't
+# have this.
+test_dir = $(shell [ -e ${dir}/include/linux ] && echo ${dir})
+KSP := $(foreach dir, ${KSP}, ${test_dir})
+
+# we will use this first valid entry in the search path
+ifeq (,${KSRC})
+  KSRC := $(firstword ${KSP})
+endif
+
+ifeq (,${KSRC})
+  $(warning *** Kernel header files not in any of the expected locations.)
+  $(warning *** Install the appropriate kernel development package, e.g.)
+  $(error kernel-devel, for building kernel modules and try again)
+else
+ifeq (/lib/modules/${BUILD_KERNEL}/source, ${KSRC})
+  KOBJ :=  /lib/modules/${BUILD_KERNEL}/build
+else
+  KOBJ :=  ${KSRC}
+endif
+endif
+
+# Version file Search Path
+VSP :=  ${KOBJ}/include/generated/utsrelease.h \
+        ${KOBJ}/include/linux/utsrelease.h \
+        ${KOBJ}/include/linux/version.h \
+        ${KOBJ}/include/generated/uapi/linux/version.h \
+        /boot/vmlinuz.version.h
+
+# Config file Search Path
+CSP :=  ${KOBJ}/include/generated/autoconf.h \
+        ${KOBJ}/include/linux/autoconf.h \
+        /boot/vmlinuz.autoconf.h
+
+# System.map Search Path (for depmod)
+MSP := ${KSRC}/System.map \
+       /boot/System.map-${BUILD_KERNEL}
+
+# prune the lists down to only files that exist
+test_file = $(shell [ -f ${1} ] && echo ${1})
+VSP := $(foreach file, ${VSP}, $(call test_file,${file}))
+CSP := $(foreach file, ${CSP}, $(call test_file,${file}))
+MSP := $(foreach file, ${MSP}, $(call test_file,${file}))
+
+
+# and use the first valid entry in the Search Paths
+ifeq (,${VERSION_FILE})
+  VERSION_FILE := $(firstword ${VSP})
+endif
+
+ifeq (,${CONFIG_FILE})
+  CONFIG_FILE := $(firstword ${CSP})
+endif
+
+ifeq (,${SYSTEM_MAP_FILE})
+  SYSTEM_MAP_FILE := $(firstword ${MSP})
+endif
+
+ifeq (,$(wildcard ${VERSION_FILE}))
+  $(error Linux kernel source not configured - missing version header file)
+endif
+
+ifeq (,$(wildcard ${CONFIG_FILE}))
+  $(error Linux kernel source not configured - missing autoconf.h)
+endif
+
+ifeq (,$(wildcard ${SYSTEM_MAP_FILE}))
+  $(warning Missing System.map file - depmod will not check for missing symbols during module installation)
+endif
+
+ifneq ($(words $(subst :, ,$(CURDIR))), 1)
+  $(error Sources directory '$(CURDIR)' cannot contain spaces nor colons. Rename directory or move sources to another path)
+endif
+
+########################
+# Extract config value #
+########################
+
+get_config_value = $(shell ${CC} -E -dM ${CONFIG_FILE} 2> /dev/null |\
+                           grep -m 1 ${1} | awk '{ print $$3 }')
+
+########################
+# Check module signing #
+########################
+
+CONFIG_MODULE_SIG_ALL := $(call get_config_value,CONFIG_MODULE_SIG_ALL)
+CONFIG_MODULE_SIG_FORCE := $(call get_config_value,CONFIG_MODULE_SIG_FORCE)
+CONFIG_MODULE_SIG_KEY := $(call get_config_value,CONFIG_MODULE_SIG_KEY)
+
+SIG_KEY_SP := ${KOBJ}/${CONFIG_MODULE_SIG_KEY} \
+              ${KOBJ}/certs/signing_key.pem
+
+SIG_KEY_FILE := $(firstword $(foreach file, ${SIG_KEY_SP}, $(call test_file,${file})))
+
+# print a warning if the kernel configuration attempts to sign modules but
+# the signing key can't be found.
+ifneq (${SIG_KEY_FILE},)
+warn_signed_modules := : ;
+else
+warn_signed_modules :=
+ifeq (${CONFIG_MODULE_SIG_ALL},1)
+warn_signed_modules += \
+    echo "*** The target kernel has CONFIG_MODULE_SIG_ALL enabled, but" ; \
+    echo "*** the signing key cannot be found. Module signing has been" ; \
+    echo "*** disabled for this build." ;
+endif # CONFIG_MODULE_SIG_ALL=y
+ifeq (${CONFIG_MODULE_SIG_FORCE},1)
+    echo "warning: The target kernel has CONFIG_MODULE_SIG_FORCE enabled," ; \
+    echo "warning: but the signing key cannot be found. The module must" ; \
+    echo "warning: be signed manually using 'scripts/sign-file'." ;
+endif # CONFIG_MODULE_SIG_FORCE
+DISABLE_MODULE_SIGNING := Yes
+endif
+
+#######################
+# Linux Version Setup #
+#######################
+
+# The following command line parameter is intended for development of KCOMPAT
+# against upstream kernels such as net-next which have broken or non-updated
+# version codes in their Makefile. They are intended for debugging and
+# development purpose only so that we can easily test new KCOMPAT early. If you
+# don't know what this means, you do not need to set this flag. There is no
+# arcane magic here.
+
+# Convert LINUX_VERSION into LINUX_VERSION_CODE
+ifneq (${LINUX_VERSION},)
+  LINUX_VERSION_CODE=$(call get_kvercode,$(call get_kver,${LINUX_VERSION},1),$(call get_kver,${LINUX_VERSION},2),$(call get_kver,${LINUX_VERSION},3))
+endif
+
+# Honor LINUX_VERSION_CODE
+ifneq (${LINUX_VERSION_CODE},)
+  $(warning Forcing target kernel to build with LINUX_VERSION_CODE of ${LINUX_VERSION_CODE}$(if ${LINUX_VERSION}, from LINUX_VERSION=${LINUX_VERSION}). Do this at your own risk.)
+  KVER_CODE := ${LINUX_VERSION_CODE}
+  EXTRA_CFLAGS += -DLINUX_VERSION_CODE=${LINUX_VERSION_CODE}
+endif
+
+# Determine SLE_KERNEL_REVISION for SuSE SLE >= 11 (needed by kcompat)
+# This assumes SuSE will continue setting CONFIG_LOCALVERSION to the string
+# appended to the stable kernel version on which their kernel is based with
+# additional versioning information (up to 3 numbers), a possible abbreviated
+# git SHA1 commit id and a kernel type, e.g. CONFIG_LOCALVERSION=-1.2.3-default
+# or CONFIG_LOCALVERSION=-999.gdeadbee-default
+#
+# SLE_LOCALVERSION_CODE is also exported to support legacy kcompat.h
+# definitions.
+ifeq (1,$(call get_config_value,CONFIG_SUSE_KERNEL))
+
+ifneq (10,$(call get_config_value,CONFIG_SLE_VERSION))
+
+  CONFIG_LOCALVERSION := $(call get_config_value,CONFIG_LOCALVERSION)
+  LOCALVERSION := $(shell echo ${CONFIG_LOCALVERSION} | \
+                    cut -d'-' -f2 | sed 's/\.g[[:xdigit:]]\{7\}//')
+  LOCALVER_A := $(shell echo ${LOCALVERSION} | cut -d'.' -f1)
+  LOCALVER_B := $(shell echo ${LOCALVERSION} | cut -s -d'.' -f2)
+  LOCALVER_C := $(shell echo ${LOCALVERSION} | cut -s -d'.' -f3)
+  SLE_LOCALVERSION_CODE := $(shell expr ${LOCALVER_A} \* 65536 + \
+                                        0${LOCALVER_B} \* 256 + 0${LOCALVER_C})
+  EXTRA_CFLAGS += -DSLE_LOCALVERSION_CODE=${SLE_LOCALVERSION_CODE}
+  EXTRA_CFLAGS += -DSLE_KERNEL_REVISION=${LOCALVER_A}
+endif
+endif
+
+EXTRA_CFLAGS += ${CFLAGS_EXTRA}
+
+# get the kernel version - we use this to find the correct install path
+KVER := $(shell ${CC} ${EXTRA_CFLAGS} -E -dM ${VERSION_FILE} | grep UTS_RELEASE | \
+        awk '{ print $$3 }' | sed 's/\"//g')
+
+# assume source symlink is the same as build, otherwise adjust KOBJ
+ifneq (,$(wildcard /lib/modules/${KVER}/build))
+  ifneq (${KSRC},$(call readlink,/lib/modules/${KVER}/build))
+    KOBJ=/lib/modules/${KVER}/build
+  endif
+endif
+
+ifeq (${KVER_CODE},)
+  KVER_CODE := $(shell ${CC} ${EXTRA_CFLAGS} -E -dM ${VSP} 2> /dev/null |\
+                 grep -m 1 LINUX_VERSION_CODE | awk '{ print $$3 }' | sed 's/\"//g')
+endif
+
+# minimum_kver_check
+#
+# helper function to provide uniform output for different drivers to abort the
+# build based on kernel version check. Usage: "$(call minimum_kver_check,2,6,XX)".
+define _minimum_kver_check
+ifeq (0,$(shell [ ${KVER_CODE} -lt $(call get_kvercode,${1},${2},${3}) ]; echo "$$?"))
+  $$(warning *** Aborting the build.)
+  $$(error This driver is not supported on kernel versions older than ${1}.${2}.${3})
+endif
+endef
+minimum_kver_check = $(eval $(call _minimum_kver_check,${1},${2},${3}))
+
+################
+# Manual Pages #
+################
+
+MANSECTION = 7
+
+ifeq (,${MANDIR})
+  # find the best place to install the man page
+  MANPATH := $(shell (manpath 2>/dev/null || echo $MANPATH) | sed 's/:/ /g')
+  ifneq (,${MANPATH})
+    # test based on inclusion in MANPATH
+    test_dir = $(findstring ${dir}, ${MANPATH})
+  else
+    # no MANPATH, test based on directory existence
+    test_dir = $(shell [ -e ${dir} ] && echo ${dir})
+  endif
+  # our preferred install path
+  # should /usr/local/man be in here ?
+  MANDIR := /usr/share/man /usr/man
+  MANDIR := $(foreach dir, ${MANDIR}, ${test_dir})
+  MANDIR := $(firstword ${MANDIR})
+endif
+ifeq (,${MANDIR})
+  # fallback to /usr/man
+  MANDIR := /usr/man
+endif
+
+####################
+# CCFLAGS variable #
+####################
+
+# set correct CCFLAGS variable for kernels older than 2.6.24
+ifeq (0,$(shell [ ${KVER_CODE} -lt $(call get_kvercode,2,6,24) ]; echo $$?))
+CCFLAGS_VAR := EXTRA_CFLAGS
+else
+CCFLAGS_VAR := ccflags-y
+endif
+
+#################
+# KBUILD_OUTPUT #
+#################
+
+# Only set KBUILD_OUTPUT if the real paths of KOBJ and KSRC differ
+ifneq ($(call readlink,${KSRC}),$(call readlink,${KOBJ}))
+export KBUILD_OUTPUT ?= ${KOBJ}
+endif
+
+############################
+# Module Install Directory #
+############################
+
+# Default to using updates/drivers/net/ethernet/intel/ path, since depmod since
+# v3.1 defaults to checking updates folder first, and only checking kernels/
+# and extra afterwards. We use updates instead of kernel/* due to desire to
+# prevent over-writing built-in modules files.
+export INSTALL_MOD_DIR ?= updates/drivers/net/ethernet/intel/${DRIVER}
+
+#################
+# Auxiliary Bus #
+#################
+
+# If the check_aux_bus script exists, then this driver depends on the
+# auxiliary module. Run the script to determine if we need to include
+# auxiliary files with this build.
+ifneq ($(call test_file,../scripts/check_aux_bus),)
+NEED_AUX_BUS := $(shell ../scripts/check_aux_bus --ksrc="${KSRC}" --build-kernel="${BUILD_KERNEL}" >/dev/null 2>&1; echo $$?)
+endif # check_aux_bus exists
+
+# The out-of-tree auxiliary module we ship should be moved into this
+# directory as part of installation.
+export INSTALL_AUX_DIR ?= updates/drivers/net/ethernet/intel/auxiliary
+
+# If we're installing auxiliary bus out-of-tree, the following steps are
+# necessary to ensure the relevant files get put in place.
+ifeq (${NEED_AUX_BUS},2)
+define auxiliary_post_install
+	install -D -m 644 Module.symvers ${INSTALL_MOD_PATH}/lib/modules/${KVER}/extern-symvers/auxiliary.symvers
+	install -d ${INSTALL_MOD_PATH}/lib/modules/${KVER}/${INSTALL_AUX_DIR}
+	mv -f ${INSTALL_MOD_PATH}/lib/modules/${KVER}/${INSTALL_MOD_DIR}/auxiliary.ko \
+	      ${INSTALL_MOD_PATH}/lib/modules/${KVER}/${INSTALL_AUX_DIR}/auxiliary.ko
+	install -D -m 644 linux/auxiliary_bus.h ${INSTALL_MOD_PATH}/${KSRC}/include/linux/auxiliary_bus.h
+endef
+else
+auxiliary_post_install =
+endif
+
+ifeq (${NEED_AUX_BUS},2)
+define auxiliary_post_uninstall
+	rm -f ${INSTALL_MOD_PATH}/lib/modules/${KVER}/extern-symvers/auxiliary.symvers
+	rm -f ${INSTALL_MOD_PATH}/lib/modules/${KVER}/${INSTALL_AUX_DIR}/auxiliary.ko
+	rm -f ${INSTALL_MOD_PATH}/${KSRC}/include/linux/auxiliary_bus.h
+endef
+else
+auxiliary_post_uninstall =
+endif
+
+######################
+# Kernel Build Macro #
+######################
+
+# kernel build function
+# ${1} is the kernel build target
+# ${2} may contain any extra rules to pass directly to the sub-make process
+#
+# This function is expected to be executed by
+#   @+$(call kernelbuild,<target>,<extra parameters>)
+# from within a Makefile recipe.
+#
+# The following variables are expected to be defined for its use:
+# GCC_I_SYS -- if set it will enable use of gcc-i-sys.sh wrapper to use -isystem
+# CCFLAGS_VAR -- the CCFLAGS variable to set extra CFLAGS
+# EXTRA_CFLAGS -- a set of extra CFLAGS to pass into the ccflags-y variable
+# KSRC -- the location of the kernel source tree to build against
+# DRIVER_UPPERCASE -- the uppercase name of the kernel module, set from DRIVER
+# W -- if set, enables the W= kernel warnings options
+# C -- if set, enables the C= kernel sparse build options
+#
+kernelbuild = $(call warn_signed_modules) \
+              ${MAKE} $(if ${GCC_I_SYS},CC="${GCC_I_SYS}") \
+                      ${CCFLAGS_VAR}="${EXTRA_CFLAGS}" \
+                      -C "${KSRC}" \
+                      CONFIG_${DRIVER_UPPERCASE}=m \
+                      $(if ${DISABLE_MODULE_SIGNING},CONFIG_MODULE_SIG=n) \
+                      $(if ${DISABLE_MODULE_SIGNING},CONFIG_MODULE_SIG_ALL=) \
+                      M="${CURDIR}" \
+                      $(if ${W},W="${W}") \
+                      $(if ${C},C="${C}") \
+                      $(if ${NEED_AUX_BUS},NEED_AUX_BUS="${NEED_AUX_BUS}") \
+                      ${2} ${1}
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_adminq.c linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_adminq.c
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_adminq.c	2024-05-10 01:26:45.325079481 -0400
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_adminq.c	2024-05-13 03:58:25.368491972 -0400
@@ -1,5 +1,5 @@
 // SPDX-License-Identifier: GPL-2.0
-/* Copyright(c) 2013 - 2018 Intel Corporation. */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
 
 #include "i40e_status.h"
 #include "i40e_type.h"
@@ -7,8 +7,6 @@
 #include "i40e_adminq.h"
 #include "i40e_prototype.h"
 
-static void i40e_resume_aq(struct i40e_hw *hw);
-
 /**
  *  i40e_adminq_init_regs - Initialize AdminQ registers
  *  @hw: pointer to the hardware structure
@@ -96,6 +94,7 @@
  **/
 static void i40e_free_adminq_asq(struct i40e_hw *hw)
 {
+	i40e_free_virt_mem(hw, &hw->aq.asq.cmd_buf);
 	i40e_free_dma_mem(hw, &hw->aq.asq.desc_buf);
 }
 
@@ -146,21 +145,21 @@
 		/* now configure the descriptors for use */
 		desc = I40E_ADMINQ_DESC(hw->aq.arq, i);
 
-		desc->flags = cpu_to_le16(I40E_AQ_FLAG_BUF);
+		desc->flags = CPU_TO_LE16(I40E_AQ_FLAG_BUF);
 		if (hw->aq.arq_buf_size > I40E_AQ_LARGE_BUF)
-			desc->flags |= cpu_to_le16(I40E_AQ_FLAG_LB);
+			desc->flags |= CPU_TO_LE16(I40E_AQ_FLAG_LB);
 		desc->opcode = 0;
 		/* This is in accordance with Admin queue design, there is no
 		 * register for buffer size configuration
 		 */
-		desc->datalen = cpu_to_le16((u16)bi->size);
+		desc->datalen = CPU_TO_LE16((u16)bi->size);
 		desc->retval = 0;
 		desc->cookie_high = 0;
 		desc->cookie_low = 0;
 		desc->params.external.addr_high =
-			cpu_to_le32(upper_32_bits(bi->pa));
+			CPU_TO_LE32(upper_32_bits(bi->pa));
 		desc->params.external.addr_low =
-			cpu_to_le32(lower_32_bits(bi->pa));
+			CPU_TO_LE32(lower_32_bits(bi->pa));
 		desc->params.external.param0 = 0;
 		desc->params.external.param1 = 0;
 	}
@@ -268,7 +267,7 @@
  **/
 static i40e_status i40e_config_asq_regs(struct i40e_hw *hw)
 {
-	i40e_status ret_code = 0;
+	i40e_status ret_code = I40E_SUCCESS;
 	u32 reg = 0;
 
 	/* Clear Head and Tail */
@@ -297,7 +296,7 @@
  **/
 static i40e_status i40e_config_arq_regs(struct i40e_hw *hw)
 {
-	i40e_status ret_code = 0;
+	i40e_status ret_code = I40E_SUCCESS;
 	u32 reg = 0;
 
 	/* Clear Head and Tail */
@@ -336,7 +335,7 @@
  **/
 static i40e_status i40e_init_asq(struct i40e_hw *hw)
 {
-	i40e_status ret_code = 0;
+	i40e_status ret_code = I40E_SUCCESS;
 
 	if (hw->aq.asq.count > 0) {
 		/* queue already initialized */
@@ -356,18 +355,18 @@
 
 	/* allocate the ring memory */
 	ret_code = i40e_alloc_adminq_asq_ring(hw);
-	if (ret_code)
+	if (ret_code != I40E_SUCCESS)
 		goto init_adminq_exit;
 
 	/* allocate buffers in the rings */
 	ret_code = i40e_alloc_asq_bufs(hw);
-	if (ret_code)
+	if (ret_code != I40E_SUCCESS)
 		goto init_adminq_free_rings;
 
 	/* initialize base registers */
 	ret_code = i40e_config_asq_regs(hw);
-	if (ret_code)
-		goto init_adminq_free_rings;
+	if (ret_code != I40E_SUCCESS)
+		goto init_config_regs;
 
 	/* success! */
 	hw->aq.asq.count = hw->aq.num_asq_entries;
@@ -375,6 +374,10 @@
 
 init_adminq_free_rings:
 	i40e_free_adminq_asq(hw);
+	return ret_code;
+
+init_config_regs:
+	i40e_free_asq_bufs(hw);
 
 init_adminq_exit:
 	return ret_code;
@@ -395,7 +398,7 @@
  **/
 static i40e_status i40e_init_arq(struct i40e_hw *hw)
 {
-	i40e_status ret_code = 0;
+	i40e_status ret_code = I40E_SUCCESS;
 
 	if (hw->aq.arq.count > 0) {
 		/* queue already initialized */
@@ -415,17 +418,17 @@
 
 	/* allocate the ring memory */
 	ret_code = i40e_alloc_adminq_arq_ring(hw);
-	if (ret_code)
+	if (ret_code != I40E_SUCCESS)
 		goto init_adminq_exit;
 
 	/* allocate buffers in the rings */
 	ret_code = i40e_alloc_arq_bufs(hw);
-	if (ret_code)
+	if (ret_code != I40E_SUCCESS)
 		goto init_adminq_free_rings;
 
 	/* initialize base registers */
 	ret_code = i40e_config_arq_regs(hw);
-	if (ret_code)
+	if (ret_code != I40E_SUCCESS)
 		goto init_adminq_free_rings;
 
 	/* success! */
@@ -447,9 +450,9 @@
  **/
 static i40e_status i40e_shutdown_asq(struct i40e_hw *hw)
 {
-	i40e_status ret_code = 0;
+	i40e_status ret_code = I40E_SUCCESS;
 
-	mutex_lock(&hw->aq.asq_mutex);
+	i40e_acquire_spinlock(&hw->aq.asq_spinlock);
 
 	if (hw->aq.asq.count == 0) {
 		ret_code = I40E_ERR_NOT_READY;
@@ -469,7 +472,7 @@
 	i40e_free_asq_bufs(hw);
 
 shutdown_asq_out:
-	mutex_unlock(&hw->aq.asq_mutex);
+	i40e_release_spinlock(&hw->aq.asq_spinlock);
 	return ret_code;
 }
 
@@ -481,9 +484,9 @@
  **/
 static i40e_status i40e_shutdown_arq(struct i40e_hw *hw)
 {
-	i40e_status ret_code = 0;
+	i40e_status ret_code = I40E_SUCCESS;
 
-	mutex_lock(&hw->aq.arq_mutex);
+	i40e_acquire_spinlock(&hw->aq.arq_spinlock);
 
 	if (hw->aq.arq.count == 0) {
 		ret_code = I40E_ERR_NOT_READY;
@@ -503,11 +506,93 @@
 	i40e_free_arq_bufs(hw);
 
 shutdown_arq_out:
-	mutex_unlock(&hw->aq.arq_mutex);
+	i40e_release_spinlock(&hw->aq.arq_spinlock);
 	return ret_code;
 }
 
 /**
+ *  i40e_resume_aq - resume AQ processing from 0
+ *  @hw: pointer to the hardware structure
+ **/
+static void i40e_resume_aq(struct i40e_hw *hw)
+{
+	/* Registers are reset after PF reset */
+	hw->aq.asq.next_to_use = 0;
+	hw->aq.asq.next_to_clean = 0;
+
+	i40e_config_asq_regs(hw);
+
+	hw->aq.arq.next_to_use = 0;
+	hw->aq.arq.next_to_clean = 0;
+
+	i40e_config_arq_regs(hw);
+}
+
+/**
+ *  i40e_set_hw_flags - set HW flags
+ *  @hw: pointer to the hardware structure
+ **/
+static void i40e_set_hw_flags(struct i40e_hw *hw)
+{
+	struct i40e_adminq_info *aq = &hw->aq;
+
+	hw->flags = 0;
+
+	switch (hw->mac.type) {
+	case I40E_MAC_XL710:
+		if (aq->api_maj_ver > 1 ||
+		    (aq->api_maj_ver == 1 &&
+		     aq->api_min_ver >= I40E_MINOR_VER_GET_LINK_INFO_XL710)) {
+			hw->flags |= I40E_HW_FLAG_AQ_PHY_ACCESS_CAPABLE;
+			hw->flags |= I40E_HW_FLAG_FW_LLDP_STOPPABLE;
+			/* The ability to RX (not drop) 802.1ad frames */
+			hw->flags |= I40E_HW_FLAG_802_1AD_CAPABLE;
+		}
+		break;
+	case I40E_MAC_X722:
+		hw->flags |= I40E_HW_FLAG_AQ_SRCTL_ACCESS_ENABLE |
+			     I40E_HW_FLAG_NVM_READ_REQUIRES_LOCK;
+
+		if (aq->api_maj_ver > 1 ||
+		    (aq->api_maj_ver == 1 &&
+		     aq->api_min_ver >= I40E_MINOR_VER_FW_LLDP_STOPPABLE_X722))
+			hw->flags |= I40E_HW_FLAG_FW_LLDP_STOPPABLE;
+
+		if (aq->api_maj_ver > 1 ||
+		    (aq->api_maj_ver == 1 &&
+		     aq->api_min_ver >= I40E_MINOR_VER_GET_LINK_INFO_X722))
+			hw->flags |= I40E_HW_FLAG_AQ_PHY_ACCESS_CAPABLE;
+
+		if (aq->api_maj_ver > 1 ||
+		    (aq->api_maj_ver == 1 &&
+		     aq->api_min_ver >= I40E_MINOR_VER_FW_REQUEST_FEC_X722))
+			hw->flags |= I40E_HW_FLAG_X722_FEC_REQUEST_CAPABLE;
+
+		/* fall through */
+	default:
+		break;
+	}
+
+	/* Newer versions of firmware require lock when reading the NVM */
+	if (aq->api_maj_ver > 1 ||
+	    (aq->api_maj_ver == 1 &&
+	     aq->api_min_ver >= 5))
+		hw->flags |= I40E_HW_FLAG_NVM_READ_REQUIRES_LOCK;
+
+	if (aq->api_maj_ver > 1 ||
+	    (aq->api_maj_ver == 1 &&
+	     aq->api_min_ver >= 8)) {
+		hw->flags |= I40E_HW_FLAG_FW_LLDP_PERSISTENT;
+		hw->flags |= I40E_HW_FLAG_DROP_MODE;
+	}
+
+	if (aq->api_maj_ver > 1 ||
+	    (aq->api_maj_ver == 1 &&
+	     aq->api_min_ver >= 9))
+		hw->flags |= I40E_HW_FLAG_AQ_PHY_ACCESS_EXTENDED;
+}
+
+/**
  *  i40e_init_adminq - main initialization routine for Admin Queue
  *  @hw: pointer to the hardware structure
  *
@@ -520,19 +605,24 @@
  **/
 i40e_status i40e_init_adminq(struct i40e_hw *hw)
 {
-	u16 cfg_ptr, oem_hi, oem_lo;
-	u16 eetrack_lo, eetrack_hi;
+	struct i40e_adminq_info *aq = &hw->aq;
 	i40e_status ret_code;
+	u16 oem_hi = 0, oem_lo = 0;
+	u16 eetrack_hi = 0;
+	u16 eetrack_lo = 0;
+	u16 cfg_ptr = 0;
 	int retry = 0;
 
 	/* verify input for valid configuration */
-	if ((hw->aq.num_arq_entries == 0) ||
-	    (hw->aq.num_asq_entries == 0) ||
-	    (hw->aq.arq_buf_size == 0) ||
-	    (hw->aq.asq_buf_size == 0)) {
+	if (aq->num_arq_entries == 0 ||
+	    aq->num_asq_entries == 0 ||
+	    aq->arq_buf_size == 0 ||
+	    aq->asq_buf_size == 0) {
 		ret_code = I40E_ERR_CONFIG;
 		goto init_adminq_exit;
 	}
+	i40e_init_spinlock(&aq->asq_spinlock);
+	i40e_init_spinlock(&aq->arq_spinlock);
 
 	/* Set up register offsets */
 	i40e_adminq_init_regs(hw);
@@ -542,12 +632,12 @@
 
 	/* allocate the ASQ */
 	ret_code = i40e_init_asq(hw);
-	if (ret_code)
-		goto init_adminq_destroy_locks;
+	if (ret_code != I40E_SUCCESS)
+		goto init_adminq_destroy_spinlocks;
 
 	/* allocate the ARQ */
 	ret_code = i40e_init_arq(hw);
-	if (ret_code)
+	if (ret_code != I40E_SUCCESS)
 		goto init_adminq_free_asq;
 
 	/* There are some cases where the firmware may not be quite ready
@@ -556,11 +646,11 @@
 	 */
 	do {
 		ret_code = i40e_aq_get_firmware_version(hw,
-							&hw->aq.fw_maj_ver,
-							&hw->aq.fw_min_ver,
-							&hw->aq.fw_build,
-							&hw->aq.api_maj_ver,
-							&hw->aq.api_min_ver,
+							&aq->fw_maj_ver,
+							&aq->fw_min_ver,
+							&aq->fw_build,
+							&aq->api_maj_ver,
+							&aq->api_min_ver,
 							NULL);
 		if (ret_code != I40E_ERR_ADMIN_QUEUE_TIMEOUT)
 			break;
@@ -571,6 +661,12 @@
 	if (ret_code != I40E_SUCCESS)
 		goto init_adminq_free_arq;
 
+	/*
+	 * Some features were introduced in different FW API version
+	 * for different MAC type.
+	 */
+	i40e_set_hw_flags(hw);
+
 	/* get the NVM version info */
 	i40e_read_nvm_word(hw, I40E_SR_NVM_DEV_STARTER_VERSION,
 			   &hw->nvm.version);
@@ -578,44 +674,11 @@
 	i40e_read_nvm_word(hw, I40E_SR_NVM_EETRACK_HI, &eetrack_hi);
 	hw->nvm.eetrack = (eetrack_hi << 16) | eetrack_lo;
 	i40e_read_nvm_word(hw, I40E_SR_BOOT_CONFIG_PTR, &cfg_ptr);
-	i40e_read_nvm_word(hw, (cfg_ptr + I40E_NVM_OEM_VER_OFF),
-			   &oem_hi);
-	i40e_read_nvm_word(hw, (cfg_ptr + (I40E_NVM_OEM_VER_OFF + 1)),
-			   &oem_lo);
+	i40e_read_nvm_word(hw, (cfg_ptr + I40E_NVM_OEM_VER_OFF), &oem_hi);
+	i40e_read_nvm_word(hw, (cfg_ptr + (I40E_NVM_OEM_VER_OFF + 1)), &oem_lo);
 	hw->nvm.oem_ver = ((u32)oem_hi << 16) | oem_lo;
 
-	if (hw->mac.type == I40E_MAC_XL710 &&
-	    hw->aq.api_maj_ver == I40E_FW_API_VERSION_MAJOR &&
-	    hw->aq.api_min_ver >= I40E_MINOR_VER_GET_LINK_INFO_XL710) {
-		hw->flags |= I40E_HW_FLAG_AQ_PHY_ACCESS_CAPABLE;
-		hw->flags |= I40E_HW_FLAG_FW_LLDP_STOPPABLE;
-	}
-	if (hw->mac.type == I40E_MAC_X722 &&
-	    hw->aq.api_maj_ver == I40E_FW_API_VERSION_MAJOR &&
-	    hw->aq.api_min_ver >= I40E_MINOR_VER_FW_LLDP_STOPPABLE_X722) {
-		hw->flags |= I40E_HW_FLAG_FW_LLDP_STOPPABLE;
-	}
-
-	/* Newer versions of firmware require lock when reading the NVM */
-	if (hw->aq.api_maj_ver > 1 ||
-	    (hw->aq.api_maj_ver == 1 &&
-	     hw->aq.api_min_ver >= 5))
-		hw->flags |= I40E_HW_FLAG_NVM_READ_REQUIRES_LOCK;
-
-	/* The ability to RX (not drop) 802.1ad frames was added in API 1.7 */
-	if (hw->aq.api_maj_ver > 1 ||
-	    (hw->aq.api_maj_ver == 1 &&
-	     hw->aq.api_min_ver >= 7))
-		hw->flags |= I40E_HW_FLAG_802_1AD_CAPABLE;
-
-	if (hw->aq.api_maj_ver > 1 ||
-	    (hw->aq.api_maj_ver == 1 &&
-	     hw->aq.api_min_ver >= 8)) {
-		hw->flags |= I40E_HW_FLAG_FW_LLDP_PERSISTENT;
-		hw->flags |= I40E_HW_FLAG_DROP_MODE;
-	}
-
-	if (hw->aq.api_maj_ver > I40E_FW_API_VERSION_MAJOR) {
+	if (aq->api_maj_ver > I40E_FW_API_VERSION_MAJOR) {
 		ret_code = I40E_ERR_FIRMWARE_API_VERSION;
 		goto init_adminq_free_arq;
 	}
@@ -625,7 +688,7 @@
 	hw->nvm_release_on_done = false;
 	hw->nvmupd_state = I40E_NVMUPD_STATE_INIT;
 
-	ret_code = 0;
+	ret_code = I40E_SUCCESS;
 
 	/* success! */
 	goto init_adminq_exit;
@@ -634,7 +697,9 @@
 	i40e_shutdown_arq(hw);
 init_adminq_free_asq:
 	i40e_shutdown_asq(hw);
-init_adminq_destroy_locks:
+init_adminq_destroy_spinlocks:
+	i40e_destroy_spinlock(&aq->asq_spinlock);
+	i40e_destroy_spinlock(&aq->arq_spinlock);
 
 init_adminq_exit:
 	return ret_code;
@@ -646,13 +711,15 @@
  **/
 i40e_status i40e_shutdown_adminq(struct i40e_hw *hw)
 {
-	i40e_status ret_code = 0;
+	i40e_status ret_code = I40E_SUCCESS;
 
 	if (i40e_check_asq_alive(hw))
 		i40e_aq_queue_shutdown(hw, true);
 
 	i40e_shutdown_asq(hw);
 	i40e_shutdown_arq(hw);
+	i40e_destroy_spinlock(&hw->aq.asq_spinlock);
+	i40e_destroy_spinlock(&hw->aq.arq_spinlock);
 
 	if (hw->nvm_buff.va)
 		i40e_free_virt_mem(hw, &hw->nvm_buff);
@@ -683,11 +750,12 @@
 		if (details->callback) {
 			I40E_ADMINQ_CALLBACK cb_func =
 					(I40E_ADMINQ_CALLBACK)details->callback;
-			desc_cb = *desc;
+			i40e_memcpy(&desc_cb, desc, sizeof(struct i40e_aq_desc),
+				    I40E_DMA_TO_DMA);
 			cb_func(hw, &desc_cb);
 		}
-		memset(desc, 0, sizeof(*desc));
-		memset(details, 0, sizeof(*details));
+		i40e_memset(desc, 0, sizeof(*desc), I40E_DMA_MEM);
+		i40e_memset(details, 0, sizeof(*details), I40E_NONDMA_MEM);
 		ntc++;
 		if (ntc == asq->count)
 			ntc = 0;
@@ -717,23 +785,26 @@
 }
 
 /**
- *  i40e_asq_send_command - send command to Admin Queue
+ *  i40e_asq_send_command_atomic_exec - send command to Admin Queue
  *  @hw: pointer to the hw struct
  *  @desc: prefilled descriptor describing the command (non DMA mem)
  *  @buff: buffer to use for indirect commands
  *  @buff_size: size of buffer for indirect commands
  *  @cmd_details: pointer to command details structure
+ *  @is_atomic_context: is the function called in an atomic context?
  *
  *  This is the main send command driver routine for the Admin Queue send
  *  queue.  It runs the queue, cleans the queue, etc
  **/
-i40e_status i40e_asq_send_command(struct i40e_hw *hw,
-				struct i40e_aq_desc *desc,
-				void *buff, /* can be NULL */
-				u16  buff_size,
-				struct i40e_asq_cmd_details *cmd_details)
+static enum i40e_status_code
+i40e_asq_send_command_atomic_exec(struct i40e_hw *hw,
+				  struct i40e_aq_desc *desc,
+				  void *buff, /* can be NULL */
+				  u16  buff_size,
+				  struct i40e_asq_cmd_details *cmd_details,
+				  bool is_atomic_context)
 {
-	i40e_status status = 0;
+	i40e_status status = I40E_SUCCESS;
 	struct i40e_dma_mem *dma_buff = NULL;
 	struct i40e_asq_cmd_details *details;
 	struct i40e_aq_desc *desc_on_ring;
@@ -741,7 +812,7 @@
 	u16  retval = 0;
 	u32  val = 0;
 
-	mutex_lock(&hw->aq.asq_mutex);
+	hw->aq.asq_last_status = I40E_AQ_RC_OK;
 
 	if (hw->aq.asq.count == 0) {
 		i40e_debug(hw, I40E_DEBUG_AQ_MESSAGE,
@@ -750,8 +821,6 @@
 		goto asq_send_command_error;
 	}
 
-	hw->aq.asq_last_status = I40E_AQ_RC_OK;
-
 	val = rd32(hw, hw->aq.asq.head);
 	if (val >= hw->aq.num_asq_entries) {
 		i40e_debug(hw, I40E_DEBUG_AQ_MESSAGE,
@@ -762,25 +831,30 @@
 
 	details = I40E_ADMINQ_DETAILS(hw->aq.asq, hw->aq.asq.next_to_use);
 	if (cmd_details) {
-		*details = *cmd_details;
+		i40e_memcpy(details,
+			    cmd_details,
+			    sizeof(struct i40e_asq_cmd_details),
+			    I40E_NONDMA_TO_NONDMA);
 
 		/* If the cmd_details are defined copy the cookie.  The
-		 * cpu_to_le32 is not needed here because the data is ignored
+		 * CPU_TO_LE32 is not needed here because the data is ignored
 		 * by the FW, only used by the driver
 		 */
 		if (details->cookie) {
 			desc->cookie_high =
-				cpu_to_le32(upper_32_bits(details->cookie));
+				CPU_TO_LE32(upper_32_bits(details->cookie));
 			desc->cookie_low =
-				cpu_to_le32(lower_32_bits(details->cookie));
+				CPU_TO_LE32(lower_32_bits(details->cookie));
 		}
 	} else {
-		memset(details, 0, sizeof(struct i40e_asq_cmd_details));
+		i40e_memset(details, 0,
+			    sizeof(struct i40e_asq_cmd_details),
+			    I40E_NONDMA_MEM);
 	}
 
 	/* clear requested flags and then set additional flags if defined */
-	desc->flags &= ~cpu_to_le16(details->flags_dis);
-	desc->flags |= cpu_to_le16(details->flags_ena);
+	desc->flags &= ~CPU_TO_LE16(details->flags_dis);
+	desc->flags |= CPU_TO_LE16(details->flags_ena);
 
 	if (buff_size > hw->aq.asq_buf_size) {
 		i40e_debug(hw,
@@ -818,22 +892,24 @@
 	desc_on_ring = I40E_ADMINQ_DESC(hw->aq.asq, hw->aq.asq.next_to_use);
 
 	/* if the desc is available copy the temp desc to the right place */
-	*desc_on_ring = *desc;
+	i40e_memcpy(desc_on_ring, desc, sizeof(struct i40e_aq_desc),
+		    I40E_NONDMA_TO_DMA);
 
 	/* if buff is not NULL assume indirect command */
 	if (buff != NULL) {
 		dma_buff = &(hw->aq.asq.r.asq_bi[hw->aq.asq.next_to_use]);
 		/* copy the user buff into the respective DMA buff */
-		memcpy(dma_buff->va, buff, buff_size);
-		desc_on_ring->datalen = cpu_to_le16(buff_size);
+		i40e_memcpy(dma_buff->va, buff, buff_size,
+			    I40E_NONDMA_TO_DMA);
+		desc_on_ring->datalen = CPU_TO_LE16(buff_size);
 
 		/* Update the address values in the desc with the pa value
 		 * for respective buffer
 		 */
 		desc_on_ring->params.external.addr_high =
-				cpu_to_le32(upper_32_bits(dma_buff->pa));
+				CPU_TO_LE32(upper_32_bits(dma_buff->pa));
 		desc_on_ring->params.external.addr_low =
-				cpu_to_le32(lower_32_bits(dma_buff->pa));
+				CPU_TO_LE32(lower_32_bits(dma_buff->pa));
 	}
 
 	/* bump the tail */
@@ -858,17 +934,22 @@
 			 */
 			if (i40e_asq_done(hw))
 				break;
-			udelay(50);
+			if (is_atomic_context)
+				udelay(50);
+			else
+				usleep_range(40, 60);
 			total_delay += 50;
 		} while (total_delay < hw->aq.asq_cmd_timeout);
 	}
 
 	/* if ready, copy the desc back to temp */
 	if (i40e_asq_done(hw)) {
-		*desc = *desc_on_ring;
+		i40e_memcpy(desc, desc_on_ring, sizeof(struct i40e_aq_desc),
+			    I40E_DMA_TO_NONDMA);
 		if (buff != NULL)
-			memcpy(buff, dma_buff->va, buff_size);
-		retval = le16_to_cpu(desc->retval);
+			i40e_memcpy(buff, dma_buff->va, buff_size,
+				    I40E_DMA_TO_NONDMA);
+		retval = LE16_TO_CPU(desc->retval);
 		if (retval != 0) {
 			i40e_debug(hw,
 				   I40E_DEBUG_AQ_MESSAGE,
@@ -880,7 +961,7 @@
 		}
 		cmd_completed = true;
 		if ((enum i40e_admin_queue_err)retval == I40E_AQ_RC_OK)
-			status = 0;
+			status = I40E_SUCCESS;
 		else if ((enum i40e_admin_queue_err)retval == I40E_AQ_RC_EBUSY)
 			status = I40E_ERR_NOT_READY;
 		else
@@ -894,7 +975,8 @@
 
 	/* save writeback aq if requested */
 	if (details->wb_desc)
-		*details->wb_desc = *desc_on_ring;
+		i40e_memcpy(details->wb_desc, desc_on_ring,
+			    sizeof(struct i40e_aq_desc), I40E_DMA_TO_NONDMA);
 
 	/* update the error if time out occurred */
 	if ((!cmd_completed) &&
@@ -911,11 +993,101 @@
 	}
 
 asq_send_command_error:
-	mutex_unlock(&hw->aq.asq_mutex);
 	return status;
 }
 
 /**
+ *  i40e_asq_send_command_atomic - send command to Admin Queue
+ *  @hw: pointer to the hw struct
+ *  @desc: prefilled descriptor describing the command (non DMA mem)
+ *  @buff: buffer to use for indirect commands
+ *  @buff_size: size of buffer for indirect commands
+ *  @cmd_details: pointer to command details structure
+ *  @is_atomic_context: is the function called in an atomic context?
+ *
+ *  Acquires the lock and calls the main send command execution
+ *  routine.
+ **/
+enum i40e_status_code
+i40e_asq_send_command_atomic(struct i40e_hw *hw,
+			     struct i40e_aq_desc *desc,
+			     void *buff, /* can be NULL */
+			     u16  buff_size,
+			     struct i40e_asq_cmd_details *cmd_details,
+			     bool is_atomic_context)
+{
+	i40e_status status = I40E_SUCCESS;
+
+	i40e_acquire_spinlock(&hw->aq.asq_spinlock);
+	status = i40e_asq_send_command_atomic_exec(hw, desc, buff, buff_size,
+						   cmd_details,
+						   is_atomic_context);
+	i40e_release_spinlock(&hw->aq.asq_spinlock);
+	return status;
+}
+
+// inline function with previous signature to avoid modifying
+// all existing calls to i40e_asq_send_command
+inline i40e_status i40e_asq_send_command(struct i40e_hw *hw,
+				struct i40e_aq_desc *desc,
+				void *buff, /* can be NULL */
+				u16  buff_size,
+				struct i40e_asq_cmd_details *cmd_details)
+{
+	return i40e_asq_send_command_atomic(hw, desc, buff, buff_size,
+					    cmd_details, true);
+}
+
+/**
+ *  i40e_asq_send_command_atomic_v2 - send command to Admin Queue
+ *  @hw: pointer to the hw struct
+ *  @desc: prefilled descriptor describing the command (non DMA mem)
+ *  @buff: buffer to use for indirect commands
+ *  @buff_size: size of buffer for indirect commands
+ *  @cmd_details: pointer to command details structure
+ *  @is_atomic_context: is the function called in an atomic context?
+ *  @aq_status: pointer to Admin Queue status return value
+ *
+ *  Acquires the lock and calls the main send command execution
+ *  routine. Returns the last Admin Queue status in aq_status
+ *  to avoid race conditions in access to hw->aq.asq_last_status.
+ **/
+enum i40e_status_code
+i40e_asq_send_command_atomic_v2(struct i40e_hw *hw,
+				struct i40e_aq_desc *desc,
+				void *buff, /* can be NULL */
+				u16  buff_size,
+				struct i40e_asq_cmd_details *cmd_details,
+				bool is_atomic_context,
+				enum i40e_admin_queue_err *aq_status)
+{
+	i40e_status status = I40E_SUCCESS;
+
+	i40e_acquire_spinlock(&hw->aq.asq_spinlock);
+	status = i40e_asq_send_command_atomic_exec(hw, desc, buff,
+						   buff_size,
+						   cmd_details,
+						   is_atomic_context);
+	if (aq_status)
+		*aq_status = hw->aq.asq_last_status;
+	i40e_release_spinlock(&hw->aq.asq_spinlock);
+	return status;
+}
+
+// inline function with previous signature to avoid modifying
+// all existing calls to i40e_asq_send_command
+inline i40e_status i40e_asq_send_command_v2(struct i40e_hw *hw,
+				struct i40e_aq_desc *desc,
+				void *buff, /* can be NULL */
+				u16  buff_size,
+				struct i40e_asq_cmd_details *cmd_details,
+				enum i40e_admin_queue_err *aq_status)
+{
+	return i40e_asq_send_command_atomic_v2(hw, desc, buff, buff_size,
+					       cmd_details, true, aq_status);
+}
+
+/**
  *  i40e_fill_default_direct_cmd_desc - AQ descriptor helper function
  *  @desc:     pointer to the temp descriptor (non DMA mem)
  *  @opcode:   the opcode can be used to decide which flags to turn off or on
@@ -926,9 +1098,10 @@
 				       u16 opcode)
 {
 	/* zero out the desc */
-	memset((void *)desc, 0, sizeof(struct i40e_aq_desc));
-	desc->opcode = cpu_to_le16(opcode);
-	desc->flags = cpu_to_le16(I40E_AQ_FLAG_SI);
+	i40e_memset((void *)desc, 0, sizeof(struct i40e_aq_desc),
+		    I40E_NONDMA_MEM);
+	desc->opcode = CPU_TO_LE16(opcode);
+	desc->flags = CPU_TO_LE16(I40E_AQ_FLAG_SI);
 }
 
 /**
@@ -945,7 +1118,7 @@
 					     struct i40e_arq_event_info *e,
 					     u16 *pending)
 {
-	i40e_status ret_code = 0;
+	i40e_status ret_code = I40E_SUCCESS;
 	u16 ntc = hw->aq.arq.next_to_clean;
 	struct i40e_aq_desc *desc;
 	struct i40e_dma_mem *bi;
@@ -955,10 +1128,10 @@
 	u16 ntu;
 
 	/* pre-clean the event info */
-	memset(&e->desc, 0, sizeof(e->desc));
+	i40e_memset(&e->desc, 0, sizeof(e->desc), I40E_NONDMA_MEM);
 
 	/* take the lock before we start messing with the ring */
-	mutex_lock(&hw->aq.arq_mutex);
+	i40e_acquire_spinlock(&hw->aq.arq_spinlock);
 
 	if (hw->aq.arq.count == 0) {
 		i40e_debug(hw, I40E_DEBUG_AQ_MESSAGE,
@@ -980,8 +1153,8 @@
 	desc_idx = ntc;
 
 	hw->aq.arq_last_status =
-		(enum i40e_admin_queue_err)le16_to_cpu(desc->retval);
-	flags = le16_to_cpu(desc->flags);
+		(enum i40e_admin_queue_err)LE16_TO_CPU(desc->retval);
+	flags = LE16_TO_CPU(desc->flags);
 	if (flags & I40E_AQ_FLAG_ERR) {
 		ret_code = I40E_ERR_ADMIN_QUEUE_ERROR;
 		i40e_debug(hw,
@@ -990,12 +1163,14 @@
 			   hw->aq.arq_last_status);
 	}
 
-	e->desc = *desc;
-	datalen = le16_to_cpu(desc->datalen);
+	i40e_memcpy(&e->desc, desc, sizeof(struct i40e_aq_desc),
+		    I40E_DMA_TO_NONDMA);
+	datalen = LE16_TO_CPU(desc->datalen);
 	e->msg_len = min(datalen, e->buf_len);
 	if (e->msg_buf != NULL && (e->msg_len != 0))
-		memcpy(e->msg_buf, hw->aq.arq.r.arq_bi[desc_idx].va,
-		       e->msg_len);
+		i40e_memcpy(e->msg_buf,
+			    hw->aq.arq.r.arq_bi[desc_idx].va,
+			    e->msg_len, I40E_DMA_TO_NONDMA);
 
 	i40e_debug(hw, I40E_DEBUG_AQ_COMMAND, "AQRX: desc and buffer:\n");
 	i40e_debug_aq(hw, I40E_DEBUG_AQ_COMMAND, (void *)desc, e->msg_buf,
@@ -1006,14 +1181,14 @@
 	 * size
 	 */
 	bi = &hw->aq.arq.r.arq_bi[ntc];
-	memset((void *)desc, 0, sizeof(struct i40e_aq_desc));
+	i40e_memset((void *)desc, 0, sizeof(struct i40e_aq_desc), I40E_DMA_MEM);
 
-	desc->flags = cpu_to_le16(I40E_AQ_FLAG_BUF);
+	desc->flags = CPU_TO_LE16(I40E_AQ_FLAG_BUF);
 	if (hw->aq.arq_buf_size > I40E_AQ_LARGE_BUF)
-		desc->flags |= cpu_to_le16(I40E_AQ_FLAG_LB);
-	desc->datalen = cpu_to_le16((u16)bi->size);
-	desc->params.external.addr_high = cpu_to_le32(upper_32_bits(bi->pa));
-	desc->params.external.addr_low = cpu_to_le32(lower_32_bits(bi->pa));
+		desc->flags |= CPU_TO_LE16(I40E_AQ_FLAG_LB);
+	desc->datalen = CPU_TO_LE16((u16)bi->size);
+	desc->params.external.addr_high = CPU_TO_LE32(upper_32_bits(bi->pa));
+	desc->params.external.addr_low = CPU_TO_LE32(lower_32_bits(bi->pa));
 
 	/* set tail = the last cleaned desc index. */
 	wr32(hw, hw->aq.arq.tail, ntc);
@@ -1024,27 +1199,14 @@
 	hw->aq.arq.next_to_clean = ntc;
 	hw->aq.arq.next_to_use = ntu;
 
-	i40e_nvmupd_check_wait_event(hw, le16_to_cpu(e->desc.opcode), &e->desc);
+	i40e_nvmupd_check_wait_event(hw, LE16_TO_CPU(e->desc.opcode), &e->desc);
 clean_arq_element_out:
 	/* Set pending if needed, unlock and return */
-	if (pending)
+	if (pending != NULL)
 		*pending = (ntc > ntu ? hw->aq.arq.count : 0) + (ntu - ntc);
 clean_arq_element_err:
-	mutex_unlock(&hw->aq.arq_mutex);
+	i40e_release_spinlock(&hw->aq.arq_spinlock);
 
 	return ret_code;
 }
 
-static void i40e_resume_aq(struct i40e_hw *hw)
-{
-	/* Registers are reset after PF reset */
-	hw->aq.asq.next_to_use = 0;
-	hw->aq.asq.next_to_clean = 0;
-
-	i40e_config_asq_regs(hw);
-
-	hw->aq.arq.next_to_use = 0;
-	hw->aq.arq.next_to_clean = 0;
-
-	i40e_config_arq_regs(hw);
-}
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_adminq_cmd.h linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_adminq_cmd.h
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_adminq_cmd.h	2024-05-10 01:26:45.325079481 -0400
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_adminq_cmd.h	2024-05-13 03:58:25.368491972 -0400
@@ -1,5 +1,5 @@
 /* SPDX-License-Identifier: GPL-2.0 */
-/* Copyright(c) 2013 - 2018 Intel Corporation. */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
 
 #ifndef _I40E_ADMINQ_CMD_H_
 #define _I40E_ADMINQ_CMD_H_
@@ -11,8 +11,8 @@
  */
 
 #define I40E_FW_API_VERSION_MAJOR	0x0001
-#define I40E_FW_API_VERSION_MINOR_X722	0x0009
-#define I40E_FW_API_VERSION_MINOR_X710	0x0009
+#define I40E_FW_API_VERSION_MINOR_X722	0x000C
+#define I40E_FW_API_VERSION_MINOR_X710	0x000F
 
 #define I40E_FW_MINOR_VERSION(_h) ((_h)->mac.type == I40E_MAC_XL710 ? \
 					I40E_FW_API_VERSION_MINOR_X710 : \
@@ -24,6 +24,8 @@
 #define I40E_MINOR_VER_GET_LINK_INFO_X722 0x0009
 /* API version 1.6 for X722 devices adds ability to stop FW LLDP agent */
 #define I40E_MINOR_VER_FW_LLDP_STOPPABLE_X722 0x0006
+/* API version 1.10 for X722 devices adds ability to request FEC encoding */
+#define I40E_MINOR_VER_FW_REQUEST_FEC_X722 0x000A
 
 struct i40e_aq_desc {
 	__le16 flags;
@@ -67,17 +69,17 @@
 #define I40E_AQ_FLAG_EI_SHIFT	14
 #define I40E_AQ_FLAG_FE_SHIFT	15
 
-#define I40E_AQ_FLAG_DD		BIT(I40E_AQ_FLAG_DD_SHIFT)  /* 0x1    */
-#define I40E_AQ_FLAG_CMP	BIT(I40E_AQ_FLAG_CMP_SHIFT) /* 0x2    */
-#define I40E_AQ_FLAG_ERR	BIT(I40E_AQ_FLAG_ERR_SHIFT) /* 0x4    */
-#define I40E_AQ_FLAG_VFE	BIT(I40E_AQ_FLAG_VFE_SHIFT) /* 0x8    */
-#define I40E_AQ_FLAG_LB		BIT(I40E_AQ_FLAG_LB_SHIFT)  /* 0x200  */
-#define I40E_AQ_FLAG_RD		BIT(I40E_AQ_FLAG_RD_SHIFT)  /* 0x400  */
-#define I40E_AQ_FLAG_VFC	BIT(I40E_AQ_FLAG_VFC_SHIFT) /* 0x800  */
-#define I40E_AQ_FLAG_BUF	BIT(I40E_AQ_FLAG_BUF_SHIFT) /* 0x1000 */
-#define I40E_AQ_FLAG_SI		BIT(I40E_AQ_FLAG_SI_SHIFT)  /* 0x2000 */
-#define I40E_AQ_FLAG_EI		BIT(I40E_AQ_FLAG_EI_SHIFT)  /* 0x4000 */
-#define I40E_AQ_FLAG_FE		BIT(I40E_AQ_FLAG_FE_SHIFT)  /* 0x8000 */
+#define I40E_AQ_FLAG_DD		(1 << I40E_AQ_FLAG_DD_SHIFT)  /* 0x1    */
+#define I40E_AQ_FLAG_CMP	(1 << I40E_AQ_FLAG_CMP_SHIFT) /* 0x2    */
+#define I40E_AQ_FLAG_ERR	(1 << I40E_AQ_FLAG_ERR_SHIFT) /* 0x4    */
+#define I40E_AQ_FLAG_VFE	(1 << I40E_AQ_FLAG_VFE_SHIFT) /* 0x8    */
+#define I40E_AQ_FLAG_LB		(1 << I40E_AQ_FLAG_LB_SHIFT)  /* 0x200  */
+#define I40E_AQ_FLAG_RD		(1 << I40E_AQ_FLAG_RD_SHIFT)  /* 0x400  */
+#define I40E_AQ_FLAG_VFC	(1 << I40E_AQ_FLAG_VFC_SHIFT) /* 0x800  */
+#define I40E_AQ_FLAG_BUF	(1 << I40E_AQ_FLAG_BUF_SHIFT) /* 0x1000 */
+#define I40E_AQ_FLAG_SI		(1 << I40E_AQ_FLAG_SI_SHIFT)  /* 0x2000 */
+#define I40E_AQ_FLAG_EI		(1 << I40E_AQ_FLAG_EI_SHIFT)  /* 0x4000 */
+#define I40E_AQ_FLAG_FE		(1 << I40E_AQ_FLAG_FE_SHIFT)  /* 0x8000 */
 
 /* error codes */
 enum i40e_admin_queue_err {
@@ -135,6 +137,7 @@
 	/* WoL commands */
 	i40e_aqc_opc_set_wol_filter	= 0x0120,
 	i40e_aqc_opc_get_wake_reason	= 0x0121,
+	i40e_aqc_opc_clear_all_wol_filters = 0x025E,
 
 	/* internal switch commands */
 	i40e_aqc_opc_get_switch_config		= 0x0200,
@@ -175,6 +178,7 @@
 	i40e_aqc_opc_add_cloud_filters		= 0x025C,
 	i40e_aqc_opc_remove_cloud_filters	= 0x025D,
 	i40e_aqc_opc_clear_wol_switch_filters	= 0x025E,
+	i40e_aqc_opc_replace_cloud_filters	= 0x025F,
 
 	i40e_aqc_opc_add_mirror_rule	= 0x0260,
 	i40e_aqc_opc_delete_mirror_rule	= 0x0261,
@@ -212,6 +216,8 @@
 	i40e_aqc_opc_set_hmc_resource_profile	= 0x0501,
 
 	/* phy commands*/
+
+	/* phy commands*/
 	i40e_aqc_opc_get_phy_abilities		= 0x0600,
 	i40e_aqc_opc_set_phy_config		= 0x0601,
 	i40e_aqc_opc_set_mac_config		= 0x0603,
@@ -235,6 +241,8 @@
 	i40e_aqc_opc_nvm_update			= 0x0703,
 	i40e_aqc_opc_nvm_config_read		= 0x0704,
 	i40e_aqc_opc_nvm_config_write		= 0x0705,
+	i40e_aqc_opc_nvm_update_in_process	= 0x0706,
+	i40e_aqc_opc_rollback_revision_update	= 0x0707,
 	i40e_aqc_opc_oem_post_update		= 0x0720,
 	i40e_aqc_opc_thermal_sensor		= 0x0721,
 
@@ -434,6 +442,7 @@
 #define I40E_AQ_CAP_ID_SDP		0x0062
 #define I40E_AQ_CAP_ID_MDIO		0x0063
 #define I40E_AQ_CAP_ID_WSR_PROT		0x0064
+#define I40E_AQ_CAP_ID_DIS_UNUSED_PORTS	0x0067
 #define I40E_AQ_CAP_ID_NVM_MGMT		0x0080
 #define I40E_AQ_CAP_ID_FLEX10		0x00F1
 #define I40E_AQ_CAP_ID_CEM		0x00F2
@@ -524,6 +533,7 @@
 #define I40E_AQC_PORT_ADDR_VALID	0x40
 #define I40E_AQC_WOL_ADDR_VALID		0x80
 #define I40E_AQC_MC_MAG_EN_VALID	0x100
+#define I40E_AQC_WOL_PRESERVE_STATUS	0x200
 #define I40E_AQC_ADDR_VALID_MASK	0x3F0
 	u8	reserved[6];
 	__le32	addr_high;
@@ -584,6 +594,7 @@
 	__le16 cmd_flags;
 #define I40E_AQC_SET_WOL_FILTER				0x8000
 #define I40E_AQC_SET_WOL_FILTER_NO_TCO_WOL		0x4000
+#define I40E_AQC_SET_WOL_FILTER_WOL_PRESERVE_ON_PFR	0x2000
 #define I40E_AQC_SET_WOL_FILTER_ACTION_CLEAR		0
 #define I40E_AQC_SET_WOL_FILTER_ACTION_SET		1
 	__le16 valid_flags;
@@ -757,6 +768,8 @@
 /* flags used for both fields below */
 #define I40E_AQ_SET_SWITCH_CFG_PROMISC		0x0001
 #define I40E_AQ_SET_SWITCH_CFG_L2_FILTER	0x0002
+#define I40E_AQ_SET_SWITCH_CFG_HW_ATR_EVICT	0x0004
+#define I40E_AQ_SET_SWITCH_CFG_OUTER_VLAN	0x0008
 	__le16	valid_flags;
 	/* The ethertype in switch_tag is dropped on ingress and used
 	 * internally by the switch. Set this to zero for the default
@@ -893,7 +906,7 @@
 	u8	sec_reserved;
 	/* VLAN section */
 	__le16	pvid; /* VLANS include priority bits */
-	__le16	fcoe_pvid;
+	__le16	outer_vlan;
 	u8	port_vlan_flags;
 #define I40E_AQ_VSI_PVLAN_MODE_SHIFT	0x00
 #define I40E_AQ_VSI_PVLAN_MODE_MASK	(0x03 << \
@@ -909,7 +922,24 @@
 #define I40E_AQ_VSI_PVLAN_EMOD_STR_UP	0x08
 #define I40E_AQ_VSI_PVLAN_EMOD_STR	0x10
 #define I40E_AQ_VSI_PVLAN_EMOD_NOTHING	0x18
-	u8	pvlan_reserved[3];
+	u8	outer_vlan_flags;
+#define I40E_AQ_VSI_OVLAN_MODE_SHIFT	0x00
+#define I40E_AQ_VSI_OVLAN_MODE_MASK	(0x03 << \
+					 I40E_AQ_VSI_OVLAN_MODE_SHIFT)
+#define I40E_AQ_VSI_OVLAN_MODE_UNTAGGED	0x01
+#define I40E_AQ_VSI_OVLAN_MODE_TAGGED	0x02
+#define I40E_AQ_VSI_OVLAN_MODE_ALL	0x03
+#define I40E_AQ_VSI_OVLAN_INSERT_PVID	0x04
+#define I40E_AQ_VSI_OVLAN_EMOD_SHIFT	0x03
+#define I40E_AQ_VSI_OVLAN_EMOD_MASK	(0x03 <<\
+					 I40E_AQ_VSI_OVLAN_EMOD_SHIFT)
+#define I40E_AQ_VSI_OVLAN_EMOD_SHOW_ALL	0x00
+#define I40E_AQ_VSI_OVLAN_EMOD_SHOW_UP	0x01
+#define I40E_AQ_VSI_OVLAN_EMOD_HIDE_ALL	0x02
+#define I40E_AQ_VSI_OVLAN_EMOD_NOTHING	0x03
+#define I40E_AQ_VSI_OVLAN_CTRL_ENA	0x04
+
+	u8	pvlan_reserved[2];
 	/* ingress egress up sections */
 	__le32	ingress_table; /* bitmap, 3 bits per up */
 #define I40E_AQ_VSI_UP_TABLE_UP0_SHIFT	0
@@ -1355,7 +1385,8 @@
 #define I40E_AQC_ADD_CLOUD_CMD_SEID_NUM_MASK	(0x3FF << \
 					I40E_AQC_ADD_CLOUD_CMD_SEID_NUM_SHIFT)
 	u8	big_buffer_flag;
-#define I40E_AQC_ADD_CLOUD_CMD_BB	1
+#define I40E_AQC_ADD_REM_CLOUD_CMD_BIG_BUFFER	1
+#define I40E_AQC_ADD_CLOUD_CMD_BB		1
 	u8	reserved2[3];
 	__le32	addr_high;
 	__le32	addr_low;
@@ -1396,6 +1427,8 @@
 #define I40E_AQC_ADD_CLOUD_FILTER_IMAC			0x000A
 #define I40E_AQC_ADD_CLOUD_FILTER_OMAC_TEN_ID_IMAC	0x000B
 #define I40E_AQC_ADD_CLOUD_FILTER_IIP			0x000C
+#define I40E_AQC_ADD_CLOUD_FILTER_OIP1			0x0010
+#define I40E_AQC_ADD_CLOUD_FILTER_OIP2			0x0012
 /* 0x000D reserved */
 /* 0x000E reserved */
 /* 0x000F reserved */
@@ -1437,6 +1470,45 @@
 	u8	response_reserved[7];
 };
 
+/* i40e_aqc_add_rm_cloud_filt_elem_ext is used when
+ * I40E_AQC_ADD_REM_CLOUD_CMD_BIG_BUFFER flag is set.
+ */
+struct i40e_aqc_add_rm_cloud_filt_elem_ext {
+	struct i40e_aqc_cloud_filters_element_data element;
+	u16     general_fields[32];
+#define I40E_AQC_ADD_CLOUD_FV_FLU_0X10_WORD0	0
+#define I40E_AQC_ADD_CLOUD_FV_FLU_0X10_WORD1	1
+#define I40E_AQC_ADD_CLOUD_FV_FLU_0X10_WORD2	2
+#define I40E_AQC_ADD_CLOUD_FV_FLU_0X11_WORD0	3
+#define I40E_AQC_ADD_CLOUD_FV_FLU_0X11_WORD1	4
+#define I40E_AQC_ADD_CLOUD_FV_FLU_0X11_WORD2	5
+#define I40E_AQC_ADD_CLOUD_FV_FLU_0X12_WORD0	6
+#define I40E_AQC_ADD_CLOUD_FV_FLU_0X12_WORD1	7
+#define I40E_AQC_ADD_CLOUD_FV_FLU_0X12_WORD2	8
+#define I40E_AQC_ADD_CLOUD_FV_FLU_0X13_WORD0	9
+#define I40E_AQC_ADD_CLOUD_FV_FLU_0X13_WORD1	10
+#define I40E_AQC_ADD_CLOUD_FV_FLU_0X13_WORD2	11
+#define I40E_AQC_ADD_CLOUD_FV_FLU_0X14_WORD0	12
+#define I40E_AQC_ADD_CLOUD_FV_FLU_0X14_WORD1	13
+#define I40E_AQC_ADD_CLOUD_FV_FLU_0X14_WORD2	14
+#define I40E_AQC_ADD_CLOUD_FV_FLU_0X16_WORD0	15
+#define I40E_AQC_ADD_CLOUD_FV_FLU_0X16_WORD1	16
+#define I40E_AQC_ADD_CLOUD_FV_FLU_0X16_WORD2	17
+#define I40E_AQC_ADD_CLOUD_FV_FLU_0X16_WORD3	18
+#define I40E_AQC_ADD_CLOUD_FV_FLU_0X16_WORD4	19
+#define I40E_AQC_ADD_CLOUD_FV_FLU_0X16_WORD5	20
+#define I40E_AQC_ADD_CLOUD_FV_FLU_0X16_WORD6	21
+#define I40E_AQC_ADD_CLOUD_FV_FLU_0X16_WORD7	22
+#define I40E_AQC_ADD_CLOUD_FV_FLU_0X17_WORD0	23
+#define I40E_AQC_ADD_CLOUD_FV_FLU_0X17_WORD1	24
+#define I40E_AQC_ADD_CLOUD_FV_FLU_0X17_WORD2	25
+#define I40E_AQC_ADD_CLOUD_FV_FLU_0X17_WORD3	26
+#define I40E_AQC_ADD_CLOUD_FV_FLU_0X17_WORD4	27
+#define I40E_AQC_ADD_CLOUD_FV_FLU_0X17_WORD5	28
+#define I40E_AQC_ADD_CLOUD_FV_FLU_0X17_WORD6	29
+#define I40E_AQC_ADD_CLOUD_FV_FLU_0X17_WORD7	30
+};
+
 I40E_CHECK_STRUCT_LEN(0x40, i40e_aqc_cloud_filters_element_data);
 
 /* i40e_aqc_cloud_filters_element_bb is used when
@@ -1503,16 +1575,17 @@
 I40E_CHECK_STRUCT_LEN(4, i40e_filter_data);
 
 struct i40e_aqc_replace_cloud_filters_cmd {
-	u8      valid_flags;
+	u8	valid_flags;
 #define I40E_AQC_REPLACE_L1_FILTER		0x0
 #define I40E_AQC_REPLACE_CLOUD_FILTER		0x1
 #define I40E_AQC_GET_CLOUD_FILTERS		0x2
 #define I40E_AQC_MIRROR_CLOUD_FILTER		0x4
 #define I40E_AQC_HIGH_PRIORITY_CLOUD_FILTER	0x8
-	u8      old_filter_type;
-	u8      new_filter_type;
-	u8      tr_bit;
-	u8      reserved[4];
+	u8	old_filter_type;
+	u8	new_filter_type;
+	u8	tr_bit;
+	u8	tr_bit2;
+	u8	reserved[3];
 	__le32 addr_high;
 	__le32 addr_low;
 };
@@ -1520,27 +1593,27 @@
 I40E_CHECK_CMD_LENGTH(i40e_aqc_replace_cloud_filters_cmd);
 
 struct i40e_aqc_replace_cloud_filters_cmd_buf {
-	u8      data[32];
+	u8	data[32];
 /* Filter type INPUT codes*/
 #define I40E_AQC_REPLACE_CLOUD_CMD_INPUT_ENTRIES_MAX	3
-#define I40E_AQC_REPLACE_CLOUD_CMD_INPUT_VALIDATED	BIT(7)
+#define I40E_AQC_REPLACE_CLOUD_CMD_INPUT_VALIDATED	(1 << 7UL)
 
 /* Field Vector offsets */
-#define I40E_AQC_REPLACE_CLOUD_CMD_INPUT_FV_MAC_DA	0
-#define I40E_AQC_REPLACE_CLOUD_CMD_INPUT_FV_STAG_ETH	6
-#define I40E_AQC_REPLACE_CLOUD_CMD_INPUT_FV_STAG	7
-#define I40E_AQC_REPLACE_CLOUD_CMD_INPUT_FV_VLAN	8
-#define I40E_AQC_REPLACE_CLOUD_CMD_INPUT_FV_STAG_OVLAN	9
-#define I40E_AQC_REPLACE_CLOUD_CMD_INPUT_FV_STAG_IVLAN	10
-#define I40E_AQC_REPLACE_CLOUD_CMD_INPUT_FV_TUNNLE_KEY	11
-#define I40E_AQC_REPLACE_CLOUD_CMD_INPUT_FV_IMAC	12
+#define I40E_AQC_REPLACE_CLOUD_CMD_INPUT_FV_MAC_DA		0
+#define I40E_AQC_REPLACE_CLOUD_CMD_INPUT_FV_STAG_ETH		6
+#define I40E_AQC_REPLACE_CLOUD_CMD_INPUT_FV_STAG		7
+#define I40E_AQC_REPLACE_CLOUD_CMD_INPUT_FV_VLAN		8
+#define I40E_AQC_REPLACE_CLOUD_CMD_INPUT_FV_STAG_OVLAN		9
+#define I40E_AQC_REPLACE_CLOUD_CMD_INPUT_FV_STAG_IVLAN		10
+#define I40E_AQC_REPLACE_CLOUD_CMD_INPUT_FV_TUNNLE_KEY		11
+#define I40E_AQC_REPLACE_CLOUD_CMD_INPUT_FV_IMAC		12
 /* big FLU */
-#define I40E_AQC_REPLACE_CLOUD_CMD_INPUT_FV_IP_DA	14
+#define I40E_AQC_REPLACE_CLOUD_CMD_INPUT_FV_IP_DA		14
 /* big FLU */
-#define I40E_AQC_REPLACE_CLOUD_CMD_INPUT_FV_OIP_DA	15
+#define I40E_AQC_REPLACE_CLOUD_CMD_INPUT_FV_OIP_DA		15
 
-#define I40E_AQC_REPLACE_CLOUD_CMD_INPUT_FV_INNER_VLAN	37
-	struct i40e_filter_data filters[8];
+#define I40E_AQC_REPLACE_CLOUD_CMD_INPUT_FV_INNER_VLAN		37
+	struct i40e_filter_data	filters[8];
 };
 
 I40E_CHECK_STRUCT_LEN(0x40, i40e_aqc_replace_cloud_filters_cmd_buf);
@@ -1893,8 +1966,10 @@
 	I40E_PHY_TYPE_25GBASE_LR		= 0x22,
 	I40E_PHY_TYPE_25GBASE_AOC		= 0x23,
 	I40E_PHY_TYPE_25GBASE_ACC		= 0x24,
-	I40E_PHY_TYPE_2_5GBASE_T		= 0x30,
-	I40E_PHY_TYPE_5GBASE_T			= 0x31,
+	I40E_PHY_TYPE_2_5GBASE_T		= 0x26,
+	I40E_PHY_TYPE_5GBASE_T			= 0x27,
+	I40E_PHY_TYPE_2_5GBASE_T_LINK_STATUS	= 0x30,
+	I40E_PHY_TYPE_5GBASE_T_LINK_STATUS	= 0x31,
 	I40E_PHY_TYPE_MAX,
 	I40E_PHY_TYPE_NOT_SUPPORTED_HIGH_TEMP	= 0xFD,
 	I40E_PHY_TYPE_EMPTY			= 0xFE,
@@ -1951,14 +2026,23 @@
 
 enum i40e_aq_link_speed {
 	I40E_LINK_SPEED_UNKNOWN	= 0,
-	I40E_LINK_SPEED_100MB	= BIT(I40E_LINK_SPEED_100MB_SHIFT),
-	I40E_LINK_SPEED_1GB	= BIT(I40E_LINK_SPEED_1000MB_SHIFT),
+	I40E_LINK_SPEED_100MB	= (1 << I40E_LINK_SPEED_100MB_SHIFT),
+	I40E_LINK_SPEED_1GB	= (1 << I40E_LINK_SPEED_1000MB_SHIFT),
 	I40E_LINK_SPEED_2_5GB	= (1 << I40E_LINK_SPEED_2_5GB_SHIFT),
 	I40E_LINK_SPEED_5GB	= (1 << I40E_LINK_SPEED_5GB_SHIFT),
-	I40E_LINK_SPEED_10GB	= BIT(I40E_LINK_SPEED_10GB_SHIFT),
-	I40E_LINK_SPEED_40GB	= BIT(I40E_LINK_SPEED_40GB_SHIFT),
-	I40E_LINK_SPEED_20GB	= BIT(I40E_LINK_SPEED_20GB_SHIFT),
-	I40E_LINK_SPEED_25GB	= BIT(I40E_LINK_SPEED_25GB_SHIFT),
+	I40E_LINK_SPEED_10GB	= (1 << I40E_LINK_SPEED_10GB_SHIFT),
+	I40E_LINK_SPEED_40GB	= (1 << I40E_LINK_SPEED_40GB_SHIFT),
+	I40E_LINK_SPEED_20GB	= (1 << I40E_LINK_SPEED_20GB_SHIFT),
+	I40E_LINK_SPEED_25GB	= (1 << I40E_LINK_SPEED_25GB_SHIFT),
+};
+
+enum i40e_prt_mac_pcs_link_speed {
+	I40E_PRT_MAC_PCS_LINK_SPEED_UNKNOWN = 0,
+	I40E_PRT_MAC_PCS_LINK_SPEED_100MB,
+	I40E_PRT_MAC_PCS_LINK_SPEED_1GB,
+	I40E_PRT_MAC_PCS_LINK_SPEED_10GB,
+	I40E_PRT_MAC_PCS_LINK_SPEED_40GB,
+	I40E_PRT_MAC_PCS_LINK_SPEED_20GB
 };
 
 struct i40e_aqc_module_desc {
@@ -1984,18 +2068,21 @@
 #define I40E_AQ_PHY_FEC_ABILITY_KR	0x40
 #define I40E_AQ_PHY_FEC_ABILITY_RS	0x80
 	__le16	eee_capability;
+#define I40E_AQ_EEE_AUTO		0x0001
 #define I40E_AQ_EEE_100BASE_TX		0x0002
 #define I40E_AQ_EEE_1000BASE_T		0x0004
 #define I40E_AQ_EEE_10GBASE_T		0x0008
 #define I40E_AQ_EEE_1000BASE_KX		0x0010
 #define I40E_AQ_EEE_10GBASE_KX4		0x0020
 #define I40E_AQ_EEE_10GBASE_KR		0x0040
+#define I40E_AQ_EEE_2_5GBASE_T		0x0100
+#define I40E_AQ_EEE_5GBASE_T		0x0200
 	__le32	eeer_val;
 	u8	d3_lpan;
 #define I40E_AQ_SET_PHY_D3_LPAN_ENA	0x01
 	u8	phy_type_ext;
-#define I40E_AQ_PHY_TYPE_EXT_25G_KR	0X01
-#define I40E_AQ_PHY_TYPE_EXT_25G_CR	0X02
+#define I40E_AQ_PHY_TYPE_EXT_25G_KR	0x01
+#define I40E_AQ_PHY_TYPE_EXT_25G_CR	0x02
 #define I40E_AQ_PHY_TYPE_EXT_25G_SR	0x04
 #define I40E_AQ_PHY_TYPE_EXT_25G_LR	0x08
 #define I40E_AQ_PHY_TYPE_EXT_25G_AOC	0x10
@@ -2035,10 +2122,6 @@
 	__le32	eeer;
 	u8	low_power_ctrl;
 	u8	phy_type_ext;
-#define I40E_AQ_PHY_TYPE_EXT_25G_KR	0X01
-#define I40E_AQ_PHY_TYPE_EXT_25G_CR	0X02
-#define I40E_AQ_PHY_TYPE_EXT_25G_SR	0x04
-#define I40E_AQ_PHY_TYPE_EXT_25G_LR	0x08
 	u8	fec_config;
 #define I40E_AQ_SET_FEC_ABILITY_KR	BIT(0)
 #define I40E_AQ_SET_FEC_ABILITY_RS	BIT(1)
@@ -2197,11 +2280,28 @@
 
 /* Set Loopback mode (0x0618) */
 struct i40e_aqc_set_lb_mode {
-	__le16	lb_mode;
+	u8	lb_level;
+#define I40E_AQ_LB_NONE	0
+#define I40E_AQ_LB_MAC	1
+#define I40E_AQ_LB_SERDES	2
+#define I40E_AQ_LB_PHY_INT	3
+#define I40E_AQ_LB_PHY_EXT	4
+#define I40E_AQ_LB_BASE_T_PCS	5
+#define I40E_AQ_LB_BASE_T_EXT	6
 #define I40E_AQ_LB_PHY_LOCAL	0x01
 #define I40E_AQ_LB_PHY_REMOTE	0x02
 #define I40E_AQ_LB_MAC_LOCAL	0x04
-	u8	reserved[14];
+	u8	lb_type;
+#define I40E_AQ_LB_LOCAL	0
+#define I40E_AQ_LB_FAR	0x01
+	u8	speed;
+#define I40E_AQ_LB_SPEED_NONE	0
+#define I40E_AQ_LB_SPEED_1G	1
+#define I40E_AQ_LB_SPEED_10G	2
+#define I40E_AQ_LB_SPEED_40G	3
+#define I40E_AQ_LB_SPEED_20G	4
+	u8	force_speed;
+	u8	reserved[12];
 };
 
 I40E_CHECK_CMD_LENGTH(i40e_aqc_set_lb_mode);
@@ -2218,7 +2318,7 @@
 #define I40E_AQ_PHY_DEBUG_RESET_EXTERNAL_SOFT	0x02
 /* Disable link manageability on a single port */
 #define I40E_AQ_PHY_DEBUG_DISABLE_LINK_FW	0x10
-/* Disable link manageability on all ports */
+/* Disable link manageability on all ports needs both bits 4 and 5 */
 #define I40E_AQ_PHY_DEBUG_DISABLE_ALL_LINK_FW	0x20
 	u8	reserved[15];
 };
@@ -2233,15 +2333,28 @@
 
 /* Run PHY Activity (0x0626) */
 struct i40e_aqc_run_phy_activity {
-	__le16  activity_id;
-	u8      flags;
-	u8      reserved1;
-	__le32  control;
-	__le32  data;
-	u8      reserved2[4];
-};
-
-I40E_CHECK_CMD_LENGTH(i40e_aqc_run_phy_activity);
+	u8	cmd_flags;
+	__le16	activity_id;
+#define I40E_AQ_RUN_PHY_ACT_ID_USR_DFND			0x10
+	u8	reserved;
+	union {
+		struct {
+			__le32  dnl_opcode;
+#define I40E_AQ_RUN_PHY_ACT_DNL_OPCODE_GET_EEE_STAT_DUR	0x801a
+#define I40E_AQ_RUN_PHY_ACT_DNL_OPCODE_GET_EEE_STAT	0x801b
+#define I40E_AQ_RUN_PHY_ACT_DNL_OPCODE_GET_EEE_DUR	0x1801b
+			__le32  data;
+			u8	reserved2[4];
+		} cmd;
+		struct {
+			__le32	cmd_status;
+#define I40E_AQ_RUN_PHY_ACT_CMD_STAT_SUCC		0x4
+#define I40E_AQ_RUN_PHY_ACT_CMD_STAT_MASK		0xFFFF
+			__le32	data0;
+			__le32	data1;
+		} resp;
+	} params;
+} __packed;
 
 /* Set PHY Register command (0x0628) */
 /* Get PHY Register command (0x0629) */
@@ -2250,8 +2363,14 @@
 #define I40E_AQ_PHY_REG_ACCESS_INTERNAL	0
 #define I40E_AQ_PHY_REG_ACCESS_EXTERNAL	1
 #define I40E_AQ_PHY_REG_ACCESS_EXTERNAL_MODULE	2
-	u8	dev_address;
-	u8	reserved1[2];
+	u8	dev_addres;
+	u8	cmd_flags;
+#define I40E_AQ_PHY_REG_ACCESS_DONT_CHANGE_QSFP_PAGE	0x01
+#define I40E_AQ_PHY_REG_ACCESS_SET_MDIO_IF_NUMBER	0x02
+#define I40E_AQ_PHY_REG_ACCESS_MDIO_IF_NUMBER_SHIFT	2
+#define I40E_AQ_PHY_REG_ACCESS_MDIO_IF_NUMBER_MASK	(0x3 << \
+		I40E_AQ_PHY_REG_ACCESS_MDIO_IF_NUMBER_SHIFT)
+	u8	reserved1;
 	__le32	reg_address;
 	__le32	reg_value;
 	u8	reserved2[4];
@@ -2285,8 +2404,8 @@
 /* NVM Config Read (indirect 0x0704) */
 struct i40e_aqc_nvm_config_read {
 	__le16	cmd_flags;
-#define I40E_AQ_ANVM_SINGLE_OR_MULTIPLE_FEATURES_MASK	1 
-#define I40E_AQ_ANVM_READ_SINGLE_FEATURE		0 
+#define I40E_AQ_ANVM_SINGLE_OR_MULTIPLE_FEATURES_MASK	1
+#define I40E_AQ_ANVM_READ_SINGLE_FEATURE		0
 #define I40E_AQ_ANVM_READ_MULTIPLE_FEATURES		1
 	__le16	element_count;
 	__le16	element_id;	/* Feature/field ID */
@@ -2311,9 +2430,9 @@
 /* Used for 0x0704 as well as for 0x0705 commands */
 #define I40E_AQ_ANVM_FEATURE_OR_IMMEDIATE_SHIFT		1
 #define I40E_AQ_ANVM_FEATURE_OR_IMMEDIATE_MASK \
-				BIT(I40E_AQ_ANVM_FEATURE_OR_IMMEDIATE_SHIFT)
+				(1 << I40E_AQ_ANVM_FEATURE_OR_IMMEDIATE_SHIFT)
 #define I40E_AQ_ANVM_FEATURE		0
-#define I40E_AQ_ANVM_IMMEDIATE_FIELD	BIT(FEATURE_OR_IMMEDIATE_SHIFT)
+#define I40E_AQ_ANVM_IMMEDIATE_FIELD	(1 << FEATURE_OR_IMMEDIATE_SHIFT)
 struct i40e_aqc_nvm_config_data_feature {
 	__le16 feature_id;
 #define I40E_AQ_ANVM_FEATURE_OPTION_OEM_ONLY		0x01
@@ -2325,6 +2444,16 @@
 
 I40E_CHECK_STRUCT_LEN(0x6, i40e_aqc_nvm_config_data_feature);
 
+/* NVM Update in Process (direct 0x0706) */
+struct i40e_aqc_nvm_update_in_process {
+	u8	command;
+#define I40E_AQ_UPDATE_FLOW_END			0x0
+#define I40E_AQ_UPDATE_FLOW_START		0x1
+	u8	reserved[15];
+};
+
+I40E_CHECK_CMD_LENGTH(i40e_aqc_nvm_update_in_process);
+
 struct i40e_aqc_nvm_config_data_immediate_field {
 	__le32 field_id;
 	__le32 field_value;
@@ -2334,6 +2463,27 @@
 
 I40E_CHECK_STRUCT_LEN(0xc, i40e_aqc_nvm_config_data_immediate_field);
 
+/* Minimal Rollback Revision Update (direct 0x0707) */
+struct i40e_aqc_rollback_revision_update {
+	u8	optin_mode; /* bool */
+#define I40E_AQ_RREV_OPTIN_MODE			0x01
+	u8	module_selected;
+#define I40E_AQ_RREV_MODULE_PCIE_ANALOG			0
+#define I40E_AQ_RREV_MODULE_PHY_ANALOG			1
+#define I40E_AQ_RREV_MODULE_OPTION_ROM			2
+#define I40E_AQ_RREV_MODULE_EMP_IMAGE			3
+#define I40E_AQ_RREV_MODULE_PE_IMAGE			4
+#define I40E_AQ_RREV_MODULE_PHY_PLL_O_CONFIGURATION	5
+#define I40E_AQ_RREV_MODULE_PHY_0_CONFIGURATION		6
+#define I40E_AQ_RREV_MODULE_PHY_PLL_1_CONFIGURATION	7
+#define I40E_AQ_RREV_MODULE_PHY_1_CONFIGURATION		8
+	u8	reserved1[2];
+	u32	min_rrev;
+	u8	reserved2[8];
+};
+
+I40E_CHECK_CMD_LENGTH(i40e_aqc_rollback_revision_update);
+
 /* OEM Post Update (indirect 0x0720)
  * no command data struct used
  */
@@ -2598,20 +2748,7 @@
 	u8	oper_tc_bw[8];
 	u8	oper_pfc_en;
 	__le16	oper_app_prio;
-#define I40E_AQC_CEE_APP_FCOE_SHIFT	0x0
-#define I40E_AQC_CEE_APP_FCOE_MASK	(0x7 << I40E_AQC_CEE_APP_FCOE_SHIFT)
-#define I40E_AQC_CEE_APP_ISCSI_SHIFT	0x3
-#define I40E_AQC_CEE_APP_ISCSI_MASK	(0x7 << I40E_AQC_CEE_APP_ISCSI_SHIFT)
-#define I40E_AQC_CEE_APP_FIP_SHIFT	0x8
-#define I40E_AQC_CEE_APP_FIP_MASK	(0x7 << I40E_AQC_CEE_APP_FIP_SHIFT)
-#define I40E_AQC_CEE_APP_FIP_MASK	(0x7 << I40E_AQC_CEE_APP_FIP_SHIFT)
 	__le32	tlv_status;
-#define I40E_AQC_CEE_PG_STATUS_SHIFT	0x0
-#define I40E_AQC_CEE_PG_STATUS_MASK	(0x7 << I40E_AQC_CEE_PG_STATUS_SHIFT)
-#define I40E_AQC_CEE_PFC_STATUS_SHIFT	0x3
-#define I40E_AQC_CEE_PFC_STATUS_MASK	(0x7 << I40E_AQC_CEE_PFC_STATUS_SHIFT)
-#define I40E_AQC_CEE_APP_STATUS_SHIFT	0x8
-#define I40E_AQC_CEE_APP_STATUS_MASK	(0x7 << I40E_AQC_CEE_APP_STATUS_SHIFT)
 	u8	reserved[12];
 };
 
@@ -2622,11 +2759,12 @@
  */
 struct i40e_aqc_lldp_set_local_mib {
 #define SET_LOCAL_MIB_AC_TYPE_DCBX_SHIFT	0
-#define SET_LOCAL_MIB_AC_TYPE_DCBX_MASK	BIT(SET_LOCAL_MIB_AC_TYPE_DCBX_SHIFT)
+#define SET_LOCAL_MIB_AC_TYPE_DCBX_MASK	(1 << \
+					SET_LOCAL_MIB_AC_TYPE_DCBX_SHIFT)
 #define SET_LOCAL_MIB_AC_TYPE_LOCAL_MIB	0x0
 #define SET_LOCAL_MIB_AC_TYPE_NON_WILLING_APPS_SHIFT	(1)
-#define SET_LOCAL_MIB_AC_TYPE_NON_WILLING_APPS_MASK \
-			BIT(SET_LOCAL_MIB_AC_TYPE_NON_WILLING_APPS_SHIFT)
+#define SET_LOCAL_MIB_AC_TYPE_NON_WILLING_APPS_MASK	(1 << \
+				SET_LOCAL_MIB_AC_TYPE_NON_WILLING_APPS_SHIFT)
 #define SET_LOCAL_MIB_AC_TYPE_NON_WILLING_APPS		0x1
 	u8	type;
 	u8	reserved0;
@@ -2638,13 +2776,21 @@
 
 I40E_CHECK_CMD_LENGTH(i40e_aqc_lldp_set_local_mib);
 
+struct i40e_aqc_lldp_set_local_mib_resp {
+#define SET_LOCAL_MIB_RESP_EVENT_TRIGGERED_MASK      0x01
+	u8  status;
+	u8  reserved[15];
+};
+
+I40E_CHECK_STRUCT_LEN(0x10, i40e_aqc_lldp_set_local_mib_resp);
+
 /*	Stop/Start LLDP Agent (direct 0x0A09)
  *	Used for stopping/starting specific LLDP agent. e.g. DCBx
  */
 struct i40e_aqc_lldp_stop_start_specific_agent {
 #define I40E_AQC_START_SPECIFIC_AGENT_SHIFT	0
 #define I40E_AQC_START_SPECIFIC_AGENT_MASK \
-				BIT(I40E_AQC_START_SPECIFIC_AGENT_SHIFT)
+				(1 << I40E_AQC_START_SPECIFIC_AGENT_SHIFT)
 	u8	command;
 	u8	reserved[15];
 };
@@ -2707,7 +2853,7 @@
 I40E_CHECK_CMD_LENGTH(i40e_aqc_del_udp_tunnel_completion);
 
 struct i40e_aqc_get_set_rss_key {
-#define I40E_AQC_SET_RSS_KEY_VSI_VALID		BIT(15)
+#define I40E_AQC_SET_RSS_KEY_VSI_VALID		(0x1 << 15)
 #define I40E_AQC_SET_RSS_KEY_VSI_ID_SHIFT	0
 #define I40E_AQC_SET_RSS_KEY_VSI_ID_MASK	(0x3FF << \
 					I40E_AQC_SET_RSS_KEY_VSI_ID_SHIFT)
@@ -2727,13 +2873,14 @@
 I40E_CHECK_STRUCT_LEN(0x34, i40e_aqc_get_set_rss_key_data);
 
 struct  i40e_aqc_get_set_rss_lut {
-#define I40E_AQC_SET_RSS_LUT_VSI_VALID		BIT(15)
+#define I40E_AQC_SET_RSS_LUT_VSI_VALID		(0x1 << 15)
 #define I40E_AQC_SET_RSS_LUT_VSI_ID_SHIFT	0
 #define I40E_AQC_SET_RSS_LUT_VSI_ID_MASK	(0x3FF << \
 					I40E_AQC_SET_RSS_LUT_VSI_ID_SHIFT)
 	__le16	vsi_id;
 #define I40E_AQC_SET_RSS_LUT_TABLE_TYPE_SHIFT	0
-#define I40E_AQC_SET_RSS_LUT_TABLE_TYPE_MASK	BIT(I40E_AQC_SET_RSS_LUT_TABLE_TYPE_SHIFT)
+#define I40E_AQC_SET_RSS_LUT_TABLE_TYPE_MASK	(0x1 << \
+					I40E_AQC_SET_RSS_LUT_TABLE_TYPE_SHIFT)
 
 #define I40E_AQC_SET_RSS_LUT_TABLE_TYPE_VSI	0
 #define I40E_AQC_SET_RSS_LUT_TABLE_TYPE_PF	1
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_adminq.h linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_adminq.h
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_adminq.h	2024-05-10 01:26:45.325079481 -0400
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_adminq.h	2024-05-13 03:58:25.368491972 -0400
@@ -1,5 +1,5 @@
 /* SPDX-License-Identifier: GPL-2.0 */
-/* Copyright(c) 2013 - 2018 Intel Corporation. */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
 
 #ifndef _I40E_ADMINQ_H_
 #define _I40E_ADMINQ_H_
@@ -75,8 +75,8 @@
 	u16 api_maj_ver;                /* api major version */
 	u16 api_min_ver;                /* api minor version */
 
-	struct mutex asq_mutex; /* Send queue lock */
-	struct mutex arq_mutex; /* Receive queue lock */
+	struct i40e_spinlock asq_spinlock; /* Send queue spinlock */
+	struct i40e_spinlock arq_spinlock; /* Receive queue spinlock */
 
 	/* last status values on send and receive queues */
 	enum i40e_admin_queue_err asq_last_status;
@@ -88,7 +88,7 @@
  * aq_ret: AdminQ handler error code can override aq_rc
  * aq_rc: AdminQ firmware error code to convert
  **/
-static inline int i40e_aq_rc_to_posix(int aq_ret, int aq_rc)
+static INLINE int i40e_aq_rc_to_posix(int aq_ret, int aq_rc)
 {
 	int aq_to_posix[] = {
 		0,           /* I40E_AQ_RC_OK */
Binary files linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_adminq.o and linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_adminq.o differ
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_alloc.h linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_alloc.h
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_alloc.h	2024-05-10 01:26:45.325079481 -0400
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_alloc.h	2024-05-13 03:58:25.368491972 -0400
@@ -1,5 +1,5 @@
 /* SPDX-License-Identifier: GPL-2.0 */
-/* Copyright(c) 2013 - 2018 Intel Corporation. */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
 
 #ifndef _I40E_ALLOC_H_
 #define _I40E_ALLOC_H_
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_client.c linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_client.c
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_client.c	2024-05-10 01:26:45.325079481 -0400
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_client.c	2024-05-13 03:58:25.368491972 -0400
@@ -1,5 +1,5 @@
 // SPDX-License-Identifier: GPL-2.0
-/* Copyright(c) 2013 - 2018 Intel Corporation. */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
 
 #include <linux/list.h>
 #include <linux/errno.h>
@@ -8,11 +8,19 @@
 #include "i40e_prototype.h"
 #include "i40e_client.h"
 
+#if IS_ENABLED(CONFIG_MFD_CORE)
+#define I40E_MFD_CELL_SIZE      ARRAY_SIZE(i40e_mfd_cells)
+
+#endif
 static const char i40e_client_interface_version_str[] = I40E_CLIENT_VERSION_STR;
 static struct i40e_client *registered_client;
 static LIST_HEAD(i40e_devices);
 static DEFINE_MUTEX(i40e_device_mutex);
 
+#if IS_ENABLED(CONFIG_MFD_CORE)
+static struct mfd_cell i40e_mfd_cells[] = ASSIGN_PEER_INFO;
+
+#endif
 static int i40e_client_virtchnl_send(struct i40e_info *ldev,
 				     struct i40e_client *client,
 				     u32 vf_id, u8 *msg, u16 len);
@@ -30,17 +38,23 @@
 				       bool is_vf, u32 vf_id,
 				       u32 flag, u32 valid_flag);
 
+static int i40e_client_device_register(struct i40e_info *ldev);
+
+static void i40e_client_device_unregister(struct i40e_info *ldev);
+
 static struct i40e_ops i40e_lan_ops = {
 	.virtchnl_send = i40e_client_virtchnl_send,
 	.setup_qvlist = i40e_client_setup_qvlist,
 	.request_reset = i40e_client_request_reset,
 	.update_vsi_ctxt = i40e_client_update_vsi_ctxt,
+	.client_device_register = i40e_client_device_register,
+	.client_device_unregister = i40e_client_device_unregister,
 };
 
 /**
  * i40e_client_get_params - Get the params that can change at runtime
  * @vsi: the VSI with the message
- * @params: client param struct
+ * @params: clinet param struct
  *
  **/
 static
@@ -275,21 +289,67 @@
 	cdev->lan_info.msix_entries = &pf->msix_entries[pf->iwarp_base_vector];
 }
 
+#if IS_ENABLED(CONFIG_MFD_CORE)
+static DEFINE_IDA(i40e_peer_index_ida);
+
+static int i40e_init_peer_mfd_devices(struct i40e_pf *pf)
+{
+	struct i40e_peer_dev_platform_data *platform_data;
+	struct pci_dev *pdev = pf->pdev;
+	int status;
+	int i;
+
+	platform_data = kcalloc(I40E_MFD_CELL_SIZE,
+				sizeof(*platform_data), GFP_KERNEL);
+	if (!platform_data)
+		return -ENOMEM;
+
+	for (i = 0; i < I40E_MFD_CELL_SIZE; i++) {
+		/* don't create an RDMA MFD device if NIC does not
+		 * support RDMA functionality
+		 */
+		if (i40e_mfd_cells[i].id == I40E_PEER_RDMA_ID &&
+		    !(I40E_FLAG_IWARP_ENABLED & pf->flags)) {
+			dev_warn(&pf->pdev->dev,
+				 "RDMA not supported with this config\n");
+			continue;
+		}
+		platform_data[i].ldev = &pf->cinst->lan_info;
+		i40e_mfd_cells[i].platform_data = &platform_data[i];
+		i40e_mfd_cells[i].pdata_size = sizeof(platform_data);
+	}
+
+	status = ida_simple_get(&i40e_peer_index_ida, 0, 0, GFP_KERNEL);
+	if (status < 0) {
+		dev_err(&pdev->dev,
+			"failed to get unique index for device\n");
+		return status;
+	}
+
+	pf->peer_idx = status;
+	status = mfd_add_devices(&pf->pdev->dev, pf->peer_idx,
+				 i40e_mfd_cells, I40E_MFD_CELL_SIZE,
+				 NULL, 0, NULL);
+
+	if (status)
+		dev_err(&pf->pdev->dev,
+			"Failure adding MFD devs for peers: %d\n", status);
+
+	kfree(platform_data);
+	return status;
+}
+
+#endif
 /**
  * i40e_client_add_instance - add a client instance struct to the instance list
  * @pf: pointer to the board struct
- * @client: pointer to a client struct in the client list.
- * @existing: if there was already an existing instance
- *
  **/
 static void i40e_client_add_instance(struct i40e_pf *pf)
 {
 	struct i40e_client_instance *cdev = NULL;
 	struct netdev_hw_addr *mac = NULL;
 	struct i40e_vsi *vsi = pf->vsi[pf->lan_vsi];
-
-	if (!registered_client || pf->cinst)
-		return;
+	int status;
 
 	cdev = kzalloc(sizeof(*cdev), GFP_KERNEL);
 	if (!cdev)
@@ -308,13 +368,10 @@
 	cdev->lan_info.fw_maj_ver = pf->hw.aq.fw_maj_ver;
 	cdev->lan_info.fw_min_ver = pf->hw.aq.fw_min_ver;
 	cdev->lan_info.fw_build = pf->hw.aq.fw_build;
-	set_bit(__I40E_CLIENT_INSTANCE_NONE, &cdev->state);
 
-	if (i40e_client_get_params(vsi, &cdev->lan_info.params)) {
-		kfree(cdev);
-		cdev = NULL;
-		return;
-	}
+	status = i40e_client_get_params(vsi, &cdev->lan_info.params);
+	if (status)
+		goto done;
 
 	mac = list_first_entry(&cdev->lan_info.netdev->dev_addrs.list,
 			       struct netdev_hw_addr, list);
@@ -326,7 +383,21 @@
 	cdev->client = registered_client;
 	pf->cinst = cdev;
 
-	i40e_client_update_msix_info(pf);
+	cdev->lan_info.msix_count = pf->num_iwarp_msix;
+	cdev->lan_info.msix_entries = &pf->msix_entries[pf->iwarp_base_vector];
+
+#if IS_ENABLED(CONFIG_MFD_CORE)
+	status = i40e_init_peer_mfd_devices(pf);
+	if (status)
+		goto done;
+
+	set_bit(__I40E_CLIENT_INSTANCE_NONE, &cdev->state);
+#endif
+	return;
+
+done:
+	kfree(cdev);
+	cdev = NULL;
 }
 
 /**
@@ -347,7 +418,7 @@
  **/
 void i40e_client_subtask(struct i40e_pf *pf)
 {
-	struct i40e_client *client = registered_client;
+	struct i40e_client *client;
 	struct i40e_client_instance *cdev;
 	struct i40e_vsi *vsi = pf->vsi[pf->lan_vsi];
 	int ret = 0;
@@ -361,9 +432,11 @@
 	    test_bit(__I40E_CONFIG_BUSY, pf->state))
 		return;
 
-	if (!client || !cdev)
+	if (!cdev || !cdev->client)
 		return;
 
+	client = cdev->client;
+
 	/* Here we handle client opens. If the client is down, and
 	 * the netdev is registered, then open the client.
 	 */
@@ -424,16 +497,7 @@
 		 pf->hw.pf_id, pf->hw.bus.bus_id,
 		 pf->hw.bus.device, pf->hw.bus.func);
 
-	/* If a client has already been registered, we need to add an instance
-	 * of it to our new LAN device.
-	 */
-	if (registered_client)
-		i40e_client_add_instance(pf);
-
-	/* Since in some cases register may have happened before a device gets
-	 * added, we can schedule a subtask to go initiate the clients if
-	 * they can be launched at probe time.
-	 */
+	i40e_client_add_instance(pf);
 	set_bit(__I40E_CLIENT_SERVICE_REQUESTED, pf->state);
 	i40e_service_event_schedule(pf);
 
@@ -453,6 +517,10 @@
 	struct i40e_device *ldev, *tmp;
 	int ret = -ENODEV;
 
+#if IS_ENABLED(CONFIG_MFD_CORE)
+	mfd_remove_devices(&pf->pdev->dev);
+
+#endif
 	/* First, remove any client instance. */
 	i40e_client_del_instance(pf);
 
@@ -468,71 +536,9 @@
 			break;
 		}
 	}
-	mutex_unlock(&i40e_device_mutex);
-	return ret;
-}
-
-/**
- * i40e_client_release - release client specific resources
- * @client: pointer to the registered client
- *
- **/
-static void i40e_client_release(struct i40e_client *client)
-{
-	struct i40e_client_instance *cdev;
-	struct i40e_device *ldev;
-	struct i40e_pf *pf;
-
-	mutex_lock(&i40e_device_mutex);
-	list_for_each_entry(ldev, &i40e_devices, list) {
-		pf = ldev->pf;
-		cdev = pf->cinst;
-		if (!cdev)
-			continue;
-
-		while (test_and_set_bit(__I40E_SERVICE_SCHED,
-					pf->state))
-			usleep_range(500, 1000);
-
-		if (test_bit(__I40E_CLIENT_INSTANCE_OPENED, &cdev->state)) {
-			if (client->ops && client->ops->close)
-				client->ops->close(&cdev->lan_info, client,
-						   false);
-			i40e_client_release_qvlist(&cdev->lan_info);
-			clear_bit(__I40E_CLIENT_INSTANCE_OPENED, &cdev->state);
-
-			dev_warn(&pf->pdev->dev,
-				 "Client %s instance for PF id %d closed\n",
-				 client->name, pf->hw.pf_id);
-		}
-		/* delete the client instance */
-		i40e_client_del_instance(pf);
-		dev_info(&pf->pdev->dev, "Deleted client instance of Client %s\n",
-			 client->name);
-		clear_bit(__I40E_SERVICE_SCHED, pf->state);
-	}
-	mutex_unlock(&i40e_device_mutex);
-}
-
-/**
- * i40e_client_prepare - prepare client specific resources
- * @client: pointer to the registered client
- *
- **/
-static void i40e_client_prepare(struct i40e_client *client)
-{
-	struct i40e_device *ldev;
-	struct i40e_pf *pf;
 
-	mutex_lock(&i40e_device_mutex);
-	list_for_each_entry(ldev, &i40e_devices, list) {
-		pf = ldev->pf;
-		i40e_client_add_instance(pf);
-		/* Start the client subtask */
-		set_bit(__I40E_CLIENT_SERVICE_REQUESTED, pf->state);
-		i40e_service_event_schedule(pf);
-	}
 	mutex_unlock(&i40e_device_mutex);
+	return ret;
 }
 
 /**
@@ -554,7 +560,7 @@
 	i40e_status err;
 
 	err = i40e_aq_send_msg_to_vf(hw, vf_id, VIRTCHNL_OP_IWARP,
-				     0, msg, len, NULL);
+				     I40E_SUCCESS, msg, len, NULL);
 	if (err)
 		dev_err(&pf->pdev->dev, "Unable to send iWarp message to VF, error %d, aq status %d\n",
 			err, hw->aq.asq_last_status);
@@ -578,9 +584,11 @@
 	struct i40e_hw *hw = &pf->hw;
 	struct i40e_qv_info *qv_info;
 	u32 v_idx, i, reg_idx, reg;
+	u32 size;
 
-	ldev->qvlist_info = kzalloc(struct_size(ldev->qvlist_info, qv_info,
-				    qvlist_info->num_vectors - 1), GFP_KERNEL);
+	size = sizeof(struct i40e_qvlist_info) +
+	       (sizeof(struct i40e_qv_info) * (qvlist_info->num_vectors - 1));
+	ldev->qvlist_info = kzalloc(size, GFP_KERNEL);
 	if (!ldev->qvlist_info)
 		return -ENOMEM;
 	ldev->qvlist_info->num_vectors = qvlist_info->num_vectors;
@@ -733,81 +741,63 @@
 	return err;
 }
 
-/**
- * i40e_register_client - Register a i40e client driver with the L2 driver
- * @client: pointer to the i40e_client struct
- *
- * Returns 0 on success or non-0 on error
- **/
-int i40e_register_client(struct i40e_client *client)
+static int i40e_client_device_register(struct i40e_info *ldev)
 {
-	int ret = 0;
+	struct i40e_client *client;
+	struct i40e_pf *pf;
 
-	if (!client) {
-		ret = -EIO;
-		goto out;
+	if (!ldev) {
+		pr_err("Failed to reg client dev: ldev ptr NULL\n");
+		return -EINVAL;
 	}
 
-	if (strlen(client->name) == 0) {
-		pr_info("i40e: Failed to register client with no name\n");
-		ret = -EIO;
-		goto out;
+	client = ldev->client;
+	pf = ldev->pf;
+	if (!client) {
+		pr_err("Failed to reg client dev: client ptr NULL\n");
+		return -EINVAL;
 	}
 
-	if (registered_client) {
-		pr_info("i40e: Client %s has already been registered!\n",
-			client->name);
-		ret = -EEXIST;
-		goto out;
+	if (!ldev->ops || !client->ops) {
+		pr_err("Failed to reg client dev: client dev peer_ops/ops NULL\n");
+		return -EINVAL;
 	}
 
-	if ((client->version.major != I40E_CLIENT_VERSION_MAJOR) ||
-	    (client->version.minor != I40E_CLIENT_VERSION_MINOR)) {
-		pr_info("i40e: Failed to register client %s due to mismatched client interface version\n",
-			client->name);
-		pr_info("Client is using version: %02d.%02d.%02d while LAN driver supports %s\n",
-			client->version.major, client->version.minor,
-			client->version.build,
-			i40e_client_interface_version_str);
-		ret = -EIO;
-		goto out;
+	if (client->version.major != I40E_CLIENT_VERSION_MAJOR ||
+	    client->version.minor != I40E_CLIENT_VERSION_MINOR) {
+		pr_err("i40e: Failed to register client %s due to mismatched client interface version\n",
+		       client->name);
+		pr_err("Client is using version: %02d.%02d.%02d while LAN driver supports %s\n",
+		       client->version.major, client->version.minor,
+		       client->version.build,
+		       i40e_client_interface_version_str);
+		return -EINVAL;
 	}
 
-	registered_client = client;
-
-	i40e_client_prepare(client);
+	pf->cinst->client = ldev->client;
+	set_bit(__I40E_CLIENT_SERVICE_REQUESTED, pf->state);
+	i40e_service_event_schedule(pf);
 
-	pr_info("i40e: Registered client %s\n", client->name);
-out:
-	return ret;
+	return 0;
 }
-EXPORT_SYMBOL(i40e_register_client);
 
-/**
- * i40e_unregister_client - Unregister a i40e client driver with the L2 driver
- * @client: pointer to the i40e_client struct
- *
- * Returns 0 on success or non-0 on error
- **/
-int i40e_unregister_client(struct i40e_client *client)
+static void i40e_client_device_unregister(struct i40e_info *ldev)
 {
-	int ret = 0;
+	struct i40e_pf *pf = ldev->pf;
+	struct i40e_client_instance *cdev = pf->cinst;
 
-	if (registered_client != client) {
-		pr_info("i40e: Client %s has not been registered\n",
-			client->name);
-		ret = -ENODEV;
-		goto out;
-	}
-	registered_client = NULL;
-	/* When a unregister request comes through we would have to send
-	 * a close for each of the client instances that were opened.
-	 * client_release function is called to handle this.
-	 */
-	i40e_client_release(client);
+	while (test_and_set_bit(__I40E_SERVICE_SCHED, pf->state))
+		usleep_range(500, 1000);
 
-	pr_info("i40e: Unregistered client %s\n", client->name);
-out:
-	return ret;
+	if (!cdev || !cdev->client || !cdev->client->ops ||
+	    !cdev->client->ops->close) {
+		dev_err(&pf->pdev->dev, "Cannot close client device\n");
+		return;
+	}
+	cdev->client->ops->close(&cdev->lan_info, cdev->client, false);
+	clear_bit(__I40E_CLIENT_INSTANCE_OPENED, &cdev->state);
+	i40e_client_release_qvlist(&cdev->lan_info);
+	pf->cinst->client = NULL;
+	clear_bit(__I40E_SERVICE_SCHED, pf->state);
 }
-EXPORT_SYMBOL(i40e_unregister_client);
+
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_client.h linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_client.h
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_client.h	2024-05-10 01:26:45.325079481 -0400
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_client.h	2024-05-13 03:58:25.368491972 -0400
@@ -1,11 +1,17 @@
 /* SPDX-License-Identifier: GPL-2.0 */
-/* Copyright(c) 2013 - 2018 Intel Corporation. */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
 
 #ifndef _I40E_CLIENT_H_
 #define _I40E_CLIENT_H_
 
+#include <linux/platform_device.h>
+#if IS_ENABLED(CONFIG_MFD_CORE)
+#include <linux/mfd/core.h>
+
+#define I40E_PEER_RDMA_NAME    "i40e_rdma"
+#define I40E_PEER_RDMA_ID      PLATFORM_DEVID_AUTO
+#endif /* CONFIG_MFD_CORE */
 #define I40E_CLIENT_STR_LENGTH 10
-
 /* Client interface version should be updated anytime there is a change in the
  * existing APIs or data structures.
  */
@@ -105,6 +111,10 @@
 	u16 fw_maj_ver;                 /* firmware major version */
 	u16 fw_min_ver;                 /* firmware minor version */
 	u32 fw_build;                   /* firmware build number */
+#if IS_ENABLED(CONFIG_MFD_CORE)
+	struct platform_device *platform_dev;
+#endif
+	struct i40e_client *client;
 };
 
 #define I40E_CLIENT_RESET_LEVEL_PF   1
@@ -132,6 +142,10 @@
 			       struct i40e_client *client,
 			       bool is_vf, u32 vf_id,
 			       u32 flag, u32 valid_flag);
+
+	int (*client_device_register)(struct i40e_info *ldev);
+
+	void (*client_device_unregister)(struct i40e_info *ldev);
 };
 
 struct i40e_client_ops {
@@ -188,7 +202,8 @@
 #define I40E_TX_FLAGS_NOTIFY_OTHER_EVENTS	BIT(2)
 	u8 type;
 #define I40E_CLIENT_IWARP 0
-	const struct i40e_client_ops *ops; /* client ops provided by the client */
+	/* client ops provided by the client */
+	const struct i40e_client_ops *ops;
 };
 
 static inline bool i40e_client_is_registered(struct i40e_client *client)
@@ -196,8 +211,15 @@
 	return test_bit(__I40E_CLIENT_REGISTERED, &client->state);
 }
 
-/* used by clients */
-int i40e_register_client(struct i40e_client *client);
-int i40e_unregister_client(struct i40e_client *client);
+#if IS_ENABLED(CONFIG_MFD_CORE)
+#define ASSIGN_PEER_INFO                                               \
+{                                                                      \
+	{ .name = I40E_PEER_RDMA_NAME, .id = I40E_PEER_RDMA_ID },       \
+}
 
+struct i40e_peer_dev_platform_data {
+	struct i40e_info *ldev;
+};
+#endif /* CONFIG_MFD_CORE */
 #endif /* _I40E_CLIENT_H_ */
+
Binary files linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_client.o and linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_client.o differ
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_common.c linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_common.c
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_common.c	2024-05-10 01:26:45.325079481 -0400
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_common.c	2024-05-13 03:58:25.372491940 -0400
@@ -1,11 +1,10 @@
 // SPDX-License-Identifier: GPL-2.0
-/* Copyright(c) 2013 - 2018 Intel Corporation. */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
 
-#include "i40e.h"
 #include "i40e_type.h"
 #include "i40e_adminq.h"
 #include "i40e_prototype.h"
-#include <linux/avf/virtchnl.h>
+#include "virtchnl.h"
 
 /**
  * i40e_set_mac_type - Sets MAC type
@@ -16,7 +15,7 @@
  **/
 i40e_status i40e_set_mac_type(struct i40e_hw *hw)
 {
-	i40e_status status = 0;
+	i40e_status status = I40E_SUCCESS;
 
 	if (hw->vendor_id == PCI_VENDOR_ID_INTEL) {
 		switch (hw->device_id) {
@@ -29,8 +28,10 @@
 		case I40E_DEV_ID_QSFP_C:
 		case I40E_DEV_ID_10G_BASE_T:
 		case I40E_DEV_ID_10G_BASE_T4:
+		case I40E_DEV_ID_10G_BASE_T_BC:
 		case I40E_DEV_ID_10G_B:
 		case I40E_DEV_ID_10G_SFP:
+		case I40E_DEV_ID_5G_BASE_T_BC:
 		case I40E_DEV_ID_20G_KR2:
 		case I40E_DEV_ID_20G_KR2_A:
 		case I40E_DEV_ID_25G_B:
@@ -128,7 +129,7 @@
 const char *i40e_stat_str(struct i40e_hw *hw, i40e_status stat_err)
 {
 	switch (stat_err) {
-	case 0:
+	case I40E_SUCCESS:
 		return "OK";
 	case I40E_ERR_NVM:
 		return "I40E_ERR_NVM";
@@ -283,40 +284,39 @@
 {
 	struct i40e_aq_desc *aq_desc = (struct i40e_aq_desc *)desc;
 	u32 effective_mask = hw->debug_mask & mask;
-	char prefix[27];
-	u16 len;
 	u8 *buf = (u8 *)buffer;
+	u16 len;
+	char prefix[27];
 
 	if (!effective_mask || !desc)
 		return;
 
-	len = le16_to_cpu(aq_desc->datalen);
+	len = LE16_TO_CPU(aq_desc->datalen);
 
 	i40e_debug(hw, mask & I40E_DEBUG_AQ_DESCRIPTOR,
 		   "AQ CMD: opcode 0x%04X, flags 0x%04X, datalen 0x%04X, retval 0x%04X\n",
-		   le16_to_cpu(aq_desc->opcode),
-		   le16_to_cpu(aq_desc->flags),
-		   le16_to_cpu(aq_desc->datalen),
-		   le16_to_cpu(aq_desc->retval));
+		   LE16_TO_CPU(aq_desc->opcode),
+		   LE16_TO_CPU(aq_desc->flags),
+		   LE16_TO_CPU(aq_desc->datalen),
+		   LE16_TO_CPU(aq_desc->retval));
 	i40e_debug(hw, mask & I40E_DEBUG_AQ_DESCRIPTOR,
 		   "\tcookie (h,l) 0x%08X 0x%08X\n",
-		   le32_to_cpu(aq_desc->cookie_high),
-		   le32_to_cpu(aq_desc->cookie_low));
+		   LE32_TO_CPU(aq_desc->cookie_high),
+		   LE32_TO_CPU(aq_desc->cookie_low));
 	i40e_debug(hw, mask & I40E_DEBUG_AQ_DESCRIPTOR,
 		   "\tparam (0,1)  0x%08X 0x%08X\n",
-		   le32_to_cpu(aq_desc->params.internal.param0),
-		   le32_to_cpu(aq_desc->params.internal.param1));
+		   LE32_TO_CPU(aq_desc->params.internal.param0),
+		   LE32_TO_CPU(aq_desc->params.internal.param1));
 	i40e_debug(hw, mask & I40E_DEBUG_AQ_DESCRIPTOR,
 		   "\taddr (h,l)   0x%08X 0x%08X\n",
-		   le32_to_cpu(aq_desc->params.external.addr_high),
-		   le32_to_cpu(aq_desc->params.external.addr_low));
+		   LE32_TO_CPU(aq_desc->params.external.addr_high),
+		   LE32_TO_CPU(aq_desc->params.external.addr_low));
 
-	if (buffer && buf_len != 0 && len != 0 &&
+	if (buffer && (buf_len != 0) && (len != 0) &&
 	    (effective_mask & I40E_DEBUG_AQ_DESC_BUFFER)) {
 		i40e_debug(hw, mask, "AQ CMD Buffer:\n");
 		if (buf_len < len)
 			len = buf_len;
-
 		snprintf(prefix, sizeof(prefix),
 			 "i40e %02x:%02x.%x: \t0x",
 			 hw->bus.bus_id,
@@ -338,9 +338,8 @@
 {
 	if (hw->aq.asq.len)
 		return !!(rd32(hw, hw->aq.asq.len) &
-			  I40E_PF_ATQLEN_ATQENABLE_MASK);
-	else
-		return false;
+			I40E_PF_ATQLEN_ATQENABLE_MASK);
+	return false;
 }
 
 /**
@@ -363,7 +362,7 @@
 					  i40e_aqc_opc_queue_shutdown);
 
 	if (unloading)
-		cmd->driver_unloading = cpu_to_le32(I40E_AQ_DRIVER_UNLOADING);
+		cmd->driver_unloading = CPU_TO_LE32(I40E_AQ_DRIVER_UNLOADING);
 	status = i40e_asq_send_command(hw, &desc, NULL, 0, NULL);
 
 	return status;
@@ -381,9 +380,9 @@
  * Internal function to get or set RSS look up table
  **/
 static i40e_status i40e_aq_get_set_rss_lut(struct i40e_hw *hw,
-					   u16 vsi_id, bool pf_lut,
-					   u8 *lut, u16 lut_size,
-					   bool set)
+						     u16 vsi_id, bool pf_lut,
+						     u8 *lut, u16 lut_size,
+						     bool set)
 {
 	i40e_status status;
 	struct i40e_aq_desc desc;
@@ -398,22 +397,22 @@
 						  i40e_aqc_opc_get_rss_lut);
 
 	/* Indirect command */
-	desc.flags |= cpu_to_le16((u16)I40E_AQ_FLAG_BUF);
-	desc.flags |= cpu_to_le16((u16)I40E_AQ_FLAG_RD);
+	desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_BUF);
+	desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_RD);
 
 	cmd_resp->vsi_id =
-			cpu_to_le16((u16)((vsi_id <<
+			CPU_TO_LE16((u16)((vsi_id <<
 					  I40E_AQC_SET_RSS_LUT_VSI_ID_SHIFT) &
 					  I40E_AQC_SET_RSS_LUT_VSI_ID_MASK));
-	cmd_resp->vsi_id |= cpu_to_le16((u16)I40E_AQC_SET_RSS_LUT_VSI_VALID);
+	cmd_resp->vsi_id |= CPU_TO_LE16((u16)I40E_AQC_SET_RSS_LUT_VSI_VALID);
 
 	if (pf_lut)
-		cmd_resp->flags |= cpu_to_le16((u16)
+		cmd_resp->flags |= CPU_TO_LE16((u16)
 					((I40E_AQC_SET_RSS_LUT_TABLE_TYPE_PF <<
 					I40E_AQC_SET_RSS_LUT_TABLE_TYPE_SHIFT) &
 					I40E_AQC_SET_RSS_LUT_TABLE_TYPE_MASK));
 	else
-		cmd_resp->flags |= cpu_to_le16((u16)
+		cmd_resp->flags |= CPU_TO_LE16((u16)
 					((I40E_AQC_SET_RSS_LUT_TABLE_TYPE_VSI <<
 					I40E_AQC_SET_RSS_LUT_TABLE_TYPE_SHIFT) &
 					I40E_AQC_SET_RSS_LUT_TABLE_TYPE_MASK));
@@ -434,7 +433,7 @@
  * get the RSS lookup table, PF or VSI type
  **/
 i40e_status i40e_aq_get_rss_lut(struct i40e_hw *hw, u16 vsi_id,
-				bool pf_lut, u8 *lut, u16 lut_size)
+					  bool pf_lut, u8 *lut, u16 lut_size)
 {
 	return i40e_aq_get_set_rss_lut(hw, vsi_id, pf_lut, lut, lut_size,
 				       false);
@@ -451,7 +450,7 @@
  * set the RSS lookup table, PF or VSI type
  **/
 i40e_status i40e_aq_set_rss_lut(struct i40e_hw *hw, u16 vsi_id,
-				bool pf_lut, u8 *lut, u16 lut_size)
+					  bool pf_lut, u8 *lut, u16 lut_size)
 {
 	return i40e_aq_get_set_rss_lut(hw, vsi_id, pf_lut, lut, lut_size, true);
 }
@@ -484,14 +483,14 @@
 						  i40e_aqc_opc_get_rss_key);
 
 	/* Indirect command */
-	desc.flags |= cpu_to_le16((u16)I40E_AQ_FLAG_BUF);
-	desc.flags |= cpu_to_le16((u16)I40E_AQ_FLAG_RD);
+	desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_BUF);
+	desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_RD);
 
 	cmd_resp->vsi_id =
-			cpu_to_le16((u16)((vsi_id <<
+			CPU_TO_LE16((u16)((vsi_id <<
 					  I40E_AQC_SET_RSS_KEY_VSI_ID_SHIFT) &
 					  I40E_AQC_SET_RSS_KEY_VSI_ID_MASK));
-	cmd_resp->vsi_id |= cpu_to_le16((u16)I40E_AQC_SET_RSS_KEY_VSI_VALID);
+	cmd_resp->vsi_id |= CPU_TO_LE16((u16)I40E_AQC_SET_RSS_KEY_VSI_VALID);
 
 	status = i40e_asq_send_command(hw, &desc, key, key_size, NULL);
 
@@ -506,8 +505,8 @@
  *
  **/
 i40e_status i40e_aq_get_rss_key(struct i40e_hw *hw,
-				u16 vsi_id,
-				struct i40e_aqc_get_set_rss_key_data *key)
+				      u16 vsi_id,
+				      struct i40e_aqc_get_set_rss_key_data *key)
 {
 	return i40e_aq_get_set_rss_key(hw, vsi_id, key, false);
 }
@@ -521,8 +520,8 @@
  * set the RSS key per VSI
  **/
 i40e_status i40e_aq_set_rss_key(struct i40e_hw *hw,
-				u16 vsi_id,
-				struct i40e_aqc_get_set_rss_key_data *key)
+				      u16 vsi_id,
+				      struct i40e_aqc_get_set_rss_key_data *key)
 {
 	return i40e_aq_get_set_rss_key(hw, vsi_id, key, true);
 }
@@ -906,7 +905,7 @@
  **/
 i40e_status i40e_init_shared_code(struct i40e_hw *hw)
 {
-	i40e_status status = 0;
+	i40e_status status = I40E_SUCCESS;
 	u32 port, ari, func_rid;
 
 	i40e_set_mac_type(hw);
@@ -933,9 +932,16 @@
 	else
 		hw->pf_id = (u8)(func_rid & 0x7);
 
-	if (hw->mac.type == I40E_MAC_X722)
-		hw->flags |= I40E_HW_FLAG_AQ_SRCTL_ACCESS_ENABLE |
-			     I40E_HW_FLAG_NVM_READ_REQUIRES_LOCK;
+	/* NVMUpdate features structure initialization */
+	hw->nvmupd_features.major = I40E_NVMUPD_FEATURES_API_VER_MAJOR;
+	hw->nvmupd_features.minor = I40E_NVMUPD_FEATURES_API_VER_MINOR;
+	hw->nvmupd_features.size = sizeof(hw->nvmupd_features);
+	i40e_memset(hw->nvmupd_features.features, 0x0,
+		    I40E_NVMUPD_FEATURES_API_FEATURES_ARRAY_LEN *
+		    sizeof(*hw->nvmupd_features.features),
+		    I40E_NONDMA_MEM);
+
+	hw->nvmupd_features.features[0] = I40E_NVMUPD_FEATURE_FLAT_NVM_SUPPORT;
 
 	status = i40e_init_nvm(hw);
 	return status;
@@ -959,11 +965,11 @@
 	i40e_status status;
 
 	i40e_fill_default_direct_cmd_desc(&desc, i40e_aqc_opc_mac_address_read);
-	desc.flags |= cpu_to_le16(I40E_AQ_FLAG_BUF);
+	desc.flags |= CPU_TO_LE16(I40E_AQ_FLAG_BUF);
 
 	status = i40e_asq_send_command(hw, &desc, addrs,
 				       sizeof(*addrs), cmd_details);
-	*flags = le16_to_cpu(cmd_data->command_flags);
+	*flags = LE16_TO_CPU(cmd_data->command_flags);
 
 	return status;
 }
@@ -986,9 +992,9 @@
 
 	i40e_fill_default_direct_cmd_desc(&desc,
 					  i40e_aqc_opc_mac_address_write);
-	cmd_data->command_flags = cpu_to_le16(flags);
-	cmd_data->mac_sah = cpu_to_le16((u16)mac_addr[0] << 8 | mac_addr[1]);
-	cmd_data->mac_sal = cpu_to_le32(((u32)mac_addr[2] << 24) |
+	cmd_data->command_flags = CPU_TO_LE16(flags);
+	cmd_data->mac_sah = CPU_TO_LE16((u16)mac_addr[0] << 8 | mac_addr[1]);
+	cmd_data->mac_sal = CPU_TO_LE32(((u32)mac_addr[2] << 24) |
 					((u32)mac_addr[3] << 16) |
 					((u32)mac_addr[4] << 8) |
 					mac_addr[5]);
@@ -1047,7 +1053,7 @@
 /**
  * i40e_pre_tx_queue_cfg - pre tx queue configure
  * @hw: pointer to the HW structure
- * @queue: target PF queue index
+ * @queue: target pf queue index
  * @enable: state change request
  *
  * Handles hw requirement to indicate intention to enable
@@ -1085,28 +1091,28 @@
  *  Reads the part number string from the EEPROM.
  **/
 i40e_status i40e_read_pba_string(struct i40e_hw *hw, u8 *pba_num,
-				 u32 pba_num_size)
+					    u32 pba_num_size)
 {
-	i40e_status status = 0;
+	i40e_status status = I40E_SUCCESS;
 	u16 pba_word = 0;
 	u16 pba_size = 0;
 	u16 pba_ptr = 0;
 	u16 i = 0;
 
 	status = i40e_read_nvm_word(hw, I40E_SR_PBA_FLAGS, &pba_word);
-	if (status || (pba_word != 0xFAFA)) {
+	if ((status != I40E_SUCCESS) || (pba_word != 0xFAFA)) {
 		hw_dbg(hw, "Failed to read PBA flags or flag is invalid.\n");
 		return status;
 	}
 
 	status = i40e_read_nvm_word(hw, I40E_SR_PBA_BLOCK_PTR, &pba_ptr);
-	if (status) {
+	if (status != I40E_SUCCESS) {
 		hw_dbg(hw, "Failed to read PBA Block pointer.\n");
 		return status;
 	}
 
 	status = i40e_read_nvm_word(hw, pba_ptr, &pba_size);
-	if (status) {
+	if (status != I40E_SUCCESS) {
 		hw_dbg(hw, "Failed to read PBA Block size.\n");
 		return status;
 	}
@@ -1122,7 +1128,7 @@
 
 	for (i = 0; i < pba_size; i++) {
 		status = i40e_read_nvm_word(hw, (pba_ptr + 1) + i, &pba_word);
-		if (status) {
+		if (status != I40E_SUCCESS) {
 			hw_dbg(hw, "Failed to read PBA Block word %d.\n", i);
 			return status;
 		}
@@ -1152,12 +1158,15 @@
 	case I40E_PHY_TYPE_40GBASE_LR4:
 	case I40E_PHY_TYPE_25GBASE_LR:
 	case I40E_PHY_TYPE_25GBASE_SR:
+	case I40E_PHY_TYPE_10GBASE_AOC:
+	case I40E_PHY_TYPE_25GBASE_AOC:
+	case I40E_PHY_TYPE_40GBASE_AOC:
 		media = I40E_MEDIA_TYPE_FIBER;
 		break;
 	case I40E_PHY_TYPE_100BASE_TX:
 	case I40E_PHY_TYPE_1000BASE_T:
-	case I40E_PHY_TYPE_2_5GBASE_T:
-	case I40E_PHY_TYPE_5GBASE_T:
+	case I40E_PHY_TYPE_2_5GBASE_T_LINK_STATUS:
+	case I40E_PHY_TYPE_5GBASE_T_LINK_STATUS:
 	case I40E_PHY_TYPE_10GBASE_T:
 		media = I40E_MEDIA_TYPE_BASET;
 		break;
@@ -1166,10 +1175,7 @@
 	case I40E_PHY_TYPE_10GBASE_CR1:
 	case I40E_PHY_TYPE_40GBASE_CR4:
 	case I40E_PHY_TYPE_10GBASE_SFPP_CU:
-	case I40E_PHY_TYPE_40GBASE_AOC:
-	case I40E_PHY_TYPE_10GBASE_AOC:
 	case I40E_PHY_TYPE_25GBASE_CR:
-	case I40E_PHY_TYPE_25GBASE_AOC:
 	case I40E_PHY_TYPE_25GBASE_ACC:
 		media = I40E_MEDIA_TYPE_DA;
 		break;
@@ -1200,14 +1206,14 @@
  * @retry_limit: how many times to retry before failure
  **/
 static i40e_status i40e_poll_globr(struct i40e_hw *hw,
-				   u32 retry_limit)
+					     u32 retry_limit)
 {
 	u32 cnt, reg = 0;
 
 	for (cnt = 0; cnt < retry_limit; cnt++) {
 		reg = rd32(hw, I40E_GLGEN_RSTAT);
 		if (!(reg & I40E_GLGEN_RSTAT_DEVSTATE_MASK))
-			return 0;
+			return I40E_SUCCESS;
 		msleep(100);
 	}
 
@@ -1217,8 +1223,7 @@
 	return I40E_ERR_RESET_FAILED;
 }
 
-#define I40E_PF_RESET_WAIT_COUNT_A0	200
-#define I40E_PF_RESET_WAIT_COUNT	200
+#define I40E_PF_RESET_WAIT_COUNT	1000
 /**
  * i40e_pf_reset - Reset the PF
  * @hw: pointer to the hardware structure
@@ -1238,13 +1243,10 @@
 	 * couple counts longer to be sure we don't just miss the end.
 	 */
 	grst_del = (rd32(hw, I40E_GLGEN_RSTCTL) &
-		    I40E_GLGEN_RSTCTL_GRSTDEL_MASK) >>
-		    I40E_GLGEN_RSTCTL_GRSTDEL_SHIFT;
+			I40E_GLGEN_RSTCTL_GRSTDEL_MASK) >>
+			I40E_GLGEN_RSTCTL_GRSTDEL_SHIFT;
 
-	/* It can take upto 15 secs for GRST steady state.
-	 * Bump it to 16 secs max to be safe.
-	 */
-	grst_del = grst_del * 20;
+	grst_del = min(grst_del * 20, 160U);
 
 	for (cnt = 0; cnt < grst_del; cnt++) {
 		reg = rd32(hw, I40E_GLGEN_RSTAT);
@@ -1281,14 +1283,11 @@
 	 */
 	if (!cnt) {
 		u32 reg2 = 0;
-		if (hw->revision_id == 0)
-			cnt = I40E_PF_RESET_WAIT_COUNT_A0;
-		else
-			cnt = I40E_PF_RESET_WAIT_COUNT;
+
 		reg = rd32(hw, I40E_PFGEN_CTRL);
 		wr32(hw, I40E_PFGEN_CTRL,
 		     (reg | I40E_PFGEN_CTRL_PFSWR_MASK));
-		for (; cnt; cnt--) {
+		for (cnt = 0; cnt < I40E_PF_RESET_WAIT_COUNT; cnt++) {
 			reg = rd32(hw, I40E_PFGEN_CTRL);
 			if (!(reg & I40E_PFGEN_CTRL_PFSWR_MASK))
 				break;
@@ -1298,7 +1297,7 @@
 			usleep_range(1000, 2000);
 		}
 		if (reg2 & I40E_GLGEN_RSTAT_DEVSTATE_MASK) {
-			if (i40e_poll_globr(hw, grst_del))
+			if (i40e_poll_globr(hw, grst_del) != I40E_SUCCESS)
 				return I40E_ERR_RESET_FAILED;
 		} else if (reg & I40E_PFGEN_CTRL_PFSWR_MASK) {
 			hw_dbg(hw, "PF reset polling failed to complete.\n");
@@ -1308,7 +1307,7 @@
 
 	i40e_clear_pxe_mode(hw);
 
-	return 0;
+	return I40E_SUCCESS;
 }
 
 /**
@@ -1329,18 +1328,18 @@
 	u32 val;
 	u32 eol = 0x7ff;
 
-	/* get number of interrupts, queues, and VFs */
+	/* get number of interrupts, queues, and vfs */
 	val = rd32(hw, I40E_GLPCI_CNF2);
 	num_pf_int = (val & I40E_GLPCI_CNF2_MSI_X_PF_N_MASK) >>
-		     I40E_GLPCI_CNF2_MSI_X_PF_N_SHIFT;
+			I40E_GLPCI_CNF2_MSI_X_PF_N_SHIFT;
 	num_vf_int = (val & I40E_GLPCI_CNF2_MSI_X_VF_N_MASK) >>
-		     I40E_GLPCI_CNF2_MSI_X_VF_N_SHIFT;
+			I40E_GLPCI_CNF2_MSI_X_VF_N_SHIFT;
 
 	val = rd32(hw, I40E_PFLAN_QALLOC);
 	base_queue = (val & I40E_PFLAN_QALLOC_FIRSTQ_MASK) >>
-		     I40E_PFLAN_QALLOC_FIRSTQ_SHIFT;
+			I40E_PFLAN_QALLOC_FIRSTQ_SHIFT;
 	j = (val & I40E_PFLAN_QALLOC_LASTQ_MASK) >>
-	    I40E_PFLAN_QALLOC_LASTQ_SHIFT;
+			I40E_PFLAN_QALLOC_LASTQ_SHIFT;
 	if (val & I40E_PFLAN_QALLOC_VALID_MASK)
 		num_queues = (j - base_queue) + 1;
 	else
@@ -1348,9 +1347,9 @@
 
 	val = rd32(hw, I40E_PF_VT_PFALLOC);
 	i = (val & I40E_PF_VT_PFALLOC_FIRSTVF_MASK) >>
-	    I40E_PF_VT_PFALLOC_FIRSTVF_SHIFT;
+			I40E_PF_VT_PFALLOC_FIRSTVF_SHIFT;
 	j = (val & I40E_PF_VT_PFALLOC_LASTVF_MASK) >>
-	    I40E_PF_VT_PFALLOC_LASTVF_SHIFT;
+			I40E_PF_VT_PFALLOC_LASTVF_SHIFT;
 	if (val & I40E_PF_VT_PFALLOC_VALID_MASK)
 		num_vfs = (j - i) + 1;
 	else
@@ -1413,20 +1412,8 @@
  **/
 void i40e_clear_pxe_mode(struct i40e_hw *hw)
 {
-	u32 reg;
-
 	if (i40e_check_asq_alive(hw))
 		i40e_aq_clear_pxe_mode(hw, NULL);
-
-	/* Clear single descriptor fetch/write-back mode */
-	reg = rd32(hw, I40E_GLLAN_RCTL_0);
-
-	if (hw->revision_id == 0) {
-		/* As a work around clear PXE_MODE instead of setting it */
-		wr32(hw, I40E_GLLAN_RCTL_0, (reg & (~I40E_GLLAN_RCTL_0_PXE_MODE_MASK)));
-	} else {
-		wr32(hw, I40E_GLLAN_RCTL_0, (reg | I40E_GLLAN_RCTL_0_PXE_MODE_MASK));
-	}
 }
 
 /**
@@ -1441,9 +1428,9 @@
 	u32 gpio_val = 0;
 	u32 port;
 
-	if (!hw->func_caps.led[idx])
+	if (!I40E_IS_X710TL_DEVICE(hw->device_id) &&
+	    !hw->func_caps.led[idx])
 		return 0;
-
 	gpio_val = rd32(hw, I40E_GLGEN_GPIO_CTL(idx));
 	port = (gpio_val & I40E_GLGEN_GPIO_CTL_PRT_NUM_MASK) >>
 		I40E_GLGEN_GPIO_CTL_PRT_NUM_SHIFT;
@@ -1462,8 +1449,15 @@
 #define I40E_FILTER_ACTIVITY 0xE
 #define I40E_LINK_ACTIVITY 0xC
 #define I40E_MAC_ACTIVITY 0xD
+#define I40E_FW_LED BIT(4)
+#define I40E_LED_MODE_VALID (I40E_GLGEN_GPIO_CTL_LED_MODE_MASK >> \
+			     I40E_GLGEN_GPIO_CTL_LED_MODE_SHIFT)
+
 #define I40E_LED0 22
 
+#define I40E_PIN_FUNC_SDP 0x0
+#define I40E_PIN_FUNC_LED 0x1
+
 /**
  * i40e_led_get - return current on/off mode
  * @hw: pointer to the hw struct
@@ -1508,8 +1502,10 @@
 {
 	int i;
 
-	if (mode & 0xfffffff0)
+	if (mode & ~I40E_LED_MODE_VALID) {
 		hw_dbg(hw, "invalid mode passed in %X\n", mode);
+		return;
+	}
 
 	/* as per the documentation GPIO 22-29 are the LED
 	 * GPIO pins named LED0..LED7
@@ -1519,6 +1515,20 @@
 
 		if (!gpio_val)
 			continue;
+
+		if (I40E_IS_X710TL_DEVICE(hw->device_id)) {
+			u32 pin_func = 0;
+
+			if (mode & I40E_FW_LED)
+				pin_func = I40E_PIN_FUNC_SDP;
+			else
+				pin_func = I40E_PIN_FUNC_LED;
+
+			gpio_val &= ~I40E_GLGEN_GPIO_CTL_PIN_FUNC_MASK;
+			gpio_val |= ((pin_func <<
+				     I40E_GLGEN_GPIO_CTL_PIN_FUNC_SHIFT) &
+				     I40E_GLGEN_GPIO_CTL_PIN_FUNC_MASK);
+		}
 		gpio_val &= ~I40E_GLGEN_GPIO_CTL_LED_MODE_MASK;
 		/* this & is a bit of paranoia, but serves as a range check */
 		gpio_val |= ((mode << I40E_GLGEN_GPIO_CTL_LED_MODE_SHIFT) &
@@ -1553,8 +1563,8 @@
 {
 	struct i40e_aq_desc desc;
 	i40e_status status;
-	u16 abilities_size = sizeof(struct i40e_aq_get_phy_abilities_resp);
 	u16 max_delay = I40E_MAX_PHY_TIMEOUT, total_delay = 0;
+	u16 abilities_size = sizeof(struct i40e_aq_get_phy_abilities_resp);
 
 	if (!abilities)
 		return I40E_ERR_PARAM;
@@ -1563,17 +1573,17 @@
 		i40e_fill_default_direct_cmd_desc(&desc,
 					       i40e_aqc_opc_get_phy_abilities);
 
-		desc.flags |= cpu_to_le16((u16)I40E_AQ_FLAG_BUF);
+		desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_BUF);
 		if (abilities_size > I40E_AQ_LARGE_BUF)
-			desc.flags |= cpu_to_le16((u16)I40E_AQ_FLAG_LB);
+			desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_LB);
 
 		if (qualified_modules)
 			desc.params.external.param0 |=
-			cpu_to_le32(I40E_AQ_PHY_REPORT_QUALIFIED_MODULES);
+			CPU_TO_LE32(I40E_AQ_PHY_REPORT_QUALIFIED_MODULES);
 
 		if (report_init)
 			desc.params.external.param0 |=
-			cpu_to_le32(I40E_AQ_PHY_REPORT_INITIAL_VALUES);
+			CPU_TO_LE32(I40E_AQ_PHY_REPORT_INITIAL_VALUES);
 
 		status = i40e_asq_send_command(hw, &desc, abilities,
 					       abilities_size, cmd_details);
@@ -1595,7 +1605,7 @@
 	} while ((hw->aq.asq_last_status == I40E_AQ_RC_EAGAIN) &&
 		(total_delay < max_delay));
 
-	if (status)
+	if (status != I40E_SUCCESS)
 		return status;
 
 	if (report_init) {
@@ -1604,7 +1614,7 @@
 		    hw->aq.api_min_ver >= I40E_MINOR_VER_GET_LINK_INFO_XL710) {
 			status = i40e_aq_get_link_info(hw, true, NULL, NULL);
 		} else {
-			hw->phy.phy_types = le32_to_cpu(abilities->phy_type);
+			hw->phy.phy_types = LE32_TO_CPU(abilities->phy_type);
 			hw->phy.phy_types |=
 					((u64)abilities->phy_type_ext << 32);
 		}
@@ -1625,14 +1635,14 @@
  * of the PHY Config parameters. This status will be indicated by the
  * command response.
  **/
-enum i40e_status_code i40e_aq_set_phy_config(struct i40e_hw *hw,
+i40e_status i40e_aq_set_phy_config(struct i40e_hw *hw,
 				struct i40e_aq_set_phy_config *config,
 				struct i40e_asq_cmd_details *cmd_details)
 {
 	struct i40e_aq_desc desc;
 	struct i40e_aq_set_phy_config *cmd =
-			(struct i40e_aq_set_phy_config *)&desc.params.raw;
-	enum i40e_status_code status;
+		(struct i40e_aq_set_phy_config *)&desc.params.raw;
+	i40e_status status;
 
 	if (!config)
 		return I40E_ERR_PARAM;
@@ -1647,15 +1657,25 @@
 	return status;
 }
 
-static noinline_for_stack enum i40e_status_code
-i40e_set_fc_status(struct i40e_hw *hw,
-		   struct i40e_aq_get_phy_abilities_resp *abilities,
-		   bool atomic_restart)
+/**
+ * i40e_set_fc
+ * @hw: pointer to the hw struct
+ * @aq_failures: buffer to return AdminQ failure information
+ * @atomic_restart: whether to enable atomic link restart
+ *
+ * Set the requested flow control mode using set_phy_config.
+ **/
+i40e_status i40e_set_fc(struct i40e_hw *hw, u8 *aq_failures,
+				  bool atomic_restart)
 {
-	struct i40e_aq_set_phy_config config;
 	enum i40e_fc_mode fc_mode = hw->fc.requested_mode;
+	struct i40e_aq_get_phy_abilities_resp abilities;
+	struct i40e_aq_set_phy_config config;
+	i40e_status status;
 	u8 pause_mask = 0x0;
 
+	*aq_failures = 0x0;
+
 	switch (fc_mode) {
 	case I40E_FC_FULL:
 		pause_mask |= I40E_AQ_PHY_FLAG_PAUSE_TX;
@@ -1671,48 +1691,6 @@
 		break;
 	}
 
-	memset(&config, 0, sizeof(struct i40e_aq_set_phy_config));
-	/* clear the old pause settings */
-	config.abilities = abilities->abilities & ~(I40E_AQ_PHY_FLAG_PAUSE_TX) &
-			   ~(I40E_AQ_PHY_FLAG_PAUSE_RX);
-	/* set the new abilities */
-	config.abilities |= pause_mask;
-	/* If the abilities have changed, then set the new config */
-	if (config.abilities == abilities->abilities)
-		return 0;
-
-	/* Auto restart link so settings take effect */
-	if (atomic_restart)
-		config.abilities |= I40E_AQ_PHY_ENABLE_ATOMIC_LINK;
-	/* Copy over all the old settings */
-	config.phy_type = abilities->phy_type;
-	config.phy_type_ext = abilities->phy_type_ext;
-	config.link_speed = abilities->link_speed;
-	config.eee_capability = abilities->eee_capability;
-	config.eeer = abilities->eeer_val;
-	config.low_power_ctrl = abilities->d3_lpan;
-	config.fec_config = abilities->fec_cfg_curr_mod_ext_info &
-			    I40E_AQ_PHY_FEC_CONFIG_MASK;
-
-	return i40e_aq_set_phy_config(hw, &config, NULL);
-}
-
-/**
- * i40e_set_fc
- * @hw: pointer to the hw struct
- * @aq_failures: buffer to return AdminQ failure information
- * @atomic_restart: whether to enable atomic link restart
- *
- * Set the requested flow control mode using set_phy_config.
- **/
-enum i40e_status_code i40e_set_fc(struct i40e_hw *hw, u8 *aq_failures,
-				  bool atomic_restart)
-{
-	struct i40e_aq_get_phy_abilities_resp abilities;
-	enum i40e_status_code status;
-
-	*aq_failures = 0x0;
-
 	/* Get the current phy config */
 	status = i40e_aq_get_phy_capabilities(hw, false, false, &abilities,
 					      NULL);
@@ -1721,10 +1699,31 @@
 		return status;
 	}
 
-	status = i40e_set_fc_status(hw, &abilities, atomic_restart);
-	if (status)
-		*aq_failures |= I40E_SET_FC_AQ_FAIL_SET;
+	memset(&config, 0, sizeof(config));
+	/* clear the old pause settings */
+	config.abilities = abilities.abilities & ~(I40E_AQ_PHY_FLAG_PAUSE_TX) &
+			   ~(I40E_AQ_PHY_FLAG_PAUSE_RX);
+	/* set the new abilities */
+	config.abilities |= pause_mask;
+	/* If the abilities have changed, then set the new config */
+	if (config.abilities != abilities.abilities) {
+		/* Auto restart link so settings take effect */
+		if (atomic_restart)
+			config.abilities |= I40E_AQ_PHY_ENABLE_ATOMIC_LINK;
+		/* Copy over all the old settings */
+		config.phy_type = abilities.phy_type;
+		config.phy_type_ext = abilities.phy_type_ext;
+		config.link_speed = abilities.link_speed;
+		config.eee_capability = abilities.eee_capability;
+		config.eeer = abilities.eeer_val;
+		config.low_power_ctrl = abilities.d3_lpan;
+		config.fec_config = abilities.fec_cfg_curr_mod_ext_info &
+				    I40E_AQ_PHY_FEC_CONFIG_MASK;
+		status = i40e_aq_set_phy_config(hw, &config, NULL);
 
+		if (status)
+			*aq_failures |= I40E_SET_FC_AQ_FAIL_SET;
+	}
 	/* Update the link info */
 	status = i40e_update_link_info(hw);
 	if (status) {
@@ -1749,7 +1748,7 @@
  * Tell the firmware that the driver is taking over from PXE
  **/
 i40e_status i40e_aq_clear_pxe_mode(struct i40e_hw *hw,
-				struct i40e_asq_cmd_details *cmd_details)
+			struct i40e_asq_cmd_details *cmd_details)
 {
 	i40e_status status;
 	struct i40e_aq_desc desc;
@@ -1777,8 +1776,7 @@
  * Sets up the link and restarts the Auto-Negotiation over the link.
  **/
 i40e_status i40e_aq_set_link_restart_an(struct i40e_hw *hw,
-					bool enable_link,
-					struct i40e_asq_cmd_details *cmd_details)
+		bool enable_link, struct i40e_asq_cmd_details *cmd_details)
 {
 	struct i40e_aq_desc desc;
 	struct i40e_aqc_set_link_restart_an *cmd =
@@ -1826,15 +1824,16 @@
 		command_flags = I40E_AQ_LSE_ENABLE;
 	else
 		command_flags = I40E_AQ_LSE_DISABLE;
-	resp->command_flags = cpu_to_le16(command_flags);
+	resp->command_flags = CPU_TO_LE16(command_flags);
 
 	status = i40e_asq_send_command(hw, &desc, NULL, 0, cmd_details);
 
-	if (status)
+	if (status != I40E_SUCCESS)
 		goto aq_get_link_info_exit;
 
 	/* save off old link status information */
-	hw->phy.link_info_old = *hw_link_info;
+	i40e_memcpy(&hw->phy.link_info_old, hw_link_info,
+		    sizeof(*hw_link_info), I40E_NONDMA_TO_NONDMA);
 
 	/* update link status */
 	hw_link_info->phy_type = (enum i40e_aq_phy_type)resp->phy_type;
@@ -1846,7 +1845,7 @@
 						 I40E_AQ_CONFIG_FEC_RS_ENA);
 	hw_link_info->ext_info = resp->ext_info;
 	hw_link_info->loopback = resp->loopback & I40E_AQ_LOOPBACK_MASK;
-	hw_link_info->max_frame_size = le16_to_cpu(resp->max_frame_size);
+	hw_link_info->max_frame_size = LE16_TO_CPU(resp->max_frame_size);
 	hw_link_info->pacing = resp->config & I40E_AQ_CONFIG_PACING_MASK;
 
 	/* update fc info */
@@ -1866,7 +1865,7 @@
 	else
 		hw_link_info->crc_enable = false;
 
-	if (resp->command_flags & cpu_to_le16(I40E_AQ_LSE_IS_ENABLED))
+	if (resp->command_flags & CPU_TO_LE16(I40E_AQ_LSE_IS_ENABLED))
 		hw_link_info->lse_enable = true;
 	else
 		hw_link_info->lse_enable = false;
@@ -1876,18 +1875,23 @@
 	     hw->aq.fw_min_ver < 40)) && hw_link_info->phy_type == 0xE)
 		hw_link_info->phy_type = I40E_PHY_TYPE_10GBASE_SFPP_CU;
 
+	/* 'Get Link Status' response data structure from X722 FW has
+	 * different format and does not contain this information
+	 */
 	if (hw->flags & I40E_HW_FLAG_AQ_PHY_ACCESS_CAPABLE &&
 	    hw->mac.type != I40E_MAC_X722) {
 		__le32 tmp;
 
-		memcpy(&tmp, resp->link_type, sizeof(tmp));
-		hw->phy.phy_types = le32_to_cpu(tmp);
+		i40e_memcpy(&tmp, resp->link_type, sizeof(tmp),
+			    I40E_NONDMA_TO_NONDMA);
+		hw->phy.phy_types = LE32_TO_CPU(tmp);
 		hw->phy.phy_types |= ((u64)resp->link_type_ext << 32);
 	}
 
 	/* save link status information */
 	if (link)
-		*link = *hw_link_info;
+		i40e_memcpy(link, hw_link_info, sizeof(*hw_link_info),
+			    I40E_NONDMA_TO_NONDMA);
 
 	/* flag cleared so helper functions don't call AQ again */
 	hw->phy.get_link_info = false;
@@ -1905,8 +1909,8 @@
  * Set link interrupt mask.
  **/
 i40e_status i40e_aq_set_phy_int_mask(struct i40e_hw *hw,
-				     u16 mask,
-				     struct i40e_asq_cmd_details *cmd_details)
+				u16 mask,
+				struct i40e_asq_cmd_details *cmd_details)
 {
 	struct i40e_aq_desc desc;
 	struct i40e_aqc_set_phy_int_mask *cmd =
@@ -1916,7 +1920,7 @@
 	i40e_fill_default_direct_cmd_desc(&desc,
 					  i40e_aqc_opc_set_phy_int_mask);
 
-	cmd->event_mask = cpu_to_le16(mask);
+	cmd->event_mask = CPU_TO_LE16(mask);
 
 	status = i40e_asq_send_command(hw, &desc, NULL, 0, cmd_details);
 
@@ -1932,7 +1936,7 @@
  * Reset the external PHY.
  **/
 i40e_status i40e_aq_set_phy_debug(struct i40e_hw *hw, u8 cmd_flags,
-				  struct i40e_asq_cmd_details *cmd_details)
+				struct i40e_asq_cmd_details *cmd_details)
 {
 	struct i40e_aq_desc desc;
 	struct i40e_aqc_set_phy_debug *cmd =
@@ -1950,18 +1954,19 @@
 }
 
 /**
- * i40e_is_aq_api_ver_ge
- * @aq: pointer to AdminQ info containing HW API version to compare
- * @maj: API major value
- * @min: API minor value
+ * i40e_hw_ver_ge
+ * @hw: pointer to the hw struct
+ * @maj: api major value
+ * @min: api minor value
  *
- * Assert whether current HW API version is greater/equal than provided.
+ * Assert whether current HW api version is greater/equal than provided.
  **/
-static bool i40e_is_aq_api_ver_ge(struct i40e_adminq_info *aq, u16 maj,
-				  u16 min)
+static bool i40e_hw_ver_ge(struct i40e_hw *hw, u16 maj, u16 min)
 {
-	return (aq->api_maj_ver > maj ||
-		(aq->api_maj_ver == maj && aq->api_min_ver >= min));
+	if (hw->aq.api_maj_ver > maj ||
+	    (hw->aq.api_maj_ver == maj && hw->aq.api_min_ver >= min))
+		return true;
+	return false;
 }
 
 /**
@@ -1987,23 +1992,24 @@
 	i40e_fill_default_direct_cmd_desc(&desc,
 					  i40e_aqc_opc_add_vsi);
 
-	cmd->uplink_seid = cpu_to_le16(vsi_ctx->uplink_seid);
+	cmd->uplink_seid = CPU_TO_LE16(vsi_ctx->uplink_seid);
 	cmd->connection_type = vsi_ctx->connection_type;
 	cmd->vf_id = vsi_ctx->vf_num;
-	cmd->vsi_flags = cpu_to_le16(vsi_ctx->flags);
+	cmd->vsi_flags = CPU_TO_LE16(vsi_ctx->flags);
 
-	desc.flags |= cpu_to_le16((u16)(I40E_AQ_FLAG_BUF | I40E_AQ_FLAG_RD));
+	desc.flags |= CPU_TO_LE16((u16)(I40E_AQ_FLAG_BUF | I40E_AQ_FLAG_RD));
 
-	status = i40e_asq_send_command(hw, &desc, &vsi_ctx->info,
-				    sizeof(vsi_ctx->info), cmd_details);
+	status = i40e_asq_send_command_atomic(hw, &desc, &vsi_ctx->info,
+					      sizeof(vsi_ctx->info),
+					      cmd_details, true);
 
-	if (status)
+	if (status != I40E_SUCCESS)
 		goto aq_add_vsi_exit;
 
-	vsi_ctx->seid = le16_to_cpu(resp->seid);
-	vsi_ctx->vsi_number = le16_to_cpu(resp->vsi_number);
-	vsi_ctx->vsis_allocated = le16_to_cpu(resp->vsi_used);
-	vsi_ctx->vsis_unallocated = le16_to_cpu(resp->vsi_free);
+	vsi_ctx->seid = LE16_TO_CPU(resp->seid);
+	vsi_ctx->vsi_number = LE16_TO_CPU(resp->vsi_number);
+	vsi_ctx->vsis_allocated = LE16_TO_CPU(resp->vsi_used);
+	vsi_ctx->vsis_unallocated = LE16_TO_CPU(resp->vsi_free);
 
 aq_add_vsi_exit:
 	return status;
@@ -2016,8 +2022,8 @@
  * @cmd_details: pointer to command details structure or NULL
  **/
 i40e_status i40e_aq_set_default_vsi(struct i40e_hw *hw,
-				    u16 seid,
-				    struct i40e_asq_cmd_details *cmd_details)
+				u16 seid,
+				struct i40e_asq_cmd_details *cmd_details)
 {
 	struct i40e_aq_desc desc;
 	struct i40e_aqc_set_vsi_promiscuous_modes *cmd =
@@ -2026,11 +2032,11 @@
 	i40e_status status;
 
 	i40e_fill_default_direct_cmd_desc(&desc,
-					  i40e_aqc_opc_set_vsi_promiscuous_modes);
+					i40e_aqc_opc_set_vsi_promiscuous_modes);
 
-	cmd->promiscuous_flags = cpu_to_le16(I40E_AQC_SET_VSI_DEFAULT);
-	cmd->valid_flags = cpu_to_le16(I40E_AQC_SET_VSI_DEFAULT);
-	cmd->seid = cpu_to_le16(seid);
+	cmd->promiscuous_flags = CPU_TO_LE16(I40E_AQC_SET_VSI_DEFAULT);
+	cmd->valid_flags = CPU_TO_LE16(I40E_AQC_SET_VSI_DEFAULT);
+	cmd->seid = CPU_TO_LE16(seid);
 
 	status = i40e_asq_send_command(hw, &desc, NULL, 0, cmd_details);
 
@@ -2044,8 +2050,8 @@
  * @cmd_details: pointer to command details structure or NULL
  **/
 i40e_status i40e_aq_clear_default_vsi(struct i40e_hw *hw,
-				      u16 seid,
-				      struct i40e_asq_cmd_details *cmd_details)
+				u16 seid,
+				struct i40e_asq_cmd_details *cmd_details)
 {
 	struct i40e_aq_desc desc;
 	struct i40e_aqc_set_vsi_promiscuous_modes *cmd =
@@ -2054,11 +2060,11 @@
 	i40e_status status;
 
 	i40e_fill_default_direct_cmd_desc(&desc,
-					  i40e_aqc_opc_set_vsi_promiscuous_modes);
+					i40e_aqc_opc_set_vsi_promiscuous_modes);
 
-	cmd->promiscuous_flags = cpu_to_le16(0);
-	cmd->valid_flags = cpu_to_le16(I40E_AQC_SET_VSI_DEFAULT);
-	cmd->seid = cpu_to_le16(seid);
+	cmd->promiscuous_flags = CPU_TO_LE16(0);
+	cmd->valid_flags = CPU_TO_LE16(I40E_AQC_SET_VSI_DEFAULT);
+	cmd->seid = CPU_TO_LE16(seid);
 
 	status = i40e_asq_send_command(hw, &desc, NULL, 0, cmd_details);
 
@@ -2089,19 +2095,20 @@
 
 	if (set) {
 		flags |= I40E_AQC_SET_VSI_PROMISC_UNICAST;
-		if (rx_only_promisc && i40e_is_aq_api_ver_ge(&hw->aq, 1, 5))
+		if (rx_only_promisc && i40e_hw_ver_ge(hw, 1, 5))
 			flags |= I40E_AQC_SET_VSI_PROMISC_RX_ONLY;
 	}
 
-	cmd->promiscuous_flags = cpu_to_le16(flags);
+	cmd->promiscuous_flags = CPU_TO_LE16(flags);
 
-	cmd->valid_flags = cpu_to_le16(I40E_AQC_SET_VSI_PROMISC_UNICAST);
-	if (i40e_is_aq_api_ver_ge(&hw->aq, 1, 5))
+	cmd->valid_flags = CPU_TO_LE16(I40E_AQC_SET_VSI_PROMISC_UNICAST);
+	if (i40e_hw_ver_ge(hw, 1, 5))
 		cmd->valid_flags |=
-			cpu_to_le16(I40E_AQC_SET_VSI_PROMISC_RX_ONLY);
+			CPU_TO_LE16(I40E_AQC_SET_VSI_PROMISC_RX_ONLY);
 
-	cmd->seid = cpu_to_le16(seid);
-	status = i40e_asq_send_command(hw, &desc, NULL, 0, cmd_details);
+	cmd->seid = CPU_TO_LE16(seid);
+	status = i40e_asq_send_command_atomic(hw, &desc, NULL, 0,
+					      cmd_details, true);
 
 	return status;
 }
@@ -2128,11 +2135,49 @@
 	if (set)
 		flags |= I40E_AQC_SET_VSI_PROMISC_MULTICAST;
 
-	cmd->promiscuous_flags = cpu_to_le16(flags);
+	cmd->promiscuous_flags = CPU_TO_LE16(flags);
+
+	cmd->valid_flags = CPU_TO_LE16(I40E_AQC_SET_VSI_PROMISC_MULTICAST);
 
-	cmd->valid_flags = cpu_to_le16(I40E_AQC_SET_VSI_PROMISC_MULTICAST);
+	cmd->seid = CPU_TO_LE16(seid);
+	status = i40e_asq_send_command_atomic(hw, &desc, NULL, 0,
+					      cmd_details, true);
+
+	return status;
+}
+
+/**
+* i40e_aq_set_vsi_full_promiscuous
+* @hw: pointer to the hw struct
+* @seid: VSI number
+* @set: set promiscuous enable/disable
+* @cmd_details: pointer to command details structure or NULL
+**/
+i40e_status i40e_aq_set_vsi_full_promiscuous(struct i40e_hw *hw,
+				u16 seid, bool set,
+				struct i40e_asq_cmd_details *cmd_details)
+{
+	struct i40e_aq_desc desc;
+	struct i40e_aqc_set_vsi_promiscuous_modes *cmd =
+		(struct i40e_aqc_set_vsi_promiscuous_modes *)&desc.params.raw;
+	i40e_status status;
+	u16 flags = 0;
 
-	cmd->seid = cpu_to_le16(seid);
+	i40e_fill_default_direct_cmd_desc(&desc,
+		i40e_aqc_opc_set_vsi_promiscuous_modes);
+
+	if (set)
+		flags = I40E_AQC_SET_VSI_PROMISC_UNICAST   |
+			I40E_AQC_SET_VSI_PROMISC_MULTICAST |
+			I40E_AQC_SET_VSI_PROMISC_BROADCAST;
+
+	cmd->promiscuous_flags = CPU_TO_LE16(flags);
+
+	cmd->valid_flags = CPU_TO_LE16(I40E_AQC_SET_VSI_PROMISC_UNICAST   |
+				       I40E_AQC_SET_VSI_PROMISC_MULTICAST |
+				       I40E_AQC_SET_VSI_PROMISC_BROADCAST);
+
+	cmd->seid = CPU_TO_LE16(seid);
 	status = i40e_asq_send_command(hw, &desc, NULL, 0, cmd_details);
 
 	return status;
@@ -2146,29 +2191,29 @@
  * @vid: The VLAN tag filter - capture any multicast packet with this VLAN tag
  * @cmd_details: pointer to command details structure or NULL
  **/
-enum i40e_status_code i40e_aq_set_vsi_mc_promisc_on_vlan(struct i40e_hw *hw,
-							 u16 seid, bool enable,
-							 u16 vid,
+i40e_status i40e_aq_set_vsi_mc_promisc_on_vlan(struct i40e_hw *hw,
+				u16 seid, bool enable, u16 vid,
 				struct i40e_asq_cmd_details *cmd_details)
 {
 	struct i40e_aq_desc desc;
 	struct i40e_aqc_set_vsi_promiscuous_modes *cmd =
 		(struct i40e_aqc_set_vsi_promiscuous_modes *)&desc.params.raw;
-	enum i40e_status_code status;
+	i40e_status status;
 	u16 flags = 0;
 
 	i40e_fill_default_direct_cmd_desc(&desc,
-					  i40e_aqc_opc_set_vsi_promiscuous_modes);
+					i40e_aqc_opc_set_vsi_promiscuous_modes);
 
 	if (enable)
 		flags |= I40E_AQC_SET_VSI_PROMISC_MULTICAST;
 
-	cmd->promiscuous_flags = cpu_to_le16(flags);
-	cmd->valid_flags = cpu_to_le16(I40E_AQC_SET_VSI_PROMISC_MULTICAST);
-	cmd->seid = cpu_to_le16(seid);
-	cmd->vlan_tag = cpu_to_le16(vid | I40E_AQC_SET_VSI_VLAN_VALID);
+	cmd->promiscuous_flags = CPU_TO_LE16(flags);
+	cmd->valid_flags = CPU_TO_LE16(I40E_AQC_SET_VSI_PROMISC_MULTICAST);
+	cmd->seid = CPU_TO_LE16(seid);
+	cmd->vlan_tag = CPU_TO_LE16(vid | I40E_AQC_SET_VSI_VLAN_VALID);
 
-	status = i40e_asq_send_command(hw, &desc, NULL, 0, cmd_details);
+	status = i40e_asq_send_command_atomic(hw, &desc, NULL, 0,
+					      cmd_details, true);
 
 	return status;
 }
@@ -2181,35 +2226,35 @@
  * @vid: The VLAN tag filter - capture any unicast packet with this VLAN tag
  * @cmd_details: pointer to command details structure or NULL
  **/
-enum i40e_status_code i40e_aq_set_vsi_uc_promisc_on_vlan(struct i40e_hw *hw,
-							 u16 seid, bool enable,
-							 u16 vid,
+i40e_status i40e_aq_set_vsi_uc_promisc_on_vlan(struct i40e_hw *hw,
+				u16 seid, bool enable, u16 vid,
 				struct i40e_asq_cmd_details *cmd_details)
 {
 	struct i40e_aq_desc desc;
 	struct i40e_aqc_set_vsi_promiscuous_modes *cmd =
 		(struct i40e_aqc_set_vsi_promiscuous_modes *)&desc.params.raw;
-	enum i40e_status_code status;
+	i40e_status status;
 	u16 flags = 0;
 
 	i40e_fill_default_direct_cmd_desc(&desc,
-					  i40e_aqc_opc_set_vsi_promiscuous_modes);
+					i40e_aqc_opc_set_vsi_promiscuous_modes);
 
 	if (enable) {
 		flags |= I40E_AQC_SET_VSI_PROMISC_UNICAST;
-		if (i40e_is_aq_api_ver_ge(&hw->aq, 1, 5))
+		if (i40e_hw_ver_ge(hw, 1, 5))
 			flags |= I40E_AQC_SET_VSI_PROMISC_RX_ONLY;
 	}
 
-	cmd->promiscuous_flags = cpu_to_le16(flags);
-	cmd->valid_flags = cpu_to_le16(I40E_AQC_SET_VSI_PROMISC_UNICAST);
-	if (i40e_is_aq_api_ver_ge(&hw->aq, 1, 5))
+	cmd->promiscuous_flags = CPU_TO_LE16(flags);
+	cmd->valid_flags = CPU_TO_LE16(I40E_AQC_SET_VSI_PROMISC_UNICAST);
+	if (i40e_hw_ver_ge(hw, 1, 5))
 		cmd->valid_flags |=
-			cpu_to_le16(I40E_AQC_SET_VSI_PROMISC_RX_ONLY);
-	cmd->seid = cpu_to_le16(seid);
-	cmd->vlan_tag = cpu_to_le16(vid | I40E_AQC_SET_VSI_VLAN_VALID);
+			CPU_TO_LE16(I40E_AQC_SET_VSI_PROMISC_RX_ONLY);
+	cmd->seid = CPU_TO_LE16(seid);
+	cmd->vlan_tag = CPU_TO_LE16(vid | I40E_AQC_SET_VSI_VLAN_VALID);
 
-	status = i40e_asq_send_command(hw, &desc, NULL, 0, cmd_details);
+	status = i40e_asq_send_command_atomic(hw, &desc, NULL, 0,
+					      cmd_details, true);
 
 	return status;
 }
@@ -2238,10 +2283,10 @@
 	if (enable)
 		flags |= I40E_AQC_SET_VSI_PROMISC_BROADCAST;
 
-	cmd->promiscuous_flags = cpu_to_le16(flags);
-	cmd->valid_flags = cpu_to_le16(I40E_AQC_SET_VSI_PROMISC_BROADCAST);
-	cmd->seid = cpu_to_le16(seid);
-	cmd->vlan_tag = cpu_to_le16(vid | I40E_AQC_SET_VSI_VLAN_VALID);
+	cmd->promiscuous_flags = CPU_TO_LE16(flags);
+	cmd->valid_flags = CPU_TO_LE16(I40E_AQC_SET_VSI_PROMISC_BROADCAST);
+	cmd->seid = CPU_TO_LE16(seid);
+	cmd->vlan_tag = CPU_TO_LE16(vid | I40E_AQC_SET_VSI_VLAN_VALID);
 
 	status = i40e_asq_send_command(hw, &desc, NULL, 0, cmd_details);
 
@@ -2271,13 +2316,13 @@
 
 	if (set_filter)
 		cmd->promiscuous_flags
-			    |= cpu_to_le16(I40E_AQC_SET_VSI_PROMISC_BROADCAST);
+			    |= CPU_TO_LE16(I40E_AQC_SET_VSI_PROMISC_BROADCAST);
 	else
 		cmd->promiscuous_flags
-			    &= cpu_to_le16(~I40E_AQC_SET_VSI_PROMISC_BROADCAST);
+			    &= CPU_TO_LE16(~I40E_AQC_SET_VSI_PROMISC_BROADCAST);
 
-	cmd->valid_flags = cpu_to_le16(I40E_AQC_SET_VSI_PROMISC_BROADCAST);
-	cmd->seid = cpu_to_le16(seid);
+	cmd->valid_flags = CPU_TO_LE16(I40E_AQC_SET_VSI_PROMISC_BROADCAST);
+	cmd->seid = CPU_TO_LE16(seid);
 	status = i40e_asq_send_command(hw, &desc, NULL, 0, cmd_details);
 
 	return status;
@@ -2291,8 +2336,8 @@
  * @cmd_details: pointer to command details structure or NULL
  **/
 i40e_status i40e_aq_set_vsi_vlan_promisc(struct i40e_hw *hw,
-				       u16 seid, bool enable,
-				       struct i40e_asq_cmd_details *cmd_details)
+				u16 seid, bool enable,
+				struct i40e_asq_cmd_details *cmd_details)
 {
 	struct i40e_aq_desc desc;
 	struct i40e_aqc_set_vsi_promiscuous_modes *cmd =
@@ -2305,9 +2350,9 @@
 	if (enable)
 		flags |= I40E_AQC_SET_VSI_PROMISC_VLAN;
 
-	cmd->promiscuous_flags = cpu_to_le16(flags);
-	cmd->valid_flags = cpu_to_le16(I40E_AQC_SET_VSI_PROMISC_VLAN);
-	cmd->seid = cpu_to_le16(seid);
+	cmd->promiscuous_flags = CPU_TO_LE16(flags);
+	cmd->valid_flags = CPU_TO_LE16(I40E_AQC_SET_VSI_PROMISC_VLAN);
+	cmd->seid = CPU_TO_LE16(seid);
 
 	status = i40e_asq_send_command(hw, &desc, NULL, 0, cmd_details);
 
@@ -2315,7 +2360,7 @@
 }
 
 /**
- * i40e_get_vsi_params - get VSI configuration info
+ * i40e_aq_get_vsi_params - get VSI configuration info
  * @hw: pointer to the hw struct
  * @vsi_ctx: pointer to a vsi context struct
  * @cmd_details: pointer to command details structure or NULL
@@ -2335,20 +2380,20 @@
 	i40e_fill_default_direct_cmd_desc(&desc,
 					  i40e_aqc_opc_get_vsi_parameters);
 
-	cmd->uplink_seid = cpu_to_le16(vsi_ctx->seid);
+	cmd->uplink_seid = CPU_TO_LE16(vsi_ctx->seid);
 
-	desc.flags |= cpu_to_le16((u16)I40E_AQ_FLAG_BUF);
+	desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_BUF);
 
 	status = i40e_asq_send_command(hw, &desc, &vsi_ctx->info,
 				    sizeof(vsi_ctx->info), NULL);
 
-	if (status)
+	if (status != I40E_SUCCESS)
 		goto aq_get_vsi_params_exit;
 
-	vsi_ctx->seid = le16_to_cpu(resp->seid);
-	vsi_ctx->vsi_number = le16_to_cpu(resp->vsi_number);
-	vsi_ctx->vsis_allocated = le16_to_cpu(resp->vsi_used);
-	vsi_ctx->vsis_unallocated = le16_to_cpu(resp->vsi_free);
+	vsi_ctx->seid = LE16_TO_CPU(resp->seid);
+	vsi_ctx->vsi_number = LE16_TO_CPU(resp->vsi_number);
+	vsi_ctx->vsis_allocated = LE16_TO_CPU(resp->vsi_used);
+	vsi_ctx->vsis_unallocated = LE16_TO_CPU(resp->vsi_free);
 
 aq_get_vsi_params_exit:
 	return status;
@@ -2376,15 +2421,16 @@
 
 	i40e_fill_default_direct_cmd_desc(&desc,
 					  i40e_aqc_opc_update_vsi_parameters);
-	cmd->uplink_seid = cpu_to_le16(vsi_ctx->seid);
+	cmd->uplink_seid = CPU_TO_LE16(vsi_ctx->seid);
 
-	desc.flags |= cpu_to_le16((u16)(I40E_AQ_FLAG_BUF | I40E_AQ_FLAG_RD));
+	desc.flags |= CPU_TO_LE16((u16)(I40E_AQ_FLAG_BUF | I40E_AQ_FLAG_RD));
 
-	status = i40e_asq_send_command(hw, &desc, &vsi_ctx->info,
-				    sizeof(vsi_ctx->info), cmd_details);
+	status = i40e_asq_send_command_atomic(hw, &desc, &vsi_ctx->info,
+					      sizeof(vsi_ctx->info),
+					      cmd_details, true);
 
-	vsi_ctx->vsis_allocated = le16_to_cpu(resp->vsi_used);
-	vsi_ctx->vsis_unallocated = le16_to_cpu(resp->vsi_free);
+	vsi_ctx->vsis_allocated = LE16_TO_CPU(resp->vsi_used);
+	vsi_ctx->vsis_unallocated = LE16_TO_CPU(resp->vsi_free);
 
 	return status;
 }
@@ -2411,13 +2457,13 @@
 
 	i40e_fill_default_direct_cmd_desc(&desc,
 					  i40e_aqc_opc_get_switch_config);
-	desc.flags |= cpu_to_le16((u16)I40E_AQ_FLAG_BUF);
+	desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_BUF);
 	if (buf_size > I40E_AQ_LARGE_BUF)
-		desc.flags |= cpu_to_le16((u16)I40E_AQ_FLAG_LB);
-	scfg->seid = cpu_to_le16(*start_seid);
+		desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_LB);
+	scfg->seid = CPU_TO_LE16(*start_seid);
 
 	status = i40e_asq_send_command(hw, &desc, buf, buf_size, cmd_details);
-	*start_seid = le16_to_cpu(scfg->seid);
+	*start_seid = LE16_TO_CPU(scfg->seid);
 
 	return status;
 }
@@ -2428,30 +2474,28 @@
  * @flags: bit flag values to set
  * @mode: cloud filter mode
  * @valid_flags: which bit flags to set
- * @mode: cloud filter mode
  * @cmd_details: pointer to command details structure or NULL
  *
  * Set switch configuration bits
  **/
-enum i40e_status_code i40e_aq_set_switch_config(struct i40e_hw *hw,
-						u16 flags,
-						u16 valid_flags, u8 mode,
+i40e_status i40e_aq_set_switch_config(struct i40e_hw *hw,
+				u16 flags, u16 valid_flags, u8 mode,
 				struct i40e_asq_cmd_details *cmd_details)
 {
 	struct i40e_aq_desc desc;
 	struct i40e_aqc_set_switch_config *scfg =
 		(struct i40e_aqc_set_switch_config *)&desc.params.raw;
-	enum i40e_status_code status;
+	i40e_status status;
 
 	i40e_fill_default_direct_cmd_desc(&desc,
 					  i40e_aqc_opc_set_switch_config);
-	scfg->flags = cpu_to_le16(flags);
-	scfg->valid_flags = cpu_to_le16(valid_flags);
+	scfg->flags = CPU_TO_LE16(flags);
+	scfg->valid_flags = CPU_TO_LE16(valid_flags);
 	scfg->mode = mode;
 	if (hw->flags & I40E_HW_FLAG_802_1AD_CAPABLE) {
-		scfg->switch_tag = cpu_to_le16(hw->switch_tag);
-		scfg->first_tag = cpu_to_le16(hw->first_tag);
-		scfg->second_tag = cpu_to_le16(hw->second_tag);
+		scfg->switch_tag = CPU_TO_LE16(hw->switch_tag);
+		scfg->first_tag = CPU_TO_LE16(hw->first_tag);
+		scfg->second_tag = CPU_TO_LE16(hw->second_tag);
 	}
 	status = i40e_asq_send_command(hw, &desc, NULL, 0, cmd_details);
 
@@ -2485,17 +2529,17 @@
 
 	status = i40e_asq_send_command(hw, &desc, NULL, 0, cmd_details);
 
-	if (!status) {
-		if (fw_major_version)
-			*fw_major_version = le16_to_cpu(resp->fw_major);
-		if (fw_minor_version)
-			*fw_minor_version = le16_to_cpu(resp->fw_minor);
-		if (fw_build)
-			*fw_build = le32_to_cpu(resp->fw_build);
-		if (api_major_version)
-			*api_major_version = le16_to_cpu(resp->api_major);
-		if (api_minor_version)
-			*api_minor_version = le16_to_cpu(resp->api_minor);
+	if (status == I40E_SUCCESS) {
+		if (fw_major_version != NULL)
+			*fw_major_version = LE16_TO_CPU(resp->fw_major);
+		if (fw_minor_version != NULL)
+			*fw_minor_version = LE16_TO_CPU(resp->fw_minor);
+		if (fw_build != NULL)
+			*fw_build = LE32_TO_CPU(resp->fw_build);
+		if (api_major_version != NULL)
+			*api_major_version = LE16_TO_CPU(resp->api_major);
+		if (api_minor_version != NULL)
+			*api_minor_version = LE16_TO_CPU(resp->api_minor);
 	}
 
 	return status;
@@ -2524,7 +2568,7 @@
 
 	i40e_fill_default_direct_cmd_desc(&desc, i40e_aqc_opc_driver_version);
 
-	desc.flags |= cpu_to_le16(I40E_AQ_FLAG_BUF | I40E_AQ_FLAG_RD);
+	desc.flags |= CPU_TO_LE16(I40E_AQ_FLAG_BUF | I40E_AQ_FLAG_RD);
 	cmd->driver_major_ver = dv->major_version;
 	cmd->driver_minor_ver = dv->minor_version;
 	cmd->driver_build_ver = dv->build_version;
@@ -2547,18 +2591,18 @@
  * @link_up: pointer to bool (true/false = linkup/linkdown)
  *
  * Variable link_up true if link is up, false if link is down.
- * The variable link_up is invalid if returned value of status != 0
+ * The variable link_up is invalid if returned value of status != I40E_SUCCESS
  *
  * Side effect: LinkStatusEvent reporting becomes enabled
  **/
 i40e_status i40e_get_link_status(struct i40e_hw *hw, bool *link_up)
 {
-	i40e_status status = 0;
+	i40e_status status = I40E_SUCCESS;
 
 	if (hw->phy.get_link_info) {
 		status = i40e_update_link_info(hw);
 
-		if (status)
+		if (status != I40E_SUCCESS)
 			i40e_debug(hw, I40E_DEBUG_LINK, "get link failed: status %d\n",
 				   status);
 	}
@@ -2569,23 +2613,26 @@
 }
 
 /**
- * i40e_updatelink_status - update status of the HW network link
+ * i40e_update_link_info - update status of the HW network link
  * @hw: pointer to the hw struct
  **/
-noinline_for_stack i40e_status i40e_update_link_info(struct i40e_hw *hw)
+i40e_status i40e_update_link_info(struct i40e_hw *hw)
 {
 	struct i40e_aq_get_phy_abilities_resp abilities;
-	i40e_status status = 0;
+	i40e_status status = I40E_SUCCESS;
 
 	status = i40e_aq_get_link_info(hw, true, NULL, NULL);
 	if (status)
 		return status;
 
 	/* extra checking needed to ensure link info to user is timely */
-	if ((hw->phy.link_info.link_info & I40E_AQ_MEDIA_AVAILABLE) &&
-	    ((hw->phy.link_info.link_info & I40E_AQ_LINK_UP) ||
-	     !(hw->phy.link_info_old.link_info & I40E_AQ_LINK_UP))) {
-		status = i40e_aq_get_phy_capabilities(hw, false, false,
+	if (((hw->phy.link_info.link_info & I40E_AQ_MEDIA_AVAILABLE) &&
+	     ((hw->phy.link_info.link_info & I40E_AQ_LINK_UP) ||
+	      !(hw->phy.link_info_old.link_info & I40E_AQ_LINK_UP))) ||
+	    hw->mac.type == I40E_MAC_X722) {
+		status = i40e_aq_get_phy_capabilities(hw, false,
+						      hw->mac.type ==
+						      I40E_MAC_X722,
 						      &abilities, NULL);
 		if (status)
 			return status;
@@ -2601,10 +2648,9 @@
 				(I40E_AQ_REQUEST_FEC_KR |
 				 I40E_AQ_REQUEST_FEC_RS);
 
-		memcpy(hw->phy.link_info.module_type, &abilities.module_type,
-		       sizeof(hw->phy.link_info.module_type));
+		i40e_memcpy(hw->phy.link_info.module_type, &abilities.module_type,
+			sizeof(hw->phy.link_info.module_type), I40E_NONDMA_TO_NONDMA);
 	}
-
 	return status;
 }
 
@@ -2642,8 +2688,8 @@
 
 	i40e_fill_default_direct_cmd_desc(&desc, i40e_aqc_opc_add_veb);
 
-	cmd->uplink_seid = cpu_to_le16(uplink_seid);
-	cmd->downlink_seid = cpu_to_le16(downlink_seid);
+	cmd->uplink_seid = CPU_TO_LE16(uplink_seid);
+	cmd->downlink_seid = CPU_TO_LE16(downlink_seid);
 	cmd->enable_tcs = enabled_tc;
 	if (!uplink_seid)
 		veb_flags |= I40E_AQC_ADD_VEB_FLOATING;
@@ -2656,12 +2702,12 @@
 	if (!enable_stats)
 		veb_flags |= I40E_AQC_ADD_VEB_ENABLE_DISABLE_STATS;
 
-	cmd->veb_flags = cpu_to_le16(veb_flags);
+	cmd->veb_flags = CPU_TO_LE16(veb_flags);
 
 	status = i40e_asq_send_command(hw, &desc, NULL, 0, cmd_details);
 
 	if (!status && veb_seid)
-		*veb_seid = le16_to_cpu(resp->veb_seid);
+		*veb_seid = LE16_TO_CPU(resp->veb_seid);
 
 	return status;
 }
@@ -2697,22 +2743,22 @@
 
 	i40e_fill_default_direct_cmd_desc(&desc,
 					  i40e_aqc_opc_get_veb_parameters);
-	cmd_resp->seid = cpu_to_le16(veb_seid);
+	cmd_resp->seid = CPU_TO_LE16(veb_seid);
 
 	status = i40e_asq_send_command(hw, &desc, NULL, 0, cmd_details);
 	if (status)
 		goto get_veb_exit;
 
 	if (switch_id)
-		*switch_id = le16_to_cpu(cmd_resp->switch_id);
+		*switch_id = LE16_TO_CPU(cmd_resp->switch_id);
 	if (statistic_index)
-		*statistic_index = le16_to_cpu(cmd_resp->statistic_index);
+		*statistic_index = LE16_TO_CPU(cmd_resp->statistic_index);
 	if (vebs_used)
-		*vebs_used = le16_to_cpu(cmd_resp->vebs_used);
+		*vebs_used = LE16_TO_CPU(cmd_resp->vebs_used);
 	if (vebs_free)
-		*vebs_free = le16_to_cpu(cmd_resp->vebs_free);
+		*vebs_free = LE16_TO_CPU(cmd_resp->vebs_free);
 	if (floating) {
-		u16 flags = le16_to_cpu(cmd_resp->veb_flags);
+		u16 flags = LE16_TO_CPU(cmd_resp->veb_flags);
 
 		if (flags & I40E_AQC_ADD_VEB_FLOATING)
 			*floating = true;
@@ -2725,6 +2771,46 @@
 }
 
 /**
+ * i40e_prepare_add_macvlan
+ * @mv_list: list of macvlans to be added
+ * @desc: pointer to AQ descriptor structure
+ * @count: length of the list
+ * @seid: VSI for the mac address
+ *
+ * Internal helper function that prepares the add macvlan request
+ * and returns the buffer size.
+ **/
+static u16
+i40e_prepare_add_macvlan(struct i40e_aqc_add_macvlan_element_data *mv_list,
+			 struct i40e_aq_desc *desc, u16 count, u16 seid)
+{
+	struct i40e_aqc_macvlan *cmd =
+		(struct i40e_aqc_macvlan *)&desc->params.raw;
+	u16 buf_size;
+	int i;
+
+	buf_size = count * sizeof(*mv_list);
+
+	/* prep the rest of the request */
+	i40e_fill_default_direct_cmd_desc(desc, i40e_aqc_opc_add_macvlan);
+	cmd->num_addresses = CPU_TO_LE16(count);
+	cmd->seid[0] = CPU_TO_LE16(I40E_AQC_MACVLAN_CMD_SEID_VALID | seid);
+	cmd->seid[1] = 0;
+	cmd->seid[2] = 0;
+
+	for (i = 0; i < count; i++)
+		if (is_multicast_ether_addr(mv_list[i].mac_addr))
+			mv_list[i].flags |=
+			    CPU_TO_LE16(I40E_AQC_MACVLAN_ADD_USE_SHARED_MAC);
+
+	desc->flags |= CPU_TO_LE16((u16)(I40E_AQ_FLAG_BUF | I40E_AQ_FLAG_RD));
+	if (buf_size > I40E_AQ_LARGE_BUF)
+		desc->flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_LB);
+
+	return buf_size;
+}
+
+/**
  * i40e_aq_add_macvlan
  * @hw: pointer to the hw struct
  * @seid: VSI for the mac address
@@ -2734,8 +2820,74 @@
  *
  * Add MAC/VLAN addresses to the HW filtering
  **/
-i40e_status i40e_aq_add_macvlan(struct i40e_hw *hw, u16 seid,
-			struct i40e_aqc_add_macvlan_element_data *mv_list,
+enum i40e_status_code
+i40e_aq_add_macvlan(struct i40e_hw *hw, u16 seid,
+		    struct i40e_aqc_add_macvlan_element_data *mv_list,
+		    u16 count, struct i40e_asq_cmd_details *cmd_details)
+{
+	struct i40e_aq_desc desc;
+	i40e_status status;
+	u16 buf_size;
+
+	if (count == 0 || !mv_list || !hw)
+		return I40E_ERR_PARAM;
+
+	buf_size = i40e_prepare_add_macvlan(mv_list, &desc, count, seid);
+
+	status = i40e_asq_send_command_atomic(hw, &desc, mv_list, buf_size,
+					      cmd_details, true);
+
+	return status;
+}
+
+/**
+ * i40e_aq_add_macvlan_v2
+ * @hw: pointer to the hw struct
+ * @seid: VSI for the mac address
+ * @mv_list: list of macvlans to be added
+ * @count: length of the list
+ * @cmd_details: pointer to command details structure or NULL
+ * @aq_status: pointer to Admin Queue status return value
+ *
+ * Add MAC/VLAN addresses to the HW filtering.
+ * The _v2 version returns the last Admin Queue status in aq_status
+ * to avoid race conditions in access to hw->aq.asq_last_status.
+ * It also calls _v2 versions of asq_send_command functions to
+ * get the aq_status on the stack.
+ **/
+enum i40e_status_code
+i40e_aq_add_macvlan_v2(struct i40e_hw *hw, u16 seid,
+		       struct i40e_aqc_add_macvlan_element_data *mv_list,
+		       u16 count, struct i40e_asq_cmd_details *cmd_details,
+		       enum i40e_admin_queue_err *aq_status)
+{
+	struct i40e_aq_desc desc;
+	i40e_status status;
+	u16 buf_size;
+
+	if (count == 0 || !mv_list || !hw)
+		return I40E_ERR_PARAM;
+
+	buf_size = i40e_prepare_add_macvlan(mv_list, &desc, count, seid);
+
+	status = i40e_asq_send_command_atomic_v2(hw, &desc, mv_list, buf_size,
+						 cmd_details, true, aq_status);
+
+	return status;
+}
+
+/**
+ * i40e_aq_remove_macvlan
+ * @hw: pointer to the hw struct
+ * @seid: VSI for the mac address
+ * @mv_list: list of macvlans to be removed
+ * @count: length of the list
+ * @cmd_details: pointer to command details structure or NULL
+ *
+ * Remove MAC/VLAN addresses from the HW filtering
+ **/
+i40e_status i40e_aq_remove_macvlan(struct i40e_hw *hw, u16 seid,
+			struct i40e_aqc_remove_macvlan_element_data *mv_list,
 			u16 count, struct i40e_asq_cmd_details *cmd_details)
 {
 	struct i40e_aq_desc desc;
@@ -2743,7 +2895,6 @@
 		(struct i40e_aqc_macvlan *)&desc.params.raw;
 	i40e_status status;
 	u16 buf_size;
-	int i;
 
 	if (count == 0 || !mv_list || !hw)
 		return I40E_ERR_PARAM;
@@ -2751,40 +2902,42 @@
 	buf_size = count * sizeof(*mv_list);
 
 	/* prep the rest of the request */
-	i40e_fill_default_direct_cmd_desc(&desc, i40e_aqc_opc_add_macvlan);
-	cmd->num_addresses = cpu_to_le16(count);
-	cmd->seid[0] = cpu_to_le16(I40E_AQC_MACVLAN_CMD_SEID_VALID | seid);
+	i40e_fill_default_direct_cmd_desc(&desc, i40e_aqc_opc_remove_macvlan);
+	cmd->num_addresses = CPU_TO_LE16(count);
+	cmd->seid[0] = CPU_TO_LE16(I40E_AQC_MACVLAN_CMD_SEID_VALID | seid);
 	cmd->seid[1] = 0;
 	cmd->seid[2] = 0;
 
-	for (i = 0; i < count; i++)
-		if (is_multicast_ether_addr(mv_list[i].mac_addr))
-			mv_list[i].flags |=
-			       cpu_to_le16(I40E_AQC_MACVLAN_ADD_USE_SHARED_MAC);
-
-	desc.flags |= cpu_to_le16((u16)(I40E_AQ_FLAG_BUF | I40E_AQ_FLAG_RD));
+	desc.flags |= CPU_TO_LE16((u16)(I40E_AQ_FLAG_BUF | I40E_AQ_FLAG_RD));
 	if (buf_size > I40E_AQ_LARGE_BUF)
-		desc.flags |= cpu_to_le16((u16)I40E_AQ_FLAG_LB);
+		desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_LB);
 
-	status = i40e_asq_send_command(hw, &desc, mv_list, buf_size,
-				       cmd_details);
+	status = i40e_asq_send_command_atomic(hw, &desc, mv_list, buf_size,
+					      cmd_details, true);
 
 	return status;
 }
 
 /**
- * i40e_aq_remove_macvlan
+ * i40e_aq_remove_macvlan_v2
  * @hw: pointer to the hw struct
  * @seid: VSI for the mac address
  * @mv_list: list of macvlans to be removed
  * @count: length of the list
  * @cmd_details: pointer to command details structure or NULL
+ * @aq_status: pointer to Admin Queue status return value
  *
- * Remove MAC/VLAN addresses from the HW filtering
+ * Remove MAC/VLAN addresses from the HW filtering.
+ * The _v2 version returns the last Admin Queue status in aq_status
+ * to avoid race conditions in access to hw->aq.asq_last_status.
+ * It also calls _v2 versions of asq_send_command functions to
+ * get the aq_status on the stack.
  **/
-i40e_status i40e_aq_remove_macvlan(struct i40e_hw *hw, u16 seid,
-			struct i40e_aqc_remove_macvlan_element_data *mv_list,
-			u16 count, struct i40e_asq_cmd_details *cmd_details)
+enum i40e_status_code
+i40e_aq_remove_macvlan_v2(struct i40e_hw *hw, u16 seid,
+			  struct i40e_aqc_remove_macvlan_element_data *mv_list,
+			  u16 count, struct i40e_asq_cmd_details *cmd_details,
+			  enum i40e_admin_queue_err *aq_status)
 {
 	struct i40e_aq_desc desc;
 	struct i40e_aqc_macvlan *cmd =
@@ -2799,17 +2952,17 @@
 
 	/* prep the rest of the request */
 	i40e_fill_default_direct_cmd_desc(&desc, i40e_aqc_opc_remove_macvlan);
-	cmd->num_addresses = cpu_to_le16(count);
-	cmd->seid[0] = cpu_to_le16(I40E_AQC_MACVLAN_CMD_SEID_VALID | seid);
+	cmd->num_addresses = CPU_TO_LE16(count);
+	cmd->seid[0] = CPU_TO_LE16(I40E_AQC_MACVLAN_CMD_SEID_VALID | seid);
 	cmd->seid[1] = 0;
 	cmd->seid[2] = 0;
 
-	desc.flags |= cpu_to_le16((u16)(I40E_AQ_FLAG_BUF | I40E_AQ_FLAG_RD));
+	desc.flags |= CPU_TO_LE16((u16)(I40E_AQ_FLAG_BUF | I40E_AQ_FLAG_RD));
 	if (buf_size > I40E_AQ_LARGE_BUF)
-		desc.flags |= cpu_to_le16((u16)I40E_AQ_FLAG_LB);
+		desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_LB);
 
-	status = i40e_asq_send_command(hw, &desc, mv_list, buf_size,
-				       cmd_details);
+	status = i40e_asq_send_command_atomic_v2(hw, &desc, mv_list, buf_size,
+						 cmd_details, true, aq_status);
 
 	return status;
 }
@@ -2832,10 +2985,10 @@
  * VEBs/VEPA elements only
  **/
 static i40e_status i40e_mirrorrule_op(struct i40e_hw *hw,
-				u16 opcode, u16 sw_seid, u16 rule_type, u16 id,
-				u16 count, __le16 *mr_list,
-				struct i40e_asq_cmd_details *cmd_details,
-				u16 *rule_id, u16 *rules_used, u16 *rules_free)
+			u16 opcode, u16 sw_seid, u16 rule_type, u16 id,
+			u16 count, __le16 *mr_list,
+			struct i40e_asq_cmd_details *cmd_details,
+			u16 *rule_id, u16 *rules_used, u16 *rules_free)
 {
 	struct i40e_aq_desc desc;
 	struct i40e_aqc_add_delete_mirror_rule *cmd =
@@ -2849,29 +3002,29 @@
 
 	/* prep the rest of the request */
 	i40e_fill_default_direct_cmd_desc(&desc, opcode);
-	cmd->seid = cpu_to_le16(sw_seid);
-	cmd->rule_type = cpu_to_le16(rule_type &
+	cmd->seid = CPU_TO_LE16(sw_seid);
+	cmd->rule_type = CPU_TO_LE16(rule_type &
 				     I40E_AQC_MIRROR_RULE_TYPE_MASK);
-	cmd->num_entries = cpu_to_le16(count);
+	cmd->num_entries = CPU_TO_LE16(count);
 	/* Dest VSI for add, rule_id for delete */
-	cmd->destination = cpu_to_le16(id);
+	cmd->destination = CPU_TO_LE16(id);
 	if (mr_list) {
-		desc.flags |= cpu_to_le16((u16)(I40E_AQ_FLAG_BUF |
+		desc.flags |= CPU_TO_LE16((u16)(I40E_AQ_FLAG_BUF |
 						I40E_AQ_FLAG_RD));
 		if (buf_size > I40E_AQ_LARGE_BUF)
-			desc.flags |= cpu_to_le16((u16)I40E_AQ_FLAG_LB);
+			desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_LB);
 	}
 
 	status = i40e_asq_send_command(hw, &desc, mr_list, buf_size,
 				       cmd_details);
-	if (!status ||
+	if (status == I40E_SUCCESS ||
 	    hw->aq.asq_last_status == I40E_AQ_RC_ENOSPC) {
 		if (rule_id)
-			*rule_id = le16_to_cpu(resp->rule_id);
+			*rule_id = LE16_TO_CPU(resp->rule_id);
 		if (rules_used)
-			*rules_used = le16_to_cpu(resp->mirror_rules_used);
+			*rules_used = LE16_TO_CPU(resp->mirror_rules_used);
 		if (rules_free)
-			*rules_free = le16_to_cpu(resp->mirror_rules_free);
+			*rules_free = LE16_TO_CPU(resp->mirror_rules_free);
 	}
 	return status;
 }
@@ -2945,7 +3098,7 @@
 /**
  * i40e_aq_send_msg_to_vf
  * @hw: pointer to the hardware structure
- * @vfid: VF id to send msg
+ * @vfid: vf id to send msg
  * @v_opcode: opcodes for VF-PF communication
  * @v_retval: return error code
  * @msg: pointer to the msg buffer
@@ -2964,16 +3117,16 @@
 	i40e_status status;
 
 	i40e_fill_default_direct_cmd_desc(&desc, i40e_aqc_opc_send_msg_to_vf);
-	cmd->id = cpu_to_le32(vfid);
-	desc.cookie_high = cpu_to_le32(v_opcode);
-	desc.cookie_low = cpu_to_le32(v_retval);
-	desc.flags |= cpu_to_le16((u16)I40E_AQ_FLAG_SI);
+	cmd->id = CPU_TO_LE32(vfid);
+	desc.cookie_high = CPU_TO_LE32(v_opcode);
+	desc.cookie_low = CPU_TO_LE32(v_retval);
+	desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_SI);
 	if (msglen) {
-		desc.flags |= cpu_to_le16((u16)(I40E_AQ_FLAG_BUF |
+		desc.flags |= CPU_TO_LE16((u16)(I40E_AQ_FLAG_BUF |
 						I40E_AQ_FLAG_RD));
 		if (msglen > I40E_AQ_LARGE_BUF)
-			desc.flags |= cpu_to_le16((u16)I40E_AQ_FLAG_LB);
-		desc.datalen = cpu_to_le16(msglen);
+			desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_LB);
+		desc.datalen = CPU_TO_LE16(msglen);
 	}
 	status = i40e_asq_send_command(hw, &desc, msg, msglen, cmd_details);
 
@@ -3003,13 +3156,13 @@
 
 	i40e_fill_default_direct_cmd_desc(&desc, i40e_aqc_opc_debug_read_reg);
 
-	cmd_resp->address = cpu_to_le32(reg_addr);
+	cmd_resp->address = CPU_TO_LE32(reg_addr);
 
 	status = i40e_asq_send_command(hw, &desc, NULL, 0, cmd_details);
 
-	if (!status) {
-		*reg_val = ((u64)le32_to_cpu(cmd_resp->value_high) << 32) |
-			   (u64)le32_to_cpu(cmd_resp->value_low);
+	if (status == I40E_SUCCESS) {
+		*reg_val = ((u64)LE32_TO_CPU(cmd_resp->value_high) << 32) |
+			   (u64)LE32_TO_CPU(cmd_resp->value_low);
 	}
 
 	return status;
@@ -3025,8 +3178,8 @@
  * Write to a register using the admin queue commands
  **/
 i40e_status i40e_aq_debug_write_register(struct i40e_hw *hw,
-					u32 reg_addr, u64 reg_val,
-					struct i40e_asq_cmd_details *cmd_details)
+				u32 reg_addr, u64 reg_val,
+				struct i40e_asq_cmd_details *cmd_details)
 {
 	struct i40e_aq_desc desc;
 	struct i40e_aqc_debug_reg_read_write *cmd =
@@ -3035,9 +3188,9 @@
 
 	i40e_fill_default_direct_cmd_desc(&desc, i40e_aqc_opc_debug_write_reg);
 
-	cmd->address = cpu_to_le32(reg_addr);
-	cmd->value_high = cpu_to_le32((u32)(reg_val >> 32));
-	cmd->value_low = cpu_to_le32((u32)(reg_val & 0xFFFFFFFF));
+	cmd->address = CPU_TO_LE32(reg_addr);
+	cmd->value_high = CPU_TO_LE32((u32)(reg_val >> 32));
+	cmd->value_low = CPU_TO_LE32((u32)(reg_val & 0xFFFFFFFF));
 
 	status = i40e_asq_send_command(hw, &desc, NULL, 0, cmd_details);
 
@@ -3068,9 +3221,9 @@
 
 	i40e_fill_default_direct_cmd_desc(&desc, i40e_aqc_opc_request_resource);
 
-	cmd_resp->resource_id = cpu_to_le16(resource);
-	cmd_resp->access_type = cpu_to_le16(access);
-	cmd_resp->resource_number = cpu_to_le32(sdp_number);
+	cmd_resp->resource_id = CPU_TO_LE16(resource);
+	cmd_resp->access_type = CPU_TO_LE16(access);
+	cmd_resp->resource_number = CPU_TO_LE32(sdp_number);
 
 	status = i40e_asq_send_command(hw, &desc, NULL, 0, cmd_details);
 	/* The completion specifies the maximum time in ms that the driver
@@ -3079,8 +3232,8 @@
 	 * busy return value and the timeout field indicates the maximum time
 	 * the current owner of the resource has to free it.
 	 */
-	if (!status || hw->aq.asq_last_status == I40E_AQ_RC_EBUSY)
-		*timeout = le32_to_cpu(cmd_resp->timeout);
+	if (status == I40E_SUCCESS || hw->aq.asq_last_status == I40E_AQ_RC_EBUSY)
+		*timeout = LE32_TO_CPU(cmd_resp->timeout);
 
 	return status;
 }
@@ -3106,8 +3259,8 @@
 
 	i40e_fill_default_direct_cmd_desc(&desc, i40e_aqc_opc_release_resource);
 
-	cmd->resource_id = cpu_to_le16(resource);
-	cmd->resource_number = cpu_to_le32(sdp_number);
+	cmd->resource_id = CPU_TO_LE16(resource);
+	cmd->resource_number = CPU_TO_LE32(sdp_number);
 
 	status = i40e_asq_send_command(hw, &desc, NULL, 0, cmd_details);
 
@@ -3148,12 +3301,12 @@
 	if (last_command)
 		cmd->command_flags |= I40E_AQ_NVM_LAST_CMD;
 	cmd->module_pointer = module_pointer;
-	cmd->offset = cpu_to_le32(offset);
-	cmd->length = cpu_to_le16(length);
+	cmd->offset = CPU_TO_LE32(offset);
+	cmd->length = CPU_TO_LE16(length);
 
-	desc.flags |= cpu_to_le16((u16)I40E_AQ_FLAG_BUF);
+	desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_BUF);
 	if (length > I40E_AQ_LARGE_BUF)
-		desc.flags |= cpu_to_le16((u16)I40E_AQ_FLAG_LB);
+		desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_LB);
 
 	status = i40e_asq_send_command(hw, &desc, data, length, cmd_details);
 
@@ -3162,6 +3315,158 @@
 }
 
 /**
+ * i40e_aq_read_nvm_config - read an nvm config block
+ * @hw: pointer to the hw struct
+ * @cmd_flags: NVM access admin command bits
+ * @field_id: field or feature id
+ * @data: buffer for result
+ * @buf_size: buffer size
+ * @element_count: pointer to count of elements read by FW
+ * @cmd_details: pointer to command details structure or NULL
+ **/
+i40e_status i40e_aq_read_nvm_config(struct i40e_hw *hw,
+				u8 cmd_flags, u32 field_id, void *data,
+				u16 buf_size, u16 *element_count,
+				struct i40e_asq_cmd_details *cmd_details)
+{
+	struct i40e_aq_desc desc;
+	struct i40e_aqc_nvm_config_read *cmd =
+		(struct i40e_aqc_nvm_config_read *)&desc.params.raw;
+	i40e_status status;
+
+	i40e_fill_default_direct_cmd_desc(&desc, i40e_aqc_opc_nvm_config_read);
+	desc.flags |= CPU_TO_LE16((u16)(I40E_AQ_FLAG_BUF));
+	if (buf_size > I40E_AQ_LARGE_BUF)
+		desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_LB);
+
+	cmd->cmd_flags = CPU_TO_LE16(cmd_flags);
+	cmd->element_id = CPU_TO_LE16((u16)(0xffff & field_id));
+	if (cmd_flags & I40E_AQ_ANVM_FEATURE_OR_IMMEDIATE_MASK)
+		cmd->element_id_msw = CPU_TO_LE16((u16)(field_id >> 16));
+	else
+		cmd->element_id_msw = 0;
+
+	status = i40e_asq_send_command(hw, &desc, data, buf_size, cmd_details);
+
+	if (!status && element_count)
+		*element_count = LE16_TO_CPU(cmd->element_count);
+
+	return status;
+}
+
+/**
+ * i40e_aq_write_nvm_config - write an nvm config block
+ * @hw: pointer to the hw struct
+ * @cmd_flags: NVM access admin command bits
+ * @data: buffer for result
+ * @buf_size: buffer size
+ * @element_count: count of elements to be written
+ * @cmd_details: pointer to command details structure or NULL
+ **/
+i40e_status i40e_aq_write_nvm_config(struct i40e_hw *hw,
+				u8 cmd_flags, void *data, u16 buf_size,
+				u16 element_count,
+				struct i40e_asq_cmd_details *cmd_details)
+{
+	struct i40e_aq_desc desc;
+	struct i40e_aqc_nvm_config_write *cmd =
+		(struct i40e_aqc_nvm_config_write *)&desc.params.raw;
+	i40e_status status;
+
+	i40e_fill_default_direct_cmd_desc(&desc, i40e_aqc_opc_nvm_config_write);
+	desc.flags |= CPU_TO_LE16((u16)(I40E_AQ_FLAG_BUF | I40E_AQ_FLAG_RD));
+	if (buf_size > I40E_AQ_LARGE_BUF)
+		desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_LB);
+
+	cmd->element_count = CPU_TO_LE16(element_count);
+	cmd->cmd_flags = CPU_TO_LE16(cmd_flags);
+	status = i40e_asq_send_command(hw, &desc, data, buf_size, cmd_details);
+
+	return status;
+}
+
+/**
+ * i40e_aq_nvm_update_in_process
+ * @hw: pointer to the hw struct
+ * @update_flow_state: True indicates that update flow starts, false that ends
+ * @cmd_details: pointer to command details structure or NULL
+ *
+ * Indicate NVM update in process.
+ **/
+i40e_status i40e_aq_nvm_update_in_process(struct i40e_hw *hw,
+				bool update_flow_state,
+				struct i40e_asq_cmd_details *cmd_details)
+{
+	struct i40e_aq_desc desc;
+	struct i40e_aqc_nvm_update_in_process *cmd =
+		(struct i40e_aqc_nvm_update_in_process *)&desc.params.raw;
+	i40e_status status;
+
+	i40e_fill_default_direct_cmd_desc(&desc,
+					  i40e_aqc_opc_nvm_update_in_process);
+
+	cmd->command = I40E_AQ_UPDATE_FLOW_END;
+
+	if (update_flow_state)
+		cmd->command |= I40E_AQ_UPDATE_FLOW_START;
+
+	status = i40e_asq_send_command(hw, &desc, NULL, 0, cmd_details);
+
+	return status;
+}
+
+/**
+ * i40e_aq_min_rollback_rev_update - triggers an ow after update
+ * @hw: pointer to the hw struct
+ * @mode: opt-in mode, 1b for single module update, 0b for bulk update
+ * @module: module to be updated. Ignored if mode is 0b
+ * @min_rrev: value of the new minimal version. Ignored if mode is 0b
+ * @cmd_details: pointer to command details structure or NULL
+ **/
+enum i40e_status_code
+i40e_aq_min_rollback_rev_update(struct i40e_hw *hw, u8 mode, u8 module,
+				u32 min_rrev,
+				struct i40e_asq_cmd_details *cmd_details)
+{
+	struct i40e_aq_desc desc;
+	struct i40e_aqc_rollback_revision_update *cmd =
+		(struct i40e_aqc_rollback_revision_update *)&desc.params.raw;
+	i40e_status status;
+
+	i40e_fill_default_direct_cmd_desc(&desc, i40e_aqc_opc_rollback_revision_update);
+	cmd->optin_mode = mode;
+	cmd->module_selected = module;
+	cmd->min_rrev = min_rrev;
+
+	status = i40e_asq_send_command(hw, &desc, NULL, 0, cmd_details);
+
+	return status;
+}
+
+/**
+ * i40e_aq_oem_post_update - triggers an OEM specific flow after update
+ * @hw: pointer to the hw struct
+ * @buff: buffer for result
+ * @buff_size: buffer size
+ * @cmd_details: pointer to command details structure or NULL
+ **/
+i40e_status i40e_aq_oem_post_update(struct i40e_hw *hw,
+				void *buff, u16 buff_size,
+				struct i40e_asq_cmd_details *cmd_details)
+{
+	struct i40e_aq_desc desc;
+	i40e_status status;
+
+
+	i40e_fill_default_direct_cmd_desc(&desc, i40e_aqc_opc_oem_post_update);
+	status = i40e_asq_send_command(hw, &desc, NULL, 0, cmd_details);
+	if (status && LE16_TO_CPU(desc.retval) == I40E_AQ_RC_ESRCH)
+		status = I40E_ERR_NOT_IMPLEMENTED;
+
+	return status;
+}
+
+/**
  * i40e_aq_erase_nvm
  * @hw: pointer to the hw struct
  * @module_pointer: module pointer location in words from the NVM beginning
@@ -3173,8 +3478,8 @@
  * Erase the NVM sector using the admin queue commands
  **/
 i40e_status i40e_aq_erase_nvm(struct i40e_hw *hw, u8 module_pointer,
-			      u32 offset, u16 length, bool last_command,
-			      struct i40e_asq_cmd_details *cmd_details)
+				u32 offset, u16 length, bool last_command,
+				struct i40e_asq_cmd_details *cmd_details)
 {
 	struct i40e_aq_desc desc;
 	struct i40e_aqc_nvm_update *cmd =
@@ -3193,8 +3498,8 @@
 	if (last_command)
 		cmd->command_flags |= I40E_AQ_NVM_LAST_CMD;
 	cmd->module_pointer = module_pointer;
-	cmd->offset = cpu_to_le32(offset);
-	cmd->length = cpu_to_le16(length);
+	cmd->offset = CPU_TO_LE32(offset);
+	cmd->length = CPU_TO_LE16(length);
 
 	status = i40e_asq_send_command(hw, &desc, NULL, 0, cmd_details);
 
@@ -3219,30 +3524,33 @@
 	u32 valid_functions, num_functions;
 	u32 number, logical_id, phys_id;
 	struct i40e_hw_capabilities *p;
-	u16 id, ocp_cfg_word0;
 	i40e_status status;
+	u16 id, ocp_cfg_word0;
 	u8 major_rev;
 	u32 i = 0;
 
 	cap = (struct i40e_aqc_list_capabilities_element_resp *) buff;
 
 	if (list_type_opc == i40e_aqc_opc_list_dev_capabilities)
-		p = &hw->dev_caps;
+		p = (struct i40e_hw_capabilities *)&hw->dev_caps;
 	else if (list_type_opc == i40e_aqc_opc_list_func_capabilities)
-		p = &hw->func_caps;
+		p = (struct i40e_hw_capabilities *)&hw->func_caps;
 	else
 		return;
 
 	for (i = 0; i < cap_count; i++, cap++) {
-		id = le16_to_cpu(cap->id);
-		number = le32_to_cpu(cap->number);
-		logical_id = le32_to_cpu(cap->logical_id);
-		phys_id = le32_to_cpu(cap->phys_id);
+		id = LE16_TO_CPU(cap->id);
+		number = LE32_TO_CPU(cap->number);
+		logical_id = LE32_TO_CPU(cap->logical_id);
+		phys_id = LE32_TO_CPU(cap->phys_id);
 		major_rev = cap->major_rev;
 
 		switch (id) {
 		case I40E_AQ_CAP_ID_SWITCH_MODE:
 			p->switch_mode = number;
+			i40e_debug(hw, I40E_DEBUG_INIT,
+				   "HW Capability: Switch mode = %d\n",
+				   p->switch_mode);
 			break;
 		case I40E_AQ_CAP_ID_MNG_MODE:
 			p->management_mode = number;
@@ -3254,38 +3562,67 @@
 			} else {
 				p->mng_protocols_over_mctp = 0;
 			}
+			i40e_debug(hw, I40E_DEBUG_INIT,
+				   "HW Capability: Management Mode = %d\n",
+				   p->management_mode);
 			break;
 		case I40E_AQ_CAP_ID_NPAR_ACTIVE:
 			p->npar_enable = number;
+			i40e_debug(hw, I40E_DEBUG_INIT,
+				   "HW Capability: NPAR enable = %d\n",
+				   p->npar_enable);
 			break;
 		case I40E_AQ_CAP_ID_OS2BMC_CAP:
 			p->os2bmc = number;
+			i40e_debug(hw, I40E_DEBUG_INIT,
+				   "HW Capability: OS2BMC = %d\n", p->os2bmc);
 			break;
 		case I40E_AQ_CAP_ID_FUNCTIONS_VALID:
 			p->valid_functions = number;
+			i40e_debug(hw, I40E_DEBUG_INIT,
+				   "HW Capability: Valid Functions = %d\n",
+				   p->valid_functions);
 			break;
 		case I40E_AQ_CAP_ID_SRIOV:
 			if (number == 1)
 				p->sr_iov_1_1 = true;
+			i40e_debug(hw, I40E_DEBUG_INIT,
+				   "HW Capability: SR-IOV = %d\n",
+				   p->sr_iov_1_1);
 			break;
 		case I40E_AQ_CAP_ID_VF:
 			p->num_vfs = number;
 			p->vf_base_id = logical_id;
+			i40e_debug(hw, I40E_DEBUG_INIT,
+				   "HW Capability: VF count = %d\n",
+				   p->num_vfs);
+			i40e_debug(hw, I40E_DEBUG_INIT,
+				   "HW Capability: VF base_id = %d\n",
+				   p->vf_base_id);
 			break;
 		case I40E_AQ_CAP_ID_VMDQ:
 			if (number == 1)
 				p->vmdq = true;
+			i40e_debug(hw, I40E_DEBUG_INIT,
+				   "HW Capability: VMDQ = %d\n", p->vmdq);
 			break;
 		case I40E_AQ_CAP_ID_8021QBG:
 			if (number == 1)
 				p->evb_802_1_qbg = true;
+			i40e_debug(hw, I40E_DEBUG_INIT,
+				   "HW Capability: 802.1Qbg = %d\n", number);
 			break;
 		case I40E_AQ_CAP_ID_8021QBR:
 			if (number == 1)
 				p->evb_802_1_qbh = true;
+			i40e_debug(hw, I40E_DEBUG_INIT,
+				   "HW Capability: 802.1Qbh = %d\n", number);
 			break;
 		case I40E_AQ_CAP_ID_VSI:
 			p->num_vsis = number;
+			i40e_debug(hw, I40E_DEBUG_INIT,
+				   "HW Capability: VSI count = %d\n",
+				   p->num_vsis);
 			break;
 		case I40E_AQ_CAP_ID_DCB:
 			if (number == 1) {
@@ -3293,27 +3630,56 @@
 				p->enabled_tcmap = logical_id;
 				p->maxtc = phys_id;
 			}
+			i40e_debug(hw, I40E_DEBUG_INIT,
+				   "HW Capability: DCB = %d\n", p->dcb);
+			i40e_debug(hw, I40E_DEBUG_INIT,
+				   "HW Capability: TC Mapping = %d\n",
+				   logical_id);
+			i40e_debug(hw, I40E_DEBUG_INIT,
+				   "HW Capability: TC Max = %d\n", p->maxtc);
 			break;
 		case I40E_AQ_CAP_ID_FCOE:
 			if (number == 1)
 				p->fcoe = true;
+			i40e_debug(hw, I40E_DEBUG_INIT,
+				   "HW Capability: FCOE = %d\n", p->fcoe);
 			break;
 		case I40E_AQ_CAP_ID_ISCSI:
 			if (number == 1)
 				p->iscsi = true;
+			i40e_debug(hw, I40E_DEBUG_INIT,
+				   "HW Capability: iSCSI = %d\n", p->iscsi);
 			break;
 		case I40E_AQ_CAP_ID_RSS:
 			p->rss = true;
 			p->rss_table_size = number;
 			p->rss_table_entry_width = logical_id;
+			i40e_debug(hw, I40E_DEBUG_INIT,
+				   "HW Capability: RSS = %d\n", p->rss);
+			i40e_debug(hw, I40E_DEBUG_INIT,
+				   "HW Capability: RSS table size = %d\n",
+				   p->rss_table_size);
+			i40e_debug(hw, I40E_DEBUG_INIT,
+				   "HW Capability: RSS table width = %d\n",
+				   p->rss_table_entry_width);
 			break;
 		case I40E_AQ_CAP_ID_RXQ:
 			p->num_rx_qp = number;
 			p->base_queue = phys_id;
+			i40e_debug(hw, I40E_DEBUG_INIT,
+				   "HW Capability: Rx QP = %d\n", number);
+			i40e_debug(hw, I40E_DEBUG_INIT,
+				   "HW Capability: base_queue = %d\n",
+				   p->base_queue);
 			break;
 		case I40E_AQ_CAP_ID_TXQ:
 			p->num_tx_qp = number;
 			p->base_queue = phys_id;
+			i40e_debug(hw, I40E_DEBUG_INIT,
+				   "HW Capability: Tx QP = %d\n", number);
+			i40e_debug(hw, I40E_DEBUG_INIT,
+				   "HW Capability: base_queue = %d\n",
+				   p->base_queue);
 			break;
 		case I40E_AQ_CAP_ID_MSIX:
 			p->num_msix_vectors = number;
@@ -3323,6 +3689,9 @@
 			break;
 		case I40E_AQ_CAP_ID_VF_MSIX:
 			p->num_msix_vectors_vf = number;
+			i40e_debug(hw, I40E_DEBUG_INIT,
+				   "HW Capability: MSIX VF vector count = %d\n",
+				   p->num_msix_vectors_vf);
 			break;
 		case I40E_AQ_CAP_ID_FLEX10:
 			if (major_rev == 1) {
@@ -3339,41 +3708,78 @@
 			}
 			p->flex10_mode = logical_id;
 			p->flex10_status = phys_id;
+			i40e_debug(hw, I40E_DEBUG_INIT,
+				   "HW Capability: Flex10 mode = %d\n",
+				   p->flex10_mode);
+			i40e_debug(hw, I40E_DEBUG_INIT,
+				   "HW Capability: Flex10 status = %d\n",
+				   p->flex10_status);
 			break;
 		case I40E_AQ_CAP_ID_CEM:
 			if (number == 1)
 				p->mgmt_cem = true;
+			i40e_debug(hw, I40E_DEBUG_INIT,
+				   "HW Capability: CEM = %d\n", p->mgmt_cem);
 			break;
 		case I40E_AQ_CAP_ID_IWARP:
 			if (number == 1)
 				p->iwarp = true;
+			i40e_debug(hw, I40E_DEBUG_INIT,
+				   "HW Capability: iWARP = %d\n", p->iwarp);
 			break;
 		case I40E_AQ_CAP_ID_LED:
 			if (phys_id < I40E_HW_CAP_MAX_GPIO)
 				p->led[phys_id] = true;
+			i40e_debug(hw, I40E_DEBUG_INIT,
+				   "HW Capability: LED - PIN %d\n", phys_id);
 			break;
 		case I40E_AQ_CAP_ID_SDP:
 			if (phys_id < I40E_HW_CAP_MAX_GPIO)
 				p->sdp[phys_id] = true;
+			i40e_debug(hw, I40E_DEBUG_INIT,
+				   "HW Capability: SDP - PIN %d\n", phys_id);
 			break;
 		case I40E_AQ_CAP_ID_MDIO:
 			if (number == 1) {
 				p->mdio_port_num = phys_id;
 				p->mdio_port_mode = logical_id;
 			}
+			i40e_debug(hw, I40E_DEBUG_INIT,
+				   "HW Capability: MDIO port number = %d\n",
+				   p->mdio_port_num);
+			i40e_debug(hw, I40E_DEBUG_INIT,
+				   "HW Capability: MDIO port mode = %d\n",
+				   p->mdio_port_mode);
 			break;
 		case I40E_AQ_CAP_ID_1588:
 			if (number == 1)
 				p->ieee_1588 = true;
+			i40e_debug(hw, I40E_DEBUG_INIT,
+				   "HW Capability: IEEE 1588 = %d\n",
+				   p->ieee_1588);
 			break;
 		case I40E_AQ_CAP_ID_FLOW_DIRECTOR:
 			p->fd = true;
 			p->fd_filters_guaranteed = number;
 			p->fd_filters_best_effort = logical_id;
+			i40e_debug(hw, I40E_DEBUG_INIT,
+				   "HW Capability: Flow Director = 1\n");
+			i40e_debug(hw, I40E_DEBUG_INIT,
+				   "HW Capability: Guaranteed FD filters = %d\n",
+				   p->fd_filters_guaranteed);
 			break;
 		case I40E_AQ_CAP_ID_WSR_PROT:
 			p->wr_csr_prot = (u64)number;
 			p->wr_csr_prot |= (u64)logical_id << 32;
+			i40e_debug(hw, I40E_DEBUG_INIT,
+				   "HW Capability: wr_csr_prot = 0x%llX\n\n",
+				   (unsigned long long)(p->wr_csr_prot & 0xffff));
+			break;
+		case I40E_AQ_CAP_ID_DIS_UNUSED_PORTS:
+			p->dis_unused_ports = (bool)number;
+			i40e_debug(hw, I40E_DEBUG_INIT,
+				   "HW Capability: dis_unused_ports = %d\n\n",
+				   p->dis_unused_ports);
 			break;
 		case I40E_AQ_CAP_ID_NVM_MGMT:
 			if (number & I40E_NVM_MGMT_SEC_REV_DISABLED)
@@ -3381,6 +3787,19 @@
 			if (number & I40E_NVM_MGMT_UPDATE_DISABLED)
 				p->update_disabled = true;
 			break;
+		case I40E_AQ_CAP_ID_WOL_AND_PROXY:
+			hw->num_wol_proxy_filters = (u16)number;
+			hw->wol_proxy_vsi_seid = (u16)logical_id;
+			p->apm_wol_support = phys_id & I40E_WOL_SUPPORT_MASK;
+			if (phys_id & I40E_ACPI_PROGRAMMING_METHOD_MASK)
+				p->acpi_prog_method = I40E_ACPI_PROGRAMMING_METHOD_AQC_FPK;
+			else
+				p->acpi_prog_method = I40E_ACPI_PROGRAMMING_METHOD_HW_FVL;
+			p->proxy_support = (phys_id & I40E_PROXY_SUPPORT_MASK) ? 1 : 0;
+			i40e_debug(hw, I40E_DEBUG_INIT,
+				   "HW Capability: WOL proxy filters = %d\n",
+				   hw->num_wol_proxy_filters);
+			break;
 		default:
 			break;
 		}
@@ -3389,11 +3808,8 @@
 	if (p->fcoe)
 		i40e_debug(hw, I40E_DEBUG_ALL, "device is FCoE capable\n");
 
-	/* Software override ensuring FCoE is disabled if npar or mfp
-	 * mode because it is not supported in these modes.
-	 */
-	if (p->npar_enable || p->flex10_enable)
-		p->fcoe = false;
+	/* Always disable FCoE if compiled without the I40E_FCOE_ENA flag */
+	p->fcoe = false;
 
 	/* count the enabled ports (aka the "not disabled" ports) */
 	hw->num_ports = 0;
@@ -3409,7 +3825,7 @@
 			hw->num_ports++;
 	}
 
-	/* OCP cards case: if a mezz is removed the Ethernet port is at
+	/* OCP cards case: if a mezz is removed the ethernet port is at
 	 * disabled state in PRTGEN_CNF register. Additional NVM read is
 	 * needed in order to check if we are dealing with OCP card.
 	 * Those cards have 4 PFs at minimum, so using PRTGEN_CNF for counting
@@ -3417,12 +3833,12 @@
 	 * not supporting WoL.
 	 */
 	if (hw->mac.type == I40E_MAC_X722) {
-		if (!i40e_acquire_nvm(hw, I40E_RESOURCE_READ)) {
+		if (i40e_acquire_nvm(hw, I40E_RESOURCE_READ) == I40E_SUCCESS) {
 			status = i40e_aq_read_nvm(hw, I40E_SR_EMP_MODULE_PTR,
 						  2 * I40E_SR_OCP_CFG_WORD0,
 						  sizeof(ocp_cfg_word0),
 						  &ocp_cfg_word0, true, NULL);
-			if (!status &&
+			if (status == I40E_SUCCESS &&
 			    (ocp_cfg_word0 & I40E_SR_OCP_ENABLED))
 				hw->num_ports = 4;
 			i40e_release_nvm(hw);
@@ -3469,7 +3885,7 @@
 {
 	struct i40e_aqc_list_capabilites *cmd;
 	struct i40e_aq_desc desc;
-	i40e_status status = 0;
+	i40e_status status = I40E_SUCCESS;
 
 	cmd = (struct i40e_aqc_list_capabilites *)&desc.params.raw;
 
@@ -3481,17 +3897,17 @@
 
 	i40e_fill_default_direct_cmd_desc(&desc, list_type_opc);
 
-	desc.flags |= cpu_to_le16((u16)I40E_AQ_FLAG_BUF);
+	desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_BUF);
 	if (buff_size > I40E_AQ_LARGE_BUF)
-		desc.flags |= cpu_to_le16((u16)I40E_AQ_FLAG_LB);
+		desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_LB);
 
 	status = i40e_asq_send_command(hw, &desc, buff, buff_size, cmd_details);
-	*data_size = le16_to_cpu(desc.datalen);
+	*data_size = LE16_TO_CPU(desc.datalen);
 
 	if (status)
 		goto exit;
 
-	i40e_parse_discover_capabilities(hw, buff, le32_to_cpu(cmd->count),
+	i40e_parse_discover_capabilities(hw, buff, LE32_TO_CPU(cmd->count),
 					 list_type_opc);
 
 exit:
@@ -3512,9 +3928,9 @@
  * Update the NVM using the admin queue commands
  **/
 i40e_status i40e_aq_update_nvm(struct i40e_hw *hw, u8 module_pointer,
-			       u32 offset, u16 length, void *data,
+				u32 offset, u16 length, void *data,
 				bool last_command, u8 preservation_flags,
-			       struct i40e_asq_cmd_details *cmd_details)
+				struct i40e_asq_cmd_details *cmd_details)
 {
 	struct i40e_aq_desc desc;
 	struct i40e_aqc_nvm_update *cmd =
@@ -3543,12 +3959,12 @@
 				 I40E_AQ_NVM_PRESERVATION_FLAGS_SHIFT);
 	}
 	cmd->module_pointer = module_pointer;
-	cmd->offset = cpu_to_le32(offset);
-	cmd->length = cpu_to_le16(length);
+	cmd->offset = CPU_TO_LE32(offset);
+	cmd->length = CPU_TO_LE16(length);
 
-	desc.flags |= cpu_to_le16((u16)(I40E_AQ_FLAG_BUF | I40E_AQ_FLAG_RD));
+	desc.flags |= CPU_TO_LE16((u16)(I40E_AQ_FLAG_BUF | I40E_AQ_FLAG_RD));
 	if (length > I40E_AQ_LARGE_BUF)
-		desc.flags |= cpu_to_le16((u16)I40E_AQ_FLAG_LB);
+		desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_LB);
 
 	status = i40e_asq_send_command(hw, &desc, data, length, cmd_details);
 
@@ -3565,8 +3981,8 @@
  * Rearrange NVM structure, available only for transition FW
  **/
 i40e_status i40e_aq_rearrange_nvm(struct i40e_hw *hw,
-				  u8 rearrange_nvm,
-				  struct i40e_asq_cmd_details *cmd_details)
+				u8 rearrange_nvm,
+				struct i40e_asq_cmd_details *cmd_details)
 {
 	struct i40e_aqc_nvm_update *cmd;
 	i40e_status status;
@@ -3621,29 +4037,68 @@
 
 	i40e_fill_default_direct_cmd_desc(&desc, i40e_aqc_opc_lldp_get_mib);
 	/* Indirect Command */
-	desc.flags |= cpu_to_le16((u16)I40E_AQ_FLAG_BUF);
+	desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_BUF);
 
 	cmd->type = mib_type & I40E_AQ_LLDP_MIB_TYPE_MASK;
 	cmd->type |= ((bridge_type << I40E_AQ_LLDP_BRIDGE_TYPE_SHIFT) &
 		       I40E_AQ_LLDP_BRIDGE_TYPE_MASK);
 
-	desc.datalen = cpu_to_le16(buff_size);
+	desc.datalen = CPU_TO_LE16(buff_size);
 
-	desc.flags |= cpu_to_le16((u16)I40E_AQ_FLAG_BUF);
+	desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_BUF);
 	if (buff_size > I40E_AQ_LARGE_BUF)
-		desc.flags |= cpu_to_le16((u16)I40E_AQ_FLAG_LB);
+		desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_LB);
 
 	status = i40e_asq_send_command(hw, &desc, buff, buff_size, cmd_details);
 	if (!status) {
 		if (local_len != NULL)
-			*local_len = le16_to_cpu(resp->local_len);
+			*local_len = LE16_TO_CPU(resp->local_len);
 		if (remote_len != NULL)
-			*remote_len = le16_to_cpu(resp->remote_len);
+			*remote_len = LE16_TO_CPU(resp->remote_len);
 	}
 
 	return status;
 }
 
+ /**
+ * i40e_aq_set_lldp_mib - Set the LLDP MIB
+ * @hw: pointer to the hw struct
+ * @mib_type: Local, Remote or both Local and Remote MIBs
+ * @buff: pointer to a user supplied buffer to store the MIB block
+ * @buff_size: size of the buffer (in bytes)
+ * @cmd_details: pointer to command details structure or NULL
+ *
+ * Set the LLDP MIB.
+ **/
+i40e_status i40e_aq_set_lldp_mib(struct i40e_hw *hw,
+				u8 mib_type, void *buff, u16 buff_size,
+				struct i40e_asq_cmd_details *cmd_details)
+{
+	struct i40e_aq_desc desc;
+	struct i40e_aqc_lldp_set_local_mib *cmd =
+		(struct i40e_aqc_lldp_set_local_mib *)&desc.params.raw;
+	i40e_status status;
+
+	if (buff_size == 0 || !buff)
+		return I40E_ERR_PARAM;
+
+	i40e_fill_default_direct_cmd_desc(&desc,
+				i40e_aqc_opc_lldp_set_local_mib);
+	/* Indirect Command */
+	desc.flags |= CPU_TO_LE16((u16)(I40E_AQ_FLAG_BUF | I40E_AQ_FLAG_RD));
+	if (buff_size > I40E_AQ_LARGE_BUF)
+		desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_LB);
+	desc.datalen = CPU_TO_LE16(buff_size);
+
+	cmd->type = mib_type;
+	cmd->length = CPU_TO_LE16(buff_size);
+	cmd->address_high = CPU_TO_LE32(upper_32_bits((u64)buff));
+	cmd->address_low =  CPU_TO_LE32(lower_32_bits((u64)buff));
+
+	status = i40e_asq_send_command(hw, &desc, buff, buff_size, cmd_details);
+	return status;
+}
+
 /**
  * i40e_aq_cfg_lldp_mib_change_event
  * @hw: pointer to the hw struct
@@ -3749,15 +4204,14 @@
 /**
  * i40e_aq_start_lldp
  * @hw: pointer to the hw struct
- * @buff: buffer for result
  * @persist: True if start of LLDP should be persistent across power cycles
- * @buff_size: buffer size
  * @cmd_details: pointer to command details structure or NULL
  *
  * Start the embedded LLDP Agent on all ports.
  **/
-i40e_status i40e_aq_start_lldp(struct i40e_hw *hw, bool persist,
-			       struct i40e_asq_cmd_details *cmd_details)
+i40e_status i40e_aq_start_lldp(struct i40e_hw *hw,
+				bool persist,
+				struct i40e_asq_cmd_details *cmd_details)
 {
 	struct i40e_aq_desc desc;
 	struct i40e_aqc_lldp_start *cmd =
@@ -3822,8 +4276,8 @@
  * Get CEE DCBX mode operational configuration from firmware
  **/
 i40e_status i40e_aq_get_cee_dcb_config(struct i40e_hw *hw,
-				       void *buff, u16 buff_size,
-				       struct i40e_asq_cmd_details *cmd_details)
+				void *buff, u16 buff_size,
+				struct i40e_asq_cmd_details *cmd_details)
 {
 	struct i40e_aq_desc desc;
 	i40e_status status;
@@ -3833,7 +4287,7 @@
 
 	i40e_fill_default_direct_cmd_desc(&desc, i40e_aqc_opc_get_cee_dcb_cfg);
 
-	desc.flags |= cpu_to_le16((u16)I40E_AQ_FLAG_BUF);
+	desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_BUF);
 	status = i40e_asq_send_command(hw, &desc, (void *)buff, buff_size,
 				       cmd_details);
 
@@ -3841,6 +4295,36 @@
 }
 
 /**
+ * i40e_aq_start_stop_dcbx - Start/Stop DCBx service in FW
+ * @hw: pointer to the hw struct
+ * @start_agent: True if DCBx Agent needs to be Started
+ *				False if DCBx Agent needs to be Stopped
+ * @cmd_details: pointer to command details structure or NULL
+ *
+ * Start/Stop the embedded dcbx Agent
+ **/
+i40e_status i40e_aq_start_stop_dcbx(struct i40e_hw *hw,
+				bool start_agent,
+				struct i40e_asq_cmd_details *cmd_details)
+{
+	struct i40e_aq_desc desc;
+	struct i40e_aqc_lldp_stop_start_specific_agent *cmd =
+		(struct i40e_aqc_lldp_stop_start_specific_agent *)
+				&desc.params.raw;
+	i40e_status status;
+
+	i40e_fill_default_direct_cmd_desc(&desc,
+				i40e_aqc_opc_lldp_stop_start_spec_agent);
+
+	if (start_agent)
+		cmd->command = I40E_AQC_START_SPECIFIC_AGENT_MASK;
+
+	status = i40e_asq_send_command(hw, &desc, NULL, 0, cmd_details);
+
+	return status;
+}
+
+/**
  * i40e_aq_add_udp_tunnel
  * @hw: pointer to the hw struct
  * @udp_port: the UDP port to add in Host byte order
@@ -3849,7 +4333,7 @@
  * @cmd_details: pointer to command details structure or NULL
  *
  * Note: Firmware expects the udp_port value to be in Little Endian format,
- * and this function will call cpu_to_le16 to convert from Host byte order to
+ * and this function will call CPU_TO_LE16 to convert from Host byte order to
  * Little Endian order.
  **/
 i40e_status i40e_aq_add_udp_tunnel(struct i40e_hw *hw,
@@ -3866,7 +4350,7 @@
 
 	i40e_fill_default_direct_cmd_desc(&desc, i40e_aqc_opc_add_udp_tunnel);
 
-	cmd->udp_port = cpu_to_le16(udp_port);
+	cmd->udp_port = CPU_TO_LE16(udp_port);
 	cmd->protocol_type = protocol_index;
 
 	status = i40e_asq_send_command(hw, &desc, NULL, 0, cmd_details);
@@ -3901,6 +4385,45 @@
 }
 
 /**
+ * i40e_aq_get_switch_resource_alloc - command (0x0204) to get allocations
+ * @hw: pointer to the hw struct
+ * @num_entries: pointer to u8 to store the number of resource entries returned
+ * @buf: pointer to a user supplied buffer.  This buffer must be large enough
+ *        to store the resource information for all resource types.  Each
+ *        resource type is a i40e_aqc_switch_resource_alloc_data structure.
+ * @count: size, in bytes, of the buffer provided
+ * @cmd_details: pointer to command details structure or NULL
+ *
+ * Query the resources allocated to a function.
+ **/
+i40e_status i40e_aq_get_switch_resource_alloc(struct i40e_hw *hw,
+			u8 *num_entries,
+			struct i40e_aqc_switch_resource_alloc_element_resp *buf,
+			u16 count,
+			struct i40e_asq_cmd_details *cmd_details)
+{
+	struct i40e_aq_desc desc;
+	struct i40e_aqc_get_switch_resource_alloc *cmd_resp =
+		(struct i40e_aqc_get_switch_resource_alloc *)&desc.params.raw;
+	i40e_status status;
+	u16 length = count * sizeof(*buf);
+
+	i40e_fill_default_direct_cmd_desc(&desc,
+					i40e_aqc_opc_get_switch_resource_alloc);
+
+	desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_BUF);
+	if (length > I40E_AQ_LARGE_BUF)
+		desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_LB);
+
+	status = i40e_asq_send_command(hw, &desc, buf, length, cmd_details);
+
+	if (!status && num_entries)
+		*num_entries = cmd_resp->num_entries;
+
+	return status;
+}
+
+/**
  * i40e_aq_delete_element - Delete switch element
  * @hw: pointer to the hw struct
  * @seid: the SEID to delete from the switch
@@ -3921,9 +4444,9 @@
 
 	i40e_fill_default_direct_cmd_desc(&desc, i40e_aqc_opc_delete_element);
 
-	cmd->seid = cpu_to_le16(seid);
-
-	status = i40e_asq_send_command(hw, &desc, NULL, 0, cmd_details);
+	cmd->seid = CPU_TO_LE16(seid);
+	status = i40e_asq_send_command_atomic(hw, &desc, NULL, 0,
+					      cmd_details, true);
 
 	return status;
 }
@@ -3933,6 +4456,14 @@
  * @hw: pointer to the hw struct
  * @cmd_details: pointer to command details structure or NULL
  *
+ * When LLDP is handled in PF this command is used by the PF
+ * to notify EMP that a DCB setting is modified.
+ * When LLDP is handled in EMP this command is used by the PF
+ * to notify EMP whenever one of the following parameters get
+ * modified:
+ *   - PFCLinkDelayAllowance in PRTDCB_GENC.PFCLDA
+ *   - PCIRTT in PRTDCB_GENC.PCIRTT
+ *   - Maximum Frame Size for non-FCoE TCs set by PRTDCB_TDPUC.MAX_TXFRAME.
  * EMP will return when the shared RPB settings have been
  * recomputed and modified. The retval field in the descriptor
  * will be set to 0 when RPB is modified.
@@ -3949,6 +4480,43 @@
 
 	return status;
 }
+/**
+ * i40e_aq_set_port_parameters - set physical port parameters.
+ * @hw: pointer to the hw struct
+ * @bad_frame_vsi: defines the VSI to which bad frames are forwarded
+ * @save_bad_pac: if set packets with errors are forwarded to the bad frames VSI
+ * @pad_short_pac: if set transmit packets smaller than 60 bytes are padded
+ * @double_vlan: if set double VLAN is enabled
+ * @cmd_details: pointer to command details structure or NULL
+ **/
+i40e_status i40e_aq_set_port_parameters(struct i40e_hw *hw,
+				u16 bad_frame_vsi, bool save_bad_pac,
+				bool pad_short_pac, bool double_vlan,
+				struct i40e_asq_cmd_details *cmd_details)
+{
+	struct i40e_aqc_set_port_parameters *cmd;
+	i40e_status status;
+	struct i40e_aq_desc desc;
+	u16 command_flags = 0;
+
+	cmd = (struct i40e_aqc_set_port_parameters *)&desc.params.raw;
+
+	i40e_fill_default_direct_cmd_desc(&desc,
+					  i40e_aqc_opc_set_port_parameters);
+
+	cmd->bad_frame_vsi = CPU_TO_LE16(bad_frame_vsi);
+	if (save_bad_pac)
+		command_flags |= I40E_AQ_SET_P_PARAMS_SAVE_BAD_PACKETS;
+	if (pad_short_pac)
+		command_flags |= I40E_AQ_SET_P_PARAMS_PAD_SHORT_PACKETS;
+	if (double_vlan)
+		command_flags |= I40E_AQ_SET_P_PARAMS_DOUBLE_VLAN_ENA;
+	cmd->command_flags = CPU_TO_LE16(command_flags);
+
+	status = i40e_asq_send_command(hw, &desc, NULL, 0, cmd_details);
+
+	return status;
+}
 
 /**
  * i40e_aq_tx_sched_cmd - generic Tx scheduler AQ command handler
@@ -3996,15 +4564,15 @@
 	i40e_fill_default_direct_cmd_desc(&desc, opcode);
 
 	/* Indirect command */
-	desc.flags |= cpu_to_le16((u16)I40E_AQ_FLAG_BUF);
+	desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_BUF);
 	if (cmd_param_flag)
-		desc.flags |= cpu_to_le16((u16)I40E_AQ_FLAG_RD);
+		desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_RD);
 	if (buff_size > I40E_AQ_LARGE_BUF)
-		desc.flags |= cpu_to_le16((u16)I40E_AQ_FLAG_LB);
+		desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_LB);
 
-	desc.datalen = cpu_to_le16(buff_size);
+	desc.datalen = CPU_TO_LE16(buff_size);
 
-	cmd->vsi_seid = cpu_to_le16(seid);
+	cmd->vsi_seid = CPU_TO_LE16(seid);
 
 	status = i40e_asq_send_command(hw, &desc, buff, buff_size, cmd_details);
 
@@ -4031,8 +4599,8 @@
 	i40e_fill_default_direct_cmd_desc(&desc,
 					  i40e_aqc_opc_configure_vsi_bw_limit);
 
-	cmd->vsi_seid = cpu_to_le16(seid);
-	cmd->credit = cpu_to_le16(credit);
+	cmd->vsi_seid = CPU_TO_LE16(seid);
+	cmd->credit = CPU_TO_LE16(credit);
 	cmd->max_credit = max_credit;
 
 	status = i40e_asq_send_command(hw, &desc, NULL, 0, cmd_details);
@@ -4041,6 +4609,23 @@
 }
 
 /**
+ * i40e_aq_config_vsi_ets_sla_bw_limit - Config VSI BW Limit per TC
+ * @hw: pointer to the hw struct
+ * @seid: VSI seid
+ * @bw_data: Buffer holding enabled TCs, per TC BW limit/credits
+ * @cmd_details: pointer to command details structure or NULL
+ **/
+i40e_status i40e_aq_config_vsi_ets_sla_bw_limit(struct i40e_hw *hw,
+			u16 seid,
+			struct i40e_aqc_configure_vsi_ets_sla_bw_data *bw_data,
+			struct i40e_asq_cmd_details *cmd_details)
+{
+	return i40e_aq_tx_sched_cmd(hw, seid, (void *)bw_data, sizeof(*bw_data),
+				    i40e_aqc_opc_configure_vsi_ets_sla_bw_limit,
+				    cmd_details);
+}
+
+/**
  * i40e_aq_config_vsi_tc_bw - Config VSI BW Allocation per TC
  * @hw: pointer to the hw struct
  * @seid: VSI seid
@@ -4093,6 +4678,23 @@
 }
 
 /**
+ * i40e_aq_config_switch_comp_ets_bw_limit - Config Switch comp BW Limit per TC
+ * @hw: pointer to the hw struct
+ * @seid: seid of the switching component
+ * @bw_data: Buffer holding enabled TCs, per TC BW limit/credits
+ * @cmd_details: pointer to command details structure or NULL
+ **/
+i40e_status i40e_aq_config_switch_comp_ets_bw_limit(
+	struct i40e_hw *hw, u16 seid,
+	struct i40e_aqc_configure_switching_comp_ets_bw_limit_data *bw_data,
+	struct i40e_asq_cmd_details *cmd_details)
+{
+	return i40e_aq_tx_sched_cmd(hw, seid, (void *)bw_data, sizeof(*bw_data),
+			    i40e_aqc_opc_configure_switching_comp_ets_bw_limit,
+			    cmd_details);
+}
+
+/**
  * i40e_aq_query_vsi_bw_config - Query VSI BW configuration
  * @hw: pointer to the hw struct
  * @seid: seid of the VSI
@@ -4186,7 +4788,7 @@
  * The function checks for the valid filter/context sizes being
  * passed for FCoE and PE.
  *
- * Returns 0 if the values passed are valid and within
+ * Returns I40E_SUCCESS if the values passed are valid and within
  * range else returns an error.
  **/
 static i40e_status i40e_validate_filter_settings(struct i40e_hw *hw,
@@ -4195,6 +4797,7 @@
 	u32 fcoe_cntx_size, fcoe_filt_size;
 	u32 pe_cntx_size, pe_filt_size;
 	u32 fcoe_fmax;
+
 	u32 val;
 
 	/* Validate FCoE settings passed */
@@ -4269,7 +4872,7 @@
 	if (fcoe_filt_size + fcoe_cntx_size >  fcoe_fmax)
 		return I40E_ERR_INVALID_SIZE;
 
-	return 0;
+	return I40E_SUCCESS;
 }
 
 /**
@@ -4284,7 +4887,7 @@
 i40e_status i40e_set_filter_control(struct i40e_hw *hw,
 				struct i40e_filter_control_settings *settings)
 {
-	i40e_status ret = 0;
+	i40e_status ret = I40E_SUCCESS;
 	u32 hash_lut_size = 0;
 	u32 val;
 
@@ -4336,7 +4939,7 @@
 
 	i40e_write_rx_ctl(hw, I40E_PFQF_CTL_0, val);
 
-	return 0;
+	return I40E_SUCCESS;
 }
 
 /**
@@ -4376,7 +4979,7 @@
 	if (is_add) {
 		i40e_fill_default_direct_cmd_desc(&desc,
 				i40e_aqc_opc_add_control_packet_filter);
-		cmd->queue = cpu_to_le16(queue);
+		cmd->queue = CPU_TO_LE16(queue);
 	} else {
 		i40e_fill_default_direct_cmd_desc(&desc,
 				i40e_aqc_opc_remove_control_packet_filter);
@@ -4385,17 +4988,17 @@
 	if (mac_addr)
 		ether_addr_copy(cmd->mac, mac_addr);
 
-	cmd->etype = cpu_to_le16(ethtype);
-	cmd->flags = cpu_to_le16(flags);
-	cmd->seid = cpu_to_le16(vsi_seid);
+	cmd->etype = CPU_TO_LE16(ethtype);
+	cmd->flags = CPU_TO_LE16(flags);
+	cmd->seid = CPU_TO_LE16(vsi_seid);
 
 	status = i40e_asq_send_command(hw, &desc, NULL, 0, cmd_details);
 
 	if (!status && stats) {
-		stats->mac_etype_used = le16_to_cpu(resp->mac_etype_used);
-		stats->etype_used = le16_to_cpu(resp->etype_used);
-		stats->mac_etype_free = le16_to_cpu(resp->mac_etype_free);
-		stats->etype_free = le16_to_cpu(resp->etype_free);
+		stats->mac_etype_used = LE16_TO_CPU(resp->mac_etype_used);
+		stats->etype_used = LE16_TO_CPU(resp->etype_used);
+		stats->mac_etype_free = LE16_TO_CPU(resp->mac_etype_free);
+		stats->etype_free = LE16_TO_CPU(resp->etype_free);
 	}
 
 	return status;
@@ -4424,6 +5027,278 @@
 }
 
 /**
+ * i40e_fix_up_geneve_vni - adjust Geneve VNI for HW issue
+ * @filters: list of cloud filters
+ * @filter_count: length of list
+ *
+ * There's an issue in the device where the Geneve VNI layout needs
+ * to be shifted 1 byte over from the VxLAN VNI
+ **/
+static void i40e_fix_up_geneve_vni(
+	struct i40e_aqc_cloud_filters_element_data *filters,
+	u8 filter_count)
+{
+	struct i40e_aqc_cloud_filters_element_data *f = filters;
+	int i;
+
+	for (i = 0; i < filter_count; i++) {
+		u16 tnl_type;
+		u32 ti;
+
+		tnl_type = (LE16_TO_CPU(f[i].flags) &
+			   I40E_AQC_ADD_CLOUD_TNL_TYPE_MASK) >>
+			   I40E_AQC_ADD_CLOUD_TNL_TYPE_SHIFT;
+		if (tnl_type == I40E_AQC_ADD_CLOUD_TNL_TYPE_GENEVE) {
+			ti = LE32_TO_CPU(f[i].tenant_id);
+			f[i].tenant_id = CPU_TO_LE32(ti << 8);
+		}
+	}
+}
+
+/**
+ * i40e_aq_add_cloud_filters
+ * @hw: pointer to the hardware structure
+ * @seid: VSI seid to add cloud filters from
+ * @filters: Buffer which contains the filters to be added
+ * @filter_count: number of filters contained in the buffer
+ *
+ * Set the cloud filters for a given VSI.  The contents of the
+ * i40e_aqc_cloud_filters_element_data are filled
+ * in by the caller of the function.
+ *
+ **/
+i40e_status i40e_aq_add_cloud_filters(struct i40e_hw *hw,
+	u16 seid,
+	struct i40e_aqc_cloud_filters_element_data *filters,
+	u8 filter_count)
+{
+	struct i40e_aq_desc desc;
+	struct i40e_aqc_add_remove_cloud_filters *cmd =
+	(struct i40e_aqc_add_remove_cloud_filters *)&desc.params.raw;
+	i40e_status status;
+	u16 buff_len;
+
+	i40e_fill_default_direct_cmd_desc(&desc,
+					  i40e_aqc_opc_add_cloud_filters);
+
+	buff_len = filter_count * sizeof(*filters);
+	desc.datalen = CPU_TO_LE16(buff_len);
+	desc.flags |= CPU_TO_LE16((u16)(I40E_AQ_FLAG_BUF | I40E_AQ_FLAG_RD));
+	cmd->num_filters = filter_count;
+	cmd->seid = CPU_TO_LE16(seid);
+
+	i40e_fix_up_geneve_vni(filters, filter_count);
+
+	status = i40e_asq_send_command(hw, &desc, filters, buff_len, NULL);
+
+	return status;
+}
+
+/**
+ * i40e_aq_add_cloud_filters_bb
+ * @hw: pointer to the hardware structure
+ * @seid: VSI seid to add cloud filters from
+ * @filters: Buffer which contains the filters in big buffer to be added
+ * @filter_count: number of filters contained in the buffer
+ *
+ * Set the cloud filters for a given VSI.  The contents of the
+ * i40e_aqc_cloud_filters_element_bb are filled in by the caller of the
+ * the function.
+ *
+ **/
+enum i40e_status_code
+i40e_aq_add_cloud_filters_bb(struct i40e_hw *hw, u16 seid,
+			     struct i40e_aqc_cloud_filters_element_bb *filters,
+			     u8 filter_count)
+{
+	struct i40e_aq_desc desc;
+	struct i40e_aqc_add_remove_cloud_filters *cmd =
+	(struct i40e_aqc_add_remove_cloud_filters *)&desc.params.raw;
+	i40e_status status;
+	u16 buff_len;
+	int i;
+
+	i40e_fill_default_direct_cmd_desc(&desc,
+					  i40e_aqc_opc_add_cloud_filters);
+
+	buff_len = filter_count * sizeof(*filters);
+	desc.datalen = CPU_TO_LE16(buff_len);
+	desc.flags |= CPU_TO_LE16((u16)(I40E_AQ_FLAG_BUF | I40E_AQ_FLAG_RD));
+	cmd->num_filters = filter_count;
+	cmd->seid = CPU_TO_LE16(seid);
+	cmd->big_buffer_flag = I40E_AQC_ADD_CLOUD_CMD_BB;
+
+	for (i = 0; i < filter_count; i++) {
+		u16 tnl_type;
+		u32 ti;
+
+		tnl_type = (LE16_TO_CPU(filters[i].element.flags) &
+			   I40E_AQC_ADD_CLOUD_TNL_TYPE_MASK) >>
+			   I40E_AQC_ADD_CLOUD_TNL_TYPE_SHIFT;
+
+		/* Due to hardware eccentricities, the VNI for Geneve is shifted
+		 * one more byte further than normally used for Tenant ID in
+		 * other tunnel types.
+		 */
+		if (tnl_type == I40E_AQC_ADD_CLOUD_TNL_TYPE_GENEVE) {
+			ti = LE32_TO_CPU(filters[i].element.tenant_id);
+			filters[i].element.tenant_id = CPU_TO_LE32(ti << 8);
+		}
+	}
+
+	status = i40e_asq_send_command(hw, &desc, filters, buff_len, NULL);
+
+	return status;
+}
+
+/**
+ * i40e_aq_rem_cloud_filters
+ * @hw: pointer to the hardware structure
+ * @seid: VSI seid to remove cloud filters from
+ * @filters: Buffer which contains the filters to be removed
+ * @filter_count: number of filters contained in the buffer
+ *
+ * Remove the cloud filters for a given VSI.  The contents of the
+ * i40e_aqc_cloud_filters_element_data are filled in by the caller
+ * of the function.
+ *
+ **/
+enum i40e_status_code
+i40e_aq_rem_cloud_filters(struct i40e_hw *hw, u16 seid,
+			  struct i40e_aqc_cloud_filters_element_data *filters,
+			  u8 filter_count)
+{
+	struct i40e_aq_desc desc;
+	struct i40e_aqc_add_remove_cloud_filters *cmd =
+	(struct i40e_aqc_add_remove_cloud_filters *)&desc.params.raw;
+	i40e_status status;
+	u16 buff_len;
+
+	i40e_fill_default_direct_cmd_desc(&desc,
+					  i40e_aqc_opc_remove_cloud_filters);
+
+	buff_len = filter_count * sizeof(*filters);
+	desc.datalen = CPU_TO_LE16(buff_len);
+	desc.flags |= CPU_TO_LE16((u16)(I40E_AQ_FLAG_BUF | I40E_AQ_FLAG_RD));
+	cmd->num_filters = filter_count;
+	cmd->seid = CPU_TO_LE16(seid);
+
+	i40e_fix_up_geneve_vni(filters, filter_count);
+
+	status = i40e_asq_send_command(hw, &desc, filters, buff_len, NULL);
+
+	return status;
+}
+
+/**
+ * i40e_aq_rem_cloud_filters_bb
+ * @hw: pointer to the hardware structure
+ * @seid: VSI seid to remove cloud filters from
+ * @filters: Buffer which contains the filters in big buffer to be removed
+ * @filter_count: number of filters contained in the buffer
+ *
+ * Remove the big buffer cloud filters for a given VSI.  The contents of the
+ * i40e_aqc_cloud_filters_element_bb are filled in by the caller of the
+ * function.
+ *
+ **/
+enum i40e_status_code
+i40e_aq_rem_cloud_filters_bb(struct i40e_hw *hw, u16 seid,
+			     struct i40e_aqc_cloud_filters_element_bb *filters,
+			     u8 filter_count)
+{
+	struct i40e_aq_desc desc;
+	struct i40e_aqc_add_remove_cloud_filters *cmd =
+	(struct i40e_aqc_add_remove_cloud_filters *)&desc.params.raw;
+	i40e_status status;
+	u16 buff_len;
+	int i;
+
+	i40e_fill_default_direct_cmd_desc(&desc,
+					  i40e_aqc_opc_remove_cloud_filters);
+
+	buff_len = filter_count * sizeof(*filters);
+	desc.datalen = CPU_TO_LE16(buff_len);
+	desc.flags |= CPU_TO_LE16((u16)(I40E_AQ_FLAG_BUF | I40E_AQ_FLAG_RD));
+	cmd->num_filters = filter_count;
+	cmd->seid = CPU_TO_LE16(seid);
+	cmd->big_buffer_flag = I40E_AQC_ADD_CLOUD_CMD_BB;
+
+	for (i = 0; i < filter_count; i++) {
+		u16 tnl_type;
+		u32 ti;
+
+		tnl_type = (LE16_TO_CPU(filters[i].element.flags) &
+			   I40E_AQC_ADD_CLOUD_TNL_TYPE_MASK) >>
+			   I40E_AQC_ADD_CLOUD_TNL_TYPE_SHIFT;
+
+		/* Due to hardware eccentricities, the VNI for Geneve is shifted
+		 * one more byte further than normally used for Tenant ID in
+		 * other tunnel types.
+		 */
+		if (tnl_type == I40E_AQC_ADD_CLOUD_TNL_TYPE_GENEVE) {
+			ti = LE32_TO_CPU(filters[i].element.tenant_id);
+			filters[i].element.tenant_id = CPU_TO_LE32(ti << 8);
+		}
+	}
+
+	status = i40e_asq_send_command(hw, &desc, filters, buff_len, NULL);
+
+	return status;
+}
+
+/**
+ * i40e_aq_replace_cloud_filters - Replace cloud filter command
+ * @hw: pointer to the hw struct
+ * @filters: pointer to the i40e_aqc_replace_cloud_filter_cmd struct
+ * @cmd_buf: pointer to the i40e_aqc_replace_cloud_filter_cmd_buf struct
+ *
+ **/
+enum
+i40e_status_code i40e_aq_replace_cloud_filters(struct i40e_hw *hw,
+	struct i40e_aqc_replace_cloud_filters_cmd *filters,
+	struct i40e_aqc_replace_cloud_filters_cmd_buf *cmd_buf)
+{
+	struct i40e_aq_desc desc;
+	struct i40e_aqc_replace_cloud_filters_cmd *cmd =
+		(struct i40e_aqc_replace_cloud_filters_cmd *)&desc.params.raw;
+	i40e_status status = I40E_SUCCESS;
+	int i = 0;
+
+	/* X722 doesn't support this command */
+	if (hw->mac.type == I40E_MAC_X722)
+		return I40E_ERR_DEVICE_NOT_SUPPORTED;
+
+	/* need FW version greater than 6.00 */
+	if (hw->aq.fw_maj_ver < 6)
+		return I40E_NOT_SUPPORTED;
+
+	i40e_fill_default_direct_cmd_desc(&desc,
+					  i40e_aqc_opc_replace_cloud_filters);
+
+	desc.datalen = CPU_TO_LE16(32);
+	desc.flags |= CPU_TO_LE16((u16)(I40E_AQ_FLAG_BUF | I40E_AQ_FLAG_RD));
+	cmd->old_filter_type = filters->old_filter_type;
+	cmd->new_filter_type = filters->new_filter_type;
+	cmd->valid_flags = filters->valid_flags;
+	cmd->tr_bit = filters->tr_bit;
+	cmd->tr_bit2 = filters->tr_bit2;
+
+	status = i40e_asq_send_command(hw, &desc, cmd_buf,
+		sizeof(struct i40e_aqc_replace_cloud_filters_cmd_buf),  NULL);
+
+	/* for get cloud filters command */
+	for (i = 0; i < 32; i += 4) {
+		cmd_buf->filters[i / 4].filter_type = cmd_buf->data[i];
+		cmd_buf->filters[i / 4].input[0] = cmd_buf->data[i + 1];
+		cmd_buf->filters[i / 4].input[1] = cmd_buf->data[i + 2];
+		cmd_buf->filters[i / 4].input[2] = cmd_buf->data[i + 3];
+	}
+
+	return status;
+}
+
+/**
  * i40e_aq_alternate_read
  * @hw: pointer to the hardware structure
  * @reg_addr0: address of first dword to be read
@@ -4436,35 +5311,60 @@
  * is not passed then only register at 'reg_addr0' is read.
  *
  **/
-static i40e_status i40e_aq_alternate_read(struct i40e_hw *hw,
-					  u32 reg_addr0, u32 *reg_val0,
-					  u32 reg_addr1, u32 *reg_val1)
+i40e_status i40e_aq_alternate_read(struct i40e_hw *hw,
+				u32 reg_addr0, u32 *reg_val0,
+				u32 reg_addr1, u32 *reg_val1)
 {
 	struct i40e_aq_desc desc;
 	struct i40e_aqc_alternate_write *cmd_resp =
 		(struct i40e_aqc_alternate_write *)&desc.params.raw;
 	i40e_status status;
 
-	if (!reg_val0)
+	if (reg_val0 == NULL)
 		return I40E_ERR_PARAM;
 
 	i40e_fill_default_direct_cmd_desc(&desc, i40e_aqc_opc_alternate_read);
-	cmd_resp->address0 = cpu_to_le32(reg_addr0);
-	cmd_resp->address1 = cpu_to_le32(reg_addr1);
+	cmd_resp->address0 = CPU_TO_LE32(reg_addr0);
+	cmd_resp->address1 = CPU_TO_LE32(reg_addr1);
 
 	status = i40e_asq_send_command(hw, &desc, NULL, 0, NULL);
 
-	if (!status) {
-		*reg_val0 = le32_to_cpu(cmd_resp->data0);
+	if (status == I40E_SUCCESS) {
+		*reg_val0 = LE32_TO_CPU(cmd_resp->data0);
 
-		if (reg_val1)
-			*reg_val1 = le32_to_cpu(cmd_resp->data1);
+		if (reg_val1 != NULL)
+			*reg_val1 = LE32_TO_CPU(cmd_resp->data1);
 	}
 
 	return status;
 }
 
 /**
+ * i40e_aq_suspend_port_tx
+ * @hw: pointer to the hardware structure
+ * @seid: port seid
+ * @cmd_details: pointer to command details structure or NULL
+ *
+ * Suspend port's Tx traffic
+ **/
+i40e_status i40e_aq_suspend_port_tx(struct i40e_hw *hw, u16 seid,
+				struct i40e_asq_cmd_details *cmd_details)
+{
+	struct i40e_aq_desc desc;
+	i40e_status status;
+	struct i40e_aqc_tx_sched_ind *cmd =
+		(struct i40e_aqc_tx_sched_ind *)&desc.params.raw;
+
+	i40e_fill_default_direct_cmd_desc(&desc, i40e_aqc_opc_suspend_port_tx);
+
+	cmd->vsi_seid = CPU_TO_LE16(seid);
+
+	status = i40e_asq_send_command(hw, &desc, NULL, 0, cmd_details);
+
+	return status;
+}
+
+/**
  * i40e_aq_resume_port_tx
  * @hw: pointer to the hardware structure
  * @cmd_details: pointer to command details structure or NULL
@@ -4472,7 +5372,7 @@
  * Resume port's Tx traffic
  **/
 i40e_status i40e_aq_resume_port_tx(struct i40e_hw *hw,
-				   struct i40e_asq_cmd_details *cmd_details)
+				struct i40e_asq_cmd_details *cmd_details)
 {
 	struct i40e_aq_desc desc;
 	i40e_status status;
@@ -4546,10 +5446,10 @@
  *
  **/
 i40e_status i40e_aq_debug_dump(struct i40e_hw *hw, u8 cluster_id,
-			       u8 table_id, u32 start_index, u16 buff_size,
-			       void *buff, u16 *ret_buff_size,
-			       u8 *ret_next_table, u32 *ret_next_index,
-			       struct i40e_asq_cmd_details *cmd_details)
+				u8 table_id, u32 start_index, u16 buff_size,
+				void *buff, u16 *ret_buff_size,
+				u8 *ret_next_table, u32 *ret_next_index,
+				struct i40e_asq_cmd_details *cmd_details)
 {
 	struct i40e_aq_desc desc;
 	struct i40e_aqc_debug_dump_internals *cmd =
@@ -4564,24 +5464,24 @@
 	i40e_fill_default_direct_cmd_desc(&desc,
 					  i40e_aqc_opc_debug_dump_internals);
 	/* Indirect Command */
-	desc.flags |= cpu_to_le16((u16)I40E_AQ_FLAG_BUF);
+	desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_BUF);
 	if (buff_size > I40E_AQ_LARGE_BUF)
-		desc.flags |= cpu_to_le16((u16)I40E_AQ_FLAG_LB);
+		desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_LB);
 
 	cmd->cluster_id = cluster_id;
 	cmd->table_id = table_id;
-	cmd->idx = cpu_to_le32(start_index);
+	cmd->idx = CPU_TO_LE32(start_index);
 
-	desc.datalen = cpu_to_le16(buff_size);
+	desc.datalen = CPU_TO_LE16(buff_size);
 
 	status = i40e_asq_send_command(hw, &desc, buff, buff_size, cmd_details);
 	if (!status) {
-		if (ret_buff_size)
-			*ret_buff_size = le16_to_cpu(desc.datalen);
-		if (ret_next_table)
+		if (ret_buff_size != NULL)
+			*ret_buff_size = LE16_TO_CPU(desc.datalen);
+		if (ret_next_table != NULL)
 			*ret_next_table = resp->table_id;
-		if (ret_next_index)
-			*ret_next_index = le32_to_cpu(resp->idx);
+		if (ret_next_index != NULL)
+			*ret_next_index = LE32_TO_CPU(resp->idx);
 	}
 
 	return status;
@@ -4598,8 +5498,8 @@
  * Read bw from the alternate ram for the given pf
  **/
 i40e_status i40e_read_bw_from_alt_ram(struct i40e_hw *hw,
-				      u32 *max_bw, u32 *min_bw,
-				      bool *min_valid, bool *max_valid)
+					u32 *max_bw, u32 *min_bw,
+					bool *min_valid, bool *max_valid)
 {
 	i40e_status status;
 	u32 max_bw_addr, min_bw_addr;
@@ -4646,19 +5546,15 @@
 	u16 bwd_size = sizeof(*bw_data);
 
 	i40e_fill_default_direct_cmd_desc(&desc,
-					  i40e_aqc_opc_configure_partition_bw);
+				i40e_aqc_opc_configure_partition_bw);
 
 	/* Indirect command */
-	desc.flags |= cpu_to_le16((u16)I40E_AQ_FLAG_BUF);
-	desc.flags |= cpu_to_le16((u16)I40E_AQ_FLAG_RD);
+	desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_BUF);
+	desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_RD);
 
-	if (bwd_size > I40E_AQ_LARGE_BUF)
-		desc.flags |= cpu_to_le16((u16)I40E_AQ_FLAG_LB);
+	desc.datalen = CPU_TO_LE16(bwd_size);
 
-	desc.datalen = cpu_to_le16(bwd_size);
-
-	status = i40e_asq_send_command(hw, &desc, bw_data, bwd_size,
-				       cmd_details);
+	status = i40e_asq_send_command(hw, &desc, bw_data, bwd_size, cmd_details);
 
 	return status;
 }
@@ -4673,7 +5569,7 @@
  * Reads specified PHY register value
  **/
 i40e_status i40e_read_phy_register_clause22(struct i40e_hw *hw,
-					    u16 reg, u8 phy_addr, u16 *value)
+					u16 reg, u8 phy_addr, u16 *value)
 {
 	i40e_status status = I40E_ERR_TIMEOUT;
 	u8 port_num = (u8)hw->func_caps.mdio_port_num;
@@ -4689,7 +5585,7 @@
 	do {
 		command = rd32(hw, I40E_GLGEN_MSCA(port_num));
 		if (!(command & I40E_GLGEN_MSCA_MDICMD_MASK)) {
-			status = 0;
+			status = I40E_SUCCESS;
 			break;
 		}
 		udelay(10);
@@ -4718,7 +5614,7 @@
  * Writes specified PHY register value
  **/
 i40e_status i40e_write_phy_register_clause22(struct i40e_hw *hw,
-					     u16 reg, u8 phy_addr, u16 value)
+					u16 reg, u8 phy_addr, u16 value)
 {
 	i40e_status status = I40E_ERR_TIMEOUT;
 	u8 port_num = (u8)hw->func_caps.mdio_port_num;
@@ -4738,7 +5634,7 @@
 	do {
 		command = rd32(hw, I40E_GLGEN_MSCA(port_num));
 		if (!(command & I40E_GLGEN_MSCA_MDICMD_MASK)) {
-			status = 0;
+			status = I40E_SUCCESS;
 			break;
 		}
 		udelay(10);
@@ -4762,9 +5658,9 @@
 				u8 page, u16 reg, u8 phy_addr, u16 *value)
 {
 	i40e_status status = I40E_ERR_TIMEOUT;
-	u32 command = 0;
+	u32 command  = 0;
 	u16 retry = 1000;
-	u8 port_num = hw->func_caps.mdio_port_num;
+	u8 port_num = (u8)hw->func_caps.mdio_port_num;
 
 	command = (reg << I40E_GLGEN_MSCA_MDIADD_SHIFT) |
 		  (page << I40E_GLGEN_MSCA_DEVADD_SHIFT) |
@@ -4777,10 +5673,10 @@
 	do {
 		command = rd32(hw, I40E_GLGEN_MSCA(port_num));
 		if (!(command & I40E_GLGEN_MSCA_MDICMD_MASK)) {
-			status = 0;
+			status = I40E_SUCCESS;
 			break;
 		}
-		usleep_range(10, 20);
+		udelay(10);
 		retry--;
 	} while (retry);
 
@@ -4802,10 +5698,10 @@
 	do {
 		command = rd32(hw, I40E_GLGEN_MSCA(port_num));
 		if (!(command & I40E_GLGEN_MSCA_MDICMD_MASK)) {
-			status = 0;
+			status = I40E_SUCCESS;
 			break;
 		}
-		usleep_range(10, 20);
+		udelay(10);
 		retry--;
 	} while (retry);
 
@@ -4836,9 +5732,9 @@
 				u8 page, u16 reg, u8 phy_addr, u16 value)
 {
 	i40e_status status = I40E_ERR_TIMEOUT;
-	u32 command = 0;
+	u32 command  = 0;
 	u16 retry = 1000;
-	u8 port_num = hw->func_caps.mdio_port_num;
+	u8 port_num = (u8)hw->func_caps.mdio_port_num;
 
 	command = (reg << I40E_GLGEN_MSCA_MDIADD_SHIFT) |
 		  (page << I40E_GLGEN_MSCA_DEVADD_SHIFT) |
@@ -4851,10 +5747,10 @@
 	do {
 		command = rd32(hw, I40E_GLGEN_MSCA(port_num));
 		if (!(command & I40E_GLGEN_MSCA_MDICMD_MASK)) {
-			status = 0;
+			status = I40E_SUCCESS;
 			break;
 		}
-		usleep_range(10, 20);
+		udelay(10);
 		retry--;
 	} while (retry);
 	if (status) {
@@ -4878,10 +5774,10 @@
 	do {
 		command = rd32(hw, I40E_GLGEN_MSCA(port_num));
 		if (!(command & I40E_GLGEN_MSCA_MDICMD_MASK)) {
-			status = 0;
+			status = I40E_SUCCESS;
 			break;
 		}
-		usleep_range(10, 20);
+		udelay(10);
 		retry--;
 	} while (retry);
 
@@ -4900,22 +5796,24 @@
  * Writes value to specified PHY register
  **/
 i40e_status i40e_write_phy_register(struct i40e_hw *hw,
-				    u8 page, u16 reg, u8 phy_addr, u16 value)
+				u8 page, u16 reg, u8 phy_addr, u16 value)
 {
 	i40e_status status;
 
 	switch (hw->device_id) {
 	case I40E_DEV_ID_1G_BASE_T_X722:
-		status = i40e_write_phy_register_clause22(hw, reg, phy_addr,
-							  value);
+		status = i40e_write_phy_register_clause22(hw,
+			reg, phy_addr, value);
 		break;
 	case I40E_DEV_ID_10G_BASE_T:
 	case I40E_DEV_ID_10G_BASE_T4:
+	case I40E_DEV_ID_10G_BASE_T_BC:
+	case I40E_DEV_ID_5G_BASE_T_BC:
 	case I40E_DEV_ID_10G_BASE_T_X722:
 	case I40E_DEV_ID_25G_B:
 	case I40E_DEV_ID_25G_SFP28:
-		status = i40e_write_phy_register_clause45(hw, page, reg,
-							  phy_addr, value);
+		status = i40e_write_phy_register_clause45(hw,
+			page, reg, phy_addr, value);
 		break;
 	default:
 		status = I40E_ERR_UNKNOWN_PHY;
@@ -4936,7 +5834,7 @@
  * Reads specified PHY register value
  **/
 i40e_status i40e_read_phy_register(struct i40e_hw *hw,
-				   u8 page, u16 reg, u8 phy_addr, u16 *value)
+				u8 page, u16 reg, u8 phy_addr, u16 *value)
 {
 	i40e_status status;
 
@@ -4948,6 +5846,7 @@
 	case I40E_DEV_ID_10G_BASE_T:
 	case I40E_DEV_ID_10G_BASE_T4:
 	case I40E_DEV_ID_10G_BASE_T_BC:
+	case I40E_DEV_ID_5G_BASE_T_BC:
 	case I40E_DEV_ID_10G_BASE_T_X722:
 	case I40E_DEV_ID_25G_B:
 	case I40E_DEV_ID_25G_SFP28:
@@ -4971,14 +5870,14 @@
  **/
 u8 i40e_get_phy_address(struct i40e_hw *hw, u8 dev_num)
 {
-	u8 port_num = hw->func_caps.mdio_port_num;
+	u8 port_num = (u8)hw->func_caps.mdio_port_num;
 	u32 reg_val = rd32(hw, I40E_GLGEN_MDIO_I2C_SEL(port_num));
 
 	return (u8)(reg_val >> ((dev_num + 1) * 5)) & 0x1f;
 }
 
 /**
- * i40e_blink_phy_led
+ * i40e_blink_phy_link_led
  * @hw: pointer to the HW structure
  * @time: time how long led will blinks in secs
  * @interval: gap between LED on and off in msecs
@@ -4986,11 +5885,11 @@
  * Blinks PHY link LED
  **/
 i40e_status i40e_blink_phy_link_led(struct i40e_hw *hw,
-				    u32 time, u32 interval)
+					      u32 time, u32 interval)
 {
-	i40e_status status = 0;
+	i40e_status status = I40E_SUCCESS;
 	u32 i;
-	u16 led_ctl;
+	u16 led_ctl = 0;
 	u16 gpio_led_port;
 	u16 led_reg;
 	u16 led_addr = I40E_PHY_LED_PROV_REG_1;
@@ -5057,26 +5956,21 @@
  * @led_addr: LED register address
  * @reg_val: read register value
  **/
-static enum i40e_status_code i40e_led_get_reg(struct i40e_hw *hw, u16 led_addr,
+static i40e_status i40e_led_get_reg(struct i40e_hw *hw, u16 led_addr,
 					      u32 *reg_val)
 {
-	enum i40e_status_code status;
+	i40e_status status;
 	u8 phy_addr = 0;
-	u8 port_num;
-	u32 i;
 
 	*reg_val = 0;
 	if (hw->flags & I40E_HW_FLAG_AQ_PHY_ACCESS_CAPABLE) {
-		status =
-		       i40e_aq_get_phy_register(hw,
+		status = i40e_aq_get_phy_register(hw,
 						I40E_AQ_PHY_REG_ACCESS_EXTERNAL,
-						I40E_PHY_COM_REG_PAGE,
+						I40E_PHY_COM_REG_PAGE, true,
 						I40E_PHY_LED_PROV_REG_1,
 						reg_val, NULL);
 	} else {
-		i = rd32(hw, I40E_PFGEN_PORTNUM);
-		port_num = (u8)(i & I40E_PFGEN_PORTNUM_PORT_NUM_MASK);
-		phy_addr = i40e_get_phy_address(hw, port_num);
+		phy_addr = i40e_get_phy_address(hw, hw->port);
 		status = i40e_read_phy_register_clause45(hw,
 							 I40E_PHY_COM_REG_PAGE,
 							 led_addr, phy_addr,
@@ -5091,25 +5985,20 @@
  * @led_addr: LED register address
  * @reg_val: register value to write
  **/
-static enum i40e_status_code i40e_led_set_reg(struct i40e_hw *hw, u16 led_addr,
+static i40e_status i40e_led_set_reg(struct i40e_hw *hw, u16 led_addr,
 					      u32 reg_val)
 {
-	enum i40e_status_code status;
+	i40e_status status;
 	u8 phy_addr = 0;
-	u8 port_num;
-	u32 i;
 
 	if (hw->flags & I40E_HW_FLAG_AQ_PHY_ACCESS_CAPABLE) {
-		status =
-		       i40e_aq_set_phy_register(hw,
+		status = i40e_aq_set_phy_register(hw,
 						I40E_AQ_PHY_REG_ACCESS_EXTERNAL,
-						I40E_PHY_COM_REG_PAGE,
+						I40E_PHY_COM_REG_PAGE, true,
 						I40E_PHY_LED_PROV_REG_1,
 						reg_val, NULL);
 	} else {
-		i = rd32(hw, I40E_PFGEN_PORTNUM);
-		port_num = (u8)(i & I40E_PFGEN_PORTNUM_PORT_NUM_MASK);
-		phy_addr = i40e_get_phy_address(hw, port_num);
+		phy_addr = i40e_get_phy_address(hw, hw->port);
 		status = i40e_write_phy_register_clause45(hw,
 							  I40E_PHY_COM_REG_PAGE,
 							  led_addr, phy_addr,
@@ -5127,33 +6016,27 @@
  *
  **/
 i40e_status i40e_led_get_phy(struct i40e_hw *hw, u16 *led_addr,
-			     u16 *val)
+				       u16 *val)
 {
-	i40e_status status = 0;
+	i40e_status status = I40E_SUCCESS;
 	u16 gpio_led_port;
+	u32 reg_val_aq;
+	u16 temp_addr;
 	u8 phy_addr = 0;
 	u16 reg_val;
-	u16 temp_addr;
-	u8 port_num;
-	u32 i;
-	u32 reg_val_aq;
 
 	if (hw->flags & I40E_HW_FLAG_AQ_PHY_ACCESS_CAPABLE) {
-		status =
-		      i40e_aq_get_phy_register(hw,
-					       I40E_AQ_PHY_REG_ACCESS_EXTERNAL,
-					       I40E_PHY_COM_REG_PAGE,
-					       I40E_PHY_LED_PROV_REG_1,
-					       &reg_val_aq, NULL);
+		status = i40e_aq_get_phy_register(hw,
+						I40E_AQ_PHY_REG_ACCESS_EXTERNAL,
+						I40E_PHY_COM_REG_PAGE, true,
+						I40E_PHY_LED_PROV_REG_1,
+						&reg_val_aq, NULL);
 		if (status == I40E_SUCCESS)
 			*val = (u16)reg_val_aq;
 		return status;
 	}
 	temp_addr = I40E_PHY_LED_PROV_REG_1;
-	i = rd32(hw, I40E_PFGEN_PORTNUM);
-	port_num = (u8)(i & I40E_PFGEN_PORTNUM_PORT_NUM_MASK);
-	phy_addr = i40e_get_phy_address(hw, port_num);
-
+	phy_addr = i40e_get_phy_address(hw, hw->port);
 	for (gpio_led_port = 0; gpio_led_port < 3; gpio_led_port++,
 	     temp_addr++) {
 		status = i40e_read_phy_register_clause45(hw,
@@ -5182,9 +6065,9 @@
  *
  **/
 i40e_status i40e_led_set_phy(struct i40e_hw *hw, bool on,
-			     u16 led_addr, u32 mode)
+				       u16 led_addr, u32 mode)
 {
-	i40e_status status = 0;
+	i40e_status status = I40E_SUCCESS;
 	u32 led_ctl = 0;
 	u32 led_reg = 0;
 
@@ -5205,7 +6088,6 @@
 		led_reg = I40E_PHY_LED_MANUAL_ON;
 	else
 		led_reg = 0;
-
 	status = i40e_led_set_reg(hw, led_addr, led_reg);
 	if (status)
 		goto restore_config;
@@ -5221,6 +6103,204 @@
 }
 
 /**
+ * i40e_get_phy_lpi_status - read LPI status from PHY or MAC register
+ * @hw: pointer to the hw struct
+ * @stat: pointer to structure with status of rx and tx lpi
+ *
+ * Read LPI state directly from external PHY register or from MAC
+ * register, depending on device ID and current link speed.
+ */
+i40e_status i40e_get_phy_lpi_status(struct i40e_hw *hw,
+					      struct i40e_hw_port_stats *stat)
+{
+	i40e_status ret = I40E_SUCCESS;
+	bool eee_mrvl_phy;
+	bool eee_bcm_phy;
+	u32 val;
+
+	stat->rx_lpi_status = 0;
+	stat->tx_lpi_status = 0;
+
+	eee_bcm_phy =
+		(hw->device_id == I40E_DEV_ID_10G_BASE_T_BC ||
+		 hw->device_id == I40E_DEV_ID_5G_BASE_T_BC) &&
+		(hw->phy.link_info.link_speed == I40E_LINK_SPEED_2_5GB ||
+		 hw->phy.link_info.link_speed == I40E_LINK_SPEED_5GB);
+	eee_mrvl_phy =
+		hw->device_id == I40E_DEV_ID_1G_BASE_T_X722;
+
+	if (eee_bcm_phy || eee_mrvl_phy) {
+		// read Clause 45 PCS Status 1 register
+		ret = i40e_aq_get_phy_register(hw,
+					       I40E_AQ_PHY_REG_ACCESS_EXTERNAL,
+					       I40E_BCM_PHY_PCS_STATUS1_PAGE,
+					       true,
+					       I40E_BCM_PHY_PCS_STATUS1_REG,
+					       &val, NULL);
+
+		if (ret != I40E_SUCCESS)
+			return ret;
+
+		stat->rx_lpi_status = !!(val & I40E_BCM_PHY_PCS_STATUS1_RX_LPI);
+		stat->tx_lpi_status = !!(val & I40E_BCM_PHY_PCS_STATUS1_TX_LPI);
+
+		return ret;
+	}
+
+	val = rd32(hw, I40E_PRTPM_EEE_STAT);
+	stat->rx_lpi_status = (val & I40E_PRTPM_EEE_STAT_RX_LPI_STATUS_MASK) >>
+			       I40E_PRTPM_EEE_STAT_RX_LPI_STATUS_SHIFT;
+	stat->tx_lpi_status = (val & I40E_PRTPM_EEE_STAT_TX_LPI_STATUS_MASK) >>
+			       I40E_PRTPM_EEE_STAT_TX_LPI_STATUS_SHIFT;
+
+	return ret;
+}
+
+/**
+ * i40e_get_lpi_counters - read LPI counters from EEE statistics
+ * @hw: pointer to the hw struct
+ * @tx_counter: pointer to memory for TX LPI counter
+ * @rx_counter: pointer to memory for RX LPI counter
+ * @is_clear:   returns true if counters are clear after read
+ *
+ * Read Low Power Idle (LPI) mode counters from Energy Efficient
+ * Ethernet (EEE) statistics.
+ **/
+i40e_status i40e_get_lpi_counters(struct i40e_hw *hw,
+					    u32 *tx_counter, u32 *rx_counter,
+					    bool *is_clear)
+{
+	/* only X710-T*L requires special handling of counters
+	 * for other devices we just read the MAC registers
+	 */
+	if ((hw->device_id == I40E_DEV_ID_10G_BASE_T_BC ||
+	     hw->device_id == I40E_DEV_ID_5G_BASE_T_BC) &&
+	     hw->phy.link_info.link_speed != I40E_LINK_SPEED_1GB) {
+		i40e_status retval;
+		u32 cmd_status;
+
+		*is_clear = false;
+		retval = i40e_aq_run_phy_activity(hw,
+				I40E_AQ_RUN_PHY_ACT_ID_USR_DFND,
+				I40E_AQ_RUN_PHY_ACT_DNL_OPCODE_GET_EEE_STAT,
+				&cmd_status, tx_counter, rx_counter, NULL);
+
+		if (!retval && cmd_status != I40E_AQ_RUN_PHY_ACT_CMD_STAT_SUCC)
+			retval = I40E_ERR_ADMIN_QUEUE_ERROR;
+
+		return retval;
+	}
+
+	*is_clear = true;
+	*tx_counter = rd32(hw, I40E_PRTPM_TLPIC);
+	*rx_counter = rd32(hw, I40E_PRTPM_RLPIC);
+
+	return I40E_SUCCESS;
+}
+
+/**
+ * i40e_get_lpi_duration - read LPI time duration from EEE statistics
+ * @hw: pointer to the hw struct
+ * @stat: pointer to structure with status of rx and tx lpi
+ * @tx_duration: pointer to memory for TX LPI time duration
+ * @rx_duration: pointer to memory for RX LPI time duration
+ *
+ * Read Low Power Idle (LPI) mode time duration from Energy Efficient
+ * Ethernet (EEE) statistics.
+ */
+i40e_status i40e_get_lpi_duration(struct i40e_hw *hw,
+					    struct i40e_hw_port_stats *stat,
+					    u64 *tx_duration, u64 *rx_duration)
+{
+	u32 tx_time_dur, rx_time_dur;
+	i40e_status retval;
+	u32 cmd_status;
+
+	if (hw->device_id != I40E_DEV_ID_10G_BASE_T_BC &&
+	    hw->device_id != I40E_DEV_ID_5G_BASE_T_BC)
+		return I40E_ERR_NOT_IMPLEMENTED;
+
+	retval = i40e_aq_run_phy_activity
+		(hw, I40E_AQ_RUN_PHY_ACT_ID_USR_DFND,
+		I40E_AQ_RUN_PHY_ACT_DNL_OPCODE_GET_EEE_DUR,
+		&cmd_status, &tx_time_dur, &rx_time_dur, NULL);
+
+	if (retval)
+		return retval;
+	if ((cmd_status & I40E_AQ_RUN_PHY_ACT_CMD_STAT_MASK) !=
+	    I40E_AQ_RUN_PHY_ACT_CMD_STAT_SUCC)
+		return I40E_ERR_ADMIN_QUEUE_ERROR;
+
+	if (hw->phy.link_info.link_speed == I40E_LINK_SPEED_1GB &&
+	    !tx_time_dur && !rx_time_dur &&
+	    stat->tx_lpi_status && stat->rx_lpi_status) {
+		retval = i40e_aq_run_phy_activity
+			(hw, I40E_AQ_RUN_PHY_ACT_ID_USR_DFND,
+			I40E_AQ_RUN_PHY_ACT_DNL_OPCODE_GET_EEE_STAT_DUR,
+			&cmd_status,
+			&tx_time_dur, &rx_time_dur, NULL);
+
+		if (retval)
+			return retval;
+		if ((cmd_status & I40E_AQ_RUN_PHY_ACT_CMD_STAT_MASK) !=
+		    I40E_AQ_RUN_PHY_ACT_CMD_STAT_SUCC)
+			return I40E_ERR_ADMIN_QUEUE_ERROR;
+		tx_time_dur = 0;
+		rx_time_dur = 0;
+	}
+
+	*tx_duration = tx_time_dur;
+	*rx_duration = rx_time_dur;
+
+	return retval;
+}
+
+/**
+ * i40e_lpi_stat_update - update LPI counters with values relative to offset
+ * @hw: pointer to the hw struct
+ * @offset_loaded: flag indicating need of writing current value to offset
+ * @tx_offset: pointer to offset of TX LPI counter
+ * @tx_stat: pointer to value of TX LPI counter
+ * @rx_offset: pointer to offset of RX LPI counter
+ * @rx_stat: pointer to value of RX LPI counter
+ *
+ * Update Low Power Idle (LPI) mode counters while having regard to passed
+ * offsets.
+ **/
+i40e_status i40e_lpi_stat_update(struct i40e_hw *hw,
+					   bool offset_loaded, u64 *tx_offset,
+					   u64 *tx_stat, u64 *rx_offset,
+					   u64 *rx_stat)
+{
+	i40e_status retval;
+	u32 tx_counter, rx_counter;
+	bool is_clear;
+
+	retval = i40e_get_lpi_counters(hw, &tx_counter, &rx_counter, &is_clear);
+	if (retval)
+		goto err;
+
+	if (is_clear) {
+		*tx_stat += tx_counter;
+		*rx_stat += rx_counter;
+	} else {
+		if (!offset_loaded) {
+			*tx_offset = tx_counter;
+			*rx_offset = rx_counter;
+		}
+
+		*tx_stat = (tx_counter >= *tx_offset) ?
+			(u32)(tx_counter - *tx_offset) :
+			(u32)((tx_counter + BIT_ULL(32)) - *tx_offset);
+		*rx_stat = (rx_counter >= *rx_offset) ?
+			(u32)(rx_counter - *rx_offset) :
+			(u32)((rx_counter + BIT_ULL(32)) - *rx_offset);
+	}
+err:
+	return retval;
+}
+
+/**
  * i40e_aq_rx_ctl_read_register - use FW to read from an Rx control register
  * @hw: pointer to the hw struct
  * @reg_addr: register address
@@ -5239,17 +6319,17 @@
 		(struct i40e_aqc_rx_ctl_reg_read_write *)&desc.params.raw;
 	i40e_status status;
 
-	if (!reg_val)
+	if (reg_val == NULL)
 		return I40E_ERR_PARAM;
 
 	i40e_fill_default_direct_cmd_desc(&desc, i40e_aqc_opc_rx_ctl_reg_read);
 
-	cmd_resp->address = cpu_to_le32(reg_addr);
+	cmd_resp->address = CPU_TO_LE32(reg_addr);
 
 	status = i40e_asq_send_command(hw, &desc, NULL, 0, cmd_details);
 
-	if (status == 0)
-		*reg_val = le32_to_cpu(cmd_resp->value);
+	if (status == I40E_SUCCESS)
+		*reg_val = LE32_TO_CPU(cmd_resp->value);
 
 	return status;
 }
@@ -5261,7 +6341,7 @@
  **/
 u32 i40e_read_rx_ctl(struct i40e_hw *hw, u32 reg_addr)
 {
-	i40e_status status = 0;
+	i40e_status status = I40E_SUCCESS;
 	bool use_register;
 	int retry = 5;
 	u32 val = 0;
@@ -5307,10 +6387,11 @@
 
 	i40e_fill_default_direct_cmd_desc(&desc, i40e_aqc_opc_rx_ctl_reg_write);
 
-	cmd->address = cpu_to_le32(reg_addr);
-	cmd->value = cpu_to_le32(reg_val);
+	cmd->address = CPU_TO_LE32(reg_addr);
+	cmd->value = CPU_TO_LE32(reg_val);
 
-	status = i40e_asq_send_command(hw, &desc, NULL, 0, cmd_details);
+	status = i40e_asq_send_command_atomic(hw, &desc, NULL, 0,
+					      cmd_details, true);
 
 	return status;
 }
@@ -5323,7 +6404,7 @@
  **/
 void i40e_write_rx_ctl(struct i40e_hw *hw, u32 reg_addr, u32 reg_val)
 {
-	i40e_status status = 0;
+	i40e_status status = I40E_SUCCESS;
 	bool use_register;
 	int retry = 5;
 
@@ -5347,20 +6428,51 @@
 }
 
 /**
- * i40e_aq_set_phy_register
+ * i40e_mdio_if_number_selection - MDIO I/F number selection
+ * @hw: pointer to the hw struct
+ * @set_mdio: use MDIO I/F number specified by mdio_num
+ * @mdio_num: MDIO I/F number
+ * @cmd: pointer to PHY Register command structure
+ **/
+static void
+i40e_mdio_if_number_selection(struct i40e_hw *hw, bool set_mdio, u8 mdio_num,
+			      struct i40e_aqc_phy_register_access *cmd)
+{
+	if (set_mdio && cmd->phy_interface == I40E_AQ_PHY_REG_ACCESS_EXTERNAL) {
+		if (hw->flags & I40E_HW_FLAG_AQ_PHY_ACCESS_EXTENDED)
+			cmd->cmd_flags |=
+				I40E_AQ_PHY_REG_ACCESS_SET_MDIO_IF_NUMBER |
+				((mdio_num <<
+				I40E_AQ_PHY_REG_ACCESS_MDIO_IF_NUMBER_SHIFT) &
+				I40E_AQ_PHY_REG_ACCESS_MDIO_IF_NUMBER_MASK);
+		else
+			i40e_debug(hw, I40E_DEBUG_PHY,
+				   "MDIO I/F number selection not supported by current FW version.\n");
+	}
+}
+
+/**
+ * i40e_aq_set_phy_register_ext
  * @hw: pointer to the hw struct
  * @phy_select: select which phy should be accessed
  * @dev_addr: PHY device address
+ * @page_change: enable auto page change
+ * @set_mdio: use MDIO I/F number specified by mdio_num
+ * @mdio_num: MDIO I/F number
  * @reg_addr: PHY register address
  * @reg_val: new register value
  * @cmd_details: pointer to command details structure or NULL
  *
  * Write the external PHY register.
+ * NOTE: In common cases MDIO I/F number should not be changed, thats why you
+ * may use simple wrapper i40e_aq_set_phy_register.
  **/
-i40e_status i40e_aq_set_phy_register(struct i40e_hw *hw,
-				     u8 phy_select, u8 dev_addr,
-				     u32 reg_addr, u32 reg_val,
-				     struct i40e_asq_cmd_details *cmd_details)
+enum i40e_status_code
+i40e_aq_set_phy_register_ext(struct i40e_hw *hw,
+			     u8 phy_select, u8 dev_addr, bool page_change,
+			     bool set_mdio, u8 mdio_num,
+			     u32 reg_addr, u32 reg_val,
+			     struct i40e_asq_cmd_details *cmd_details)
 {
 	struct i40e_aq_desc desc;
 	struct i40e_aqc_phy_register_access *cmd =
@@ -5371,9 +6483,14 @@
 					  i40e_aqc_opc_set_phy_register);
 
 	cmd->phy_interface = phy_select;
-	cmd->dev_address = dev_addr;
-	cmd->reg_address = cpu_to_le32(reg_addr);
-	cmd->reg_value = cpu_to_le32(reg_val);
+	cmd->dev_addres = dev_addr;
+	cmd->reg_address = CPU_TO_LE32(reg_addr);
+	cmd->reg_value = CPU_TO_LE32(reg_val);
+
+	if (!page_change)
+		cmd->cmd_flags = I40E_AQ_PHY_REG_ACCESS_DONT_CHANGE_QSFP_PAGE;
+
+	i40e_mdio_if_number_selection(hw, set_mdio, mdio_num, cmd);
 
 	status = i40e_asq_send_command(hw, &desc, NULL, 0, cmd_details);
 
@@ -5381,20 +6498,27 @@
 }
 
 /**
- * i40e_aq_get_phy_register
+ * i40e_aq_get_phy_register_ext
  * @hw: pointer to the hw struct
  * @phy_select: select which phy should be accessed
  * @dev_addr: PHY device address
+ * @page_change: enable auto page change
+ * @set_mdio: use MDIO I/F number specified by mdio_num
+ * @mdio_num: MDIO I/F number
  * @reg_addr: PHY register address
  * @reg_val: read register value
  * @cmd_details: pointer to command details structure or NULL
  *
  * Read the external PHY register.
+ * NOTE: In common cases MDIO I/F number should not be changed, thats why you
+ * may use simple wrapper i40e_aq_get_phy_register.
  **/
-i40e_status i40e_aq_get_phy_register(struct i40e_hw *hw,
-				     u8 phy_select, u8 dev_addr,
-				     u32 reg_addr, u32 *reg_val,
-				     struct i40e_asq_cmd_details *cmd_details)
+enum i40e_status_code
+i40e_aq_get_phy_register_ext(struct i40e_hw *hw,
+			     u8 phy_select, u8 dev_addr, bool page_change,
+			     bool set_mdio, u8 mdio_num,
+			     u32 reg_addr, u32 *reg_val,
+			     struct i40e_asq_cmd_details *cmd_details)
 {
 	struct i40e_aq_desc desc;
 	struct i40e_aqc_phy_register_access *cmd =
@@ -5405,12 +6529,252 @@
 					  i40e_aqc_opc_get_phy_register);
 
 	cmd->phy_interface = phy_select;
-	cmd->dev_address = dev_addr;
-	cmd->reg_address = cpu_to_le32(reg_addr);
+	cmd->dev_addres = dev_addr;
+	cmd->reg_address = CPU_TO_LE32(reg_addr);
 
-	status = i40e_asq_send_command(hw, &desc, NULL, 0, cmd_details);
+	if (!page_change)
+		cmd->cmd_flags = I40E_AQ_PHY_REG_ACCESS_DONT_CHANGE_QSFP_PAGE;
+
+	i40e_mdio_if_number_selection(hw, set_mdio, mdio_num, cmd);
+
+	status = i40e_asq_send_command_atomic(hw, &desc, NULL, 0,
+					      cmd_details, false);
 	if (!status)
-		*reg_val = le32_to_cpu(cmd->reg_value);
+		*reg_val = LE32_TO_CPU(cmd->reg_value);
+
+	return status;
+}
+
+/**
+ * i40e_aq_run_phy_activity
+ * @hw: pointer to the hw struct
+ * @activity_id: ID of DNL activity to run
+ * @dnl_opcode: opcode passed to DNL script
+ * @cmd_status: pointer to memory to write return value of DNL script
+ * @data0: pointer to memory for first 4 bytes of data returned by DNL script
+ * @data1: pointer to memory for last 4 bytes of data returned by DNL script
+ * @cmd_details: pointer to command details structure or NULL
+ *
+ * Run DNL admin command.
+ **/
+enum i40e_status_code
+i40e_aq_run_phy_activity(struct i40e_hw *hw, u16 activity_id, u32 dnl_opcode,
+			 u32 *cmd_status, u32 *data0, u32 *data1,
+			 struct i40e_asq_cmd_details *cmd_details)
+{
+	struct i40e_aqc_run_phy_activity *cmd;
+	i40e_status retval;
+	struct i40e_aq_desc desc;
+
+	cmd = (struct i40e_aqc_run_phy_activity *)&desc.params.raw;
+
+	if (!cmd_status || !data0 || !data1) {
+		retval = I40E_ERR_PARAM;
+		goto err;
+	}
+
+	i40e_fill_default_direct_cmd_desc(&desc,
+					  i40e_aqc_opc_run_phy_activity);
+
+	cmd->activity_id = CPU_TO_LE16(activity_id);
+	cmd->params.cmd.dnl_opcode = CPU_TO_LE32(dnl_opcode);
+
+	retval = i40e_asq_send_command(hw, &desc, NULL, 0, cmd_details);
+	if (retval)
+		goto err;
+
+	*cmd_status = LE32_TO_CPU(cmd->params.resp.cmd_status);
+	*data0 = LE32_TO_CPU(cmd->params.resp.data0);
+	*data1 = LE32_TO_CPU(cmd->params.resp.data1);
+err:
+	return retval;
+}
+
+/**
+ * i40e_aq_set_arp_proxy_config
+ * @hw: pointer to the HW structure
+ * @proxy_config: pointer to proxy config command table struct
+ * @cmd_details: pointer to command details
+ *
+ * Set ARP offload parameters from pre-populated
+ * i40e_aqc_arp_proxy_data struct
+ **/
+i40e_status i40e_aq_set_arp_proxy_config(struct i40e_hw *hw,
+				struct i40e_aqc_arp_proxy_data *proxy_config,
+				struct i40e_asq_cmd_details *cmd_details)
+{
+	struct i40e_aq_desc desc;
+	i40e_status status;
+
+	if (!proxy_config)
+		return I40E_ERR_PARAM;
+
+	i40e_fill_default_direct_cmd_desc(&desc, i40e_aqc_opc_set_proxy_config);
+
+	desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_BUF);
+	desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_RD);
+	desc.params.external.addr_high =
+				  CPU_TO_LE32(upper_32_bits((u64)proxy_config));
+	desc.params.external.addr_low =
+				  CPU_TO_LE32(lower_32_bits((u64)proxy_config));
+	desc.datalen = CPU_TO_LE16(sizeof(struct i40e_aqc_arp_proxy_data));
+
+	status = i40e_asq_send_command(hw, &desc, proxy_config,
+				       sizeof(struct i40e_aqc_arp_proxy_data),
+				       cmd_details);
+
+	return status;
+}
+
+/**
+ * i40e_aq_set_ns_proxy_table_entry
+ * @hw: pointer to the HW structure
+ * @ns_proxy_table_entry: pointer to NS table entry command struct
+ * @cmd_details: pointer to command details
+ *
+ * Set IPv6 Neighbor Solicitation (NS) protocol offload parameters
+ * from pre-populated i40e_aqc_ns_proxy_data struct
+ **/
+i40e_status i40e_aq_set_ns_proxy_table_entry(struct i40e_hw *hw,
+			struct i40e_aqc_ns_proxy_data *ns_proxy_table_entry,
+			struct i40e_asq_cmd_details *cmd_details)
+{
+	struct i40e_aq_desc desc;
+	i40e_status status;
+
+	if (!ns_proxy_table_entry)
+		return I40E_ERR_PARAM;
+
+	i40e_fill_default_direct_cmd_desc(&desc,
+				i40e_aqc_opc_set_ns_proxy_table_entry);
+
+	desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_BUF);
+	desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_RD);
+	desc.params.external.addr_high =
+		CPU_TO_LE32(upper_32_bits((u64)ns_proxy_table_entry));
+	desc.params.external.addr_low =
+		CPU_TO_LE32(lower_32_bits((u64)ns_proxy_table_entry));
+	desc.datalen = CPU_TO_LE16(sizeof(struct i40e_aqc_ns_proxy_data));
+
+	status = i40e_asq_send_command(hw, &desc, ns_proxy_table_entry,
+				       sizeof(struct i40e_aqc_ns_proxy_data),
+				       cmd_details);
+
+	return status;
+}
+
+/**
+ * i40e_aq_set_clear_wol_filter
+ * @hw: pointer to the hw struct
+ * @filter_index: index of filter to modify (0-7)
+ * @filter: buffer containing filter to be set
+ * @set_filter: true to set filter, false to clear filter
+ * @no_wol_tco: if true, pass through packets cannot cause wake-up
+ *		if false, pass through packets may cause wake-up
+ * @filter_valid: true if filter action is valid
+ * @no_wol_tco_valid: true if no WoL in TCO traffic action valid
+ * @cmd_details: pointer to command details structure or NULL
+ *
+ * Set or clear WoL filter for port attached to the PF
+ **/
+i40e_status i40e_aq_set_clear_wol_filter(struct i40e_hw *hw,
+				u8 filter_index,
+				struct i40e_aqc_set_wol_filter_data *filter,
+				bool set_filter, bool no_wol_tco,
+				bool filter_valid, bool no_wol_tco_valid,
+				struct i40e_asq_cmd_details *cmd_details)
+{
+	struct i40e_aq_desc desc;
+	struct i40e_aqc_set_wol_filter *cmd =
+		(struct i40e_aqc_set_wol_filter *)&desc.params.raw;
+	i40e_status status;
+	u16 cmd_flags = 0;
+	u16 valid_flags = 0;
+	u16 buff_len = 0;
+
+	i40e_fill_default_direct_cmd_desc(&desc, i40e_aqc_opc_set_wol_filter);
+
+	if (filter_index >= I40E_AQC_MAX_NUM_WOL_FILTERS)
+		return  I40E_ERR_PARAM;
+	cmd->filter_index = CPU_TO_LE16(filter_index);
+
+	if (set_filter) {
+		if (!filter)
+			return  I40E_ERR_PARAM;
+
+		cmd_flags |= I40E_AQC_SET_WOL_FILTER;
+		cmd_flags |= I40E_AQC_SET_WOL_FILTER_WOL_PRESERVE_ON_PFR;
+	}
+
+	if (no_wol_tco)
+		cmd_flags |= I40E_AQC_SET_WOL_FILTER_NO_TCO_WOL;
+	cmd->cmd_flags = CPU_TO_LE16(cmd_flags);
+
+	if (filter_valid)
+		valid_flags |= I40E_AQC_SET_WOL_FILTER_ACTION_VALID;
+	if (no_wol_tco_valid)
+		valid_flags |= I40E_AQC_SET_WOL_FILTER_NO_TCO_ACTION_VALID;
+	cmd->valid_flags = CPU_TO_LE16(valid_flags);
+
+	buff_len = sizeof(*filter);
+	desc.datalen = CPU_TO_LE16(buff_len);
+
+	desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_BUF);
+	desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_RD);
+
+	cmd->address_high = CPU_TO_LE32(upper_32_bits((u64)filter));
+	cmd->address_low = CPU_TO_LE32(lower_32_bits((u64)filter));
+
+	status = i40e_asq_send_command(hw, &desc, filter,
+				       buff_len, cmd_details);
+
+	return status;
+}
+
+/**
+ * i40e_aq_get_wake_event_reason
+ * @hw: pointer to the hw struct
+ * @wake_reason: return value, index of matching filter
+ * @cmd_details: pointer to command details structure or NULL
+ *
+ * Get information for the reason of a Wake Up event
+ **/
+i40e_status i40e_aq_get_wake_event_reason(struct i40e_hw *hw,
+				u16 *wake_reason,
+				struct i40e_asq_cmd_details *cmd_details)
+{
+	struct i40e_aq_desc desc;
+	struct i40e_aqc_get_wake_reason_completion *resp =
+		(struct i40e_aqc_get_wake_reason_completion *)&desc.params.raw;
+	i40e_status status;
+
+	i40e_fill_default_direct_cmd_desc(&desc, i40e_aqc_opc_get_wake_reason);
+
+	status = i40e_asq_send_command(hw, &desc, NULL, 0, cmd_details);
+
+	if (status == I40E_SUCCESS)
+		*wake_reason = LE16_TO_CPU(resp->wake_reason);
+
+	return status;
+}
+
+/**
+* i40e_aq_clear_all_wol_filters
+* @hw: pointer to the hw struct
+* @cmd_details: pointer to command details structure or NULL
+*
+* Get information for the reason of a Wake Up event
+**/
+i40e_status i40e_aq_clear_all_wol_filters(struct i40e_hw *hw,
+	struct i40e_asq_cmd_details *cmd_details)
+{
+	struct i40e_aq_desc desc;
+	i40e_status status;
+
+	i40e_fill_default_direct_cmd_desc(&desc,
+					  i40e_aqc_opc_clear_all_wol_filters);
+
+	status = i40e_asq_send_command(hw, &desc, NULL, 0, cmd_details);
 
 	return status;
 }
@@ -5439,23 +6803,23 @@
 	i40e_status status;
 
 	i40e_fill_default_direct_cmd_desc(&desc,
-					  i40e_aqc_opc_write_personalization_profile);
+				  i40e_aqc_opc_write_personalization_profile);
 
-	desc.flags |= cpu_to_le16(I40E_AQ_FLAG_BUF | I40E_AQ_FLAG_RD);
+	desc.flags |= CPU_TO_LE16(I40E_AQ_FLAG_BUF | I40E_AQ_FLAG_RD);
 	if (buff_size > I40E_AQ_LARGE_BUF)
-		desc.flags |= cpu_to_le16((u16)I40E_AQ_FLAG_LB);
+		desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_LB);
 
-	desc.datalen = cpu_to_le16(buff_size);
+	desc.datalen = CPU_TO_LE16(buff_size);
 
-	cmd->profile_track_id = cpu_to_le32(track_id);
+	cmd->profile_track_id = CPU_TO_LE32(track_id);
 
 	status = i40e_asq_send_command(hw, &desc, buff, buff_size, cmd_details);
 	if (!status) {
 		resp = (struct i40e_aqc_write_ddp_resp *)&desc.params.raw;
 		if (error_offset)
-			*error_offset = le32_to_cpu(resp->error_offset);
+			*error_offset = LE32_TO_CPU(resp->error_offset);
 		if (error_info)
-			*error_info = le32_to_cpu(resp->error_info);
+			*error_info = LE32_TO_CPU(resp->error_info);
 	}
 
 	return status;
@@ -5480,12 +6844,12 @@
 	i40e_status status;
 
 	i40e_fill_default_direct_cmd_desc(&desc,
-					  i40e_aqc_opc_get_personalization_profile_list);
+			  i40e_aqc_opc_get_personalization_profile_list);
 
-	desc.flags |= cpu_to_le16((u16)I40E_AQ_FLAG_BUF);
+	desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_BUF);
 	if (buff_size > I40E_AQ_LARGE_BUF)
-		desc.flags |= cpu_to_le16((u16)I40E_AQ_FLAG_LB);
-	desc.datalen = cpu_to_le16(buff_size);
+		desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_LB);
+	desc.datalen = CPU_TO_LE16(buff_size);
 
 	cmd->flags = flags;
 
@@ -5586,22 +6950,23 @@
 	u16 msglen;
 
 	i40e_fill_default_direct_cmd_desc(&desc, aq->opcode);
-	desc.flags |= cpu_to_le16(aq->flags);
-	memcpy(desc.params.raw, aq->param, sizeof(desc.params.raw));
+	desc.flags |= CPU_TO_LE16(aq->flags);
+	i40e_memcpy(desc.params.raw, aq->param, sizeof(desc.params.raw),
+		    I40E_NONDMA_TO_NONDMA);
 
 	msglen = aq->datalen;
 	if (msglen) {
-		desc.flags |= cpu_to_le16((u16)(I40E_AQ_FLAG_BUF |
+		desc.flags |= CPU_TO_LE16((u16)(I40E_AQ_FLAG_BUF |
 						I40E_AQ_FLAG_RD));
 		if (msglen > I40E_AQ_LARGE_BUF)
-			desc.flags |= cpu_to_le16((u16)I40E_AQ_FLAG_LB);
-		desc.datalen = cpu_to_le16(msglen);
+			desc.flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_LB);
+		desc.datalen = CPU_TO_LE16(msglen);
 		msg = &aq->data[0];
 	}
 
 	status = i40e_asq_send_command(hw, &desc, msg, msglen, NULL);
 
-	if (status) {
+	if (status != I40E_SUCCESS) {
 		i40e_debug(hw, I40E_DEBUG_PACKAGE,
 			   "unable to exec DDP AQ opcode %u, error %d\n",
 			   aq->opcode, status);
@@ -5609,9 +6974,10 @@
 	}
 
 	/* copy returned desc to aq_buf */
-	memcpy(aq->param, desc.params.raw, sizeof(desc.params.raw));
+	i40e_memcpy(aq->param, desc.params.raw, sizeof(desc.params.raw),
+		    I40E_NONDMA_TO_NONDMA);
 
-	return 0;
+	return I40E_SUCCESS;
 }
 
 /**
@@ -5628,7 +6994,7 @@
 		      u32 track_id, bool rollback)
 {
 	struct i40e_profile_section_header *sec = NULL;
-	i40e_status status = 0;
+	i40e_status status = I40E_SUCCESS;
 	struct i40e_section_table *sec_tbl;
 	u32 vendor_dev_id;
 	u32 dev_cnt;
@@ -5647,7 +7013,7 @@
 		    hw->device_id == (vendor_dev_id & 0xFFFF))
 			break;
 	}
-	if (dev_cnt && i == dev_cnt) {
+	if (dev_cnt && (i == dev_cnt)) {
 		i40e_debug(hw, I40E_DEBUG_PACKAGE,
 			   "Device doesn't support DDP\n");
 		return I40E_ERR_DEVICE_NOT_SUPPORTED;
@@ -5692,7 +7058,7 @@
 i40e_write_profile(struct i40e_hw *hw, struct i40e_profile_segment *profile,
 		   u32 track_id)
 {
-	i40e_status status = 0;
+	i40e_status status = I40E_SUCCESS;
 	struct i40e_section_table *sec_tbl;
 	struct i40e_profile_section_header *sec = NULL;
 	struct i40e_profile_aq_section *ddp_aq;
@@ -5756,7 +7122,7 @@
 		      u32 track_id)
 {
 	struct i40e_profile_section_header *sec = NULL;
-	i40e_status status = 0;
+	i40e_status status = I40E_SUCCESS;
 	struct i40e_section_table *sec_tbl;
 	u32 offset = 0, info = 0;
 	u32 section_size = 0;
@@ -5808,7 +7174,7 @@
 		       struct i40e_profile_segment *profile,
 		       u8 *profile_info_sec, u32 track_id)
 {
-	i40e_status status = 0;
+	i40e_status status = I40E_SUCCESS;
 	struct i40e_profile_section_header *sec = NULL;
 	struct i40e_profile_info *pinfo;
 	u32 offset = 0, info = 0;
@@ -5825,198 +7191,10 @@
 	pinfo->track_id = track_id;
 	pinfo->version = profile->version;
 	pinfo->op = I40E_DDP_ADD_TRACKID;
-	memcpy(pinfo->name, profile->name, I40E_DDP_NAME_SIZE);
+	i40e_memcpy(pinfo->name, profile->name, I40E_DDP_NAME_SIZE,
+		    I40E_NONDMA_TO_NONDMA);
 
 	status = i40e_aq_write_ddp(hw, (void *)sec, sec->data_end,
 				   track_id, &offset, &info, NULL);
-
-	return status;
-}
-
-/**
- * i40e_aq_add_cloud_filters
- * @hw: pointer to the hardware structure
- * @seid: VSI seid to add cloud filters from
- * @filters: Buffer which contains the filters to be added
- * @filter_count: number of filters contained in the buffer
- *
- * Set the cloud filters for a given VSI.  The contents of the
- * i40e_aqc_cloud_filters_element_data are filled in by the caller
- * of the function.
- *
- **/
-enum i40e_status_code
-i40e_aq_add_cloud_filters(struct i40e_hw *hw, u16 seid,
-			  struct i40e_aqc_cloud_filters_element_data *filters,
-			  u8 filter_count)
-{
-	struct i40e_aq_desc desc;
-	struct i40e_aqc_add_remove_cloud_filters *cmd =
-	(struct i40e_aqc_add_remove_cloud_filters *)&desc.params.raw;
-	enum i40e_status_code status;
-	u16 buff_len;
-
-	i40e_fill_default_direct_cmd_desc(&desc,
-					  i40e_aqc_opc_add_cloud_filters);
-
-	buff_len = filter_count * sizeof(*filters);
-	desc.datalen = cpu_to_le16(buff_len);
-	desc.flags |= cpu_to_le16((u16)(I40E_AQ_FLAG_BUF | I40E_AQ_FLAG_RD));
-	cmd->num_filters = filter_count;
-	cmd->seid = cpu_to_le16(seid);
-
-	status = i40e_asq_send_command(hw, &desc, filters, buff_len, NULL);
-
-	return status;
-}
-
-/**
- * i40e_aq_add_cloud_filters_bb
- * @hw: pointer to the hardware structure
- * @seid: VSI seid to add cloud filters from
- * @filters: Buffer which contains the filters in big buffer to be added
- * @filter_count: number of filters contained in the buffer
- *
- * Set the big buffer cloud filters for a given VSI.  The contents of the
- * i40e_aqc_cloud_filters_element_bb are filled in by the caller of the
- * function.
- *
- **/
-enum i40e_status_code
-i40e_aq_add_cloud_filters_bb(struct i40e_hw *hw, u16 seid,
-			     struct i40e_aqc_cloud_filters_element_bb *filters,
-			     u8 filter_count)
-{
-	struct i40e_aq_desc desc;
-	struct i40e_aqc_add_remove_cloud_filters *cmd =
-	(struct i40e_aqc_add_remove_cloud_filters *)&desc.params.raw;
-	i40e_status status;
-	u16 buff_len;
-	int i;
-
-	i40e_fill_default_direct_cmd_desc(&desc,
-					  i40e_aqc_opc_add_cloud_filters);
-
-	buff_len = filter_count * sizeof(*filters);
-	desc.datalen = cpu_to_le16(buff_len);
-	desc.flags |= cpu_to_le16((u16)(I40E_AQ_FLAG_BUF | I40E_AQ_FLAG_RD));
-	cmd->num_filters = filter_count;
-	cmd->seid = cpu_to_le16(seid);
-	cmd->big_buffer_flag = I40E_AQC_ADD_CLOUD_CMD_BB;
-
-	for (i = 0; i < filter_count; i++) {
-		u16 tnl_type;
-		u32 ti;
-
-		tnl_type = (le16_to_cpu(filters[i].element.flags) &
-			   I40E_AQC_ADD_CLOUD_TNL_TYPE_MASK) >>
-			   I40E_AQC_ADD_CLOUD_TNL_TYPE_SHIFT;
-
-		/* Due to hardware eccentricities, the VNI for Geneve is shifted
-		 * one more byte further than normally used for Tenant ID in
-		 * other tunnel types.
-		 */
-		if (tnl_type == I40E_AQC_ADD_CLOUD_TNL_TYPE_GENEVE) {
-			ti = le32_to_cpu(filters[i].element.tenant_id);
-			filters[i].element.tenant_id = cpu_to_le32(ti << 8);
-		}
-	}
-
-	status = i40e_asq_send_command(hw, &desc, filters, buff_len, NULL);
-
-	return status;
-}
-
-/**
- * i40e_aq_rem_cloud_filters
- * @hw: pointer to the hardware structure
- * @seid: VSI seid to remove cloud filters from
- * @filters: Buffer which contains the filters to be removed
- * @filter_count: number of filters contained in the buffer
- *
- * Remove the cloud filters for a given VSI.  The contents of the
- * i40e_aqc_cloud_filters_element_data are filled in by the caller
- * of the function.
- *
- **/
-enum i40e_status_code
-i40e_aq_rem_cloud_filters(struct i40e_hw *hw, u16 seid,
-			  struct i40e_aqc_cloud_filters_element_data *filters,
-			  u8 filter_count)
-{
-	struct i40e_aq_desc desc;
-	struct i40e_aqc_add_remove_cloud_filters *cmd =
-	(struct i40e_aqc_add_remove_cloud_filters *)&desc.params.raw;
-	enum i40e_status_code status;
-	u16 buff_len;
-
-	i40e_fill_default_direct_cmd_desc(&desc,
-					  i40e_aqc_opc_remove_cloud_filters);
-
-	buff_len = filter_count * sizeof(*filters);
-	desc.datalen = cpu_to_le16(buff_len);
-	desc.flags |= cpu_to_le16((u16)(I40E_AQ_FLAG_BUF | I40E_AQ_FLAG_RD));
-	cmd->num_filters = filter_count;
-	cmd->seid = cpu_to_le16(seid);
-
-	status = i40e_asq_send_command(hw, &desc, filters, buff_len, NULL);
-
-	return status;
-}
-
-/**
- * i40e_aq_rem_cloud_filters_bb
- * @hw: pointer to the hardware structure
- * @seid: VSI seid to remove cloud filters from
- * @filters: Buffer which contains the filters in big buffer to be removed
- * @filter_count: number of filters contained in the buffer
- *
- * Remove the big buffer cloud filters for a given VSI.  The contents of the
- * i40e_aqc_cloud_filters_element_bb are filled in by the caller of the
- * function.
- *
- **/
-enum i40e_status_code
-i40e_aq_rem_cloud_filters_bb(struct i40e_hw *hw, u16 seid,
-			     struct i40e_aqc_cloud_filters_element_bb *filters,
-			     u8 filter_count)
-{
-	struct i40e_aq_desc desc;
-	struct i40e_aqc_add_remove_cloud_filters *cmd =
-	(struct i40e_aqc_add_remove_cloud_filters *)&desc.params.raw;
-	i40e_status status;
-	u16 buff_len;
-	int i;
-
-	i40e_fill_default_direct_cmd_desc(&desc,
-					  i40e_aqc_opc_remove_cloud_filters);
-
-	buff_len = filter_count * sizeof(*filters);
-	desc.datalen = cpu_to_le16(buff_len);
-	desc.flags |= cpu_to_le16((u16)(I40E_AQ_FLAG_BUF | I40E_AQ_FLAG_RD));
-	cmd->num_filters = filter_count;
-	cmd->seid = cpu_to_le16(seid);
-	cmd->big_buffer_flag = I40E_AQC_ADD_CLOUD_CMD_BB;
-
-	for (i = 0; i < filter_count; i++) {
-		u16 tnl_type;
-		u32 ti;
-
-		tnl_type = (le16_to_cpu(filters[i].element.flags) &
-			   I40E_AQC_ADD_CLOUD_TNL_TYPE_MASK) >>
-			   I40E_AQC_ADD_CLOUD_TNL_TYPE_SHIFT;
-
-		/* Due to hardware eccentricities, the VNI for Geneve is shifted
-		 * one more byte further than normally used for Tenant ID in
-		 * other tunnel types.
-		 */
-		if (tnl_type == I40E_AQC_ADD_CLOUD_TNL_TYPE_GENEVE) {
-			ti = le32_to_cpu(filters[i].element.tenant_id);
-			filters[i].element.tenant_id = cpu_to_le32(ti << 8);
-		}
-	}
-
-	status = i40e_asq_send_command(hw, &desc, filters, buff_len, NULL);
-
 	return status;
 }
Binary files linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_common.o and linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_common.o differ
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_dcb.c linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_dcb.c
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_dcb.c	2024-05-10 01:26:45.325079481 -0400
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_dcb.c	2024-05-13 03:58:25.372491940 -0400
@@ -1,5 +1,5 @@
 // SPDX-License-Identifier: GPL-2.0
-/* Copyright(c) 2013 - 2018 Intel Corporation. */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
 
 #include "i40e_adminq.h"
 #include "i40e_prototype.h"
@@ -23,7 +23,7 @@
 	*status = (u16)((reg & I40E_PRTDCB_GENS_DCBX_STATUS_MASK) >>
 			I40E_PRTDCB_GENS_DCBX_STATUS_SHIFT);
 
-	return 0;
+	return I40E_SUCCESS;
 }
 
 /**
@@ -234,7 +234,7 @@
 }
 
 /**
- * i40e_parse_ieee_etsrec_tlv
+ * i40e_parse_ieee_tlv
  * @tlv: IEEE 802.1Qaz TLV
  * @dcbcfg: Local store to update ETS REC data
  *
@@ -314,9 +314,15 @@
 	 *        |pg0|pg1|pg2|pg3|pg4|pg5|pg6|pg7|
 	 *        ---------------------------------
 	 */
-	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++)
+	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
 		etscfg->tcbwtable[i] = buf[offset++];
 
+		if (etscfg->prioritytable[i] == I40E_CEE_PGID_STRICT)
+			dcbcfg->etscfg.tsatable[i] = I40E_IEEE_TSA_STRICT;
+		else
+			dcbcfg->etscfg.tsatable[i] = I40E_IEEE_TSA_ETS;
+	}
+
 	/* Number of TCs supported (1 octet) */
 	etscfg->maxtcs = buf[offset];
 }
@@ -364,7 +370,6 @@
 		       I40E_LLDP_TLV_LEN_SHIFT);
 
 	dcbcfg->numapps = length / sizeof(*app);
-
 	if (!dcbcfg->numapps)
 		return;
 	if (dcbcfg->numapps > I40E_DCBX_MAX_APPS)
@@ -500,7 +505,7 @@
 i40e_status i40e_lldp_to_dcb_config(u8 *lldpmib,
 				    struct i40e_dcbx_config *dcbcfg)
 {
-	i40e_status ret = 0;
+	i40e_status ret = I40E_SUCCESS;
 	struct i40e_lldp_org_tlv *tlv;
 	u16 type;
 	u16 length;
@@ -555,7 +560,7 @@
 				   u8 bridgetype,
 				   struct i40e_dcbx_config *dcbcfg)
 {
-	i40e_status ret = 0;
+	i40e_status ret = I40E_SUCCESS;
 	struct i40e_virt_mem mem;
 	u8 *lldpmib;
 
@@ -590,8 +595,8 @@
 			struct i40e_aqc_get_cee_dcb_cfg_v1_resp *cee_cfg,
 			struct i40e_dcbx_config *dcbcfg)
 {
-	u16 status, tlv_status = le16_to_cpu(cee_cfg->tlv_status);
-	u16 app_prio = le16_to_cpu(cee_cfg->oper_app_prio);
+	u16 status, tlv_status = LE16_TO_CPU(cee_cfg->tlv_status);
+	u16 app_prio = LE16_TO_CPU(cee_cfg->oper_app_prio);
 	u8 i, tc, err;
 
 	/* CEE PG data to ETS config */
@@ -604,7 +609,7 @@
 		tc = (u8)((cee_cfg->oper_prio_tc[i] &
 			 I40E_CEE_PGID_PRIO_0_MASK) >>
 			 I40E_CEE_PGID_PRIO_0_SHIFT);
-		dcbcfg->etscfg.prioritytable[i * 2] =  tc;
+		dcbcfg->etscfg.prioritytable[i*2] =  tc;
 		tc = (u8)((cee_cfg->oper_prio_tc[i] &
 			 I40E_CEE_PGID_PRIO_1_MASK) >>
 			 I40E_CEE_PGID_PRIO_1_SHIFT);
@@ -671,8 +676,8 @@
 				struct i40e_aqc_get_cee_dcb_cfg_resp *cee_cfg,
 				struct i40e_dcbx_config *dcbcfg)
 {
-	u32 status, tlv_status = le32_to_cpu(cee_cfg->tlv_status);
-	u16 app_prio = le16_to_cpu(cee_cfg->oper_app_prio);
+	u32 status, tlv_status = LE32_TO_CPU(cee_cfg->tlv_status);
+	u16 app_prio = LE16_TO_CPU(cee_cfg->oper_app_prio);
 	u8 i, tc, err, sync, oper;
 
 	/* CEE PG data to ETS config */
@@ -685,11 +690,11 @@
 		tc = (u8)((cee_cfg->oper_prio_tc[i] &
 			 I40E_CEE_PGID_PRIO_0_MASK) >>
 			 I40E_CEE_PGID_PRIO_0_SHIFT);
-		dcbcfg->etscfg.prioritytable[i * 2] =  tc;
+		dcbcfg->etscfg.prioritytable[i*2] =  tc;
 		tc = (u8)((cee_cfg->oper_prio_tc[i] &
 			 I40E_CEE_PGID_PRIO_1_MASK) >>
 			 I40E_CEE_PGID_PRIO_1_SHIFT);
-		dcbcfg->etscfg.prioritytable[i * 2 + 1] = tc;
+		dcbcfg->etscfg.prioritytable[i*2 + 1] = tc;
 	}
 
 	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++)
@@ -769,7 +774,7 @@
  **/
 static i40e_status i40e_get_ieee_dcb_config(struct i40e_hw *hw)
 {
-	i40e_status ret = 0;
+	i40e_status ret = I40E_SUCCESS;
 
 	/* IEEE mode */
 	hw->local_dcbx_config.dcbx_mode = I40E_DCBX_MODE_IEEE;
@@ -785,7 +790,7 @@
 				     &hw->remote_dcbx_config);
 	/* Don't treat ENOENT as an error for Remote MIBs */
 	if (hw->aq.asq_last_status == I40E_AQ_RC_ENOENT)
-		ret = 0;
+		ret = I40E_SUCCESS;
 
 out:
 	return ret;
@@ -799,7 +804,7 @@
  **/
 i40e_status i40e_get_dcb_config(struct i40e_hw *hw)
 {
-	i40e_status ret = 0;
+	i40e_status ret = I40E_SUCCESS;
 	struct i40e_aqc_get_cee_dcb_cfg_resp cee_cfg;
 	struct i40e_aqc_get_cee_dcb_cfg_v1_resp cee_v1_cfg;
 
@@ -814,22 +819,22 @@
 	    ((hw->aq.fw_maj_ver == 4) && (hw->aq.fw_min_ver == 33))) {
 		ret = i40e_aq_get_cee_dcb_config(hw, &cee_v1_cfg,
 						 sizeof(cee_v1_cfg), NULL);
-		if (!ret) {
+		if (ret == I40E_SUCCESS) {
 			/* CEE mode */
 			hw->local_dcbx_config.dcbx_mode = I40E_DCBX_MODE_CEE;
 			hw->local_dcbx_config.tlv_status =
-					le16_to_cpu(cee_v1_cfg.tlv_status);
+					LE16_TO_CPU(cee_v1_cfg.tlv_status);
 			i40e_cee_to_dcb_v1_config(&cee_v1_cfg,
 						  &hw->local_dcbx_config);
 		}
 	} else {
 		ret = i40e_aq_get_cee_dcb_config(hw, &cee_cfg,
 						 sizeof(cee_cfg), NULL);
-		if (!ret) {
+		if (ret == I40E_SUCCESS) {
 			/* CEE mode */
 			hw->local_dcbx_config.dcbx_mode = I40E_DCBX_MODE_CEE;
 			hw->local_dcbx_config.tlv_status =
-					le32_to_cpu(cee_cfg.tlv_status);
+					LE32_TO_CPU(cee_cfg.tlv_status);
 			i40e_cee_to_dcb_config(&cee_cfg,
 					       &hw->local_dcbx_config);
 		}
@@ -839,7 +844,7 @@
 	if (hw->aq.asq_last_status == I40E_AQ_RC_ENOENT)
 		return i40e_get_ieee_dcb_config(hw);
 
-	if (ret)
+	if (ret != I40E_SUCCESS)
 		goto out;
 
 	/* Get CEE DCB Desired Config */
@@ -850,11 +855,11 @@
 
 	/* Get Remote DCB Config */
 	ret = i40e_aq_get_dcb_config(hw, I40E_AQ_LLDP_MIB_REMOTE,
-				     I40E_AQ_LLDP_BRIDGE_TYPE_NEAREST_BRIDGE,
-				     &hw->remote_dcbx_config);
+			     I40E_AQ_LLDP_BRIDGE_TYPE_NEAREST_BRIDGE,
+			     &hw->remote_dcbx_config);
 	/* Don't treat ENOENT as an error for Remote MIBs */
 	if (hw->aq.asq_last_status == I40E_AQ_RC_ENOENT)
-		ret = 0;
+		ret = I40E_SUCCESS;
 
 out:
 	return ret;
@@ -869,7 +874,7 @@
  **/
 i40e_status i40e_init_dcb(struct i40e_hw *hw, bool enable_mib_change)
 {
-	i40e_status ret = 0;
+	i40e_status ret = I40E_SUCCESS;
 	struct i40e_lldp_variables lldp_cfg;
 	u8 adminstatus = 0;
 
@@ -889,7 +894,9 @@
 
 		ret = i40e_read_nvm_module_data(hw,
 						I40E_SR_EMP_SR_SETTINGS_PTR,
-						offset, 1,
+						offset,
+						I40E_LLDP_CURRENT_STATUS_OFFSET,
+						I40E_LLDP_CURRENT_STATUS_SIZE,
 						&lldp_cfg.adminstatus);
 	} else {
 		ret = i40e_read_lldp_cfg(hw, &lldp_cfg);
@@ -931,6 +938,967 @@
 }
 
 /**
+ * i40e_get_fw_lldp_status
+ * @hw: pointer to the hw struct
+ * @lldp_status: pointer to the status enum
+ *
+ * Get status of FW Link Layer Discovery Protocol (LLDP) Agent.
+ * Status of agent is reported via @lldp_status parameter.
+ **/
+enum i40e_status_code
+i40e_get_fw_lldp_status(struct i40e_hw *hw,
+			enum i40e_get_fw_lldp_status_resp *lldp_status)
+{
+	i40e_status ret;
+	struct i40e_virt_mem mem;
+	u8 *lldpmib;
+
+	if (!lldp_status)
+		return I40E_ERR_PARAM;
+
+	/* Allocate buffer for the LLDPDU */
+	ret = i40e_allocate_virt_mem(hw, &mem, I40E_LLDPDU_SIZE);
+	if (ret)
+		return ret;
+
+	lldpmib = (u8 *)mem.va;
+	ret = i40e_aq_get_lldp_mib(hw, 0, 0, (void *)lldpmib,
+				   I40E_LLDPDU_SIZE, NULL, NULL, NULL);
+
+	if (ret == I40E_SUCCESS) {
+		*lldp_status = I40E_GET_FW_LLDP_STATUS_ENABLED;
+	} else if (hw->aq.asq_last_status == I40E_AQ_RC_ENOENT) {
+		/* MIB is not available yet but the agent is running */
+		*lldp_status = I40E_GET_FW_LLDP_STATUS_ENABLED;
+		ret = I40E_SUCCESS;
+	} else if (hw->aq.asq_last_status == I40E_AQ_RC_EPERM) {
+		*lldp_status = I40E_GET_FW_LLDP_STATUS_DISABLED;
+		ret = I40E_SUCCESS;
+	}
+
+	i40e_free_virt_mem(hw, &mem);
+	return ret;
+}
+
+/**
+ * i40e_add_ieee_ets_tlv - Prepare ETS TLV in IEEE format
+ * @tlv: Fill the ETS config data in IEEE format
+ * @dcbcfg: Local store which holds the DCB Config
+ *
+ * Prepare IEEE 802.1Qaz ETS CFG TLV
+ **/
+static void i40e_add_ieee_ets_tlv(struct i40e_lldp_org_tlv *tlv,
+				  struct i40e_dcbx_config *dcbcfg)
+{
+	u8 priority0, priority1, maxtcwilling = 0;
+	struct i40e_dcb_ets_config *etscfg;
+	u16 offset = 0, typelength, i;
+	u8 *buf = tlv->tlvinfo;
+	u32 ouisubtype;
+
+	typelength = (u16)((I40E_TLV_TYPE_ORG << I40E_LLDP_TLV_TYPE_SHIFT) |
+			I40E_IEEE_ETS_TLV_LENGTH);
+	tlv->typelength = htons(typelength);
+
+	ouisubtype = (u32)((I40E_IEEE_8021QAZ_OUI << I40E_LLDP_TLV_OUI_SHIFT) |
+			I40E_IEEE_SUBTYPE_ETS_CFG);
+	tlv->ouisubtype = htonl(ouisubtype);
+
+	/* First Octet post subtype
+	 * --------------------------
+	 * |will-|CBS  | Re-  | Max |
+	 * |ing  |     |served| TCs |
+	 * --------------------------
+	 * |1bit | 1bit|3 bits|3bits|
+	 */
+	etscfg = &dcbcfg->etscfg;
+	if (etscfg->willing)
+		maxtcwilling = BIT(I40E_IEEE_ETS_WILLING_SHIFT);
+	maxtcwilling |= etscfg->maxtcs & I40E_IEEE_ETS_MAXTC_MASK;
+	buf[offset] = maxtcwilling;
+
+	/* Move offset to Priority Assignment Table */
+	offset++;
+
+	/* Priority Assignment Table (4 octets)
+	 * Octets:|    1    |    2    |    3    |    4    |
+	 *        -----------------------------------------
+	 *        |pri0|pri1|pri2|pri3|pri4|pri5|pri6|pri7|
+	 *        -----------------------------------------
+	 *   Bits:|7  4|3  0|7  4|3  0|7  4|3  0|7  4|3  0|
+	 *        -----------------------------------------
+	 */
+	for (i = 0; i < 4; i++) {
+		priority0 = etscfg->prioritytable[i * 2] & 0xF;
+		priority1 = etscfg->prioritytable[i * 2 + 1] & 0xF;
+		buf[offset] = (priority0 << I40E_IEEE_ETS_PRIO_1_SHIFT) |
+				priority1;
+		offset++;
+	}
+
+	/* TC Bandwidth Table (8 octets)
+	 * Octets:| 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 |
+	 *        ---------------------------------
+	 *        |tc0|tc1|tc2|tc3|tc4|tc5|tc6|tc7|
+	 *        ---------------------------------
+	 */
+	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++)
+		buf[offset++] = etscfg->tcbwtable[i];
+
+	/* TSA Assignment Table (8 octets)
+	 * Octets:| 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 |
+	 *        ---------------------------------
+	 *        |tc0|tc1|tc2|tc3|tc4|tc5|tc6|tc7|
+	 *        ---------------------------------
+	 */
+	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++)
+		buf[offset++] = etscfg->tsatable[i];
+}
+
+/**
+ * i40e_add_ieee_etsrec_tlv - Prepare ETS Recommended TLV in IEEE format
+ * @tlv: Fill ETS Recommended TLV in IEEE format
+ * @dcbcfg: Local store which holds the DCB Config
+ *
+ * Prepare IEEE 802.1Qaz ETS REC TLV
+ **/
+static void i40e_add_ieee_etsrec_tlv(struct i40e_lldp_org_tlv *tlv,
+				     struct i40e_dcbx_config *dcbcfg)
+{
+	struct i40e_dcb_ets_config *etsrec;
+	u16 offset = 0, typelength, i;
+	u8 priority0, priority1;
+	u8 *buf = tlv->tlvinfo;
+	u32 ouisubtype;
+
+	typelength = (u16)((I40E_TLV_TYPE_ORG << I40E_LLDP_TLV_TYPE_SHIFT) |
+			I40E_IEEE_ETS_TLV_LENGTH);
+	tlv->typelength = htons(typelength);
+
+	ouisubtype = (u32)((I40E_IEEE_8021QAZ_OUI << I40E_LLDP_TLV_OUI_SHIFT) |
+			I40E_IEEE_SUBTYPE_ETS_REC);
+	tlv->ouisubtype = htonl(ouisubtype);
+
+	etsrec = &dcbcfg->etsrec;
+	/* First Octet is reserved */
+	/* Move offset to Priority Assignment Table */
+	offset++;
+
+	/* Priority Assignment Table (4 octets)
+	 * Octets:|    1    |    2    |    3    |    4    |
+	 *        -----------------------------------------
+	 *        |pri0|pri1|pri2|pri3|pri4|pri5|pri6|pri7|
+	 *        -----------------------------------------
+	 *   Bits:|7  4|3  0|7  4|3  0|7  4|3  0|7  4|3  0|
+	 *        -----------------------------------------
+	 */
+	for (i = 0; i < 4; i++) {
+		priority0 = etsrec->prioritytable[i * 2] & 0xF;
+		priority1 = etsrec->prioritytable[i * 2 + 1] & 0xF;
+		buf[offset] = (priority0 << I40E_IEEE_ETS_PRIO_1_SHIFT) |
+				priority1;
+		offset++;
+	}
+
+	/* TC Bandwidth Table (8 octets)
+	 * Octets:| 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 |
+	 *        ---------------------------------
+	 *        |tc0|tc1|tc2|tc3|tc4|tc5|tc6|tc7|
+	 *        ---------------------------------
+	 */
+	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++)
+		buf[offset++] = etsrec->tcbwtable[i];
+
+	/* TSA Assignment Table (8 octets)
+	 * Octets:| 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 |
+	 *        ---------------------------------
+	 *        |tc0|tc1|tc2|tc3|tc4|tc5|tc6|tc7|
+	 *        ---------------------------------
+	 */
+	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++)
+		buf[offset++] = etsrec->tsatable[i];
+}
+
+ /**
+ * i40e_add_ieee_pfc_tlv - Prepare PFC TLV in IEEE format
+ * @tlv: Fill PFC TLV in IEEE format
+ * @dcbcfg: Local store to get PFC CFG data
+ *
+ * Prepare IEEE 802.1Qaz PFC CFG TLV
+ **/
+static void i40e_add_ieee_pfc_tlv(struct i40e_lldp_org_tlv *tlv,
+				  struct i40e_dcbx_config *dcbcfg)
+{
+	u8 *buf = tlv->tlvinfo;
+	u32 ouisubtype;
+	u16 typelength;
+
+	typelength = (u16)((I40E_TLV_TYPE_ORG << I40E_LLDP_TLV_TYPE_SHIFT) |
+			I40E_IEEE_PFC_TLV_LENGTH);
+	tlv->typelength = htons(typelength);
+
+	ouisubtype = (u32)((I40E_IEEE_8021QAZ_OUI << I40E_LLDP_TLV_OUI_SHIFT) |
+			I40E_IEEE_SUBTYPE_PFC_CFG);
+	tlv->ouisubtype = htonl(ouisubtype);
+
+	/* ----------------------------------------
+	 * |will-|MBC  | Re-  | PFC |  PFC Enable  |
+	 * |ing  |     |served| cap |              |
+	 * -----------------------------------------
+	 * |1bit | 1bit|2 bits|4bits| 1 octet      |
+	 */
+	if (dcbcfg->pfc.willing)
+		buf[0] = BIT(I40E_IEEE_PFC_WILLING_SHIFT);
+
+	if (dcbcfg->pfc.mbc)
+		buf[0] |= BIT(I40E_IEEE_PFC_MBC_SHIFT);
+
+	buf[0] |= dcbcfg->pfc.pfccap & 0xF;
+	buf[1] = dcbcfg->pfc.pfcenable;
+}
+
+/**
+ * i40e_add_ieee_app_pri_tlv -  Prepare APP TLV in IEEE format
+ * @tlv: Fill APP TLV in IEEE format
+ * @dcbcfg: Local store to get APP CFG data
+ *
+ * Prepare IEEE 802.1Qaz APP CFG TLV
+ **/
+static void i40e_add_ieee_app_pri_tlv(struct i40e_lldp_org_tlv *tlv,
+				      struct i40e_dcbx_config *dcbcfg)
+{
+	u16 typelength, length, offset = 0;
+	u8 priority, selector, i = 0;
+	u8 *buf = tlv->tlvinfo;
+	u32 ouisubtype;
+
+	/* No APP TLVs then just return */
+	if (dcbcfg->numapps == 0)
+		return;
+	ouisubtype = (u32)((I40E_IEEE_8021QAZ_OUI << I40E_LLDP_TLV_OUI_SHIFT) |
+			I40E_IEEE_SUBTYPE_APP_PRI);
+	tlv->ouisubtype = htonl(ouisubtype);
+
+	/* Move offset to App Priority Table */
+	offset++;
+	/* Application Priority Table (3 octets)
+	 * Octets:|         1          |    2    |    3    |
+	 *        -----------------------------------------
+	 *        |Priority|Rsrvd| Sel |    Protocol ID    |
+	 *        -----------------------------------------
+	 *   Bits:|23    21|20 19|18 16|15                0|
+	 *        -----------------------------------------
+	 */
+	while (i < dcbcfg->numapps) {
+		priority = dcbcfg->app[i].priority & 0x7;
+		selector = dcbcfg->app[i].selector & 0x7;
+		buf[offset] = (priority << I40E_IEEE_APP_PRIO_SHIFT) | selector;
+		buf[offset + 1] = (dcbcfg->app[i].protocolid >> 0x8) & 0xFF;
+		buf[offset + 2] =  dcbcfg->app[i].protocolid & 0xFF;
+		/* Move to next app */
+		offset += 3;
+		i++;
+		if (i >= I40E_DCBX_MAX_APPS)
+			break;
+	}
+	/* length includes size of ouisubtype + 1 reserved + 3*numapps */
+	length = sizeof(tlv->ouisubtype) + 1 + (i*3);
+	typelength = (u16)((I40E_TLV_TYPE_ORG << I40E_LLDP_TLV_TYPE_SHIFT) |
+		(length & 0x1FF));
+	tlv->typelength = htons(typelength);
+}
+
+ /**
+ * i40e_add_dcb_tlv - Add all IEEE TLVs
+ * @tlv: pointer to org tlv
+ *
+ * add tlv information
+ **/
+static void i40e_add_dcb_tlv(struct i40e_lldp_org_tlv *tlv,
+			     struct i40e_dcbx_config *dcbcfg,
+			     u16 tlvid)
+{
+	switch (tlvid) {
+	case I40E_IEEE_TLV_ID_ETS_CFG:
+		i40e_add_ieee_ets_tlv(tlv, dcbcfg);
+		break;
+	case I40E_IEEE_TLV_ID_ETS_REC:
+		i40e_add_ieee_etsrec_tlv(tlv, dcbcfg);
+		break;
+	case I40E_IEEE_TLV_ID_PFC_CFG:
+		i40e_add_ieee_pfc_tlv(tlv, dcbcfg);
+		break;
+	case I40E_IEEE_TLV_ID_APP_PRI:
+		i40e_add_ieee_app_pri_tlv(tlv, dcbcfg);
+		break;
+	default:
+		break;
+	}
+}
+
+ /**
+ * i40e_set_dcb_config - Set the local LLDP MIB to FW
+ * @hw: pointer to the hw struct
+ *
+ * Set DCB configuration to the Firmware
+ **/
+i40e_status i40e_set_dcb_config(struct i40e_hw *hw)
+{
+	i40e_status ret = I40E_SUCCESS;
+	struct i40e_dcbx_config *dcbcfg;
+	struct i40e_virt_mem mem;
+	u8 mib_type, *lldpmib;
+	u16 miblen;
+
+	/* update the hw local config */
+	dcbcfg = &hw->local_dcbx_config;
+	/* Allocate the LLDPDU */
+	ret = i40e_allocate_virt_mem(hw, &mem, I40E_LLDPDU_SIZE);
+	if (ret)
+		return ret;
+
+	mib_type = SET_LOCAL_MIB_AC_TYPE_LOCAL_MIB;
+	if (dcbcfg->app_mode == I40E_DCBX_APPS_NON_WILLING) {
+		mib_type |= SET_LOCAL_MIB_AC_TYPE_NON_WILLING_APPS <<
+			    SET_LOCAL_MIB_AC_TYPE_NON_WILLING_APPS_SHIFT;
+	}
+	lldpmib = (u8 *)mem.va;
+	ret = i40e_dcb_config_to_lldp(lldpmib, &miblen, dcbcfg);
+	ret = i40e_aq_set_lldp_mib(hw, mib_type, (void *)lldpmib, miblen, NULL);
+
+	i40e_free_virt_mem(hw, &mem);
+	return ret;
+}
+
+/**
+ * i40e_dcb_config_to_lldp - Convert Dcbconfig to MIB format
+ * @lldpmib: pointer to mib to be output
+ * @miblen: pointer to u16 for length of lldpmib
+ * @dcbcfg: store for LLDPDU data
+ *
+ * send DCB configuration to FW
+ **/
+i40e_status i40e_dcb_config_to_lldp(u8 *lldpmib, u16 *miblen,
+					      struct i40e_dcbx_config *dcbcfg)
+{
+	u16 length, offset = 0, tlvid = I40E_TLV_ID_START;
+	i40e_status ret = I40E_SUCCESS;
+	struct i40e_lldp_org_tlv *tlv;
+	u16 typelength;
+
+	tlv = (struct i40e_lldp_org_tlv *)lldpmib;
+	while (1) {
+		i40e_add_dcb_tlv(tlv, dcbcfg, tlvid++);
+		typelength = ntohs(tlv->typelength);
+		length = (u16)((typelength & I40E_LLDP_TLV_LEN_MASK) >>
+				I40E_LLDP_TLV_LEN_SHIFT);
+		if (length)
+			offset += length + 2;
+		/* END TLV or beyond LLDPDU size */
+		if ((tlvid >= I40E_TLV_ID_END_OF_LLDPPDU) ||
+		    (offset > I40E_LLDPDU_SIZE))
+			break;
+		/* Move to next TLV */
+		if (length)
+			tlv = (struct i40e_lldp_org_tlv *)((char *)tlv +
+			      sizeof(tlv->typelength) + length);
+	}
+	*miblen = offset;
+	return ret;
+}
+
+/**
+ * i40e_process_lldp_event
+ * @hw: pointer to the hw struct
+ * @e: event data to be processed (LLDPDU)
+ *
+ * Process LLDP MIB Change event from the Firmware
+ **/
+i40e_status i40e_process_lldp_event(struct i40e_hw *hw,
+					      struct i40e_arq_event_info *e)
+{
+	i40e_status ret = I40E_SUCCESS;
+
+	return ret;
+}
+
+/**
+ * i40e_dcb_hw_rx_fifo_config
+ * @hw: pointer to the hw struct
+ * @ets_mode: Strict Priority or Round Robin mode
+ * @non_ets_mode: Strict Priority or Round Robin
+ * @max_exponent: Exponent to calculate max refill credits
+ * @lltc_map: Low latency TC bitmap
+ *
+ * Configure HW Rx FIFO as part of DCB configuration.
+ **/
+void i40e_dcb_hw_rx_fifo_config(struct i40e_hw *hw,
+				enum i40e_dcb_arbiter_mode ets_mode,
+				enum i40e_dcb_arbiter_mode non_ets_mode,
+				u32 max_exponent,
+				u8 lltc_map)
+{
+	u32 reg = 0;
+
+	reg = rd32(hw, I40E_PRTDCB_RETSC);
+
+	reg &= ~I40E_PRTDCB_RETSC_ETS_MODE_MASK;
+	reg |= ((u32)ets_mode << I40E_PRTDCB_RETSC_ETS_MODE_SHIFT) &
+		I40E_PRTDCB_RETSC_ETS_MODE_MASK;
+
+	reg &= ~I40E_PRTDCB_RETSC_NON_ETS_MODE_MASK;
+	reg |= ((u32)non_ets_mode << I40E_PRTDCB_RETSC_NON_ETS_MODE_SHIFT) &
+		I40E_PRTDCB_RETSC_NON_ETS_MODE_MASK;
+
+	reg &= ~I40E_PRTDCB_RETSC_ETS_MAX_EXP_MASK;
+	reg |= (max_exponent << I40E_PRTDCB_RETSC_ETS_MAX_EXP_SHIFT) &
+		I40E_PRTDCB_RETSC_ETS_MAX_EXP_MASK;
+
+	reg &= ~I40E_PRTDCB_RETSC_LLTC_MASK;
+	reg |= (lltc_map << I40E_PRTDCB_RETSC_LLTC_SHIFT) &
+		I40E_PRTDCB_RETSC_LLTC_MASK;
+	wr32(hw, I40E_PRTDCB_RETSC, reg);
+}
+
+/**
+ * i40e_dcb_hw_rx_cmd_monitor_config
+ * @hw: pointer to the hw struct
+ * @num_tc: Total number of traffic class
+ * @num_ports: Total number of ports on device
+ *
+ * Configure HW Rx command monitor as part of DCB configuration.
+ **/
+void i40e_dcb_hw_rx_cmd_monitor_config(struct i40e_hw *hw,
+				       u8 num_tc, u8 num_ports)
+{
+	u32 threshold = 0;
+	u32 fifo_size = 0;
+	u32 reg = 0;
+
+	/* Set the threshold and fifo_size based on number of ports */
+	switch (num_ports) {
+	case 1:
+		threshold = 0xF;
+		fifo_size = 0x10;
+		break;
+	case 2:
+		if (num_tc > 4) {
+			threshold = 0xC;
+			fifo_size = 0x8;
+		} else {
+			threshold = 0xF;
+			fifo_size = 0x10;
+		}
+		break;
+	case 4:
+		if (num_tc > 4) {
+			threshold = 0x6;
+			fifo_size = 0x4;
+		} else {
+			threshold = 0x9;
+			fifo_size = 0x8;
+		}
+		break;
+	}
+
+	/* The hardware manual describes setting up of I40E_PRT_SWR_PM_THR
+	 * based on the number of ports and traffic classes for a given port as
+	 * part of DCB configuration.
+	 */
+	reg = rd32(hw, I40E_PRT_SWR_PM_THR);
+	reg &= ~I40E_PRT_SWR_PM_THR_THRESHOLD_MASK;
+	reg |= (threshold << I40E_PRT_SWR_PM_THR_THRESHOLD_SHIFT) &
+		I40E_PRT_SWR_PM_THR_THRESHOLD_MASK;
+	wr32(hw, I40E_PRT_SWR_PM_THR, reg);
+
+	reg = rd32(hw, I40E_PRTDCB_RPPMC);
+	reg &= ~I40E_PRTDCB_RPPMC_RX_FIFO_SIZE_MASK;
+	reg |= (fifo_size << I40E_PRTDCB_RPPMC_RX_FIFO_SIZE_SHIFT) &
+		I40E_PRTDCB_RPPMC_RX_FIFO_SIZE_MASK;
+	wr32(hw, I40E_PRTDCB_RPPMC, reg);
+}
+
+/**
+ * i40e_dcb_hw_pfc_config
+ * @hw: pointer to the hw struct
+ * @pfc_en: Bitmap of PFC enabled priorities
+ * @prio_tc: priority to tc assignment indexed by priority
+ *
+ * Configure HW Priority Flow Controller as part of DCB configuration.
+ **/
+void i40e_dcb_hw_pfc_config(struct i40e_hw *hw,
+			    u8 pfc_en, u8 *prio_tc)
+{
+	u16 pause_time = I40E_DEFAULT_PAUSE_TIME;
+	u16 refresh_time = pause_time/2;
+	u8 first_pfc_prio = 0;
+	u32 link_speed = 0;
+	u8 num_pfc_tc = 0;
+	u8 tc2pfc = 0;
+	u32 reg = 0;
+	u8 i;
+
+	/* Get Number of PFC TCs and TC2PFC map */
+	for (i = 0; i < I40E_MAX_USER_PRIORITY; i++) {
+		if (pfc_en & BIT(i)) {
+			if (!first_pfc_prio)
+				first_pfc_prio = i;
+			/* Set bit for the PFC TC */
+			tc2pfc |= BIT(prio_tc[i]);
+			num_pfc_tc++;
+		}
+	}
+
+	link_speed = hw->phy.link_info.link_speed;
+	switch (link_speed) {
+	case I40E_LINK_SPEED_10GB:
+		reg = rd32(hw, I40E_PRTDCB_MFLCN);
+		reg |= BIT(I40E_PRTDCB_MFLCN_DPF_SHIFT) &
+			I40E_PRTDCB_MFLCN_DPF_MASK;
+		reg &= ~I40E_PRTDCB_MFLCN_RFCE_MASK;
+		reg &= ~I40E_PRTDCB_MFLCN_RPFCE_MASK;
+		if (pfc_en) {
+			reg |= BIT(I40E_PRTDCB_MFLCN_RPFCM_SHIFT) &
+				I40E_PRTDCB_MFLCN_RPFCM_MASK;
+			reg |= ((u32)pfc_en << I40E_PRTDCB_MFLCN_RPFCE_SHIFT) &
+				I40E_PRTDCB_MFLCN_RPFCE_MASK;
+		}
+		wr32(hw, I40E_PRTDCB_MFLCN, reg);
+
+		reg = rd32(hw, I40E_PRTDCB_FCCFG);
+		reg &= ~I40E_PRTDCB_FCCFG_TFCE_MASK;
+		if (pfc_en)
+			reg |= (2 << I40E_PRTDCB_FCCFG_TFCE_SHIFT) &
+				I40E_PRTDCB_FCCFG_TFCE_MASK;
+		wr32(hw, I40E_PRTDCB_FCCFG, reg);
+
+		/* FCTTV and FCRTV to be set by default */
+		break;
+	case I40E_LINK_SPEED_40GB:
+		reg = rd32(hw, I40E_PRTMAC_HSEC_CTL_RX_ENABLE_GPP);
+		reg &= ~I40E_PRTMAC_HSEC_CTL_RX_ENABLE_GPP_HSEC_CTL_RX_ENABLE_GPP_MASK;
+		wr32(hw, I40E_PRTMAC_HSEC_CTL_RX_ENABLE_GPP, reg);
+
+		reg = rd32(hw, I40E_PRTMAC_HSEC_CTL_RX_ENABLE_PPP);
+		reg &= ~I40E_PRTMAC_HSEC_CTL_RX_ENABLE_GPP_HSEC_CTL_RX_ENABLE_GPP_MASK;
+		reg |= BIT(I40E_PRTMAC_HSEC_CTL_RX_ENABLE_PPP_HSEC_CTL_RX_ENABLE_PPP_SHIFT) &
+			I40E_PRTMAC_HSEC_CTL_RX_ENABLE_PPP_HSEC_CTL_RX_ENABLE_PPP_MASK;
+		wr32(hw, I40E_PRTMAC_HSEC_CTL_RX_ENABLE_PPP, reg);
+
+		reg = rd32(hw, I40E_PRTMAC_HSEC_CTL_RX_PAUSE_ENABLE);
+		reg &= ~I40E_PRTMAC_HSEC_CTL_RX_PAUSE_ENABLE_HSEC_CTL_RX_PAUSE_ENABLE_MASK;
+		reg |= ((u32)pfc_en <<
+			   I40E_PRTMAC_HSEC_CTL_RX_PAUSE_ENABLE_HSEC_CTL_RX_PAUSE_ENABLE_SHIFT) &
+			I40E_PRTMAC_HSEC_CTL_RX_PAUSE_ENABLE_HSEC_CTL_RX_PAUSE_ENABLE_MASK;
+		wr32(hw, I40E_PRTMAC_HSEC_CTL_RX_PAUSE_ENABLE, reg);
+
+		reg = rd32(hw, I40E_PRTMAC_HSEC_CTL_TX_PAUSE_ENABLE);
+		reg &= ~I40E_PRTMAC_HSEC_CTL_TX_PAUSE_ENABLE_HSEC_CTL_TX_PAUSE_ENABLE_MASK;
+		reg |= ((u32)pfc_en <<
+			   I40E_PRTMAC_HSEC_CTL_TX_PAUSE_ENABLE_HSEC_CTL_TX_PAUSE_ENABLE_SHIFT) &
+			I40E_PRTMAC_HSEC_CTL_TX_PAUSE_ENABLE_HSEC_CTL_TX_PAUSE_ENABLE_MASK;
+		wr32(hw, I40E_PRTMAC_HSEC_CTL_TX_PAUSE_ENABLE, reg);
+
+		for (i = 0; i < I40E_PRTMAC_HSEC_CTL_TX_PAUSE_REFRESH_TIMER_MAX_INDEX; i++) {
+			reg = rd32(hw, I40E_PRTMAC_HSEC_CTL_TX_PAUSE_REFRESH_TIMER(i));
+			reg &= ~I40E_PRTMAC_HSEC_CTL_TX_PAUSE_REFRESH_TIMER_HSEC_CTL_TX_PAUSE_REFRESH_TIMER_MASK;
+			if (pfc_en) {
+				reg |= ((u32)refresh_time <<
+					I40E_PRTMAC_HSEC_CTL_TX_PAUSE_REFRESH_TIMER_HSEC_CTL_TX_PAUSE_REFRESH_TIMER_SHIFT) &
+					I40E_PRTMAC_HSEC_CTL_TX_PAUSE_REFRESH_TIMER_HSEC_CTL_TX_PAUSE_REFRESH_TIMER_MASK;
+			}
+			wr32(hw, I40E_PRTMAC_HSEC_CTL_TX_PAUSE_REFRESH_TIMER(i), reg);
+		}
+		/*
+		 * PRTMAC_HSEC_CTL_TX_PAUSE_QUANTA default value is 0xFFFF
+		 * for all user priorities
+		 */
+		break;
+	}
+
+	reg = rd32(hw, I40E_PRTDCB_TC2PFC);
+	reg &= ~I40E_PRTDCB_TC2PFC_TC2PFC_MASK;
+	reg |= ((u32)tc2pfc << I40E_PRTDCB_TC2PFC_TC2PFC_SHIFT) &
+		I40E_PRTDCB_TC2PFC_TC2PFC_MASK;
+	wr32(hw, I40E_PRTDCB_TC2PFC, reg);
+
+	reg = rd32(hw, I40E_PRTDCB_RUP);
+	reg &= ~I40E_PRTDCB_RUP_NOVLANUP_MASK;
+	reg |= ((u32)first_pfc_prio << I40E_PRTDCB_RUP_NOVLANUP_SHIFT) &
+		 I40E_PRTDCB_RUP_NOVLANUP_MASK;
+	wr32(hw, I40E_PRTDCB_RUP, reg);
+
+	reg = rd32(hw, I40E_PRTDCB_TDPMC);
+	reg &= ~I40E_PRTDCB_TDPMC_TCPM_MODE_MASK;
+	if (num_pfc_tc > 2) {
+		reg |= BIT(I40E_PRTDCB_TDPMC_TCPM_MODE_SHIFT) &
+			I40E_PRTDCB_TDPMC_TCPM_MODE_MASK;
+	}
+	wr32(hw, I40E_PRTDCB_TDPMC, reg);
+
+	reg = rd32(hw, I40E_PRTDCB_TCPMC);
+	reg &= ~I40E_PRTDCB_TCPMC_TCPM_MODE_MASK;
+	if (num_pfc_tc > 2) {
+		reg |= BIT(I40E_PRTDCB_TCPMC_TCPM_MODE_SHIFT) &
+			I40E_PRTDCB_TCPMC_TCPM_MODE_MASK;
+	}
+	wr32(hw, I40E_PRTDCB_TCPMC, reg);
+}
+
+/**
+ * i40e_dcb_hw_set_num_tc
+ * @hw: pointer to the hw struct
+ * @num_tc: number of traffic classes
+ *
+ * Configure number of traffic classes in HW
+ **/
+void i40e_dcb_hw_set_num_tc(struct i40e_hw *hw, u8 num_tc)
+{
+	u32 reg = rd32(hw, I40E_PRTDCB_GENC);
+
+	reg &= ~I40E_PRTDCB_GENC_NUMTC_MASK;
+	reg |= ((u32)num_tc << I40E_PRTDCB_GENC_NUMTC_SHIFT) &
+		I40E_PRTDCB_GENC_NUMTC_MASK;
+	wr32(hw, I40E_PRTDCB_GENC, reg);
+}
+
+/**
+ * i40e_dcb_hw_get_num_tc
+ * @hw: pointer to the hw struct
+ *
+ * Returns number of traffic classes configured in HW
+ **/
+u8 i40e_dcb_hw_get_num_tc(struct i40e_hw *hw)
+{
+	u32 reg = rd32(hw, I40E_PRTDCB_GENC);
+
+	return (u8)((reg & I40E_PRTDCB_GENC_NUMTC_MASK) >>
+		I40E_PRTDCB_GENC_NUMTC_SHIFT);
+}
+
+/**
+ * i40e_dcb_hw_rx_ets_bw_config
+ * @hw: pointer to the hw struct
+ * @bw_share: Bandwidth share indexed per traffic class
+ * @mode: Strict Priority or Round Robin mode between UP sharing same
+ * traffic class
+ * @prio_type: TC is ETS enabled or strict priority
+ *
+ * Configure HW Rx ETS bandwidth as part of DCB configuration.
+ **/
+void i40e_dcb_hw_rx_ets_bw_config(struct i40e_hw *hw, u8 *bw_share,
+				  u8 *mode, u8 *prio_type)
+{
+	u32 reg = 0;
+	u8 i = 0;
+
+	for (i = 0; i <= I40E_PRTDCB_RETSTCC_MAX_INDEX; i++) {
+		reg = rd32(hw, I40E_PRTDCB_RETSTCC(i));
+		reg &= ~(I40E_PRTDCB_RETSTCC_BWSHARE_MASK     |
+			 I40E_PRTDCB_RETSTCC_UPINTC_MODE_MASK |
+			 I40E_PRTDCB_RETSTCC_ETSTC_SHIFT);
+		reg |= ((u32)bw_share[i] << I40E_PRTDCB_RETSTCC_BWSHARE_SHIFT) &
+			 I40E_PRTDCB_RETSTCC_BWSHARE_MASK;
+		reg |= ((u32)mode[i] << I40E_PRTDCB_RETSTCC_UPINTC_MODE_SHIFT) &
+			 I40E_PRTDCB_RETSTCC_UPINTC_MODE_MASK;
+		reg |= ((u32)prio_type[i] << I40E_PRTDCB_RETSTCC_ETSTC_SHIFT) &
+			 I40E_PRTDCB_RETSTCC_ETSTC_MASK;
+		wr32(hw, I40E_PRTDCB_RETSTCC(i), reg);
+	}
+}
+
+/**
+ * i40e_dcb_hw_rx_up2tc_config
+ * @hw: pointer to the hw struct
+ * @prio_tc: priority to tc assignment indexed by priority
+ *
+ * Configure HW Rx UP2TC map as part of DCB configuration.
+ **/
+void i40e_dcb_hw_rx_up2tc_config(struct i40e_hw *hw, u8 *prio_tc)
+{
+	u32 reg = 0;
+
+#define I40E_UP2TC_REG(val, i) \
+		((val << I40E_PRTDCB_RUP2TC_UP##i##TC_SHIFT) & \
+		  I40E_PRTDCB_RUP2TC_UP##i##TC_MASK)
+
+	reg = rd32(hw, I40E_PRTDCB_RUP2TC);
+	reg |= I40E_UP2TC_REG(prio_tc[0], 0);
+	reg |= I40E_UP2TC_REG(prio_tc[1], 1);
+	reg |= I40E_UP2TC_REG(prio_tc[2], 2);
+	reg |= I40E_UP2TC_REG(prio_tc[3], 3);
+	reg |= I40E_UP2TC_REG(prio_tc[4], 4);
+	reg |= I40E_UP2TC_REG(prio_tc[5], 5);
+	reg |= I40E_UP2TC_REG(prio_tc[6], 6);
+	reg |= I40E_UP2TC_REG(prio_tc[7], 7);
+	wr32(hw, I40E_PRTDCB_RUP2TC, reg);
+}
+
+/**
+ * i40e_dcb_hw_calculate_pool_sizes - configure dcb pool sizes
+ * @hw: pointer to the hw struct
+ * @num_ports: Number of available ports on the device
+ * @eee_enabled: EEE enabled for the given port
+ * @pfc_en: Bit map of PFC enabled traffic classes
+ * @mfs_tc: Array of max frame size for each traffic class
+ * @pb_cfg: pointer to packet buffer configuration
+ *
+ * Calculate the shared and dedicated per TC pool sizes,
+ * watermarks and threshold values.
+ **/
+void i40e_dcb_hw_calculate_pool_sizes(struct i40e_hw *hw,
+				      u8 num_ports, bool eee_enabled,
+				      u8 pfc_en, u32 *mfs_tc,
+				      struct i40e_rx_pb_config *pb_cfg)
+{
+	u32 pool_size[I40E_MAX_TRAFFIC_CLASS];
+	u32 high_wm[I40E_MAX_TRAFFIC_CLASS];
+	u32 low_wm[I40E_MAX_TRAFFIC_CLASS];
+	int shared_pool_size = 0; /* Need signed variable */
+	u32 total_pool_size = 0;
+	u32 port_pb_size = 0;
+	u32 mfs_max = 0;
+	u32 pcirtt = 0;
+	u8 i = 0;
+
+	/* Get the MFS(max) for the port */
+	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
+		if (mfs_tc[i] > mfs_max)
+			mfs_max = mfs_tc[i];
+	}
+
+	pcirtt = I40E_BT2B(I40E_PCIRTT_LINK_SPEED_10G);
+
+	/* Calculate effective Rx PB size per port */
+	port_pb_size = (I40E_DEVICE_RPB_SIZE/num_ports);
+	if (eee_enabled)
+		port_pb_size -= I40E_BT2B(I40E_EEE_TX_LPI_EXIT_TIME);
+	port_pb_size -= mfs_max;
+
+	/* Step 1 Calculating tc pool/shared pool sizes and watermarks */
+	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
+		if (pfc_en & BIT(i)) {
+			low_wm[i] = (2 * mfs_tc[i]) + pcirtt;
+			high_wm[i] = low_wm[i];
+			high_wm[i] += ((mfs_max > I40E_MAX_FRAME_SIZE)
+					? mfs_max : I40E_MAX_FRAME_SIZE);
+			pool_size[i] = high_wm[i];
+			pool_size[i] += I40E_BT2B(I40E_STD_DV_TC(mfs_max,
+								mfs_tc[i]));
+		} else {
+			low_wm[i] = 0;
+			pool_size[i] = (2 * mfs_tc[i]) + pcirtt;
+			high_wm[i] = pool_size[i];
+		}
+		total_pool_size += pool_size[i];
+	}
+
+	shared_pool_size = port_pb_size - total_pool_size;
+	if (shared_pool_size > 0) {
+		pb_cfg->shared_pool_size = shared_pool_size;
+		pb_cfg->shared_pool_high_wm = shared_pool_size;
+		pb_cfg->shared_pool_low_wm = 0;
+		for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
+			pb_cfg->shared_pool_low_thresh[i] = 0;
+			pb_cfg->shared_pool_high_thresh[i] = shared_pool_size;
+			pb_cfg->tc_pool_size[i] = pool_size[i];
+			pb_cfg->tc_pool_high_wm[i] = high_wm[i];
+			pb_cfg->tc_pool_low_wm[i] = low_wm[i];
+		}
+
+	} else {
+		i40e_debug(hw, I40E_DEBUG_DCB,
+			   "The shared pool size for the port is negative %d.\n",
+			   shared_pool_size);
+	}
+}
+
+/**
+ * i40e_dcb_hw_rx_pb_config
+ * @hw: pointer to the hw struct
+ * @old_pb_cfg: Existing Rx Packet buffer configuration
+ * @new_pb_cfg: New Rx Packet buffer configuration
+ *
+ * Program the Rx Packet Buffer registers.
+ **/
+void i40e_dcb_hw_rx_pb_config(struct i40e_hw *hw,
+			      struct i40e_rx_pb_config *old_pb_cfg,
+			      struct i40e_rx_pb_config *new_pb_cfg)
+{
+	u32 old_val = 0;
+	u32 new_val = 0;
+	u32 reg = 0;
+	u8 i = 0;
+
+	/* The Rx Packet buffer register programming needs to be done in a
+	 * certain order and the following code is based on that
+	 * requirement.
+	 */
+
+	/* Program the shared pool low water mark per port if decreasing */
+	old_val = old_pb_cfg->shared_pool_low_wm;
+	new_val = new_pb_cfg->shared_pool_low_wm;
+	if (new_val < old_val) {
+		reg = rd32(hw, I40E_PRTRPB_SLW);
+		reg &= ~I40E_PRTRPB_SLW_SLW_MASK;
+		reg |= (new_val << I40E_PRTRPB_SLW_SLW_SHIFT) &
+			I40E_PRTRPB_SLW_SLW_MASK;
+		wr32(hw, I40E_PRTRPB_SLW, reg);
+	}
+
+	/* Program the shared pool low threshold and tc pool
+	 * low water mark per TC that are decreasing.
+	 */
+	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
+		old_val = old_pb_cfg->shared_pool_low_thresh[i];
+		new_val = new_pb_cfg->shared_pool_low_thresh[i];
+		if (new_val < old_val) {
+			reg = rd32(hw, I40E_PRTRPB_SLT(i));
+			reg &= ~I40E_PRTRPB_SLT_SLT_TCN_MASK;
+			reg |= (new_val << I40E_PRTRPB_SLT_SLT_TCN_SHIFT) &
+				I40E_PRTRPB_SLT_SLT_TCN_MASK;
+			wr32(hw, I40E_PRTRPB_SLT(i), reg);
+		}
+
+		old_val = old_pb_cfg->tc_pool_low_wm[i];
+		new_val = new_pb_cfg->tc_pool_low_wm[i];
+		if (new_val < old_val) {
+			reg = rd32(hw, I40E_PRTRPB_DLW(i));
+			reg &= ~I40E_PRTRPB_DLW_DLW_TCN_MASK;
+			reg |= (new_val << I40E_PRTRPB_DLW_DLW_TCN_SHIFT) &
+				I40E_PRTRPB_DLW_DLW_TCN_MASK;
+			wr32(hw, I40E_PRTRPB_DLW(i), reg);
+		}
+	}
+
+	/* Program the shared pool high water mark per port if decreasing */
+	old_val = old_pb_cfg->shared_pool_high_wm;
+	new_val = new_pb_cfg->shared_pool_high_wm;
+	if (new_val < old_val) {
+		reg = rd32(hw, I40E_PRTRPB_SHW);
+		reg &= ~I40E_PRTRPB_SHW_SHW_MASK;
+		reg |= (new_val << I40E_PRTRPB_SHW_SHW_SHIFT) &
+			I40E_PRTRPB_SHW_SHW_MASK;
+		wr32(hw, I40E_PRTRPB_SHW, reg);
+	}
+
+	/* Program the shared pool high threshold and tc pool
+	 * high water mark per TC that are decreasing.
+	 */
+	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
+		old_val = old_pb_cfg->shared_pool_high_thresh[i];
+		new_val = new_pb_cfg->shared_pool_high_thresh[i];
+		if (new_val < old_val) {
+			reg = rd32(hw, I40E_PRTRPB_SHT(i));
+			reg &= ~I40E_PRTRPB_SHT_SHT_TCN_MASK;
+			reg |= (new_val << I40E_PRTRPB_SHT_SHT_TCN_SHIFT) &
+				I40E_PRTRPB_SHT_SHT_TCN_MASK;
+			wr32(hw, I40E_PRTRPB_SHT(i), reg);
+		}
+
+		old_val = old_pb_cfg->tc_pool_high_wm[i];
+		new_val = new_pb_cfg->tc_pool_high_wm[i];
+		if (new_val < old_val) {
+			reg = rd32(hw, I40E_PRTRPB_DHW(i));
+			reg &= ~I40E_PRTRPB_DHW_DHW_TCN_MASK;
+			reg |= (new_val << I40E_PRTRPB_DHW_DHW_TCN_SHIFT) &
+				I40E_PRTRPB_DHW_DHW_TCN_MASK;
+			wr32(hw, I40E_PRTRPB_DHW(i), reg);
+		}
+	}
+
+	/* Write Dedicated Pool Sizes per TC */
+	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
+		new_val = new_pb_cfg->tc_pool_size[i];
+		reg = rd32(hw, I40E_PRTRPB_DPS(i));
+		reg &= ~I40E_PRTRPB_DPS_DPS_TCN_MASK;
+		reg |= (new_val << I40E_PRTRPB_DPS_DPS_TCN_SHIFT) &
+			I40E_PRTRPB_DPS_DPS_TCN_MASK;
+		wr32(hw, I40E_PRTRPB_DPS(i), reg);
+	}
+
+	/* Write Shared Pool Size per port */
+	new_val = new_pb_cfg->shared_pool_size;
+	reg = rd32(hw, I40E_PRTRPB_SPS);
+	reg &= ~I40E_PRTRPB_SPS_SPS_MASK;
+	reg |= (new_val << I40E_PRTRPB_SPS_SPS_SHIFT) &
+		I40E_PRTRPB_SPS_SPS_MASK;
+	wr32(hw, I40E_PRTRPB_SPS, reg);
+
+	/* Program the shared pool low water mark per port if increasing */
+	old_val = old_pb_cfg->shared_pool_low_wm;
+	new_val = new_pb_cfg->shared_pool_low_wm;
+	if (new_val > old_val) {
+		reg = rd32(hw, I40E_PRTRPB_SLW);
+		reg &= ~I40E_PRTRPB_SLW_SLW_MASK;
+		reg |= (new_val << I40E_PRTRPB_SLW_SLW_SHIFT) &
+			I40E_PRTRPB_SLW_SLW_MASK;
+		wr32(hw, I40E_PRTRPB_SLW, reg);
+	}
+
+	/* Program the shared pool low threshold and tc pool
+	 * low water mark per TC that are increasing.
+	 */
+	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
+		old_val = old_pb_cfg->shared_pool_low_thresh[i];
+		new_val = new_pb_cfg->shared_pool_low_thresh[i];
+		if (new_val > old_val) {
+			reg = rd32(hw, I40E_PRTRPB_SLT(i));
+			reg &= ~I40E_PRTRPB_SLT_SLT_TCN_MASK;
+			reg |= (new_val << I40E_PRTRPB_SLT_SLT_TCN_SHIFT) &
+				I40E_PRTRPB_SLT_SLT_TCN_MASK;
+			wr32(hw, I40E_PRTRPB_SLT(i), reg);
+		}
+
+		old_val = old_pb_cfg->tc_pool_low_wm[i];
+		new_val = new_pb_cfg->tc_pool_low_wm[i];
+		if (new_val > old_val) {
+			reg = rd32(hw, I40E_PRTRPB_DLW(i));
+			reg &= ~I40E_PRTRPB_DLW_DLW_TCN_MASK;
+			reg |= (new_val << I40E_PRTRPB_DLW_DLW_TCN_SHIFT) &
+				I40E_PRTRPB_DLW_DLW_TCN_MASK;
+			wr32(hw, I40E_PRTRPB_DLW(i), reg);
+		}
+	}
+
+	/* Program the shared pool high water mark per port if increasing */
+	old_val = old_pb_cfg->shared_pool_high_wm;
+	new_val = new_pb_cfg->shared_pool_high_wm;
+	if (new_val > old_val) {
+		reg = rd32(hw, I40E_PRTRPB_SHW);
+		reg &= ~I40E_PRTRPB_SHW_SHW_MASK;
+		reg |= (new_val << I40E_PRTRPB_SHW_SHW_SHIFT) &
+			I40E_PRTRPB_SHW_SHW_MASK;
+		wr32(hw, I40E_PRTRPB_SHW, reg);
+	}
+
+	/* Program the shared pool high threshold and tc pool
+	 * high water mark per TC that are increasing.
+	 */
+	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
+		old_val = old_pb_cfg->shared_pool_high_thresh[i];
+		new_val = new_pb_cfg->shared_pool_high_thresh[i];
+		if (new_val > old_val) {
+			reg = rd32(hw, I40E_PRTRPB_SHT(i));
+			reg &= ~I40E_PRTRPB_SHT_SHT_TCN_MASK;
+			reg |= (new_val << I40E_PRTRPB_SHT_SHT_TCN_SHIFT) &
+				I40E_PRTRPB_SHT_SHT_TCN_MASK;
+			wr32(hw, I40E_PRTRPB_SHT(i), reg);
+		}
+
+		old_val = old_pb_cfg->tc_pool_high_wm[i];
+		new_val = new_pb_cfg->tc_pool_high_wm[i];
+		if (new_val > old_val) {
+			reg = rd32(hw, I40E_PRTRPB_DHW(i));
+			reg &= ~I40E_PRTRPB_DHW_DHW_TCN_MASK;
+			reg |= (new_val << I40E_PRTRPB_DHW_DHW_TCN_SHIFT) &
+				I40E_PRTRPB_DHW_DHW_TCN_MASK;
+			wr32(hw, I40E_PRTRPB_DHW(i), reg);
+		}
+	}
+}
+
+/**
  * _i40e_read_lldp_cfg - generic read of LLDP Configuration data from NVM
  * @hw: pointer to the HW structure
  * @lldp_cfg: pointer to hold lldp configuration variables
@@ -940,8 +1908,8 @@
  * Reads the LLDP configuration data from NVM using passed addresses
  **/
 static i40e_status _i40e_read_lldp_cfg(struct i40e_hw *hw,
-				       struct i40e_lldp_variables *lldp_cfg,
-				       u8 module, u32 word_offset)
+					  struct i40e_lldp_variables *lldp_cfg,
+					  u8 module, u32 word_offset)
 {
 	u32 address, offset = (2 * word_offset);
 	i40e_status ret;
@@ -949,16 +1917,16 @@
 	u16 mem;
 
 	ret = i40e_acquire_nvm(hw, I40E_RESOURCE_READ);
-	if (ret)
+	if (ret != I40E_SUCCESS)
 		return ret;
 
 	ret = i40e_aq_read_nvm(hw, 0x0, module * 2, sizeof(raw_mem), &raw_mem,
 			       true, NULL);
 	i40e_release_nvm(hw);
-	if (ret)
+	if (ret != I40E_SUCCESS)
 		return ret;
 
-	mem = le16_to_cpu(raw_mem);
+	mem = LE16_TO_CPU(raw_mem);
 	/* Check if this pointer needs to be read in word size or 4K sector
 	 * units.
 	 */
@@ -968,21 +1936,21 @@
 		address = (0x7FFF & mem) * 2;
 
 	ret = i40e_acquire_nvm(hw, I40E_RESOURCE_READ);
-	if (ret)
+	if (ret != I40E_SUCCESS)
 		goto err_lldp_cfg;
 
 	ret = i40e_aq_read_nvm(hw, module, offset, sizeof(raw_mem), &raw_mem,
 			       true, NULL);
 	i40e_release_nvm(hw);
-	if (ret)
+	if (ret != I40E_SUCCESS)
 		return ret;
 
-	mem = le16_to_cpu(raw_mem);
+	mem = LE16_TO_CPU(raw_mem);
 	offset = mem + word_offset;
 	offset *= 2;
 
 	ret = i40e_acquire_nvm(hw, I40E_RESOURCE_READ);
-	if (ret)
+	if (ret != I40E_SUCCESS)
 		goto err_lldp_cfg;
 
 	ret = i40e_aq_read_nvm(hw, 0, address + offset,
@@ -1002,22 +1970,22 @@
  * Reads the LLDP configuration data from NVM
  **/
 i40e_status i40e_read_lldp_cfg(struct i40e_hw *hw,
-			       struct i40e_lldp_variables *lldp_cfg)
+					 struct i40e_lldp_variables *lldp_cfg)
 {
-	i40e_status ret = 0;
+	i40e_status ret = I40E_SUCCESS;
 	u32 mem;
 
 	if (!lldp_cfg)
 		return I40E_ERR_PARAM;
 
 	ret = i40e_acquire_nvm(hw, I40E_RESOURCE_READ);
-	if (ret)
+	if (ret != I40E_SUCCESS)
 		return ret;
 
 	ret = i40e_aq_read_nvm(hw, I40E_SR_NVM_CONTROL_WORD, 0, sizeof(mem),
 			       &mem, true, NULL);
 	i40e_release_nvm(hw);
-	if (ret)
+	if (ret != I40E_SUCCESS)
 		return ret;
 
 	/* Read a bit that holds information whether we are running flat or
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_dcb.h linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_dcb.h
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_dcb.h	2024-05-10 01:26:45.341079560 -0400
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_dcb.h	2024-05-13 03:58:25.372491940 -0400
@@ -1,5 +1,5 @@
 /* SPDX-License-Identifier: GPL-2.0 */
-/* Copyright(c) 2013 - 2018 Intel Corporation. */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
 
 #ifndef _I40E_DCB_H_
 #define _I40E_DCB_H_
@@ -32,6 +32,9 @@
 #define I40E_CEE_MAX_FEAT_TYPE		3
 #define I40E_LLDP_CURRENT_STATUS_XL710_OFFSET	0x2B
 #define I40E_LLDP_CURRENT_STATUS_X722_OFFSET	0x31
+#define I40E_LLDP_CURRENT_STATUS_OFFSET		1
+#define I40E_LLDP_CURRENT_STATUS_SIZE		1
+
 /* Defines for LLDP TLV header */
 #define I40E_LLDP_TLV_LEN_SHIFT		0
 #define I40E_LLDP_TLV_LEN_MASK		(0x01FF << I40E_LLDP_TLV_LEN_SHIFT)
@@ -77,15 +80,27 @@
 #define I40E_IEEE_APP_PRIO_SHIFT	5
 #define I40E_IEEE_APP_PRIO_MASK		(0x7 << I40E_IEEE_APP_PRIO_SHIFT)
 
-
-#pragma pack(1)
+/* TLV definitions for preparing MIB */
+#define I40E_TLV_ID_CHASSIS_ID		0
+#define I40E_TLV_ID_PORT_ID		1
+#define I40E_TLV_ID_TIME_TO_LIVE	2
+#define I40E_IEEE_TLV_ID_ETS_CFG	3
+#define I40E_IEEE_TLV_ID_ETS_REC	4
+#define I40E_IEEE_TLV_ID_PFC_CFG	5
+#define I40E_IEEE_TLV_ID_APP_PRI	6
+#define I40E_TLV_ID_END_OF_LLDPPDU	7
+#define I40E_TLV_ID_START		I40E_IEEE_TLV_ID_ETS_CFG
+
+#define I40E_IEEE_ETS_TLV_LENGTH	25
+#define I40E_IEEE_PFC_TLV_LENGTH	6
+#define I40E_IEEE_APP_TLV_LENGTH	11
 
 /* IEEE 802.1AB LLDP Organization specific TLV */
 struct i40e_lldp_org_tlv {
 	__be16 typelength;
 	__be32 ouisubtype;
 	u8 tlvinfo[1];
-};
+} __packed;
 
 struct i40e_cee_tlv_hdr {
 	__be16 typelen;
@@ -115,9 +130,114 @@
 #define I40E_CEE_APP_SELECTOR_MASK	0x03
 	__be16 lower_oui;
 	u8 prio_map;
+} __packed;
+
+enum i40e_get_fw_lldp_status_resp {
+	I40E_GET_FW_LLDP_STATUS_DISABLED = 0,
+	I40E_GET_FW_LLDP_STATUS_ENABLED = 1
+};
+
+/* Data structures to pass for SW DCBX */
+struct i40e_rx_pb_config {
+	u32	shared_pool_size;
+	u32	shared_pool_high_wm;
+	u32	shared_pool_low_wm;
+	u32	shared_pool_high_thresh[I40E_MAX_TRAFFIC_CLASS];
+	u32	shared_pool_low_thresh[I40E_MAX_TRAFFIC_CLASS];
+	u32	tc_pool_size[I40E_MAX_TRAFFIC_CLASS];
+	u32	tc_pool_high_wm[I40E_MAX_TRAFFIC_CLASS];
+	u32	tc_pool_low_wm[I40E_MAX_TRAFFIC_CLASS];
 };
-#pragma pack()
 
+enum i40e_dcb_arbiter_mode {
+	I40E_DCB_ARB_MODE_STRICT_PRIORITY = 0,
+	I40E_DCB_ARB_MODE_ROUND_ROBIN = 1
+};
+
+#define I40E_DEFAULT_PAUSE_TIME			0xffff
+#define I40E_MAX_FRAME_SIZE			4608 /* 4.5 KB */
+
+#define I40E_DEVICE_RPB_SIZE			968000 /* 968 KB */
+
+/* BitTimes (BT) conversion */
+#define I40E_BT2KB(BT) ((BT + (8 * 1024 - 1)) / (8 * 1024))
+#define I40E_B2BT(BT) (BT * 8)
+#define I40E_BT2B(BT) ((BT + (8 - 1)) / (8))
+
+/* Max Frame(TC) = MFS(max) + MFS(TC) */
+#define I40E_MAX_FRAME_TC(mfs_max, mfs_tc)	I40E_B2BT(mfs_max + mfs_tc)
+
+/* EEE Tx LPI Exit time in Bit Times */
+#define I40E_EEE_TX_LPI_EXIT_TIME		142500
+
+/* PCI Round Trip Time in Bit Times */
+#define I40E_PCIRTT_LINK_SPEED_10G		20000
+#define I40E_PCIRTT_BYTE_LINK_SPEED_20G		40000
+#define I40E_PCIRTT_BYTE_LINK_SPEED_40G		80000
+
+/* PFC Frame Delay Bit Times */
+#define I40E_PFC_FRAME_DELAY			672
+
+/* Worst case Cable (10GBase-T) Delay Bit Times */
+#define I40E_CABLE_DELAY			5556
+
+/* Higher Layer Delay @10G Bit Times */
+#define I40E_HIGHER_LAYER_DELAY_10G		6144
+
+/* Interface Delays in Bit Times */
+/* TODO: Add for other link speeds 20G/40G/etc. */
+#define I40E_INTERFACE_DELAY_10G_MAC_CONTROL	8192
+#define I40E_INTERFACE_DELAY_10G_MAC		8192
+#define I40E_INTERFACE_DELAY_10G_RS		8192
+
+#define I40E_INTERFACE_DELAY_XGXS		2048
+#define I40E_INTERFACE_DELAY_XAUI		2048
+
+#define I40E_INTERFACE_DELAY_10G_BASEX_PCS	2048
+#define I40E_INTERFACE_DELAY_10G_BASER_PCS	3584
+#define I40E_INTERFACE_DELAY_LX4_PMD		512
+#define I40E_INTERFACE_DELAY_CX4_PMD		512
+#define I40E_INTERFACE_DELAY_SERIAL_PMA		512
+#define I40E_INTERFACE_DELAY_PMD		512
+
+#define I40E_INTERFACE_DELAY_10G_BASET		25600
+
+/* delay values for with 10G BaseT in Bit Times */
+#define I40E_INTERFACE_DELAY_10G_COPPER	\
+	(I40E_INTERFACE_DELAY_10G_MAC + (2 * I40E_INTERFACE_DELAY_XAUI) \
+	 + I40E_INTERFACE_DELAY_10G_BASET)
+#define I40E_DV_TC(mfs_max, mfs_tc) \
+		((2 * I40E_MAX_FRAME_TC(mfs_max, mfs_tc)) \
+		  + I40E_PFC_FRAME_DELAY \
+		  + (2 * I40E_CABLE_DELAY) \
+		  + (2 * I40E_INTERFACE_DELAY_10G_COPPER) \
+		  + I40E_HIGHER_LAYER_DELAY_10G)
+#define I40E_STD_DV_TC(mfs_max, mfs_tc) \
+		(I40E_DV_TC(mfs_max, mfs_tc) + I40E_B2BT(mfs_max))
+
+i40e_status i40e_process_lldp_event(struct i40e_hw *hw,
+					      struct i40e_arq_event_info *e);
+/* APIs for SW DCBX */
+void i40e_dcb_hw_rx_fifo_config(struct i40e_hw *hw,
+				enum i40e_dcb_arbiter_mode ets_mode,
+				enum i40e_dcb_arbiter_mode non_ets_mode,
+				u32 max_exponent, u8 lltc_map);
+void i40e_dcb_hw_rx_cmd_monitor_config(struct i40e_hw *hw,
+				       u8 num_tc, u8 num_ports);
+void i40e_dcb_hw_pfc_config(struct i40e_hw *hw,
+			    u8 pfc_en, u8 *prio_tc);
+void i40e_dcb_hw_set_num_tc(struct i40e_hw *hw, u8 num_tc);
+u8 i40e_dcb_hw_get_num_tc(struct i40e_hw *hw);
+void i40e_dcb_hw_rx_ets_bw_config(struct i40e_hw *hw, u8 *bw_share,
+				  u8 *mode, u8 *prio_type);
+void i40e_dcb_hw_rx_up2tc_config(struct i40e_hw *hw, u8 *prio_tc);
+void i40e_dcb_hw_calculate_pool_sizes(struct i40e_hw *hw,
+				      u8 num_ports, bool eee_enabled,
+				      u8 pfc_en, u32 *mfs_tc,
+				      struct i40e_rx_pb_config *pb_cfg);
+void i40e_dcb_hw_rx_pb_config(struct i40e_hw *hw,
+			      struct i40e_rx_pb_config *old_pb_cfg,
+			      struct i40e_rx_pb_config *new_pb_cfg);
 i40e_status i40e_get_dcbx_status(struct i40e_hw *hw,
 					   u16 *status);
 i40e_status i40e_lldp_to_dcb_config(u8 *lldpmib,
@@ -126,5 +246,12 @@
 					     u8 bridgetype,
 					     struct i40e_dcbx_config *dcbcfg);
 i40e_status i40e_get_dcb_config(struct i40e_hw *hw);
-i40e_status i40e_init_dcb(struct i40e_hw *hw, bool enable_mib_change);
+i40e_status i40e_init_dcb(struct i40e_hw *hw,
+				    bool enable_mib_change);
+enum i40e_status_code
+i40e_get_fw_lldp_status(struct i40e_hw *hw,
+			enum i40e_get_fw_lldp_status_resp *lldp_status);
+i40e_status i40e_set_dcb_config(struct i40e_hw *hw);
+i40e_status i40e_dcb_config_to_lldp(u8 *lldpmib, u16 *miblen,
+					      struct i40e_dcbx_config *dcbcfg);
 #endif /* _I40E_DCB_H_ */
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_dcb_nl.c linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_dcb_nl.c
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_dcb_nl.c	2024-05-10 01:26:45.341079560 -0400
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_dcb_nl.c	2024-05-13 03:58:25.372491940 -0400
@@ -1,10 +1,15 @@
 // SPDX-License-Identifier: GPL-2.0
-/* Copyright(c) 2013 - 2018 Intel Corporation. */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
 
-#ifdef CONFIG_I40E_DCB
+#ifdef CONFIG_DCB
 #include "i40e.h"
 #include <net/dcbnl.h>
 
+#ifdef HAVE_DCBNL_IEEE
+#define I40E_DCBNL_STATUS_SUCCESS	0
+#define I40E_DCBNL_STATUS_ERROR		1
+static bool i40e_dcbnl_find_app(struct i40e_dcbx_config *cfg,
+				struct i40e_dcb_app_priority_table *app);
 /**
  * i40e_get_pfc_delay - retrieve PFC Link Delay
  * @hw: pointer to hardware struct
@@ -40,7 +45,7 @@
 
 	dcbxcfg = &hw->local_dcbx_config;
 	ets->willing = dcbxcfg->etscfg.willing;
-	ets->ets_cap = dcbxcfg->etscfg.maxtcs;
+	ets->ets_cap = I40E_MAX_TRAFFIC_CLASS;
 	ets->cbs = dcbxcfg->etscfg.cbs;
 	memcpy(ets->tc_tx_bw, dcbxcfg->etscfg.tcbwtable,
 		sizeof(ets->tc_tx_bw));
@@ -84,7 +89,7 @@
 	pfc->mbc = dcbxcfg->pfc.mbc;
 	i40e_get_pfc_delay(hw, &pfc->delay);
 
-	/* Get Requests/Indicatiosn */
+	/* Get Requests/Indications */
 	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
 		pfc->requests[i] = pf->stats.priority_xoff_tx[i];
 		pfc->indications[i] = pf->stats.priority_xoff_rx[i];
@@ -94,6 +99,727 @@
 }
 
 /**
+ * i40e_dcbnl_ieee_setets - set IEEE ETS configuration
+ * @netdev : the corresponding netdev
+ * @ets : structure to hold the ETS information
+ *
+ * Set IEEE ETS configuration
+ **/
+static int i40e_dcbnl_ieee_setets(struct net_device *netdev,
+				  struct ieee_ets *ets)
+{
+	struct i40e_pf *pf = i40e_netdev_to_pf(netdev);
+	struct i40e_hw *hw = &pf->hw;
+	struct i40e_dcbx_config *old_cfg = &hw->local_dcbx_config;
+	int i, ret;
+
+	if (!(pf->dcbx_cap & DCB_CAP_DCBX_VER_IEEE) ||
+	    (pf->dcbx_cap & DCB_CAP_DCBX_LLD_MANAGED))
+		return -EINVAL;
+
+	/* Copy current config into temp */
+	pf->tmp_cfg = *old_cfg;
+
+	/* Update the ETS configuration for temp */
+	pf->tmp_cfg.etscfg.willing = ets->willing;
+	pf->tmp_cfg.etscfg.maxtcs = I40E_MAX_TRAFFIC_CLASS;
+	pf->tmp_cfg.etscfg.cbs = ets->cbs;
+	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
+		pf->tmp_cfg.etscfg.tcbwtable[i] = ets->tc_tx_bw[i];
+		pf->tmp_cfg.etscfg.tsatable[i] = ets->tc_tsa[i];
+		pf->tmp_cfg.etscfg.prioritytable[i] = ets->prio_tc[i];
+		pf->tmp_cfg.etsrec.tcbwtable[i] = ets->tc_reco_bw[i];
+		pf->tmp_cfg.etsrec.tsatable[i] = ets->tc_reco_tsa[i];
+		pf->tmp_cfg.etsrec.prioritytable[i] = ets->reco_prio_tc[i];
+	}
+
+	/* Commit changes to HW */
+	ret = i40e_hw_dcb_config(pf, &pf->tmp_cfg);
+	if (ret) {
+		dev_info(&pf->pdev->dev,
+			 "Failed setting DCB ETS configuration err %s aq_err %s\n",
+			 i40e_stat_str(&pf->hw, ret),
+			 i40e_aq_str(&pf->hw, pf->hw.aq.asq_last_status));
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+/**
+ * i40e_dcbnl_ieee_setpfc - set local IEEE PFC configuration
+ * @netdev : the corresponding netdev
+ * @pfc : structure to hold the PFC information
+ *
+ * Sets local IEEE PFC configuration
+ **/
+static int i40e_dcbnl_ieee_setpfc(struct net_device *netdev,
+				  struct ieee_pfc *pfc)
+{
+	struct i40e_pf *pf = i40e_netdev_to_pf(netdev);
+	struct i40e_hw *hw = &pf->hw;
+	struct i40e_dcbx_config *old_cfg = &hw->local_dcbx_config;
+	int ret;
+
+	if (!(pf->dcbx_cap & DCB_CAP_DCBX_VER_IEEE) ||
+	    (pf->dcbx_cap & DCB_CAP_DCBX_LLD_MANAGED))
+		return -EINVAL;
+
+	/* Copy current config into temp */
+	pf->tmp_cfg = *old_cfg;
+	if (pfc->pfc_cap)
+		pf->tmp_cfg.pfc.pfccap = pfc->pfc_cap;
+	else
+		pf->tmp_cfg.pfc.pfccap = I40E_MAX_TRAFFIC_CLASS;
+	pf->tmp_cfg.pfc.pfcenable = pfc->pfc_en;
+
+	ret = i40e_hw_dcb_config(pf, &pf->tmp_cfg);
+	if (ret) {
+		dev_info(&pf->pdev->dev,
+			 "Failed setting DCB PFC configuration err %s aq_err %s\n",
+			 i40e_stat_str(&pf->hw, ret),
+			 i40e_aq_str(&pf->hw, pf->hw.aq.asq_last_status));
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+/**
+ * i40e_dcbnl_ieee_setapp - set local IEEE App configuration
+ * @netdev : the corresponding netdev
+ * @app : structure to hold the Application information
+ *
+ * Sets local IEEE App configuration
+ **/
+static int i40e_dcbnl_ieee_setapp(struct net_device *netdev,
+				  struct dcb_app *app)
+{
+	struct i40e_pf *pf = i40e_netdev_to_pf(netdev);
+	struct i40e_dcb_app_priority_table new_app;
+	struct i40e_hw *hw = &pf->hw;
+	struct i40e_dcbx_config *old_cfg = &hw->local_dcbx_config;
+	int ret;
+
+	if (!(pf->dcbx_cap & DCB_CAP_DCBX_VER_IEEE) ||
+	    (pf->dcbx_cap & DCB_CAP_DCBX_LLD_MANAGED))
+		return -EINVAL;
+
+	if (old_cfg->numapps == I40E_DCBX_MAX_APPS)
+		return -EINVAL;
+
+	ret = dcb_ieee_setapp(netdev, app);
+	if (ret)
+		return ret;
+
+	new_app.selector = app->selector;
+	new_app.protocolid = app->protocol;
+	new_app.priority = app->priority;
+	/* Already internally available */
+	if (i40e_dcbnl_find_app(old_cfg, &new_app))
+		return 0;
+
+	/* Copy current config into temp */
+	pf->tmp_cfg = *old_cfg;
+	/* Add the app */
+	pf->tmp_cfg.app[pf->tmp_cfg.numapps++] = new_app;
+
+	ret = i40e_hw_dcb_config(pf, &pf->tmp_cfg);
+	if (ret) {
+		dev_info(&pf->pdev->dev,
+			 "Failed setting DCB configuration err %s aq_err %s\n",
+			 i40e_stat_str(&pf->hw, ret),
+			 i40e_aq_str(&pf->hw, pf->hw.aq.asq_last_status));
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+/**
+ * i40e_dcbnl_ieee_delapp - delete local IEEE App configuration
+ * @netdev : the corresponding netdev
+ * @app : structure to hold the Application information
+ *
+ * Deletes local IEEE App configuration other than the first application
+ * required by firmware
+ **/
+static int i40e_dcbnl_ieee_delapp(struct net_device *netdev,
+				  struct dcb_app *app)
+{
+	struct i40e_pf *pf = i40e_netdev_to_pf(netdev);
+	struct i40e_hw *hw = &pf->hw;
+	struct i40e_dcbx_config *old_cfg = &hw->local_dcbx_config;
+	int i, j, ret;
+
+	if (!(pf->dcbx_cap & DCB_CAP_DCBX_VER_IEEE) ||
+	    (pf->dcbx_cap & DCB_CAP_DCBX_LLD_MANAGED))
+		return -EINVAL;
+
+	ret = dcb_ieee_delapp(netdev, app);
+	if (ret)
+		return ret;
+
+	/* Need one app for FW so keep it */
+	if (old_cfg->numapps == 1)
+		return 0;
+
+	/* Copy current config into temp */
+	pf->tmp_cfg = *old_cfg;
+
+	/* Find and reset the app */
+	for (i = 1; i < pf->tmp_cfg.numapps; i++) {
+		if (app->selector == pf->tmp_cfg.app[i].selector &&
+		    app->protocol == pf->tmp_cfg.app[i].protocolid &&
+		    app->priority == pf->tmp_cfg.app[i].priority) {
+			/* Reset the app data */
+			pf->tmp_cfg.app[i].selector = 0;
+			pf->tmp_cfg.app[i].protocolid = 0;
+			pf->tmp_cfg.app[i].priority = 0;
+			break;
+		}
+	}
+
+	/* If the specific DCB app not found */
+	if (i == pf->tmp_cfg.numapps)
+		return -EINVAL;
+
+	pf->tmp_cfg.numapps--;
+	/* Overwrite the tmp_cfg app */
+	for (j = i; j < pf->tmp_cfg.numapps; j++)
+		pf->tmp_cfg.app[j] = old_cfg->app[j + 1];
+
+	ret = i40e_hw_dcb_config(pf, &pf->tmp_cfg);
+	if (ret) {
+		dev_info(&pf->pdev->dev,
+			 "Failed setting DCB configuration err %s aq_err %s\n",
+			 i40e_stat_str(&pf->hw, ret),
+			 i40e_aq_str(&pf->hw, pf->hw.aq.asq_last_status));
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+/**
+ * i40e_dcbnl_getstate - Get DCB enabled state
+ * @netdev : the corresponding netdev
+ *
+ * Get the current DCB enabled state
+ **/
+static u8 i40e_dcbnl_getstate(struct net_device *netdev)
+{
+	struct i40e_pf *pf = i40e_netdev_to_pf(netdev);
+
+	dev_dbg(&pf->pdev->dev, "DCB state=%d\n",
+		!!(pf->flags & I40E_FLAG_DCB_ENABLED));
+	return !!(pf->flags & I40E_FLAG_DCB_ENABLED);
+}
+
+/**
+ * i40e_dcbnl_setstate - Set DCB state
+ * @netdev : the corresponding netdev
+ * @state: enable or disable
+ *
+ * Set the DCB state
+ **/
+static u8 i40e_dcbnl_setstate(struct net_device *netdev, u8 state)
+{
+	struct i40e_pf *pf = i40e_netdev_to_pf(netdev);
+	int ret = I40E_DCBNL_STATUS_SUCCESS;
+
+	if (!(pf->dcbx_cap & DCB_CAP_DCBX_VER_CEE) ||
+	    (pf->dcbx_cap & DCB_CAP_DCBX_LLD_MANAGED))
+		return ret;
+
+	dev_dbg(&pf->pdev->dev, "new state=%d current state=%d\n",
+		state, (pf->flags & I40E_FLAG_DCB_ENABLED) ? 1 : 0);
+	/* Nothing to do */
+	if (!state == !(pf->flags & I40E_FLAG_DCB_ENABLED))
+		return ret;
+
+	if (i40e_is_sw_dcb(pf)) {
+		if (state) {
+			pf->flags |= I40E_FLAG_DCB_ENABLED;
+			memcpy(&pf->hw.desired_dcbx_config,
+			       &pf->hw.local_dcbx_config,
+			       sizeof(struct i40e_dcbx_config));
+		} else {
+			pf->flags &= ~I40E_FLAG_DCB_ENABLED;
+		}
+	} else {
+		/* Cannot directly manipulate FW LLDP Agent */
+		ret = I40E_DCBNL_STATUS_ERROR;
+	}
+	return ret;
+}
+
+/**
+ * i40e_dcbnl_set_pg_tc_cfg_tx - Set CEE PG Tx config
+ * @netdev : the corresponding netdev
+ * @tc: the corresponding traffic class
+ * @prio_type: the traffic priority type
+ * @bwg_id: the BW group id the traffic class belongs to
+ * @bw_pct: the BW percentage for the corresponding BWG
+ * @up_map: prio mapped to corresponding tc
+ *
+ * Set Tx PG settings for CEE mode
+ **/
+static void i40e_dcbnl_set_pg_tc_cfg_tx(struct net_device *netdev, int tc,
+					u8 prio_type, u8 bwg_id, u8 bw_pct,
+					u8 up_map)
+{
+	struct i40e_pf *pf = i40e_netdev_to_pf(netdev);
+	int i;
+
+	if (!(pf->dcbx_cap & DCB_CAP_DCBX_VER_CEE) ||
+	    (pf->dcbx_cap & DCB_CAP_DCBX_LLD_MANAGED))
+		return;
+
+	/* LLTC not supported yet */
+	if (tc >= I40E_MAX_TRAFFIC_CLASS)
+		return;
+
+	/* prio_type, bwg_id and bw_pct per UP are not supported */
+
+	/* Use only up_map to map tc */
+	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
+		if (up_map & BIT(i))
+			pf->tmp_cfg.etscfg.prioritytable[i] = tc;
+	}
+	pf->tmp_cfg.etscfg.tsatable[tc] = I40E_IEEE_TSA_ETS;
+	dev_dbg(&pf->pdev->dev,
+		"Set PG config tc=%d bwg_id=%d prio_type=%d bw_pct=%d up_map=%d\n",
+		 tc, bwg_id, prio_type, bw_pct, up_map);
+}
+
+/**
+ * i40e_dcbnl_set_pg_bwg_cfg_tx - Set CEE PG Tx BW config
+ * @netdev : the corresponding netdev
+ * @pgid: the corresponding traffic class
+ * @bw_pct: the BW percentage for the specified traffic class
+ *
+ * Set Tx BW settings for CEE mode
+ **/
+static void i40e_dcbnl_set_pg_bwg_cfg_tx(struct net_device *netdev, int pgid,
+					 u8 bw_pct)
+{
+	struct i40e_pf *pf = i40e_netdev_to_pf(netdev);
+
+	if (!(pf->dcbx_cap & DCB_CAP_DCBX_VER_CEE) ||
+	    (pf->dcbx_cap & DCB_CAP_DCBX_LLD_MANAGED))
+		return;
+
+	/* LLTC not supported yet */
+	if (pgid >= I40E_MAX_TRAFFIC_CLASS)
+		return;
+
+	pf->tmp_cfg.etscfg.tcbwtable[pgid] = bw_pct;
+	dev_dbg(&pf->pdev->dev, "Set PG BW config tc=%d bw_pct=%d\n",
+		pgid, bw_pct);
+}
+
+/**
+ * i40e_dcbnl_set_pg_tc_cfg_rx - Set CEE PG Rx config
+ * @netdev : the corresponding netdev
+ * @prio: the corresponding traffic class
+ * @prio_type: the traffic priority type
+ * @pgid: the BW group id the traffic class belongs to
+ * @bw_pct: the BW percentage for the corresponding BWG
+ * @up_map: prio mapped to corresponding tc
+ *
+ * Set Rx BW settings for CEE mode. The hardware does not support this
+ * so we won't allow setting of this parameter.
+ **/
+static void i40e_dcbnl_set_pg_tc_cfg_rx(struct net_device *netdev,
+					int __always_unused prio,
+					u8 __always_unused prio_type,
+					u8 __always_unused pgid,
+					u8 __always_unused bw_pct,
+					u8 __always_unused up_map)
+{
+	struct i40e_pf *pf = i40e_netdev_to_pf(netdev);
+
+	dev_dbg(&pf->pdev->dev, "Rx TC PG Config Not Supported.\n");
+}
+
+/**
+ * i40e_dcbnl_set_pg_bwg_cfg_rx - Set CEE PG Rx config
+ * @netdev : the corresponding netdev
+ * @pgid: the corresponding traffic class
+ * @bw_pct: the BW percentage for the specified traffic class
+ *
+ * Set Rx BW settings for CEE mode. The hardware does not support this
+ * so we won't allow setting of this parameter.
+ **/
+static void i40e_dcbnl_set_pg_bwg_cfg_rx(struct net_device *netdev, int pgid,
+					 u8 bw_pct)
+{
+	struct i40e_pf *pf = i40e_netdev_to_pf(netdev);
+
+	dev_dbg(&pf->pdev->dev, "Rx BWG PG Config Not Supported.\n");
+}
+
+/**
+ * i40e_dcbnl_get_pg_tc_cfg_tx - Get CEE PG Tx config
+ * @netdev : the corresponding netdev
+ * @prio: the corresponding user priority
+ * @prio_type: traffic priority type
+ * @pgid: the BW group ID the traffic class belongs to
+ * @bw_pct: BW percentage for the corresponding BWG
+ * @up_map: prio mapped to corresponding TC
+ *
+ * Get Tx PG settings for CEE mode
+ **/
+static void i40e_dcbnl_get_pg_tc_cfg_tx(struct net_device *netdev, int prio,
+					u8 __always_unused *prio_type,
+					u8 *pgid,
+					u8 __always_unused *bw_pct,
+					u8 __always_unused *up_map)
+{
+	struct i40e_pf *pf = i40e_netdev_to_pf(netdev);
+
+	if (!(pf->dcbx_cap & DCB_CAP_DCBX_VER_CEE) ||
+	    (pf->dcbx_cap & DCB_CAP_DCBX_LLD_MANAGED))
+		return;
+
+	if (prio >= I40E_MAX_USER_PRIORITY)
+		return;
+
+	*pgid = pf->hw.local_dcbx_config.etscfg.prioritytable[prio];
+	dev_dbg(&pf->pdev->dev, "Get PG config prio=%d tc=%d\n",
+		prio, *pgid);
+}
+
+/**
+ * i40e_dcbnl_get_pg_bwg_cfg_tx - Get CEE PG BW config
+ * @netdev : the corresponding netdev
+ * @pgid: the corresponding traffic class
+ * @bw_pct: the BW percentage for the corresponding TC
+ *
+ * Get Tx BW settings for given TC in CEE mode
+ **/
+static void i40e_dcbnl_get_pg_bwg_cfg_tx(struct net_device *netdev, int pgid,
+					 u8 *bw_pct)
+{
+	struct i40e_pf *pf = i40e_netdev_to_pf(netdev);
+
+	if (!(pf->dcbx_cap & DCB_CAP_DCBX_VER_CEE) ||
+	    (pf->dcbx_cap & DCB_CAP_DCBX_LLD_MANAGED))
+		return;
+
+	if (pgid >= I40E_MAX_TRAFFIC_CLASS)
+		return;
+
+	*bw_pct = pf->hw.local_dcbx_config.etscfg.tcbwtable[pgid];
+	dev_dbg(&pf->pdev->dev, "Get PG BW config tc=%d bw_pct=%d\n",
+		pgid, *bw_pct);
+}
+
+/**
+ * i40e_dcbnl_get_pg_tc_cfg_rx - Get CEE PG Rx config
+ * @netdev : the corresponding netdev
+ * @prio: the corresponding user priority
+ * @prio_type: the traffic priority type
+ * @pgid: the PG ID
+ * @bw_pct: the BW percentage for the corresponding BWG
+ * @up_map: prio mapped to corresponding TC
+ *
+ * Get Rx PG settings for CEE mode. The UP2TC map is applied in same
+ * manner for Tx and Rx (symmetrical) so return the TC information for
+ * given priority accordingly.
+ **/
+static void i40e_dcbnl_get_pg_tc_cfg_rx(struct net_device *netdev, int prio,
+					u8 *prio_type, u8 *pgid, u8 *bw_pct,
+					u8 *up_map)
+{
+	struct i40e_pf *pf = i40e_netdev_to_pf(netdev);
+
+	if (!(pf->dcbx_cap & DCB_CAP_DCBX_VER_CEE) ||
+	    (pf->dcbx_cap & DCB_CAP_DCBX_LLD_MANAGED))
+		return;
+
+	if (prio >= I40E_MAX_USER_PRIORITY)
+		return;
+
+	*pgid = pf->hw.local_dcbx_config.etscfg.prioritytable[prio];
+}
+
+/**
+ * i40e_dcbnl_get_pg_bwg_cfg_rx - Get CEE PG BW Rx config
+ * @netdev : the corresponding netdev
+ * @pgid: the corresponding traffic class
+ * @bw_pct: the BW percentage for the corresponding TC
+ *
+ * Get Rx BW settings for given TC in CEE mode
+ * The adapter doesn't support Rx ETS and runs in strict priority
+ * mode in Rx path and hence just return 0.
+ **/
+static void i40e_dcbnl_get_pg_bwg_cfg_rx(struct net_device *netdev, int pgid,
+					 u8 *bw_pct)
+{
+	struct i40e_pf *pf = i40e_netdev_to_pf(netdev);
+
+	if (!(pf->dcbx_cap & DCB_CAP_DCBX_VER_CEE) ||
+	    (pf->dcbx_cap & DCB_CAP_DCBX_LLD_MANAGED))
+		return;
+	*bw_pct = 0;
+}
+
+/**
+ * i40e_dcbnl_set_pfc_cfg - Set CEE PFC configuration
+ * @netdev: the corresponding netdev
+ * @prio: the corresponding user priority
+ * @setting: the PFC setting for given priority
+ *
+ * Set the PFC enabled/disabled setting for given user priority
+ **/
+static void i40e_dcbnl_set_pfc_cfg(struct net_device *netdev, int prio,
+				   u8 setting)
+{
+	struct i40e_pf *pf = i40e_netdev_to_pf(netdev);
+
+	if (!(pf->dcbx_cap & DCB_CAP_DCBX_VER_CEE) ||
+	    (pf->dcbx_cap & DCB_CAP_DCBX_LLD_MANAGED))
+		return;
+
+	if (prio >= I40E_MAX_USER_PRIORITY)
+		return;
+
+	pf->tmp_cfg.pfc.pfccap = I40E_MAX_TRAFFIC_CLASS;
+	if (setting)
+		pf->tmp_cfg.pfc.pfcenable |= BIT(prio);
+	else
+		pf->tmp_cfg.pfc.pfcenable &= ~BIT(prio);
+	dev_dbg(&pf->pdev->dev,
+		"Set PFC Config up=%d setting=%d pfcenable=0x%x\n",
+		prio, setting, pf->tmp_cfg.pfc.pfcenable);
+}
+
+/**
+ * i40e_dcbnl_get_pfc_cfg - Get CEE PFC configuration
+ * @netdev: the corresponding netdev
+ * @prio: the corresponding user priority
+ * @setting: the PFC setting for given priority
+ *
+ * Get the PFC enabled/disabled setting for given user priority
+ **/
+static void i40e_dcbnl_get_pfc_cfg(struct net_device *netdev, int prio,
+				   u8 *setting)
+{
+	struct i40e_pf *pf = i40e_netdev_to_pf(netdev);
+
+	if (!(pf->dcbx_cap & DCB_CAP_DCBX_VER_CEE) ||
+	    (pf->dcbx_cap & DCB_CAP_DCBX_LLD_MANAGED))
+		return;
+
+	if (prio >= I40E_MAX_USER_PRIORITY)
+		return;
+
+	*setting = (pf->hw.local_dcbx_config.pfc.pfcenable >> prio) & 0x1;
+	dev_dbg(&pf->pdev->dev,
+		"Get PFC Config up=%d setting=%d pfcenable=0x%x\n",
+		prio, *setting, pf->hw.local_dcbx_config.pfc.pfcenable);
+}
+
+/**
+ * i40e_dcbnl_cee_set_all - Commit CEE DCB settings to hardware
+ * @netdev: the corresponding netdev
+ *
+ * Commit the current DCB configuration to hardware
+ **/
+static u8 i40e_dcbnl_cee_set_all(struct net_device *netdev)
+{
+	struct i40e_pf *pf = i40e_netdev_to_pf(netdev);
+	int err;
+
+	if (!(pf->dcbx_cap & DCB_CAP_DCBX_VER_CEE) ||
+	    (pf->dcbx_cap & DCB_CAP_DCBX_LLD_MANAGED))
+		return I40E_DCBNL_STATUS_ERROR;
+
+	dev_dbg(&pf->pdev->dev, "Commit DCB Configuration to the hardware\n");
+	err = i40e_hw_dcb_config(pf, &pf->tmp_cfg);
+
+	return err ? I40E_DCBNL_STATUS_ERROR : I40E_DCBNL_STATUS_SUCCESS;
+}
+
+/**
+ * i40e_dcbnl_get_cap - Get DCBX capabilities of adapter
+ * @netdev: the corresponding netdev
+ * @capid: the capability type
+ * @cap: the capability value
+ *
+ * Return the capability value for a given capability type
+ **/
+static u8 i40e_dcbnl_get_cap(struct net_device *netdev, int capid, u8 *cap)
+{
+	struct i40e_pf *pf = i40e_netdev_to_pf(netdev);
+
+	if (!(pf->flags & I40E_FLAG_DCB_CAPABLE))
+		return I40E_DCBNL_STATUS_ERROR;
+
+	switch (capid) {
+	case DCB_CAP_ATTR_PG:
+		*cap = true;
+		break;
+	case DCB_CAP_ATTR_PFC:
+		*cap = true;
+		break;
+	case DCB_CAP_ATTR_UP2TC:
+		*cap = false;
+		break;
+	case DCB_CAP_ATTR_PG_TCS:
+		*cap = 0x80;
+		break;
+	case DCB_CAP_ATTR_PFC_TCS:
+		*cap = 0x80;
+		break;
+	case DCB_CAP_ATTR_GSP:
+		*cap = false;
+		break;
+	case DCB_CAP_ATTR_BCN:
+		*cap = false;
+		break;
+	case DCB_CAP_ATTR_DCBX:
+		*cap = pf->dcbx_cap;
+		break;
+	default:
+		*cap = false;
+		break;
+	}
+
+	dev_dbg(&pf->pdev->dev, "Get Capability cap=%d capval=0x%x\n",
+		capid, *cap);
+	return I40E_DCBNL_STATUS_SUCCESS;
+}
+
+/**
+ * i40e_dcbnl_getnumtcs - Get max number of traffic classes supported
+ * @netdev: the corresponding netdev
+ * @tcid: the TC id
+ * @num: total number of TCs supported by the device
+ *
+ * Return the total number of TCs supported by the adapter
+ **/
+static int i40e_dcbnl_getnumtcs(struct net_device *netdev, int tcid, u8 *num)
+{
+	struct i40e_pf *pf = i40e_netdev_to_pf(netdev);
+
+	if (!(pf->flags & I40E_FLAG_DCB_CAPABLE))
+		return -EINVAL;
+
+	*num = I40E_MAX_TRAFFIC_CLASS;
+	return 0;
+}
+
+/**
+ * i40e_dcbnl_setnumtcs - Set CEE number of traffic classes
+ * @netdev: the corresponding netdev
+ * @tcid: the TC id
+ * @num: total number of TCs
+ *
+ * Set the total number of TCs (Unsupported)
+ **/
+static int i40e_dcbnl_setnumtcs(struct net_device *netdev, int tcid, u8 num)
+{
+	return -EINVAL;
+}
+
+/**
+ * i40e_dcbnl_getpfcstate - Get CEE PFC mode
+ * @netdev: the corresponding netdev
+ *
+ * Get the current PFC enabled state
+ **/
+static u8 i40e_dcbnl_getpfcstate(struct net_device *netdev)
+{
+	struct i40e_pf *pf = i40e_netdev_to_pf(netdev);
+
+	/* Return enabled if any PFC enabled UP */
+	if (pf->hw.local_dcbx_config.pfc.pfcenable)
+		return 1;
+	else
+		return 0;
+}
+
+/**
+ * i40e_dcbnl_setpfcstate - Set CEE PFC mode
+ * @netdev: the corresponding netdev
+ * @state: required state
+ *
+ * The PFC state to be set; this is enabled/disabled based on the PFC
+ * priority settings and not via this call for i40e driver
+ **/
+static void i40e_dcbnl_setpfcstate(struct net_device *netdev, u8 state)
+{
+	struct i40e_pf *pf = i40e_netdev_to_pf(netdev);
+
+	dev_dbg(&pf->pdev->dev, "PFC State is modified via PFC config.\n");
+}
+
+/**
+ * i40e_dcbnl_getapp - Get CEE APP
+ * @netdev: the corresponding netdev
+ * @idtype: the App selector
+ * @id: the App ethtype or port number
+ *
+ * Return the CEE mode app for the given idtype and id
+ **/
+#ifdef HAVE_DCBNL_OPS_SETAPP_RETURN_INT
+static int i40e_dcbnl_getapp(struct net_device *netdev, u8 idtype, u16 id)
+#else
+static u8 i40e_dcbnl_getapp(struct net_device *netdev, u8 idtype, u16 id)
+#endif
+{
+	struct i40e_pf *pf = i40e_netdev_to_pf(netdev);
+	struct dcb_app app = {
+				.selector = idtype,
+				.protocol = id,
+			     };
+
+	if (!(pf->dcbx_cap & DCB_CAP_DCBX_VER_CEE) ||
+	    (pf->dcbx_cap & DCB_CAP_DCBX_LLD_MANAGED))
+		return -EINVAL;
+
+	return dcb_getapp(netdev, &app);
+}
+
+/**
+ * i40e_dcbnl_setdcbx - set required DCBx capability
+ * @netdev: the corresponding netdev
+ * @mode: new DCB mode managed or CEE+IEEE
+ *
+ * Set DCBx capability features
+ **/
+static u8 i40e_dcbnl_setdcbx(struct net_device *netdev, u8 mode)
+{
+	struct i40e_pf *pf = i40e_netdev_to_pf(netdev);
+
+	/* Do not allow to set mode if managed by Firmware */
+	if (pf->dcbx_cap & DCB_CAP_DCBX_LLD_MANAGED)
+		return I40E_DCBNL_STATUS_ERROR;
+
+	/* No support for LLD_MANAGED modes or CEE+IEEE */
+	if ((mode & DCB_CAP_DCBX_LLD_MANAGED) ||
+	    ((mode & DCB_CAP_DCBX_VER_IEEE) && (mode & DCB_CAP_DCBX_VER_CEE)) ||
+	    !(mode & DCB_CAP_DCBX_HOST))
+		return I40E_DCBNL_STATUS_ERROR;
+
+	/* Already set to the given mode no change */
+	if (mode == pf->dcbx_cap)
+		return I40E_DCBNL_STATUS_SUCCESS;
+
+	pf->dcbx_cap = mode;
+	if (mode & DCB_CAP_DCBX_VER_CEE)
+		pf->hw.local_dcbx_config.dcbx_mode = I40E_DCBX_MODE_CEE;
+	else
+		pf->hw.local_dcbx_config.dcbx_mode = I40E_DCBX_MODE_IEEE;
+
+	dev_dbg(&pf->pdev->dev, "mode=%d\n", mode);
+	return I40E_DCBNL_STATUS_SUCCESS;
+}
+
+/**
  * i40e_dcbnl_getdcbx - retrieve current DCBx capability
  * @dev: the corresponding netdev
  *
@@ -132,7 +858,31 @@
 	.ieee_getets	= i40e_dcbnl_ieee_getets,
 	.ieee_getpfc	= i40e_dcbnl_ieee_getpfc,
 	.getdcbx	= i40e_dcbnl_getdcbx,
-	.getpermhwaddr  = i40e_dcbnl_get_perm_hw_addr,
+	.getpermhwaddr	= i40e_dcbnl_get_perm_hw_addr,
+	.ieee_setets	= i40e_dcbnl_ieee_setets,
+	.ieee_setpfc	= i40e_dcbnl_ieee_setpfc,
+	.ieee_setapp	= i40e_dcbnl_ieee_setapp,
+	.ieee_delapp	= i40e_dcbnl_ieee_delapp,
+	.getstate	= i40e_dcbnl_getstate,
+	.setstate	= i40e_dcbnl_setstate,
+	.setpgtccfgtx	= i40e_dcbnl_set_pg_tc_cfg_tx,
+	.setpgbwgcfgtx	= i40e_dcbnl_set_pg_bwg_cfg_tx,
+	.setpgtccfgrx	= i40e_dcbnl_set_pg_tc_cfg_rx,
+	.setpgbwgcfgrx	= i40e_dcbnl_set_pg_bwg_cfg_rx,
+	.getpgtccfgtx	= i40e_dcbnl_get_pg_tc_cfg_tx,
+	.getpgbwgcfgtx	= i40e_dcbnl_get_pg_bwg_cfg_tx,
+	.getpgtccfgrx	= i40e_dcbnl_get_pg_tc_cfg_rx,
+	.getpgbwgcfgrx	= i40e_dcbnl_get_pg_bwg_cfg_rx,
+	.setpfccfg	= i40e_dcbnl_set_pfc_cfg,
+	.getpfccfg	= i40e_dcbnl_get_pfc_cfg,
+	.setall		= i40e_dcbnl_cee_set_all,
+	.getcap		= i40e_dcbnl_get_cap,
+	.getnumtcs	= i40e_dcbnl_getnumtcs,
+	.setnumtcs	= i40e_dcbnl_setnumtcs,
+	.getpfcstate	= i40e_dcbnl_getpfcstate,
+	.setpfcstate	= i40e_dcbnl_setpfcstate,
+	.getapp		= i40e_dcbnl_getapp,
+	.setdcbx	= i40e_dcbnl_setdcbx,
 };
 
 /**
@@ -152,6 +902,10 @@
 	u8 prio, tc_map;
 	int i;
 
+	/* SW DCB taken care by DCBNL set calls */
+	if (pf->dcbx_cap & DCB_CAP_DCBX_HOST)
+		return;
+
 	/* DCB not enabled */
 	if (!(pf->flags & I40E_FLAG_DCB_ENABLED))
 		return;
@@ -176,8 +930,10 @@
 		}
 	}
 
+#ifdef HAVE_DCBNL_IEEE_DELAPP
 	/* Notify user-space of the changes */
 	dcbnl_ieee_notify(dev, RTM_SETDCB, DCB_CMD_IEEE_SET, 0, 0);
+#endif
 }
 
 /**
@@ -295,4 +1051,5 @@
 	/* Set initial IEEE DCB settings */
 	i40e_dcbnl_set_all(vsi);
 }
-#endif /* CONFIG_I40E_DCB */
+#endif /* HAVE_DCBNL_IEEE */
+#endif /* CONFIG_DCB */
Binary files linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_dcb_nl.o and linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_dcb_nl.o differ
Binary files linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_dcb.o and linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_dcb.o differ
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_ddp.c linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_ddp.c
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_ddp.c	2024-05-10 01:26:45.341079560 -0400
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_ddp.c	2024-05-13 03:58:25.372491940 -0400
@@ -1,5 +1,5 @@
 // SPDX-License-Identifier: GPL-2.0
-/* Copyright(c) 2013 - 2018 Intel Corporation. */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
 
 #include "i40e.h"
 
@@ -41,7 +41,7 @@
 
 	status = i40e_aq_get_ddp_list(hw, buff, I40E_PROFILE_LIST_SIZE, 0,
 				      NULL);
-	if (status)
+	if (status != I40E_SUCCESS)
 		return -1;
 
 	profile_list = (struct i40e_ddp_profile_list *)buff;
@@ -77,7 +77,7 @@
 }
 
 /**
- * i40e_ddp_does_profiles_ - checks if DDP overlaps with existing one.
+ * i40e_ddp_does_profile_overlap - checks if DDP overlaps with existing one.
  * @hw: HW data structure
  * @pinfo: DDP profile information structure
  *
@@ -96,7 +96,7 @@
 
 	status = i40e_aq_get_ddp_list(hw, buff, I40E_PROFILE_LIST_SIZE, 0,
 				      NULL);
-	if (status)
+	if (status != I40E_SUCCESS)
 		return -EIO;
 
 	profile_list = (struct i40e_ddp_profile_list *)buff;
@@ -116,7 +116,7 @@
  * @track_id: package tracking id
  *
  * Register a profile to the list of loaded profiles.
- */
+ **/
 static enum i40e_status_code
 i40e_add_pinfo(struct i40e_hw *hw, struct i40e_profile_segment *profile,
 	       u8 *profile_info_sec, u32 track_id)
@@ -141,7 +141,8 @@
 
 	/* Clear reserved field */
 	memset(pinfo->reserved, 0, sizeof(pinfo->reserved));
-	memcpy(pinfo->name, profile->name, I40E_DDP_NAME_SIZE);
+	i40e_memcpy(pinfo->name, profile->name, I40E_DDP_NAME_SIZE,
+		    I40E_NONDMA_TO_NONDMA);
 
 	status = i40e_aq_write_ddp(hw, (void *)sec, sec->data_end,
 				   track_id, &offset, &info, NULL);
@@ -235,14 +236,12 @@
 		u32 offset = pkg_hdr->segment_offset[segment];
 
 		if (0xFU & offset) {
-			netdev_err(netdev,
-				   "Invalid DDP profile %u segment alignment",
+			netdev_err(netdev, "Invalid DDP profile %u segment alignment",
 				   segment);
 			return false;
 		}
 		if (pkg_hdr_size > offset || offset >= size) {
-			netdev_err(netdev,
-				   "Invalid DDP profile %u segment offset",
+			netdev_err(netdev, "Invalid DDP profile %u segment offset",
 				   segment);
 			return false;
 		}
@@ -345,8 +344,7 @@
 		status = i40e_write_profile(&pf->hw, profile_hdr, track_id);
 		if (status) {
 			if (status == I40E_ERR_DEVICE_NOT_SUPPORTED) {
-				netdev_err(netdev,
-					   "Profile is not supported by the device.");
+				netdev_err(netdev, "Profile is not supported by the device.");
 				return -EPERM;
 			}
 			netdev_err(netdev, "Failed to write DDP profile.");
@@ -405,6 +403,9 @@
 	return status;
 }
 
+#define I40E_DDP_PROFILE_NAME_MAX \
+	(sizeof(I40E_DDP_PROFILE_PATH) + ETHTOOL_FLASH_MAX_FILENAME)
+
 /**
  * i40e_ddp_flash - callback function for ethtool flash feature
  * @netdev: net device structure
@@ -435,13 +436,12 @@
 	 */
 	if (strncmp(flash->data, "-", 2) != 0) {
 		struct i40e_ddp_old_profile_list *list_entry;
-		char profile_name[sizeof(I40E_DDP_PROFILE_PATH)
-				  + I40E_DDP_PROFILE_NAME_MAX];
+		char profile_name[I40E_DDP_PROFILE_NAME_MAX];
 
 		profile_name[sizeof(profile_name) - 1] = 0;
 		strncpy(profile_name, I40E_DDP_PROFILE_PATH,
 			sizeof(profile_name) - 1);
-		strncat(profile_name, flash->data, I40E_DDP_PROFILE_NAME_MAX);
+		strncat(profile_name, flash->data, ETHTOOL_FLASH_MAX_FILENAME);
 		/* Load DDP recipe. */
 		status = request_firmware(&ddp_config, profile_name,
 					  &netdev->dev);
@@ -454,12 +454,10 @@
 				       ddp_config->size, true);
 
 		if (!status) {
-			list_entry =
-			  kzalloc(sizeof(struct i40e_ddp_old_profile_list) +
-				  ddp_config->size, GFP_KERNEL);
+			list_entry = kzalloc(sizeof(*list_entry) +
+					     ddp_config->size, GFP_KERNEL);
 			if (!list_entry) {
-				netdev_info(netdev, "Failed to allocate memory for previous DDP profile data.");
-				netdev_info(netdev, "New profile loaded but roll-back will be impossible.");
+				netdev_info(netdev, "Failed to allocate memory for previous DDP profile data. New profile loaded but roll-back will be impossible.");
 			} else {
 				memcpy(list_entry->old_ddp_buf,
 				       ddp_config->data, ddp_config->size);
Binary files linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_ddp.o and linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_ddp.o differ
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_debugfs.c linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_debugfs.c
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_debugfs.c	2024-05-10 01:26:45.341079560 -0400
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_debugfs.c	2024-05-13 03:58:25.372491940 -0400
@@ -1,5 +1,5 @@
 // SPDX-License-Identifier: GPL-2.0
-/* Copyright(c) 2013 - 2018 Intel Corporation. */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
 
 #ifdef CONFIG_DEBUG_FS
 
@@ -65,7 +65,7 @@
 {
 	struct i40e_pf *pf = filp->private_data;
 	int bytes_not_copied;
-	int buf_size = 256;
+	size_t buf_size = 256;
 	char *buf;
 	int len;
 
@@ -99,19 +99,61 @@
 	"ACTIVE",
 	"FAILED",
 	"REMOVE",
+	"INACTIVE",
 };
 
 /**
+ * i40e_dbg_dump_vsi_filters - handles dump of mac/vlan filters for a VSI
+ * @pf: the i40e_pf created in command write
+ * @vsi: the vsi to dump
+ */
+static void i40e_dbg_dump_vsi_filters(struct i40e_pf *pf, struct i40e_vsi *vsi)
+{
+	struct i40e_mac_filter *f;
+	int bkt;
+
+	hash_for_each(vsi->mac_filter_hash, bkt, f, hlist) {
+		dev_info(&pf->pdev->dev,
+			 "    mac_filter_hash: %pM vid=%d, state %s\n",
+			 f->macaddr, f->vlan,
+			 i40e_filter_state_string[f->state]);
+	}
+	dev_info(&pf->pdev->dev, "    active_filters %u, promisc_threshold %u, overflow promisc %s\n",
+		 vsi->active_filters, vsi->promisc_threshold,
+		 (test_bit(__I40E_VSI_OVERFLOW_PROMISC, vsi->state) ?
+		  "ON" : "OFF"));
+}
+
+/**
+ * i40e_dbg_dump_all_vsi_filters - dump mac/vlan filters for all VSI on a PF
+ * @pf: the i40e_pf created in command write
+ */
+static void i40e_dbg_dump_all_vsi_filters(struct i40e_pf *pf)
+{
+	int i;
+
+	for (i = 0; i < pf->num_alloc_vsi; i++)
+		if (pf->vsi[i]) {
+			dev_info(&pf->pdev->dev, "vsi seid %d\n",
+				 pf->vsi[i]->seid);
+			i40e_dbg_dump_vsi_filters(pf, pf->vsi[i]);
+		}
+}
+
+/**
  * i40e_dbg_dump_vsi_seid - handles dump vsi seid write into command datum
  * @pf: the i40e_pf created in command write
  * @seid: the seid the user put in
  **/
 static void i40e_dbg_dump_vsi_seid(struct i40e_pf *pf, int seid)
 {
+#ifdef HAVE_NDO_GET_STATS64
 	struct rtnl_link_stats64 *nstat;
-	struct i40e_mac_filter *f;
+#else
+	struct net_device_stats *nstat;
+#endif
 	struct i40e_vsi *vsi;
-	int i, bkt;
+	int i;
 
 	vsi = i40e_dbg_find_vsi(pf, seid);
 	if (!vsi) {
@@ -122,18 +164,37 @@
 	dev_info(&pf->pdev->dev, "vsi seid %d\n", seid);
 	if (vsi->netdev) {
 		struct net_device *nd = vsi->netdev;
+#ifdef HAVE_RHEL6_NET_DEVICE_OPS_EXT
+		u32 hw_features;
+#endif
 
 		dev_info(&pf->pdev->dev, "    netdev: name = %s, state = %lu, flags = 0x%08x\n",
 			 nd->name, nd->state, nd->flags);
 		dev_info(&pf->pdev->dev, "        features      = 0x%08lx\n",
 			 (unsigned long int)nd->features);
+#ifdef HAVE_NDO_SET_FEATURES
+#ifdef HAVE_RHEL6_NET_DEVICE_OPS_EXT
+		hw_features = get_netdev_hw_features(vsi->netdev);
+		dev_info(&pf->pdev->dev, "        hw_features   = 0x%08x\n",
+			 hw_features);
+#else
 		dev_info(&pf->pdev->dev, "        hw_features   = 0x%08lx\n",
 			 (unsigned long int)nd->hw_features);
+#endif
+#endif
+#ifdef HAVE_NETDEV_VLAN_FEATURES
 		dev_info(&pf->pdev->dev, "        vlan_features = 0x%08lx\n",
 			 (unsigned long int)nd->vlan_features);
+#endif
 	}
+#ifdef HAVE_VLAN_RX_REGISTER
+	dev_info(&pf->pdev->dev, "    vlgrp is %s\n",
+		 vsi->vlgrp ? "<valid>" : "<null>");
+#else
+	dev_info(&pf->pdev->dev, "    active_vlans is <valid>\n");
+#endif /* HAVE_VLAN_RX_REGISTER */
 	dev_info(&pf->pdev->dev,
-		 "    flags = 0x%08lx, netdev_registered = %i, current_netdev_flags = 0x%04x\n",
+		 "    flags = 0x%016llx, netdev_registered = %i, current_netdev_flags = 0x%04x\n",
 		 vsi->flags, vsi->netdev_registered, vsi->current_netdev_flags);
 	for (i = 0; i < BITS_TO_LONGS(__I40E_VSI_STATE_SIZE__); i++)
 		dev_info(&pf->pdev->dev,
@@ -144,16 +205,7 @@
 			 pf->hw.mac.addr,
 			 pf->hw.mac.san_addr,
 			 pf->hw.mac.port_addr);
-	hash_for_each(vsi->mac_filter_hash, bkt, f, hlist) {
-		dev_info(&pf->pdev->dev,
-			 "    mac_filter_hash: %pM vid=%d, state %s\n",
-			 f->macaddr, f->vlan,
-			 i40e_filter_state_string[f->state]);
-	}
-	dev_info(&pf->pdev->dev, "    active_filters %u, promisc_threshold %u, overflow promisc %s\n",
-		 vsi->active_filters, vsi->promisc_threshold,
-		 (test_bit(__I40E_VSI_OVERFLOW_PROMISC, vsi->state) ?
-		  "ON" : "OFF"));
+	i40e_dbg_dump_vsi_filters(pf, vsi);
 	nstat = i40e_get_vsi_stats_struct(vsi);
 	dev_info(&pf->pdev->dev,
 		 "    net_stats: rx_packets = %lu, rx_bytes = %lu, rx_errors = %lu, rx_dropped = %lu\n",
@@ -277,9 +329,10 @@
 			 "    rx_rings[%i]: size = %i\n",
 			 i, rx_ring->size);
 		dev_info(&pf->pdev->dev,
-			 "    rx_rings[%i]: itr_setting = %d (%s)\n",
+			 "    rx_rings[%i]: rx_itr_setting = %d (%s)\n",
 			 i, rx_ring->itr_setting,
-			 ITR_IS_DYNAMIC(rx_ring->itr_setting) ? "dynamic" : "fixed");
+			 ITR_IS_DYNAMIC(rx_ring->itr_setting) ?
+				"dynamic" : "fixed");
 	}
 	for (i = 0; i < vsi->num_queue_pairs; i++) {
 		struct i40e_ring *tx_ring = READ_ONCE(vsi->tx_rings[i]);
@@ -315,9 +368,10 @@
 			 "    tx_rings[%i]: DCB tc = %d\n",
 			 i, tx_ring->dcb_tc);
 		dev_info(&pf->pdev->dev,
-			 "    tx_rings[%i]: itr_setting = %d (%s)\n",
+			 "    tx_rings[%i]: tx_itr_setting = %d (%s)\n",
 			 i, tx_ring->itr_setting,
-			 ITR_IS_DYNAMIC(tx_ring->itr_setting) ? "dynamic" : "fixed");
+			 ITR_IS_DYNAMIC(tx_ring->itr_setting) ?
+				"dynamic" : "fixed");
 	}
 	rcu_read_unlock();
 	dev_info(&pf->pdev->dev,
@@ -349,13 +403,12 @@
 		 "    info: sec_flags = 0x%02x, sec_reserved = 0x%02x\n",
 		 vsi->info.sec_flags, vsi->info.sec_reserved);
 	dev_info(&pf->pdev->dev,
-		 "    info: pvid = 0x%04x, fcoe_pvid = 0x%04x, port_vlan_flags = 0x%02x\n",
-		 vsi->info.pvid, vsi->info.fcoe_pvid,
-		 vsi->info.port_vlan_flags);
-	dev_info(&pf->pdev->dev,
-		 "    info: pvlan_reserved[] = 0x%02x 0x%02x 0x%02x\n",
-		 vsi->info.pvlan_reserved[0], vsi->info.pvlan_reserved[1],
-		 vsi->info.pvlan_reserved[2]);
+		 "    info: pvid = 0x%04x, outer_vlan = 0x%04x, port_vlan_flags = 0x%02x, outer_vlan_flags = 0x%02x\n",
+		 vsi->info.pvid, vsi->info.outer_vlan,
+		 vsi->info.port_vlan_flags, vsi->info.outer_vlan_flags);
+	dev_info(&pf->pdev->dev,
+		 "    info: pvlan_reserved[] = 0x%02x 0x%02x\n",
+		 vsi->info.pvlan_reserved[0], vsi->info.pvlan_reserved[1]);
 	dev_info(&pf->pdev->dev,
 		 "    info: ingress_table = 0x%08x, egress_table = 0x%08x\n",
 		 vsi->info.ingress_table, vsi->info.egress_table);
@@ -482,6 +535,15 @@
 	}
 }
 
+/* Helper macros for printing upper half of the 32byte descriptor. */
+#ifdef I40E_32BYTE_RX
+#define RXD_RSVD1(_rxd) ((_rxd)->read.rsvd1)
+#define RXD_RSVD2(_rxd) ((_rxd)->read.rsvd2)
+#else
+#define RXD_RSVD1(_rxd) 0ULL
+#define RXD_RSVD2(_rxd) 0ULL
+#endif
+
 /**
  * i40e_dbg_dump_desc - handles dump desc write into command datum
  * @cnt: number of arguments that the user supplied
@@ -496,7 +558,7 @@
 {
 	struct i40e_tx_desc *txd;
 	union i40e_rx_desc *rxd;
-	struct i40e_ring *ring;
+	struct i40e_ring ring;
 	struct i40e_vsi *vsi;
 	int i;
 
@@ -505,6 +567,14 @@
 		dev_info(&pf->pdev->dev, "vsi %d not found\n", vsi_seid);
 		return;
 	}
+	if (vsi->type != I40E_VSI_MAIN &&
+	    vsi->type != I40E_VSI_FDIR &&
+	    vsi->type != I40E_VSI_VMDQ2) {
+		dev_info(&pf->pdev->dev,
+			 "vsi %d type %d descriptor rings not available\n",
+			 vsi_seid, vsi->type);
+		return;
+	}
 	if (ring_id >= vsi->num_queue_pairs || ring_id < 0) {
 		dev_info(&pf->pdev->dev, "ring %d not found\n", ring_id);
 		return;
@@ -515,58 +585,59 @@
 			 vsi_seid);
 		return;
 	}
-
-	ring = kmemdup(is_rx_ring
-		       ? vsi->rx_rings[ring_id] : vsi->tx_rings[ring_id],
-		       sizeof(*ring), GFP_KERNEL);
-	if (!ring)
-		return;
-
+	if (is_rx_ring)
+		ring = *vsi->rx_rings[ring_id];
+	else
+		ring = *vsi->tx_rings[ring_id];
 	if (cnt == 2) {
+		void *head = (struct i40e_tx_desc *)ring.desc + ring.count;
+		u32 tx_head = le32_to_cpu(*(volatile __le32 *)head);
+
 		dev_info(&pf->pdev->dev, "vsi = %02i %s ring = %02i\n",
 			 vsi_seid, is_rx_ring ? "rx" : "tx", ring_id);
-		for (i = 0; i < ring->count; i++) {
+		dev_info(&pf->pdev->dev, "head = %04x tail = %04x\n",
+			 is_rx_ring ? 0 : tx_head, readl(ring.tail));
+		dev_info(&pf->pdev->dev, "ntc = %04x ntu = %04x\n",
+			 ring.next_to_clean, ring.next_to_use);
+		for (i = 0; i < ring.count; i++) {
 			if (!is_rx_ring) {
-				txd = I40E_TX_DESC(ring, i);
+				txd = I40E_TX_DESC(&ring, i);
 				dev_info(&pf->pdev->dev,
 					 "   d[%03x] = 0x%016llx 0x%016llx\n",
 					 i, txd->buffer_addr,
 					 txd->cmd_type_offset_bsz);
 			} else {
-				rxd = I40E_RX_DESC(ring, i);
+				rxd = I40E_RX_DESC(&ring, i);
 				dev_info(&pf->pdev->dev,
 					 "   d[%03x] = 0x%016llx 0x%016llx 0x%016llx 0x%016llx\n",
 					 i, rxd->read.pkt_addr,
 					 rxd->read.hdr_addr,
-					 rxd->read.rsvd1, rxd->read.rsvd2);
+					 RXD_RSVD1(rxd), RXD_RSVD2(rxd));
 			}
 		}
 	} else if (cnt == 3) {
-		if (desc_n >= ring->count || desc_n < 0) {
+		if (desc_n >= ring.count || desc_n < 0) {
 			dev_info(&pf->pdev->dev,
 				 "descriptor %d not found\n", desc_n);
-			goto out;
+			return;
 		}
 		if (!is_rx_ring) {
-			txd = I40E_TX_DESC(ring, desc_n);
+			txd = I40E_TX_DESC(&ring, desc_n);
 			dev_info(&pf->pdev->dev,
 				 "vsi = %02i tx ring = %02i d[%03x] = 0x%016llx 0x%016llx\n",
 				 vsi_seid, ring_id, desc_n,
 				 txd->buffer_addr, txd->cmd_type_offset_bsz);
 		} else {
-			rxd = I40E_RX_DESC(ring, desc_n);
+			rxd = I40E_RX_DESC(&ring, desc_n);
 			dev_info(&pf->pdev->dev,
 				 "vsi = %02i rx ring = %02i d[%03x] = 0x%016llx 0x%016llx 0x%016llx 0x%016llx\n",
 				 vsi_seid, ring_id, desc_n,
 				 rxd->read.pkt_addr, rxd->read.hdr_addr,
-				 rxd->read.rsvd1, rxd->read.rsvd2);
+				 RXD_RSVD1(rxd), RXD_RSVD2(rxd));
 		}
 	} else {
 		dev_info(&pf->pdev->dev, "dump desc rx/tx <vsi_seid> <ring_id> [<desc_n>]\n");
 	}
-
-out:
-	kfree(ring);
 }
 
 /**
@@ -584,7 +655,161 @@
 }
 
 /**
- * i40e_dbg_dump_stats - handles dump stats write into command datum
+ * i40e_dbg_dump_resources - handles dump resources request
+ * @pf: the i40e_pf created in command write
+ **/
+static void i40e_dbg_dump_resources(struct i40e_pf *pf)
+{
+	struct i40e_aqc_switch_resource_alloc_element_resp *buf;
+	int buf_len;
+	u16 count = 32;
+	u8 num_entries;
+	int ret, i;
+
+	buf_len = count * sizeof(*buf);
+	buf = kzalloc(buf_len, GFP_KERNEL);
+	if (!buf) {
+		dev_err(&pf->pdev->dev, "Can't get memory\n");
+		return;
+	}
+
+	ret = i40e_aq_get_switch_resource_alloc(&pf->hw, &num_entries,
+						buf, count, NULL);
+	if (ret) {
+		dev_err(&pf->pdev->dev,
+			"fail to get resources, err %s aq_err %s\n",
+			i40e_stat_str(&pf->hw, ret),
+			i40e_aq_str(&pf->hw, pf->hw.aq.asq_last_status));
+		kfree(buf);
+		return;
+	}
+
+	dev_info(&pf->pdev->dev, "  resources:\n");
+	dev_info(&pf->pdev->dev, "  guar  total  used unalloc   name\n");
+	for (i = 0; i < num_entries; i++) {
+		char *p;
+
+		switch (buf[i].resource_type) {
+		case I40E_AQ_RESOURCE_TYPE_VEB:
+			p = "vebs";
+			break;
+		case I40E_AQ_RESOURCE_TYPE_VSI:
+			p = "vsis";
+			break;
+		case I40E_AQ_RESOURCE_TYPE_MACADDR:
+			p = "macaddrs";
+			break;
+		case I40E_AQ_RESOURCE_TYPE_STAG:
+			p = "stags";
+			break;
+		case I40E_AQ_RESOURCE_TYPE_ETAG:
+			p = "etags";
+			break;
+		case I40E_AQ_RESOURCE_TYPE_MULTICAST_HASH:
+			p = "multicast hash";
+			break;
+		case I40E_AQ_RESOURCE_TYPE_UNICAST_HASH:
+			p = "unicast hash";
+			break;
+		case I40E_AQ_RESOURCE_TYPE_VLAN:
+			p = "vlans";
+			break;
+		case I40E_AQ_RESOURCE_TYPE_VSI_LIST_ENTRY:
+			p = "vsi list entries";
+			break;
+		case I40E_AQ_RESOURCE_TYPE_ETAG_LIST_ENTRY:
+			p = "etag list entries";
+			break;
+		case I40E_AQ_RESOURCE_TYPE_VLAN_STAT_POOL:
+			p = "vlan stat pools";
+			break;
+		case I40E_AQ_RESOURCE_TYPE_MIRROR_RULE:
+			p = "mirror rules";
+			break;
+		case I40E_AQ_RESOURCE_TYPE_QUEUE_SETS:
+			p = "queue sets";
+			break;
+		case I40E_AQ_RESOURCE_TYPE_VLAN_FILTERS:
+			p = "vlan filters";
+			break;
+		case I40E_AQ_RESOURCE_TYPE_INNER_MAC_FILTERS:
+			p = "inner mac filters";
+			break;
+		case I40E_AQ_RESOURCE_TYPE_IP_FILTERS:
+			p = "ip filters";
+			break;
+		case I40E_AQ_RESOURCE_TYPE_GRE_VN_KEYS:
+			p = "gre vn keys";
+			break;
+		case I40E_AQ_RESOURCE_TYPE_VN2_KEYS:
+			p = "vn2 keys";
+			break;
+		case I40E_AQ_RESOURCE_TYPE_TUNNEL_PORTS:
+			p = "tunnel ports";
+			break;
+		default:
+			p = "unknown";
+			break;
+		}
+
+		dev_info(&pf->pdev->dev, "  %4d   %4d  %4d  %4d   %s\n",
+			 buf[i].guaranteed, buf[i].total, buf[i].used,
+			 buf[i].total_unalloced, p);
+	}
+
+	kfree(buf);
+}
+
+/**
+ * i40e_dbg_dump_capabilities - handles dump capabilities request
+ * @pf: the i40e_pf created in command write
+ **/
+static void i40e_dbg_dump_capabilities(struct i40e_pf *pf)
+{
+	struct i40e_hw_capabilities *p;
+
+	p = (struct i40e_hw_capabilities *)&pf->hw.func_caps;
+	dev_info(&pf->pdev->dev, "  capabilities:\n");
+	dev_info(&pf->pdev->dev,
+		 "    switch_mode = %d\tmgmt_mode = %d\tnpar = %d\tos2bmc = %d\n",
+		 p->switch_mode, p->management_mode, p->npar_enable, p->os2bmc);
+	dev_info(&pf->pdev->dev,
+		 "    valid_functions = 0x%04x\tsr_iov_1_1 = %d\tnum_vfs = %d\tvf_base_id = %d\n",
+		 p->valid_functions, p->sr_iov_1_1, p->num_vfs, p->vf_base_id);
+	dev_info(&pf->pdev->dev, "    nvm_image_type = %d\n", p->nvm_image_type);
+	dev_info(&pf->pdev->dev,
+		 "    num_vsis = %d\tvmdq = %d\tflex10_enable = %d\tflex10_capable = %d\n",
+		 p->num_vsis, p->vmdq, p->flex10_enable, p->flex10_capable);
+	dev_info(&pf->pdev->dev,
+		 "    evb_802_1_qbg = %d\tevb_802_1_qbh = %d\tmgmt_cem = %d\tieee_1588 = %d\n",
+		 p->evb_802_1_qbg, p->evb_802_1_qbh, p->mgmt_cem, p->ieee_1588);
+	dev_info(&pf->pdev->dev,
+		 "    fcoe = %d\tiwarp = %d\tmdio_port_num = %d\tmdio_port_mode = %d\n",
+		 p->fcoe, p->iwarp, p->mdio_port_num, p->mdio_port_mode);
+	dev_info(&pf->pdev->dev,
+		 "    dcb = %d\tenabled_tcmap = %d\tmaxtc = %d\tiscsi = %d\n",
+		 p->dcb, p->enabled_tcmap, p->maxtc, p->iscsi);
+	dev_info(&pf->pdev->dev,
+		 "    fd = %d\tfd_filters_guaranteed = %d\tfd_filters_best_effort = %d\tnum_flow_director_filters = %d\n",
+		 p->fd, p->fd_filters_guaranteed, p->fd_filters_best_effort,
+		 p->num_flow_director_filters);
+	dev_info(&pf->pdev->dev,
+		 "    rss = %d\trss_table_size = %d\trss_table_entry_width = %d\n",
+		 p->rss, p->rss_table_size, p->rss_table_entry_width);
+	dev_info(&pf->pdev->dev,
+		 "    led[0] = %d\tsdp[0] = %d\tled_pin_num = %d\tsdp_pin_num = %d\n",
+		 p->led[0], p->sdp[0], p->led_pin_num, p->sdp_pin_num);
+	dev_info(&pf->pdev->dev,
+		 "    num_rx_qp = %d\tnum_tx_qp = %d\tbase_queue = %d\n",
+		 p->num_rx_qp, p->num_tx_qp, p->base_queue);
+	dev_info(&pf->pdev->dev,
+		 "    num_msix_vectors = %d\tnum_msix_vectors_vf = %d\trx_buf_chain_len = %d\n",
+		 p->num_msix_vectors, p->num_msix_vectors_vf,
+		 p->rx_buf_chain_len);
+}
+
+/**
+ * i40e_dbg_dump_eth_stats - handles dump stats write into command datum
  * @pf: the i40e_pf created in command write
  * @estats: the eth stats structure to be dumped
  **/
@@ -617,17 +842,35 @@
 static void i40e_dbg_dump_veb_seid(struct i40e_pf *pf, int seid)
 {
 	struct i40e_veb *veb;
+	int i;
 
 	veb = i40e_dbg_find_veb(pf, seid);
 	if (!veb) {
 		dev_info(&pf->pdev->dev, "can't find veb %d\n", seid);
 		return;
 	}
+#ifdef HAVE_BRIDGE_ATTRIBS
 	dev_info(&pf->pdev->dev,
 		 "veb idx=%d,%d stats_ic=%d  seid=%d uplink=%d mode=%s\n",
 		 veb->idx, veb->veb_idx, veb->stats_idx, veb->seid,
 		 veb->uplink_seid,
 		 veb->bridge_mode == BRIDGE_MODE_VEPA ? "VEPA" : "VEB");
+#else
+	dev_info(&pf->pdev->dev,
+		 "veb idx=%d,%d stats_ic=%d  seid=%d uplink=%d mode=%s\n",
+		 veb->idx, veb->veb_idx, veb->stats_idx, veb->seid,
+		 veb->uplink_seid,
+		"VEB");
+#endif
+	dev_info(&pf->pdev->dev,
+		 "veb bw: enabled_tc=0x%x bw_limit=%d bw_max_quanta=%d is_abs_credits=%d\n",
+		 veb->enabled_tc, veb->bw_limit, veb->bw_max_quanta,
+		 veb->is_abs_credits);
+	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
+		dev_info(&pf->pdev->dev, "veb bw: tc=%d bw_share=%d bw_limit=%d max_quanta=%d\n",
+			 i, veb->bw_tc_share_credits[i],
+			 veb->bw_tc_limit_credits[i], veb->bw_tc_max_quanta[i]);
+	}
 	i40e_dbg_dump_eth_stats(pf, &veb->stats);
 }
 
@@ -688,6 +931,100 @@
 			i40e_dbg_dump_vf(pf, i);
 }
 
+/**
+ * i40e_dbg_dump_dcb_cfg - Dump DCB config data struct
+ * @pf: the corresponding PF
+ * @cfg: DCB Config data structure
+ * @prefix: Prefix string
+ **/
+static void i40e_dbg_dump_dcb_cfg(struct i40e_pf *pf,
+				  struct i40e_dcbx_config *cfg,
+				  char *prefix)
+{
+	int i;
+
+	dev_info(&pf->pdev->dev,
+		 "%s ets_cfg: willing=%d cbs=%d, maxtcs=%d\n",
+		 prefix, cfg->etscfg.willing, cfg->etscfg.cbs,
+		 cfg->etscfg.maxtcs);
+	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
+		dev_info(&pf->pdev->dev, "%s ets_cfg: up=%d tc=%d\n",
+			 prefix, i, cfg->etscfg.prioritytable[i]);
+	}
+	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
+		dev_info(&pf->pdev->dev, "%s ets_cfg: tc=%d tcbw=%d tctsa=%d\n",
+			 prefix, i, cfg->etscfg.tcbwtable[i],
+			 cfg->etscfg.tsatable[i]);
+	}
+	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
+		dev_info(&pf->pdev->dev, "%s ets_rec: up=%d tc=%d\n",
+			 prefix, i, cfg->etsrec.prioritytable[i]);
+	}
+	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
+		dev_info(&pf->pdev->dev, "%s ets_rec: tc=%d tcbw=%d tctsa=%d\n",
+			 prefix, i, cfg->etsrec.tcbwtable[i],
+			 cfg->etsrec.tsatable[i]);
+	}
+	dev_info(&pf->pdev->dev,
+		 "%s pfc_cfg: willing=%d mbc=%d, pfccap=%d pfcenable=0x%x\n",
+		 prefix, cfg->pfc.willing, cfg->pfc.mbc,
+		 cfg->pfc.pfccap, cfg->pfc.pfcenable);
+
+	dev_info(&pf->pdev->dev,
+		 "%s app_table: num_apps=%d\n", prefix, (int)cfg->numapps);
+	for (i = 0; i < (int)cfg->numapps; i++) {
+		dev_info(&pf->pdev->dev, "%s app_table: %d prio=%d selector=%d protocol=0x%x\n",
+			 prefix, i, cfg->app[i].priority,
+			 cfg->app[i].selector,
+			 cfg->app[i].protocolid);
+	}
+}
+
+/**
+ * i40e_dbg_dump_fdir_filter - Dump out flow director filter contents
+ * @pf: the corresponding PF
+ * @f: the flow director filter
+ **/
+static inline void i40e_dbg_dump_fdir_filter(struct i40e_pf *pf,
+					     struct i40e_fdir_filter *f)
+{
+	dev_info(&pf->pdev->dev, "fdir filter %d:\n", f->fd_id);
+	dev_info(&pf->pdev->dev, "    flow_type=%d ipl4_proto=%d\n",
+		 f->flow_type, f->ipl4_proto);
+	dev_info(&pf->pdev->dev, "    dst_ip= %pi4  dst_port=%d\n",
+		 &f->dst_ip, f->dst_port);
+	dev_info(&pf->pdev->dev, "    src_ip= %pi4  src_port=%d\n",
+		 &f->src_ip, f->src_port);
+	dev_info(&pf->pdev->dev, "    sctp_v_tag=%d q_index=%d\n",
+		 f->sctp_v_tag, f->q_index);
+	if (f->flex_filter)
+		dev_info(&pf->pdev->dev, "    flex_word=%04x flex_offset=%d\n",
+			 f->flex_word, f->flex_offset);
+	dev_info(&pf->pdev->dev, "    pctype=%d dest_vsi=%d dest_ctl=%d\n",
+		 f->pctype, f->dest_vsi, f->dest_ctl);
+	dev_info(&pf->pdev->dev, "    fd_status=%d cnt_index=%d\n",
+		 f->fd_status, f->cnt_index);
+}
+
+/**
+ * i40e_dbg_dump_cloud_filter - Dump out cloud filter contents
+ * @pf: the corresponding PF
+ * @f: the flow director filter
+ **/
+static inline void i40e_dbg_dump_cloud_filter(struct i40e_pf *pf,
+					      struct i40e_cloud_filter *f)
+{
+	dev_info(&pf->pdev->dev, "cloud filter %d:\n", f->id);
+	dev_info(&pf->pdev->dev, "    outer_mac[]=%pM  inner_mac=%pM\n",
+		 f->outer_mac, f->inner_mac);
+	dev_info(&pf->pdev->dev, "    inner_vlan %d, inner_ip[0] %pi4\n",
+		 be16_to_cpu(f->inner_vlan), f->inner_ip);
+	dev_info(&pf->pdev->dev, "    tenant_id=%d flags=0x%02x, tunnel_type=0x%02x\n",
+		 f->tenant_id, f->flags, f->tunnel_type);
+	dev_info(&pf->pdev->dev, "    seid=%d queue_id=%d\n",
+		 f->seid, f->queue_id);
+}
+
 #define I40E_MAX_DEBUG_OUT_BUFFER (4096*4)
 /**
  * i40e_dbg_command_write - write into command datum
@@ -701,6 +1038,7 @@
 				      size_t count, loff_t *ppos)
 {
 	struct i40e_pf *pf = filp->private_data;
+	const size_t buf_size_max = 256;
 	char *cmd_buf, *cmd_buf_tmp;
 	int bytes_not_copied;
 	struct i40e_vsi *vsi;
@@ -712,6 +1050,9 @@
 	/* don't allow partial writes */
 	if (*ppos != 0)
 		return 0;
+	/* don't cross maximal possible value */
+	if (count >= buf_size_max)
+		return -ENOSPC;
 
 	cmd_buf = kzalloc(count + 1, GFP_KERNEL);
 	if (!cmd_buf)
@@ -729,7 +1070,48 @@
 		count = cmd_buf_tmp - cmd_buf + 1;
 	}
 
-	if (strncmp(cmd_buf, "add vsi", 7) == 0) {
+	if (strncmp(cmd_buf, "read", 4) == 0) {
+		u32 address;
+		u32 value;
+
+		cnt = sscanf(&cmd_buf[4], "%i", &address);
+		if (cnt != 1) {
+			dev_info(&pf->pdev->dev, "read <reg>\n");
+			goto command_write_done;
+		}
+
+		/* check the range on address */
+		if (address > (pf->ioremap_len - sizeof(u32))) {
+			dev_info(&pf->pdev->dev, "read reg address 0x%08x too large, max=0x%08lx\n",
+				 address, (pf->ioremap_len - sizeof(u32)));
+			goto command_write_done;
+		}
+
+		value = rd32(&pf->hw, address);
+		dev_info(&pf->pdev->dev, "read: 0x%08x = 0x%08x\n",
+			 address, value);
+
+	} else if (strncmp(cmd_buf, "write", 5) == 0) {
+		u32 address, value;
+
+		cnt = sscanf(&cmd_buf[5], "%i %i", &address, &value);
+		if (cnt != 2) {
+			dev_info(&pf->pdev->dev, "write <reg> <value>\n");
+			goto command_write_done;
+		}
+
+		/* check the range on address */
+		if (address > (pf->ioremap_len - sizeof(u32))) {
+			dev_info(&pf->pdev->dev, "write reg address 0x%08x too large, max=0x%08lx\n",
+				 address, (pf->ioremap_len - sizeof(u32)));
+			goto command_write_done;
+		}
+		wr32(&pf->hw, address, value);
+		value = rd32(&pf->hw, address);
+		dev_info(&pf->pdev->dev, "write: 0x%08x = 0x%08x\n",
+			 address, value);
+
+	} else if (strncmp(cmd_buf, "add vsi", 7) == 0) {
 		vsi_seid = -1;
 		cnt = sscanf(&cmd_buf[7], "%i", &vsi_seid);
 		if (cnt == 0) {
@@ -818,6 +1200,7 @@
 
 	} else if (strncmp(cmd_buf, "del relay", 9) == 0) {
 		int i;
+
 		cnt = sscanf(&cmd_buf[9], "%i", &veb_seid);
 		if (cnt != 1) {
 			dev_info(&pf->pdev->dev,
@@ -845,9 +1228,9 @@
 	} else if (strncmp(cmd_buf, "add pvid", 8) == 0) {
 		i40e_status ret;
 		u16 vid;
-		unsigned int v;
+		int v;
 
-		cnt = sscanf(&cmd_buf[8], "%i %u", &vsi_seid, &v);
+		cnt = sscanf(&cmd_buf[8], "%i %d", &vsi_seid, &v);
 		if (cnt != 2) {
 			dev_info(&pf->pdev->dev,
 				 "add pvid: bad command string, cnt=%d\n", cnt);
@@ -861,7 +1244,7 @@
 			goto command_write_done;
 		}
 
-		vid = v;
+		vid = (unsigned)v;
 		ret = i40e_vsi_add_pvid(vsi, vid);
 		if (!ret)
 			dev_info(&pf->pdev->dev,
@@ -896,6 +1279,10 @@
 	} else if (strncmp(cmd_buf, "dump", 4) == 0) {
 		if (strncmp(&cmd_buf[5], "switch", 6) == 0) {
 			i40e_fetch_switch_configuration(pf, true);
+		} else if (strncmp(&cmd_buf[5], "resources", 9) == 0) {
+			i40e_dbg_dump_resources(pf);
+		} else if (strncmp(&cmd_buf[5], "capabilities", 7) == 0) {
+			i40e_dbg_dump_capabilities(pf);
 		} else if (strncmp(&cmd_buf[5], "vsi", 3) == 0) {
 			cnt = sscanf(&cmd_buf[8], "%i", &vsi_seid);
 			if (cnt > 0)
@@ -916,6 +1303,7 @@
 				i40e_dbg_dump_vf_all(pf);
 		} else if (strncmp(&cmd_buf[5], "desc", 4) == 0) {
 			int ring_id, desc_n;
+
 			if (strncmp(&cmd_buf[10], "rx", 2) == 0) {
 				cnt = sscanf(&cmd_buf[12], "%i %i %i",
 					     &vsi_seid, &ring_id, &desc_n);
@@ -954,6 +1342,8 @@
 						&pf->hw.local_dcbx_config;
 			struct i40e_dcbx_config *r_cfg =
 						&pf->hw.remote_dcbx_config;
+			struct i40e_dcbx_config *d_cfg =
+						&pf->hw.desired_dcbx_config;
 			int i, ret;
 			u16 switch_id;
 
@@ -996,68 +1386,18 @@
 			kfree(bw_data);
 			bw_data = NULL;
 
-			dev_info(&pf->pdev->dev,
-				 "port dcbx_mode=%d\n", cfg->dcbx_mode);
-			dev_info(&pf->pdev->dev,
-				 "port ets_cfg: willing=%d cbs=%d, maxtcs=%d\n",
-				 cfg->etscfg.willing, cfg->etscfg.cbs,
-				 cfg->etscfg.maxtcs);
-			for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
-				dev_info(&pf->pdev->dev, "port ets_cfg: %d prio_tc=%d tcbw=%d tctsa=%d\n",
-					 i, cfg->etscfg.prioritytable[i],
-					 cfg->etscfg.tcbwtable[i],
-					 cfg->etscfg.tsatable[i]);
-			}
-			for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
-				dev_info(&pf->pdev->dev, "port ets_rec: %d prio_tc=%d tcbw=%d tctsa=%d\n",
-					 i, cfg->etsrec.prioritytable[i],
-					 cfg->etsrec.tcbwtable[i],
-					 cfg->etsrec.tsatable[i]);
-			}
-			dev_info(&pf->pdev->dev,
-				 "port pfc_cfg: willing=%d mbc=%d, pfccap=%d pfcenable=0x%x\n",
-				 cfg->pfc.willing, cfg->pfc.mbc,
-				 cfg->pfc.pfccap, cfg->pfc.pfcenable);
-			dev_info(&pf->pdev->dev,
-				 "port app_table: num_apps=%d\n", cfg->numapps);
-			for (i = 0; i < cfg->numapps; i++) {
-				dev_info(&pf->pdev->dev, "port app_table: %d prio=%d selector=%d protocol=0x%x\n",
-					 i, cfg->app[i].priority,
-					 cfg->app[i].selector,
-					 cfg->app[i].protocolid);
-			}
-			/* Peer TLV DCBX data */
-			dev_info(&pf->pdev->dev,
-				 "remote port ets_cfg: willing=%d cbs=%d, maxtcs=%d\n",
-				 r_cfg->etscfg.willing,
-				 r_cfg->etscfg.cbs, r_cfg->etscfg.maxtcs);
-			for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
-				dev_info(&pf->pdev->dev, "remote port ets_cfg: %d prio_tc=%d tcbw=%d tctsa=%d\n",
-					 i, r_cfg->etscfg.prioritytable[i],
-					 r_cfg->etscfg.tcbwtable[i],
-					 r_cfg->etscfg.tsatable[i]);
-			}
-			for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
-				dev_info(&pf->pdev->dev, "remote port ets_rec: %d prio_tc=%d tcbw=%d tctsa=%d\n",
-					 i, r_cfg->etsrec.prioritytable[i],
-					 r_cfg->etsrec.tcbwtable[i],
-					 r_cfg->etsrec.tsatable[i]);
-			}
-			dev_info(&pf->pdev->dev,
-				 "remote port pfc_cfg: willing=%d mbc=%d, pfccap=%d pfcenable=0x%x\n",
-				 r_cfg->pfc.willing,
-				 r_cfg->pfc.mbc,
-				 r_cfg->pfc.pfccap,
-				 r_cfg->pfc.pfcenable);
-			dev_info(&pf->pdev->dev,
-				 "remote port app_table: num_apps=%d\n",
-				 r_cfg->numapps);
-			for (i = 0; i < r_cfg->numapps; i++) {
-				dev_info(&pf->pdev->dev, "remote port app_table: %d prio=%d selector=%d protocol=0x%x\n",
-					 i, r_cfg->app[i].priority,
-					 r_cfg->app[i].selector,
-					 r_cfg->app[i].protocolid);
+			if (cfg->dcbx_mode == I40E_DCBX_MODE_CEE) {
+				dev_info(&pf->pdev->dev,
+					 "CEE DCBX mode with Oper TLV Status = 0x%x\n",
+					 cfg->tlv_status);
+				i40e_dbg_dump_dcb_cfg(pf, d_cfg, "DesiredCfg");
+			} else {
+				dev_info(&pf->pdev->dev, "IEEE DCBX mode\n");
 			}
+
+			i40e_dbg_dump_dcb_cfg(pf, cfg, "OperCfg");
+			i40e_dbg_dump_dcb_cfg(pf, r_cfg, "PeerCfg");
+
 		} else if (strncmp(&cmd_buf[5], "debug fwdata", 12) == 0) {
 			int cluster_id, table_id;
 			int index, ret;
@@ -1102,69 +1442,65 @@
 				       buff, rlen, true);
 			kfree(buff);
 			buff = NULL;
+		} else if (strncmp(&cmd_buf[5], "filters", 7) == 0) {
+			struct i40e_fdir_filter *f_rule;
+			struct i40e_cloud_filter *c_rule;
+			struct hlist_node *node2;
+
+			hlist_for_each_entry_safe(f_rule, node2,
+						  &pf->fdir_filter_list,
+						  fdir_node) {
+				i40e_dbg_dump_fdir_filter(pf, f_rule);
+			}
+
+			/* find the cloud filter rule ids */
+			hlist_for_each_entry_safe(c_rule, node2,
+						  &pf->cloud_filter_list,
+						  cloud_node) {
+				i40e_dbg_dump_cloud_filter(pf, c_rule);
+			}
+			i40e_dbg_dump_all_vsi_filters(pf);
 		} else {
 			dev_info(&pf->pdev->dev,
 				 "dump desc tx <vsi_seid> <ring_id> [<desc_n>], dump desc rx <vsi_seid> <ring_id> [<desc_n>],\n");
 			dev_info(&pf->pdev->dev, "dump switch\n");
 			dev_info(&pf->pdev->dev, "dump vsi [seid]\n");
+			dev_info(&pf->pdev->dev, "dump capabilities\n");
+			dev_info(&pf->pdev->dev, "dump resources\n");
 			dev_info(&pf->pdev->dev, "dump reset stats\n");
 			dev_info(&pf->pdev->dev, "dump port\n");
-			dev_info(&pf->pdev->dev, "dump vf [vf_id]\n");
+			dev_info(&pf->pdev->dev, "dump VF [vf_id]\n");
 			dev_info(&pf->pdev->dev,
 				 "dump debug fwdata <cluster_id> <table_id> <index>\n");
+			dev_info(&pf->pdev->dev, "dump filters\n");
 		}
-	} else if (strncmp(cmd_buf, "pfr", 3) == 0) {
-		dev_info(&pf->pdev->dev, "debugfs: forcing PFR\n");
-		i40e_do_reset_safe(pf, BIT(__I40E_PF_RESET_REQUESTED));
-
-	} else if (strncmp(cmd_buf, "corer", 5) == 0) {
-		dev_info(&pf->pdev->dev, "debugfs: forcing CoreR\n");
-		i40e_do_reset_safe(pf, BIT(__I40E_CORE_RESET_REQUESTED));
-
-	} else if (strncmp(cmd_buf, "globr", 5) == 0) {
-		dev_info(&pf->pdev->dev, "debugfs: forcing GlobR\n");
-		i40e_do_reset_safe(pf, BIT(__I40E_GLOBAL_RESET_REQUESTED));
-
-	} else if (strncmp(cmd_buf, "read", 4) == 0) {
-		u32 address;
-		u32 value;
-
-		cnt = sscanf(&cmd_buf[4], "%i", &address);
-		if (cnt != 1) {
-			dev_info(&pf->pdev->dev, "read <reg>\n");
-			goto command_write_done;
-		}
-
-		/* check the range on address */
-		if (address > (pf->ioremap_len - sizeof(u32))) {
-			dev_info(&pf->pdev->dev, "read reg address 0x%08x too large, max=0x%08lx\n",
-				 address, (unsigned long int)(pf->ioremap_len - sizeof(u32)));
-			goto command_write_done;
-		}
-
-		value = rd32(&pf->hw, address);
-		dev_info(&pf->pdev->dev, "read: 0x%08x = 0x%08x\n",
-			 address, value);
 
-	} else if (strncmp(cmd_buf, "write", 5) == 0) {
-		u32 address, value;
+	} else if (strncmp(cmd_buf, "msg_enable", 10) == 0) {
+		u32 level;
 
-		cnt = sscanf(&cmd_buf[5], "%i %i", &address, &value);
-		if (cnt != 2) {
-			dev_info(&pf->pdev->dev, "write <reg> <value>\n");
-			goto command_write_done;
-		}
-
-		/* check the range on address */
-		if (address > (pf->ioremap_len - sizeof(u32))) {
-			dev_info(&pf->pdev->dev, "write reg address 0x%08x too large, max=0x%08lx\n",
-				 address, (unsigned long int)(pf->ioremap_len - sizeof(u32)));
-			goto command_write_done;
+		cnt = sscanf(&cmd_buf[10], "%i", &level);
+		if (cnt) {
+			if (I40E_DEBUG_USER & level) {
+				pf->hw.debug_mask = level;
+				dev_info(&pf->pdev->dev,
+					 "set hw.debug_mask = 0x%08x\n",
+					 pf->hw.debug_mask);
+			}
+			pf->msg_enable = level;
+			dev_info(&pf->pdev->dev, "set msg_enable = 0x%08x\n",
+				 pf->msg_enable);
+		} else {
+			dev_info(&pf->pdev->dev, "msg_enable = 0x%08x\n",
+				 pf->msg_enable);
 		}
-		wr32(&pf->hw, address, value);
-		value = rd32(&pf->hw, address);
-		dev_info(&pf->pdev->dev, "write: 0x%08x = 0x%08x\n",
-			 address, value);
+	} else if (strncmp(cmd_buf, "defport on", 10) == 0) {
+		dev_info(&pf->pdev->dev, "debugfs: forcing PFR with defport enabled\n");
+		pf->cur_promisc = true;
+		i40e_do_reset_safe(pf, BIT(__I40E_PF_RESET_REQUESTED));
+	} else if (strncmp(cmd_buf, "defport off", 11) == 0) {
+		dev_info(&pf->pdev->dev, "debugfs: forcing PFR with defport disabled\n");
+		pf->cur_promisc = false;
+		i40e_do_reset_safe(pf, BIT(__I40E_PF_RESET_REQUESTED));
 	} else if (strncmp(cmd_buf, "clear_stats", 11) == 0) {
 		if (strncmp(&cmd_buf[12], "vsi", 3) == 0) {
 			cnt = sscanf(&cmd_buf[15], "%i", &vsi_seid);
@@ -1203,7 +1539,7 @@
 		struct i40e_aq_desc *desc;
 		i40e_status ret;
 
-		desc = kzalloc(sizeof(struct i40e_aq_desc), GFP_KERNEL);
+		desc = kzalloc(sizeof(*desc), GFP_KERNEL);
 		if (!desc)
 			goto command_write_done;
 		cnt = sscanf(&cmd_buf[11],
@@ -1251,7 +1587,7 @@
 		u16 buffer_len;
 		u8 *buff;
 
-		desc = kzalloc(sizeof(struct i40e_aq_desc), GFP_KERNEL);
+		desc = kzalloc(sizeof(*desc), GFP_KERNEL);
 		if (!desc)
 			goto command_write_done;
 		cnt = sscanf(&cmd_buf[20],
@@ -1282,7 +1618,7 @@
 			desc = NULL;
 			goto command_write_done;
 		}
-		desc->flags |= cpu_to_le16((u16)I40E_AQ_FLAG_BUF);
+		desc->flags |= CPU_TO_LE16((u16)I40E_AQ_FLAG_BUF);
 		ret = i40e_asq_send_command(&pf->hw, desc, buff,
 					    buffer_len, NULL);
 		if (!ret) {
@@ -1314,10 +1650,82 @@
 	} else if (strncmp(cmd_buf, "fd current cnt", 14) == 0) {
 		dev_info(&pf->pdev->dev, "FD current total filter count for this interface: %d\n",
 			 i40e_get_current_fd_count(pf));
+	/* vf base mode on/off hooks needs to be used by validation only to
+	 * make sure vf base mode driver is not broken
+	 */
+	} else if (strncmp(cmd_buf, "vf base mode on", 15) == 0) {
+		if (!pf->num_alloc_vfs) {
+			pf->vf_base_mode_only = true;
+			dev_info(&pf->pdev->dev, "VF Base mode is enabled\n");
+		} else
+			dev_info(&pf->pdev->dev,
+				 "cannot configure VF Base mode when VFs are allocated\n");
+	} else if (strncmp(cmd_buf, "vf base mode off", 16) == 0) {
+		if (!pf->num_alloc_vfs) {
+			pf->vf_base_mode_only = false;
+			dev_info(&pf->pdev->dev, "VF Base mode is disabled\n");
+		} else
+			dev_info(&pf->pdev->dev,
+				 "cannot configure VF Base mode when VFs are allocated\n");
+	} else if ((strncmp(cmd_buf, "add ethtype filter", 18) == 0) ||
+		   (strncmp(cmd_buf, "rem ethtype filter", 18) == 0)) {
+		u16 ethtype;
+		u16 queue;
+		bool add = false;
+		int ret;
+
+		if (strncmp(cmd_buf, "add", 3) == 0)
+			add = true;
+
+		cnt = sscanf(&cmd_buf[18],
+			     "%hi %hi",
+			     &ethtype, &queue);
+		if (cnt != 2) {
+			dev_info(&pf->pdev->dev,
+				 "%s ethtype filter: bad command string, cnt=%d\n",
+				 add ? "add" : "rem",
+				 cnt);
+			goto command_write_done;
+		}
+		ret = i40e_aq_add_rem_control_packet_filter(&pf->hw,
+					pf->hw.mac.addr,
+					ethtype, 0,
+					pf->vsi[pf->lan_vsi]->seid,
+					queue, add, NULL, NULL);
+		if (ret) {
+			dev_info(&pf->pdev->dev,
+				"%s: add/rem Control Packet Filter AQ command failed =0x%x\n",
+				add ? "add" : "rem",
+				pf->hw.aq.asq_last_status);
+			goto command_write_done;
+		}
+
+	} else if (strncmp(cmd_buf, "dcb off", 7) == 0) {
+		u8 tc = i40e_pf_get_num_tc(pf);
+		/* Allow disabling only when in single TC mode */
+		if (tc > 1) {
+			dev_info(&pf->pdev->dev, "Failed to disable DCB as TC count(%d) is greater than 1.\n",
+				 tc);
+			goto command_write_done;
+		}
+		pf->flags &= ~I40E_FLAG_DCB_ENABLED;
+	} else if (strncmp(cmd_buf, "dcb on", 6) == 0) {
+		if (pf->flags & I40E_FLAG_TC_MQPRIO) {
+			dev_info(&pf->pdev->dev,
+				 "Failed to enable DCB when TCs are configured through mqprio\n");
+			goto command_write_done;
+		}
+		pf->flags |= I40E_FLAG_DCB_ENABLED;
 	} else if (strncmp(cmd_buf, "lldp", 4) == 0) {
 		if (strncmp(&cmd_buf[5], "stop", 4) == 0) {
 			int ret;
 
+			if (pf->hw.flags & I40E_HW_FLAG_FW_LLDP_STOPPABLE) {
+				dev_info(&pf->pdev->dev,
+					 "Use ethtool to disable LLDP firmware agent:"\
+					 "\"ethtool --set-priv-flags <interface> disable-fw-lldp on\".\n");
+				goto command_write_done;
+			}
 			ret = i40e_aq_stop_lldp(&pf->hw, false, false, NULL);
 			if (ret) {
 				dev_info(&pf->pdev->dev,
@@ -1327,34 +1735,41 @@
 			}
 			ret = i40e_aq_add_rem_control_packet_filter(&pf->hw,
 						pf->hw.mac.addr,
-						ETH_P_LLDP, 0,
+						I40E_ETH_P_LLDP, 0,
 						pf->vsi[pf->lan_vsi]->seid,
 						0, true, NULL, NULL);
 			if (ret) {
 				dev_info(&pf->pdev->dev,
-					"%s: Add Control Packet Filter AQ command failed =0x%x\n",
-					__func__, pf->hw.aq.asq_last_status);
+					 "%s: Add Control Packet Filter AQ command failed =0x%x\n",
+					 __func__, pf->hw.aq.asq_last_status);
 				goto command_write_done;
 			}
-#ifdef CONFIG_I40E_DCB
+#ifdef CONFIG_DCB
+#ifdef HAVE_DCBNL_IEEE
 			pf->dcbx_cap = DCB_CAP_DCBX_HOST |
 				       DCB_CAP_DCBX_VER_IEEE;
-#endif /* CONFIG_I40E_DCB */
+#endif /* HAVE_DCBNL_IEEE */
+#endif /* CONFIG_DCB */
 		} else if (strncmp(&cmd_buf[5], "start", 5) == 0) {
 			int ret;
 
+			if (pf->hw.flags & I40E_HW_FLAG_FW_LLDP_STOPPABLE) {
+				dev_info(&pf->pdev->dev,
+					 "Use ethtool to enable LLDP firmware agent:"\
+					 "\"ethtool --set-priv-flags <interface> disable-fw-lldp off\".\n");
+				goto command_write_done;
+			}
 			ret = i40e_aq_add_rem_control_packet_filter(&pf->hw,
 						pf->hw.mac.addr,
-						ETH_P_LLDP, 0,
+						I40E_ETH_P_LLDP, 0,
 						pf->vsi[pf->lan_vsi]->seid,
 						0, false, NULL, NULL);
 			if (ret) {
 				dev_info(&pf->pdev->dev,
-					"%s: Remove Control Packet Filter AQ command failed =0x%x\n",
-					__func__, pf->hw.aq.asq_last_status);
+					 "%s: Remove Control Packet Filter AQ command failed =0x%x\n",
+					 __func__, pf->hw.aq.asq_last_status);
 				/* Continue and start FW LLDP anyways */
 			}
-
 			ret = i40e_aq_start_lldp(&pf->hw, false, NULL);
 			if (ret) {
 				dev_info(&pf->pdev->dev,
@@ -1362,10 +1777,12 @@
 					 pf->hw.aq.asq_last_status);
 				goto command_write_done;
 			}
-#ifdef CONFIG_I40E_DCB
+#ifdef CONFIG_DCB
+#ifdef HAVE_DCBNL_IEEE
 			pf->dcbx_cap = DCB_CAP_DCBX_LLD_MANAGED |
 				       DCB_CAP_DCBX_VER_IEEE;
-#endif /* CONFIG_I40E_DCB */
+#endif /* HAVE_DCBNL_IEEE */
+#endif /* CONFIG_DCB */
 		} else if (strncmp(&cmd_buf[5],
 			   "get local", 9) == 0) {
 			u16 llen, rlen;
@@ -1507,6 +1924,251 @@
 		}
 		kfree(buff);
 		buff = NULL;
+	} else if (strncmp(cmd_buf, "set rss_size", 12) == 0) {
+		int q_count;
+
+		cnt = sscanf(&cmd_buf[12], "%i", &q_count);
+		if (cnt != 1) {
+			dev_info(&pf->pdev->dev,
+				 "set rss_size: bad command string, cnt=%d\n", cnt);
+			goto command_write_done;
+		}
+		if (q_count <= 0) {
+			dev_info(&pf->pdev->dev,
+				 "set rss_size: %d is too small\n",
+				 q_count);
+			goto command_write_done;
+		}
+		dev_info(&pf->pdev->dev,
+			 "set rss_size requesting %d queues\n", q_count);
+		rtnl_lock();
+		i40e_reconfig_rss_queues(pf, q_count);
+		rtnl_unlock();
+		dev_info(&pf->pdev->dev, "new rss_size %d\n",
+			 pf->alloc_rss_size);
+	} else if (strncmp(cmd_buf, "get bw", 6) == 0) {
+		i40e_status status;
+		u32 max_bw, min_bw;
+		bool min_valid, max_valid;
+
+		status = i40e_read_bw_from_alt_ram(&pf->hw, &max_bw, &min_bw,
+						   &min_valid, &max_valid);
+
+		if (status) {
+			dev_info(&pf->pdev->dev, "get bw failed with status %d\n",
+				status);
+			goto command_write_done;
+		}
+		if (!min_valid) {
+			dev_info(&pf->pdev->dev, "min bw invalid\n");
+		} else if (min_bw & I40E_ALT_BW_RELATIVE_MASK) {
+			dev_info(&pf->pdev->dev, "relative min bw = %d%%\n",
+				min_bw & I40E_ALT_BW_VALUE_MASK);
+		} else {
+			dev_info(&pf->pdev->dev, "absolute min bw = %dMb/s\n",
+				(min_bw & I40E_ALT_BW_VALUE_MASK)*128);
+		}
+		if (!max_valid) {
+			dev_info(&pf->pdev->dev, "max bw invalid\n");
+		} else if (max_bw & I40E_ALT_BW_RELATIVE_MASK) {
+			dev_info(&pf->pdev->dev, "relative max bw = %d%%\n",
+				max_bw & I40E_ALT_BW_VALUE_MASK);
+		} else {
+			dev_info(&pf->pdev->dev, "absolute max bw = %dMb/s\n",
+				(max_bw & I40E_ALT_BW_VALUE_MASK)*128);
+		}
+	} else if (strncmp(cmd_buf, "set bw", 6) == 0) {
+		struct i40e_aqc_configure_partition_bw_data bw_data;
+		i40e_status status;
+		u32 max_bw, min_bw;
+
+		/* Set the valid bit for this PF */
+		bw_data.pf_valid_bits = cpu_to_le16(BIT(pf->hw.pf_id));
+
+		/* Get the bw's */
+		cnt = sscanf(&cmd_buf[7], "%u %u", &max_bw, &min_bw);
+		if (cnt != 2) {
+			dev_info(&pf->pdev->dev,"set bw <MAX> <MIN>\n");
+			goto command_write_done;
+		}
+		bw_data.max_bw[pf->hw.pf_id] = max_bw;
+		bw_data.min_bw[pf->hw.pf_id] = min_bw;
+
+		/* Set the new bandwidths */
+		status = i40e_aq_configure_partition_bw(&pf->hw, &bw_data, NULL);
+		if (status) {
+			dev_info(&pf->pdev->dev, "configure partition bw failed with status %d\n",
+				 status);
+			goto command_write_done;
+		}
+	} else if (strncmp(cmd_buf, "commit bw", 9) == 0) {
+		/* Commit temporary BW setting to permanent NVM image */
+		enum i40e_admin_queue_err last_aq_status;
+		i40e_status aq_status;
+		u16 nvm_word;
+
+		if (pf->hw.partition_id != 1) {
+			dev_info(&pf->pdev->dev,
+				 "Commit BW only works on first partition!\n");
+			goto command_write_done;
+		}
+
+		/* Acquire NVM for read access */
+		aq_status = i40e_acquire_nvm(&pf->hw, I40E_RESOURCE_READ);
+		if (aq_status) {
+			dev_info(&pf->pdev->dev,
+				 "Error %d: Cannot acquire NVM for Read Access\n",
+				 aq_status);
+			goto command_write_done;
+		}
+
+		/* Read word 0x10 of NVM - SW compatibility word 1 */
+		aq_status = i40e_aq_read_nvm(&pf->hw,
+					     I40E_SR_NVM_CONTROL_WORD,
+					     0x10, sizeof(nvm_word), &nvm_word,
+					     false, NULL);
+		/* Save off last admin queue command status before releasing
+		 * the NVM
+		 */
+		last_aq_status = pf->hw.aq.asq_last_status;
+		i40e_release_nvm(&pf->hw);
+		if (aq_status) {
+			dev_info(&pf->pdev->dev, "NVM read error %d:%d\n",
+				 aq_status, last_aq_status);
+			goto command_write_done;
+		}
+
+		/* Wait a bit for NVM release to complete */
+		msleep(100);
+
+		/* Acquire NVM for write access */
+		aq_status = i40e_acquire_nvm(&pf->hw, I40E_RESOURCE_WRITE);
+		if (aq_status) {
+			dev_info(&pf->pdev->dev,
+				 "Error %d: Cannot acquire NVM for Write Access\n",
+				 aq_status);
+			goto command_write_done;
+		}
+		/* Write it back out unchanged to initiate update NVM,
+		 * which will force a write of the shadow (alt) RAM to
+		 * the NVM - thus storing the bandwidth values permanently.
+		 */
+		aq_status = i40e_aq_update_nvm(&pf->hw,
+					       I40E_SR_NVM_CONTROL_WORD,
+					       0x10, sizeof(nvm_word),
+					       &nvm_word, true, 0, NULL);
+		/* Save off last admin queue command status before releasing
+		 * the NVM
+		 */
+		last_aq_status = pf->hw.aq.asq_last_status;
+		i40e_release_nvm(&pf->hw);
+		if (aq_status)
+			dev_info(&pf->pdev->dev,
+				 "BW settings NOT SAVED - error %d:%d updating NVM\n",
+				 aq_status, last_aq_status);
+	} else if (strncmp(cmd_buf, "add switch ingress mirror", 25) == 0) {
+		u16 rule_type = I40E_AQC_MIRROR_RULE_TYPE_ALL_INGRESS;
+		u16 switch_seid, dst_vsi_seid, rule_id;
+		i40e_status aq_status;
+
+		cnt = sscanf(&cmd_buf[25], "%hu %hu",
+			     &switch_seid, &dst_vsi_seid);
+		if (cnt != 2) {
+			dev_info(&pf->pdev->dev,
+				 "add mirror: bad command string, cnt=%d\n",
+				 cnt);
+			goto command_write_done;
+		}
+
+		aq_status =
+			i40e_aq_add_mirrorrule(&pf->hw,
+					       switch_seid, rule_type,
+					       dst_vsi_seid, 0, NULL, NULL,
+					       &rule_id, NULL, NULL);
+		if (aq_status)
+			dev_info(&pf->pdev->dev,
+				 "add ingress mirror failed with status %d\n",
+				 aq_status);
+		else
+			dev_info(&pf->pdev->dev,
+				 "Ingress mirror rule %d added\n", rule_id);
+	} else if (strncmp(cmd_buf, "add switch egress mirror", 24) == 0) {
+		u16 rule_type = I40E_AQC_MIRROR_RULE_TYPE_ALL_EGRESS;
+		u16 switch_seid, dst_vsi_seid, rule_id;
+		i40e_status aq_status;
+
+		cnt = sscanf(&cmd_buf[24], "%hu %hu",
+			     &switch_seid, &dst_vsi_seid);
+		if (cnt != 2) {
+			dev_info(&pf->pdev->dev,
+				 "add mirror: bad command string, cnt=%d\n",
+				 cnt);
+			goto command_write_done;
+		}
+
+		aq_status =
+			i40e_aq_add_mirrorrule(&pf->hw,
+					       switch_seid, rule_type,
+					       dst_vsi_seid, 0, NULL, NULL,
+					       &rule_id, NULL, NULL);
+		if (aq_status)
+			dev_info(&pf->pdev->dev,
+				 "add egress mirror failed with status %d\n",
+				 aq_status);
+		else
+			dev_info(&pf->pdev->dev,
+				 "Egress mirror rule %d added\n", rule_id);
+	} else if (strncmp(cmd_buf, "del switch ingress mirror", 25) == 0) {
+		u16 rule_type = I40E_AQC_MIRROR_RULE_TYPE_ALL_INGRESS;
+		i40e_status aq_status;
+		u16 switch_seid, rule_id;
+
+		cnt = sscanf(&cmd_buf[25], "%hu %hu",
+			     &switch_seid, &rule_id);
+		if (cnt != 2) {
+			dev_info(&pf->pdev->dev,
+				 "del mirror: bad command string, cnt=%d\n",
+				 cnt);
+			goto command_write_done;
+		}
+
+		aq_status =
+			i40e_aq_delete_mirrorrule(&pf->hw, switch_seid,
+						  rule_type, rule_id, 0, NULL,
+						  NULL, NULL, NULL);
+		if (aq_status)
+			dev_info(&pf->pdev->dev,
+				 "mirror rule remove failed with status %d\n",
+				 aq_status);
+		else
+			dev_info(&pf->pdev->dev,
+				 "Mirror rule %d removed\n", rule_id);
+	} else if (strncmp(cmd_buf, "del switch egress mirror", 24) == 0) {
+		u16 rule_type = I40E_AQC_MIRROR_RULE_TYPE_ALL_EGRESS;
+		i40e_status aq_status;
+		u16 switch_seid, rule_id;
+
+		cnt = sscanf(&cmd_buf[24], "%hu %hu",
+			     &switch_seid, &rule_id);
+		if (cnt != 2) {
+			dev_info(&pf->pdev->dev,
+				 "del mirror: bad command string, cnt=%d\n",
+				 cnt);
+			goto command_write_done;
+		}
+
+		aq_status =
+			i40e_aq_delete_mirrorrule(&pf->hw, switch_seid,
+						  rule_type, rule_id, 0, NULL,
+						  NULL, NULL, NULL);
+		if (aq_status)
+			dev_info(&pf->pdev->dev,
+				 "mirror rule remove failed with status %d\n",
+				 aq_status);
+		else
+			dev_info(&pf->pdev->dev,
+				 "Mirror rule %d removed\n", rule_id);
+
 	} else {
 		dev_info(&pf->pdev->dev, "unknown command '%s'\n", cmd_buf);
 		dev_info(&pf->pdev->dev, "available commands\n");
@@ -1518,21 +2180,27 @@
 		dev_info(&pf->pdev->dev, "  del pvid <vsi_seid>\n");
 		dev_info(&pf->pdev->dev, "  dump switch\n");
 		dev_info(&pf->pdev->dev, "  dump vsi [seid]\n");
+		dev_info(&pf->pdev->dev, "  dump capabilities\n");
+		dev_info(&pf->pdev->dev, "  dump resources\n");
 		dev_info(&pf->pdev->dev, "  dump desc tx <vsi_seid> <ring_id> [<desc_n>]\n");
 		dev_info(&pf->pdev->dev, "  dump desc rx <vsi_seid> <ring_id> [<desc_n>]\n");
 		dev_info(&pf->pdev->dev, "  dump desc aq\n");
 		dev_info(&pf->pdev->dev, "  dump reset stats\n");
 		dev_info(&pf->pdev->dev, "  dump debug fwdata <cluster_id> <table_id> <index>\n");
+		dev_info(&pf->pdev->dev, "  msg_enable [level]\n");
 		dev_info(&pf->pdev->dev, "  read <reg>\n");
 		dev_info(&pf->pdev->dev, "  write <reg> <value>\n");
 		dev_info(&pf->pdev->dev, "  clear_stats vsi [seid]\n");
 		dev_info(&pf->pdev->dev, "  clear_stats port\n");
-		dev_info(&pf->pdev->dev, "  pfr\n");
-		dev_info(&pf->pdev->dev, "  corer\n");
-		dev_info(&pf->pdev->dev, "  globr\n");
+		dev_info(&pf->pdev->dev, "  defport on\n");
+		dev_info(&pf->pdev->dev, "  defport off\n");
 		dev_info(&pf->pdev->dev, "  send aq_cmd <flags> <opcode> <datalen> <retval> <cookie_h> <cookie_l> <param0> <param1> <param2> <param3>\n");
 		dev_info(&pf->pdev->dev, "  send indirect aq_cmd <flags> <opcode> <datalen> <retval> <cookie_h> <cookie_l> <param0> <param1> <param2> <param3> <buffer_len>\n");
 		dev_info(&pf->pdev->dev, "  fd current cnt");
+		dev_info(&pf->pdev->dev, "  vf base mode on\n");
+		dev_info(&pf->pdev->dev, "  vf base mode off\n");
+		dev_info(&pf->pdev->dev, "  add ethtype filter <ethtype> <to_queue>");
+		dev_info(&pf->pdev->dev, "  rem ethtype filter <ethtype> <to_queue>");
 		dev_info(&pf->pdev->dev, "  lldp start\n");
 		dev_info(&pf->pdev->dev, "  lldp stop\n");
 		dev_info(&pf->pdev->dev, "  lldp get local\n");
@@ -1540,6 +2208,16 @@
 		dev_info(&pf->pdev->dev, "  lldp event on\n");
 		dev_info(&pf->pdev->dev, "  lldp event off\n");
 		dev_info(&pf->pdev->dev, "  nvm read [module] [word_offset] [word_count]\n");
+		dev_info(&pf->pdev->dev, "  set rss_size <count>\n");
+		dev_info(&pf->pdev->dev, "  dcb off\n");
+		dev_info(&pf->pdev->dev, "  dcb on\n");
+		dev_info(&pf->pdev->dev, "  get bw\n");
+		dev_info(&pf->pdev->dev, "  set bw <MAX> <MIN>\n");
+		dev_info(&pf->pdev->dev, "  commit bw\n");
+		dev_info(&pf->pdev->dev, "  add switch ingress mirror <sw_seid> <dst_seid>\n");
+		dev_info(&pf->pdev->dev, "  add switch egress mirror <sw_seid> <dst_seid>\n");
+		dev_info(&pf->pdev->dev, "  del switch ingress mirror <sw_seid> <rule_id>\n");
+		dev_info(&pf->pdev->dev, "  del switch egress mirror <sw_seid> <rule_id>\n");
 	}
 
 command_write_done:
@@ -1563,7 +2241,7 @@
 static char i40e_dbg_netdev_ops_buf[256] = "";
 
 /**
- * i40e_dbg_netdev_ops - read for netdev_ops datum
+ * i40e_dbg_netdev_ops_read - read for netdev_ops datum
  * @filp: the opened file
  * @buffer: where to write the data for the user to read
  * @count: the size of the user's buffer
@@ -1574,7 +2252,7 @@
 {
 	struct i40e_pf *pf = filp->private_data;
 	int bytes_not_copied;
-	int buf_size = 256;
+	size_t buf_size = 256;
 	char *buf;
 	int len;
 
@@ -1656,8 +2334,13 @@
 			dev_info(&pf->pdev->dev, "change_mtu: no netdev for VSI %d\n",
 				 vsi_seid);
 		} else if (rtnl_trylock()) {
+#ifdef HAVE_RHEL7_EXTENDED_MIN_MAX_MTU
+			vsi->netdev->netdev_ops->extended.ndo_change_mtu(
+						 vsi->netdev, mtu);
+#else
 			vsi->netdev->netdev_ops->ndo_change_mtu(vsi->netdev,
 								mtu);
+#endif
 			rtnl_unlock();
 			dev_info(&pf->pdev->dev, "change_mtu called\n");
 		} else {
@@ -1703,6 +2386,25 @@
 				napi_schedule(&vsi->q_vectors[i]->napi);
 			dev_info(&pf->pdev->dev, "napi called\n");
 		}
+	} else if (strncmp(i40e_dbg_netdev_ops_buf,
+			   "toggle_tx_timeout", 17) == 0) {
+		cnt = sscanf(&i40e_dbg_netdev_ops_buf[17], "%i", &vsi_seid);
+		if (cnt != 1) {
+			dev_info(&pf->pdev->dev, "toggle_tx_timeout <vsi_seid>\n");
+			goto netdev_ops_write_done;
+		}
+		vsi = i40e_dbg_find_vsi(pf, vsi_seid);
+		if (!vsi) {
+			dev_info(&pf->pdev->dev, "toggle_tx_timeout: VSI %d not found\n",
+				 vsi_seid);
+		} else {
+			if (vsi->block_tx_timeout)
+				vsi->block_tx_timeout = false;
+			else
+				vsi->block_tx_timeout = true;
+			dev_info(&pf->pdev->dev, "toggle_tx_timeout: block_tx_timeout = %d\n",
+				 vsi->block_tx_timeout);
+		}
 	} else {
 		dev_info(&pf->pdev->dev, "unknown command '%s'\n",
 			 i40e_dbg_netdev_ops_buf);
@@ -1710,6 +2412,7 @@
 		dev_info(&pf->pdev->dev, "  change_mtu <vsi_seid> <mtu>\n");
 		dev_info(&pf->pdev->dev, "  set_rx_mode <vsi_seid>\n");
 		dev_info(&pf->pdev->dev, "  napi <vsi_seid>\n");
+		dev_info(&pf->pdev->dev, "  toggle_tx_timeout <vsi_seid>\n");
 	}
 netdev_ops_write_done:
 	return count;
@@ -1728,15 +2431,29 @@
  **/
 void i40e_dbg_pf_init(struct i40e_pf *pf)
 {
+	struct dentry *pfile;
 	const char *name = pci_name(pf->pdev);
+	const struct device *dev = &pf->pdev->dev;
 
 	pf->i40e_dbg_pf = debugfs_create_dir(name, i40e_dbg_root);
+	if (!pf->i40e_dbg_pf)
+		return;
 
-	debugfs_create_file("command", 0600, pf->i40e_dbg_pf, pf,
-			    &i40e_dbg_command_fops);
-
-	debugfs_create_file("netdev_ops", 0600, pf->i40e_dbg_pf, pf,
-			    &i40e_dbg_netdev_ops_fops);
+	pfile = debugfs_create_file("command", 0600, pf->i40e_dbg_pf, pf,
+				    &i40e_dbg_command_fops);
+	if (!pfile)
+		goto create_failed;
+
+	pfile = debugfs_create_file("netdev_ops", 0600, pf->i40e_dbg_pf, pf,
+				    &i40e_dbg_netdev_ops_fops);
+	if (!pfile)
+		goto create_failed;
+
+	return;
+
+create_failed:
+	dev_info(dev, "debugfs dir/file for %s failed\n", name);
+	debugfs_remove_recursive(pf->i40e_dbg_pf);
 }
 
 /**
Binary files linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_debugfs.o and linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_debugfs.o differ
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_devids.h linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_devids.h
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_devids.h	2024-05-10 01:26:45.341079560 -0400
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_devids.h	2024-05-13 03:58:25.372491940 -0400
@@ -1,7 +1,6 @@
 /* SPDX-License-Identifier: GPL-2.0 */
-/* Copyright(c) 2013 - 2018 Intel Corporation. */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
 
-#ifndef _I40E_DEVIDS_H_
 #define _I40E_DEVIDS_H_
 
 /* Device IDs */
@@ -23,6 +22,10 @@
 #define I40E_DEV_ID_10G_BASE_T_BC	0x15FF
 #define I40E_DEV_ID_10G_B		0x104F
 #define I40E_DEV_ID_10G_SFP		0x104E
+#define I40E_DEV_ID_5G_BASE_T_BC	0x101F
+#define I40E_IS_X710TL_DEVICE(d) \
+	(((d) == I40E_DEV_ID_10G_BASE_T_BC) || \
+	((d) == I40E_DEV_ID_5G_BASE_T_BC))
 #define I40E_DEV_ID_KX_X722		0x37CE
 #define I40E_DEV_ID_QSFP_X722		0x37CF
 #define I40E_DEV_ID_SFP_X722		0x37D0
@@ -34,4 +37,6 @@
 					 (d) == I40E_DEV_ID_QSFP_B  || \
 					 (d) == I40E_DEV_ID_QSFP_C)
 
-#endif /* _I40E_DEVIDS_H_ */
+#define i40e_is_25G_device(d)		((d) == I40E_DEV_ID_25G_B  || \
+					 (d) == I40E_DEV_ID_25G_SFP28)
+
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_diag.c linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_diag.c
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_diag.c	2024-05-10 01:26:45.341079560 -0400
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_diag.c	2024-05-13 03:58:25.372491940 -0400
@@ -1,5 +1,5 @@
 // SPDX-License-Identifier: GPL-2.0
-/* Copyright(c) 2013 - 2018 Intel Corporation. */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
 
 #include "i40e_diag.h"
 #include "i40e_prototype.h"
@@ -13,9 +13,7 @@
 static i40e_status i40e_diag_reg_pattern_test(struct i40e_hw *hw,
 							u32 reg, u32 mask)
 {
-	static const u32 patterns[] = {
-		0x5A5A5A5A, 0xA5A5A5A5, 0x00000000, 0xFFFFFFFF
-	};
+	const u32 patterns[] = {0x5A5A5A5A, 0xA5A5A5A5, 0x00000000, 0xFFFFFFFF};
 	u32 pat, val, orig_val;
 	int i;
 
@@ -25,9 +23,11 @@
 		wr32(hw, reg, (pat & mask));
 		val = rd32(hw, reg);
 		if ((val & mask) != (pat & mask)) {
+#ifdef ETHTOOL_TEST
 			i40e_debug(hw, I40E_DEBUG_DIAG,
 				   "%s: reg pattern test failed - reg 0x%08x pat 0x%08x val 0x%08x\n",
 				   __func__, reg, pat, val);
+#endif
 			return I40E_ERR_DIAG_TEST_FAILED;
 		}
 	}
@@ -35,35 +35,29 @@
 	wr32(hw, reg, orig_val);
 	val = rd32(hw, reg);
 	if (val != orig_val) {
+#ifdef ETHTOOL_TEST
 		i40e_debug(hw, I40E_DEBUG_DIAG,
 			   "%s: reg restore test failed - reg 0x%08x orig_val 0x%08x val 0x%08x\n",
 			   __func__, reg, orig_val, val);
+#endif
 		return I40E_ERR_DIAG_TEST_FAILED;
 	}
 
-	return 0;
+	return I40E_SUCCESS;
 }
 
 struct i40e_diag_reg_test_info i40e_reg_list[] = {
 	/* offset               mask         elements   stride */
-	{I40E_QTX_CTL(0),       0x0000FFBF, 1,
-		I40E_QTX_CTL(1) - I40E_QTX_CTL(0)},
-	{I40E_PFINT_ITR0(0),    0x00000FFF, 3,
-		I40E_PFINT_ITR0(1) - I40E_PFINT_ITR0(0)},
-	{I40E_PFINT_ITRN(0, 0), 0x00000FFF, 1,
-		I40E_PFINT_ITRN(0, 1) - I40E_PFINT_ITRN(0, 0)},
-	{I40E_PFINT_ITRN(1, 0), 0x00000FFF, 1,
-		I40E_PFINT_ITRN(1, 1) - I40E_PFINT_ITRN(1, 0)},
-	{I40E_PFINT_ITRN(2, 0), 0x00000FFF, 1,
-		I40E_PFINT_ITRN(2, 1) - I40E_PFINT_ITRN(2, 0)},
+	{I40E_QTX_CTL(0),       0x0000FFBF, 1, I40E_QTX_CTL(1) - I40E_QTX_CTL(0)},
+	{I40E_PFINT_ITR0(0),    0x00000FFF, 3, I40E_PFINT_ITR0(1) - I40E_PFINT_ITR0(0)},
+	{I40E_PFINT_ITRN(0, 0), 0x00000FFF, 1, I40E_PFINT_ITRN(0, 1) - I40E_PFINT_ITRN(0, 0)},
+	{I40E_PFINT_ITRN(1, 0), 0x00000FFF, 1, I40E_PFINT_ITRN(1, 1) - I40E_PFINT_ITRN(1, 0)},
+	{I40E_PFINT_ITRN(2, 0), 0x00000FFF, 1, I40E_PFINT_ITRN(2, 1) - I40E_PFINT_ITRN(2, 0)},
 	{I40E_PFINT_STAT_CTL0,  0x0000000C, 1, 0},
 	{I40E_PFINT_LNKLST0,    0x00001FFF, 1, 0},
-	{I40E_PFINT_LNKLSTN(0), 0x000007FF, 1,
-		I40E_PFINT_LNKLSTN(1) - I40E_PFINT_LNKLSTN(0)},
-	{I40E_QINT_TQCTL(0),    0x000000FF, 1,
-		I40E_QINT_TQCTL(1) - I40E_QINT_TQCTL(0)},
-	{I40E_QINT_RQCTL(0),    0x000000FF, 1,
-		I40E_QINT_RQCTL(1) - I40E_QINT_RQCTL(0)},
+	{I40E_PFINT_LNKLSTN(0), 0x000007FF, 1, I40E_PFINT_LNKLSTN(1) - I40E_PFINT_LNKLSTN(0)},
+	{I40E_QINT_TQCTL(0),    0x000000FF, 1, I40E_QINT_TQCTL(1) - I40E_QINT_TQCTL(0)},
+	{I40E_QINT_RQCTL(0),    0x000000FF, 1, I40E_QINT_RQCTL(1) - I40E_QINT_RQCTL(0)},
 	{I40E_PFINT_ICR0_ENA,   0xF7F20000, 1, 0},
 	{ 0 }
 };
@@ -76,12 +70,12 @@
  **/
 i40e_status i40e_diag_reg_test(struct i40e_hw *hw)
 {
-	i40e_status ret_code = 0;
+	i40e_status ret_code = I40E_SUCCESS;
 	u32 reg, mask;
 	u32 i, j;
 
 	for (i = 0; i40e_reg_list[i].offset != 0 &&
-					     !ret_code; i++) {
+					     ret_code == I40E_SUCCESS; i++) {
 
 		/* set actual reg range for dynamically allocated resources */
 		if (i40e_reg_list[i].offset == I40E_QTX_CTL(0) &&
@@ -98,9 +92,10 @@
 
 		/* test register access */
 		mask = i40e_reg_list[i].mask;
-		for (j = 0; j < i40e_reg_list[i].elements && !ret_code; j++) {
-			reg = i40e_reg_list[i].offset +
-			      (j * i40e_reg_list[i].stride);
+		for (j = 0; j < i40e_reg_list[i].elements &&
+			    ret_code == I40E_SUCCESS; j++) {
+			reg = i40e_reg_list[i].offset
+				+ (j * i40e_reg_list[i].stride);
 			ret_code = i40e_diag_reg_pattern_test(hw, reg, mask);
 		}
 	}
@@ -121,7 +116,7 @@
 
 	/* read NVM control word and if NVM valid, validate EEPROM checksum*/
 	ret_code = i40e_read_nvm_word(hw, I40E_SR_NVM_CONTROL_WORD, &reg_val);
-	if (!ret_code &&
+	if ((ret_code == I40E_SUCCESS) &&
 	    ((reg_val & I40E_SR_CONTROL_WORD_1_MASK) ==
 	     BIT(I40E_SR_CONTROL_WORD_1_SHIFT)))
 		return i40e_validate_nvm_checksum(hw, NULL);
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_diag.h linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_diag.h
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_diag.h	2024-05-10 01:26:45.341079560 -0400
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_diag.h	2024-05-13 03:58:25.372491940 -0400
@@ -1,5 +1,5 @@
 /* SPDX-License-Identifier: GPL-2.0 */
-/* Copyright(c) 2013 - 2018 Intel Corporation. */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
 
 #ifndef _I40E_DIAG_H_
 #define _I40E_DIAG_H_
Binary files linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_diag.o and linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_diag.o differ
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_ethtool.c linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_ethtool.c
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_ethtool.c	2024-05-10 01:26:45.341079560 -0400
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_ethtool.c	2024-05-13 03:58:25.372491940 -0400
@@ -1,230 +1,19 @@
 // SPDX-License-Identifier: GPL-2.0
-/* Copyright(c) 2013 - 2018 Intel Corporation. */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
 
 /* ethtool support for i40e */
 
 #include "i40e.h"
 #include "i40e_diag.h"
-#include "i40e_txrx_common.h"
 
-/* ethtool statistics helpers */
+#ifdef SIOCETHTOOL
+#ifndef ETH_GSTRING_LEN
+#define ETH_GSTRING_LEN 32
 
-/**
- * struct i40e_stats - definition for an ethtool statistic
- * @stat_string: statistic name to display in ethtool -S output
- * @sizeof_stat: the sizeof() the stat, must be no greater than sizeof(u64)
- * @stat_offset: offsetof() the stat from a base pointer
- *
- * This structure defines a statistic to be added to the ethtool stats buffer.
- * It defines a statistic as offset from a common base pointer. Stats should
- * be defined in constant arrays using the I40E_STAT macro, with every element
- * of the array using the same _type for calculating the sizeof_stat and
- * stat_offset.
- *
- * The @sizeof_stat is expected to be sizeof(u8), sizeof(u16), sizeof(u32) or
- * sizeof(u64). Other sizes are not expected and will produce a WARN_ONCE from
- * the i40e_add_ethtool_stat() helper function.
- *
- * The @stat_string is interpreted as a format string, allowing formatted
- * values to be inserted while looping over multiple structures for a given
- * statistics array. Thus, every statistic string in an array should have the
- * same type and number of format specifiers, to be formatted by variadic
- * arguments to the i40e_add_stat_string() helper function.
- **/
-struct i40e_stats {
-	char stat_string[ETH_GSTRING_LEN];
-	int sizeof_stat;
-	int stat_offset;
-};
-
-/* Helper macro to define an i40e_stat structure with proper size and type.
- * Use this when defining constant statistics arrays. Note that @_type expects
- * only a type name and is used multiple times.
- */
-#define I40E_STAT(_type, _name, _stat) { \
-	.stat_string = _name, \
-	.sizeof_stat = FIELD_SIZEOF(_type, _stat), \
-	.stat_offset = offsetof(_type, _stat) \
-}
-
-/* Helper macro for defining some statistics directly copied from the netdev
- * stats structure.
- */
-#define I40E_NETDEV_STAT(_net_stat) \
-	I40E_STAT(struct rtnl_link_stats64, #_net_stat, _net_stat)
-
-/* Helper macro for defining some statistics related to queues */
-#define I40E_QUEUE_STAT(_name, _stat) \
-	I40E_STAT(struct i40e_ring, _name, _stat)
-
-/* Stats associated with a Tx or Rx ring */
-static const struct i40e_stats i40e_gstrings_queue_stats[] = {
-	I40E_QUEUE_STAT("%s-%u.packets", stats.packets),
-	I40E_QUEUE_STAT("%s-%u.bytes", stats.bytes),
-};
-
-/**
- * i40e_add_one_ethtool_stat - copy the stat into the supplied buffer
- * @data: location to store the stat value
- * @pointer: basis for where to copy from
- * @stat: the stat definition
- *
- * Copies the stat data defined by the pointer and stat structure pair into
- * the memory supplied as data. Used to implement i40e_add_ethtool_stats and
- * i40e_add_queue_stats. If the pointer is null, data will be zero'd.
- */
-static void
-i40e_add_one_ethtool_stat(u64 *data, void *pointer,
-			  const struct i40e_stats *stat)
-{
-	char *p;
-
-	if (!pointer) {
-		/* ensure that the ethtool data buffer is zero'd for any stats
-		 * which don't have a valid pointer.
-		 */
-		*data = 0;
-		return;
-	}
-
-	p = (char *)pointer + stat->stat_offset;
-	switch (stat->sizeof_stat) {
-	case sizeof(u64):
-		*data = *((u64 *)p);
-		break;
-	case sizeof(u32):
-		*data = *((u32 *)p);
-		break;
-	case sizeof(u16):
-		*data = *((u16 *)p);
-		break;
-	case sizeof(u8):
-		*data = *((u8 *)p);
-		break;
-	default:
-		WARN_ONCE(1, "unexpected stat size for %s",
-			  stat->stat_string);
-		*data = 0;
-	}
-}
-
-/**
- * __i40e_add_ethtool_stats - copy stats into the ethtool supplied buffer
- * @data: ethtool stats buffer
- * @pointer: location to copy stats from
- * @stats: array of stats to copy
- * @size: the size of the stats definition
- *
- * Copy the stats defined by the stats array using the pointer as a base into
- * the data buffer supplied by ethtool. Updates the data pointer to point to
- * the next empty location for successive calls to __i40e_add_ethtool_stats.
- * If pointer is null, set the data values to zero and update the pointer to
- * skip these stats.
- **/
-static void
-__i40e_add_ethtool_stats(u64 **data, void *pointer,
-			 const struct i40e_stats stats[],
-			 const unsigned int size)
-{
-	unsigned int i;
-
-	for (i = 0; i < size; i++)
-		i40e_add_one_ethtool_stat((*data)++, pointer, &stats[i]);
-}
-
-/**
- * i40e_add_ethtool_stats - copy stats into ethtool supplied buffer
- * @data: ethtool stats buffer
- * @pointer: location where stats are stored
- * @stats: static const array of stat definitions
- *
- * Macro to ease the use of __i40e_add_ethtool_stats by taking a static
- * constant stats array and passing the ARRAY_SIZE(). This avoids typos by
- * ensuring that we pass the size associated with the given stats array.
- *
- * The parameter @stats is evaluated twice, so parameters with side effects
- * should be avoided.
- **/
-#define i40e_add_ethtool_stats(data, pointer, stats) \
-	__i40e_add_ethtool_stats(data, pointer, stats, ARRAY_SIZE(stats))
-
-/**
- * i40e_add_queue_stats - copy queue statistics into supplied buffer
- * @data: ethtool stats buffer
- * @ring: the ring to copy
- *
- * Queue statistics must be copied while protected by
- * u64_stats_fetch_begin_irq, so we can't directly use i40e_add_ethtool_stats.
- * Assumes that queue stats are defined in i40e_gstrings_queue_stats. If the
- * ring pointer is null, zero out the queue stat values and update the data
- * pointer. Otherwise safely copy the stats from the ring into the supplied
- * buffer and update the data pointer when finished.
- *
- * This function expects to be called while under rcu_read_lock().
- **/
-static void
-i40e_add_queue_stats(u64 **data, struct i40e_ring *ring)
-{
-	const unsigned int size = ARRAY_SIZE(i40e_gstrings_queue_stats);
-	const struct i40e_stats *stats = i40e_gstrings_queue_stats;
-	unsigned int start;
-	unsigned int i;
-
-	/* To avoid invalid statistics values, ensure that we keep retrying
-	 * the copy until we get a consistent value according to
-	 * u64_stats_fetch_retry_irq. But first, make sure our ring is
-	 * non-null before attempting to access its syncp.
-	 */
-	do {
-		start = !ring ? 0 : u64_stats_fetch_begin_irq(&ring->syncp);
-		for (i = 0; i < size; i++) {
-			i40e_add_one_ethtool_stat(&(*data)[i], ring,
-						  &stats[i]);
-		}
-	} while (ring && u64_stats_fetch_retry_irq(&ring->syncp, start));
-
-	/* Once we successfully copy the stats in, update the data pointer */
-	*data += size;
-}
-
-/**
- * __i40e_add_stat_strings - copy stat strings into ethtool buffer
- * @p: ethtool supplied buffer
- * @stats: stat definitions array
- * @size: size of the stats array
- *
- * Format and copy the strings described by stats into the buffer pointed at
- * by p.
- **/
-static void __i40e_add_stat_strings(u8 **p, const struct i40e_stats stats[],
-				    const unsigned int size, ...)
-{
-	unsigned int i;
-
-	for (i = 0; i < size; i++) {
-		va_list args;
-
-		va_start(args, size);
-		vsnprintf(*p, ETH_GSTRING_LEN, stats[i].stat_string, args);
-		*p += ETH_GSTRING_LEN;
-		va_end(args);
-	}
-}
+#endif
+#ifdef ETHTOOL_GSTATS
 
-/**
- * 40e_add_stat_strings - copy stat strings into ethtool buffer
- * @p: ethtool supplied buffer
- * @stats: stat definitions array
- *
- * Format and copy the strings described by the const static stats value into
- * the buffer pointed at by p.
- *
- * The parameter @stats is evaluated twice, so parameters with side effects
- * should be avoided. Additionally, stats must be an array such that
- * ARRAY_SIZE can be called on it.
- **/
-#define i40e_add_stat_strings(p, stats, ...) \
-	__i40e_add_stat_strings(p, stats, ARRAY_SIZE(stats), ## __VA_ARGS__)
+#include "i40e_ethtool_stats.h"
 
 #define I40E_PF_STAT(_name, _stat) \
 	I40E_STAT(struct i40e_pf, _name, _stat)
@@ -232,6 +21,8 @@
 	I40E_STAT(struct i40e_vsi, _name, _stat)
 #define I40E_VEB_STAT(_name, _stat) \
 	I40E_STAT(struct i40e_veb, _name, _stat)
+#define I40E_VEB_TC_STAT(_name, _stat) \
+	I40E_STAT(struct i40e_cp_veb_tc_stats, _name, _stat)
 #define I40E_PFC_STAT(_name, _stat) \
 	I40E_STAT(struct i40e_pfc_stats, _name, _stat)
 #define I40E_QUEUE_STAT(_name, _stat) \
@@ -266,11 +57,18 @@
 	I40E_VEB_STAT("veb.rx_unknown_protocol", stats.rx_unknown_protocol),
 };
 
+struct i40e_cp_veb_tc_stats {
+	u64 tc_rx_packets;
+	u64 tc_rx_bytes;
+	u64 tc_tx_packets;
+	u64 tc_tx_bytes;
+};
+
 static const struct i40e_stats i40e_gstrings_veb_tc_stats[] = {
-	I40E_VEB_STAT("veb.tc_%u_tx_packets", tc_stats.tc_tx_packets),
-	I40E_VEB_STAT("veb.tc_%u_tx_bytes", tc_stats.tc_tx_bytes),
-	I40E_VEB_STAT("veb.tc_%u_rx_packets", tc_stats.tc_rx_packets),
-	I40E_VEB_STAT("veb.tc_%u_rx_bytes", tc_stats.tc_rx_bytes),
+	I40E_VEB_TC_STAT("veb.tc_%u_tx_packets", tc_tx_packets),
+	I40E_VEB_TC_STAT("veb.tc_%u_tx_bytes", tc_tx_bytes),
+	I40E_VEB_TC_STAT("veb.tc_%u_rx_packets", tc_rx_packets),
+	I40E_VEB_TC_STAT("veb.tc_%u_rx_bytes", tc_rx_bytes),
 };
 
 static const struct i40e_stats i40e_gstrings_misc_stats[] = {
@@ -341,21 +139,41 @@
 	I40E_PF_STAT("port.rx_jabber", stats.rx_jabber),
 	I40E_PF_STAT("port.VF_admin_queue_requests", vf_aq_requests),
 	I40E_PF_STAT("port.arq_overflows", arq_overflows),
+#ifdef HAVE_PTP_1588_CLOCK
 	I40E_PF_STAT("port.tx_hwtstamp_timeouts", tx_hwtstamp_timeouts),
 	I40E_PF_STAT("port.rx_hwtstamp_cleared", rx_hwtstamp_cleared),
 	I40E_PF_STAT("port.tx_hwtstamp_skipped", tx_hwtstamp_skipped),
+#endif /* HAVE_PTP_1588_CLOCK */
 	I40E_PF_STAT("port.fdir_flush_cnt", fd_flush_cnt),
 	I40E_PF_STAT("port.fdir_atr_match", stats.fd_atr_match),
 	I40E_PF_STAT("port.fdir_atr_tunnel_match", stats.fd_atr_tunnel_match),
 	I40E_PF_STAT("port.fdir_atr_status", stats.fd_atr_status),
 	I40E_PF_STAT("port.fdir_sb_match", stats.fd_sb_match),
 	I40E_PF_STAT("port.fdir_sb_status", stats.fd_sb_status),
+#ifdef I40E_ADD_PROBES
+	I40E_PF_STAT("port.tx_tcp_segments", tcp_segs),
+	I40E_PF_STAT("port.tx_tcp_cso", tx_tcp_cso),
+	I40E_PF_STAT("port.tx_udp_cso", tx_udp_cso),
+	I40E_PF_STAT("port.tx_sctp_cso", tx_sctp_cso),
+	I40E_PF_STAT("port.tx_ip4_cso", tx_ip4_cso),
+	I40E_PF_STAT("port.rx_tcp_cso", rx_tcp_cso),
+	I40E_PF_STAT("port.rx_udp_cso", rx_udp_cso),
+	I40E_PF_STAT("port.rx_sctp_cso", rx_sctp_cso),
+	I40E_PF_STAT("port.rx_ip4_cso", rx_ip4_cso),
+	I40E_PF_STAT("port.rx_csum_offload_outer", hw_csum_rx_outer),
+	I40E_PF_STAT("port.rx_tcp_cso_error", rx_tcp_cso_err),
+	I40E_PF_STAT("port.rx_udp_cso_error", rx_udp_cso_err),
+	I40E_PF_STAT("port.rx_sctp_cso_error", rx_sctp_cso_err),
+	I40E_PF_STAT("port.rx_ip4_cso_error", rx_ip4_cso_err),
+#endif
 
 	/* LPI stats */
 	I40E_PF_STAT("port.tx_lpi_status", stats.tx_lpi_status),
 	I40E_PF_STAT("port.rx_lpi_status", stats.rx_lpi_status),
 	I40E_PF_STAT("port.tx_lpi_count", stats.tx_lpi_count),
 	I40E_PF_STAT("port.rx_lpi_count", stats.rx_lpi_count),
+	I40E_PF_STAT("port.tx_lpi_duration", stats.tx_lpi_duration),
+	I40E_PF_STAT("port.rx_lpi_duration", stats.rx_lpi_duration),
 };
 
 struct i40e_pfc_stats {
@@ -389,6 +207,7 @@
 
 #define I40E_GLOBAL_STATS_LEN	ARRAY_SIZE(i40e_gstrings_stats)
 
+/* Length (number) of PF core stats only (i.e. without queues / extra stats): */
 #define I40E_PF_STATS_LEN	(I40E_GLOBAL_STATS_LEN + \
 				 I40E_PFC_STATS_LEN + \
 				 I40E_VEB_STATS_LEN + \
@@ -396,7 +215,53 @@
 
 /* Length of stats for a single queue */
 #define I40E_QUEUE_STATS_LEN	ARRAY_SIZE(i40e_gstrings_queue_stats)
+#ifdef HAVE_XDP_SUPPORT
+#define I40E_QUEUE_STATS_XDP_LEN ARRAY_SIZE(i40e_gstrings_rx_queue_xdp_stats)
+#endif
+
+#ifndef I40E_PF_EXTRA_STATS_OFF
+
+#define I40E_STATS_NAME_VFID_EXTRA "vf___."
+#define I40E_STATS_NAME_VFID_EXTRA_LEN (sizeof(I40E_STATS_NAME_VFID_EXTRA) - 1)
+
+static struct i40e_stats i40e_gstrings_eth_stats_extra[] = {
+	I40E_VSI_STAT(I40E_STATS_NAME_VFID_EXTRA
+		      "rx_bytes", eth_stats.rx_bytes),
+	I40E_VSI_STAT(I40E_STATS_NAME_VFID_EXTRA
+		      "rx_unicast", eth_stats.rx_unicast),
+	I40E_VSI_STAT(I40E_STATS_NAME_VFID_EXTRA
+		      "rx_multicast", eth_stats.rx_multicast),
+	I40E_VSI_STAT(I40E_STATS_NAME_VFID_EXTRA
+		      "rx_broadcast", eth_stats.rx_broadcast),
+	I40E_VSI_STAT(I40E_STATS_NAME_VFID_EXTRA
+		      "rx_discards", eth_stats.rx_discards),
+	I40E_VSI_STAT(I40E_STATS_NAME_VFID_EXTRA
+		      "rx_unknown_protocol", eth_stats.rx_unknown_protocol),
+	I40E_VSI_STAT(I40E_STATS_NAME_VFID_EXTRA
+		      "tx_bytes", eth_stats.tx_bytes),
+	I40E_VSI_STAT(I40E_STATS_NAME_VFID_EXTRA
+		      "tx_unicast", eth_stats.tx_unicast),
+	I40E_VSI_STAT(I40E_STATS_NAME_VFID_EXTRA
+		      "tx_multicast", eth_stats.tx_multicast),
+	I40E_VSI_STAT(I40E_STATS_NAME_VFID_EXTRA
+		      "tx_broadcast", eth_stats.tx_broadcast),
+	I40E_VSI_STAT(I40E_STATS_NAME_VFID_EXTRA
+		      "tx_discards", eth_stats.tx_discards),
+	I40E_VSI_STAT(I40E_STATS_NAME_VFID_EXTRA
+		      "tx_errors", eth_stats.tx_errors),
+};
 
+#define I40E_STATS_EXTRA_COUNT	128  /* as for now only I40E_MAX_VF_COUNT */
+/* Following length value does not include the length values for queues stats */
+#define I40E_STATS_EXTRA_LEN	ARRAY_SIZE(i40e_gstrings_eth_stats_extra)
+/* Length (number) of PF extra stats only (i.e. without core stats / queues): */
+#define I40E_PF_STATS_EXTRA_LEN (I40E_STATS_EXTRA_COUNT * I40E_STATS_EXTRA_LEN)
+/* Length (number) of enhanced/all PF stats (i.e. core with extra stats): */
+#define I40E_PF_STATS_ENHANCE_LEN (I40E_PF_STATS_LEN + I40E_PF_STATS_EXTRA_LEN)
+
+#endif /* !I40E_PF_EXTRA_STATS_OFF */
+#endif /* ETHTOOL_GSTATS */
+#ifdef ETHTOOL_TEST
 enum i40e_ethtool_test_id {
 	I40E_ETH_TEST_REG = 0,
 	I40E_ETH_TEST_EEPROM,
@@ -413,6 +278,9 @@
 
 #define I40E_TEST_LEN (sizeof(i40e_gstrings_test) / ETH_GSTRING_LEN)
 
+#endif /* ETHTOOL_TEST */
+
+#ifdef HAVE_ETHTOOL_GET_SSET_COUNT
 struct i40e_priv_flags {
 	char flag_string[ETH_GSTRING_LEN];
 	u64 flag;
@@ -428,18 +296,25 @@
 static const struct i40e_priv_flags i40e_gstrings_priv_flags[] = {
 	/* NOTE: MFP setting cannot be changed */
 	I40E_PRIV_FLAG("MFP", I40E_FLAG_MFP_ENABLED, 1),
+	I40E_PRIV_FLAG("total-port-shutdown", I40E_FLAG_TOTAL_PORT_SHUTDOWN, 1),
 	I40E_PRIV_FLAG("LinkPolling", I40E_FLAG_LINK_POLLING_ENABLED, 0),
 	I40E_PRIV_FLAG("flow-director-atr", I40E_FLAG_FD_ATR_ENABLED, 0),
 	I40E_PRIV_FLAG("veb-stats", I40E_FLAG_VEB_STATS_ENABLED, 0),
 	I40E_PRIV_FLAG("hw-atr-eviction", I40E_FLAG_HW_ATR_EVICT_ENABLED, 0),
 	I40E_PRIV_FLAG("link-down-on-close",
 		       I40E_FLAG_LINK_DOWN_ON_CLOSE_ENABLED, 0),
+#ifdef HAVE_SWIOTLB_SKIP_CPU_SYNC
 	I40E_PRIV_FLAG("legacy-rx", I40E_FLAG_LEGACY_RX, 0),
+#endif
 	I40E_PRIV_FLAG("disable-source-pruning",
 		       I40E_FLAG_SOURCE_PRUNING_DISABLED, 0),
 	I40E_PRIV_FLAG("disable-fw-lldp", I40E_FLAG_DISABLE_FW_LLDP, 0),
 	I40E_PRIV_FLAG("rs-fec", I40E_FLAG_RS_FEC, 0),
 	I40E_PRIV_FLAG("base-r-fec", I40E_FLAG_BASE_R_FEC, 0),
+	I40E_PRIV_FLAG("multiple-traffic-classes",
+		       I40E_FLAG_MULTIPLE_TRAFFIC_CLASSES, 0),
+	I40E_PRIV_FLAG("vf-vlan-prune-disable",
+		       I40E_FLAG_VF_VLAN_PRUNE_DISABLE, 0),
 };
 
 #define I40E_PRIV_FLAGS_STR_LEN ARRAY_SIZE(i40e_gstrings_priv_flags)
@@ -452,6 +327,7 @@
 
 #define I40E_GL_PRIV_FLAGS_STR_LEN ARRAY_SIZE(i40e_gl_gstrings_priv_flags)
 
+#endif /* HAVE_ETHTOOL_GET_SSET_COUNT */
 /**
  * i40e_partition_setting_complaint - generic complaint for MFP restriction
  * @pf: the PF struct
@@ -508,6 +384,7 @@
 			ethtool_link_ksettings_add_link_mode(ks, advertising,
 							     10000baseT_Full);
 	}
+#ifdef HAVE_ETHTOOL_NEW_2500MB_BITS
 	if (phy_types & I40E_CAP_PHY_TYPE_2_5GBASE_T) {
 		ethtool_link_ksettings_add_link_mode(ks, supported,
 						     2500baseT_Full);
@@ -515,6 +392,8 @@
 			ethtool_link_ksettings_add_link_mode(ks, advertising,
 							     2500baseT_Full);
 	}
+#endif /* HAVE_ETHTOOL_NEW_2500MB_BITS */
+#ifdef HAVE_ETHTOOL_5G_BITS
 	if (phy_types & I40E_CAP_PHY_TYPE_5GBASE_T) {
 		ethtool_link_ksettings_add_link_mode(ks, supported,
 						     5000baseT_Full);
@@ -522,6 +401,7 @@
 			ethtool_link_ksettings_add_link_mode(ks, advertising,
 							     5000baseT_Full);
 	}
+#endif /* HAVE_ETHTOOL_5G_BITS */
 	if (phy_types & I40E_CAP_PHY_TYPE_XLAUI ||
 	    phy_types & I40E_CAP_PHY_TYPE_XLPPI ||
 	    phy_types & I40E_CAP_PHY_TYPE_40GBASE_AOC)
@@ -597,6 +477,7 @@
 			ethtool_link_ksettings_add_link_mode(ks, advertising,
 							     1000baseKX_Full);
 	}
+#ifdef HAVE_ETHTOOL_25G_BITS
 	/* need to add 25G PHY types */
 	if (phy_types & I40E_CAP_PHY_TYPE_25GBASE_KR) {
 		ethtool_link_ksettings_add_link_mode(ks, supported,
@@ -628,6 +509,7 @@
 			ethtool_link_ksettings_add_link_mode(ks, advertising,
 							     25000baseCR_Full);
 	}
+#ifdef ETHTOOL_GFECPARAM
 	if (phy_types & I40E_CAP_PHY_TYPE_25GBASE_KR ||
 	    phy_types & I40E_CAP_PHY_TYPE_25GBASE_CR ||
 	    phy_types & I40E_CAP_PHY_TYPE_25GBASE_SR ||
@@ -646,6 +528,9 @@
 							     FEC_BASER);
 		}
 	}
+#endif /* ETHTOOL_GFECPARAM */
+#endif /* HAVE_ETHTOOL_25G_BITS */
+#ifdef HAVE_ETHTOOL_NEW_10G_BITS
 	/* need to add new 10G PHY types */
 	if (phy_types & I40E_CAP_PHY_TYPE_10GBASE_CR1 ||
 	    phy_types & I40E_CAP_PHY_TYPE_10GBASE_CR1_CU) {
@@ -678,6 +563,28 @@
 			ethtool_link_ksettings_add_link_mode(ks, advertising,
 							     1000baseX_Full);
 	}
+#else
+	/* need to keep backward compatibility with older kernels */
+	if (phy_types & I40E_CAP_PHY_TYPE_10GBASE_CR1 ||
+	    phy_types & I40E_CAP_PHY_TYPE_10GBASE_CR1_CU ||
+	    phy_types & I40E_CAP_PHY_TYPE_10GBASE_SR ||
+	    phy_types & I40E_CAP_PHY_TYPE_10GBASE_LR) {
+		ethtool_link_ksettings_add_link_mode(ks, supported,
+						     10000baseT_Full);
+		if (hw_link_info->requested_speeds & I40E_LINK_SPEED_10GB)
+			ethtool_link_ksettings_add_link_mode(ks, advertising,
+							     10000baseT_Full);
+	}
+	if (phy_types & I40E_CAP_PHY_TYPE_1000BASE_SX ||
+	    phy_types & I40E_CAP_PHY_TYPE_1000BASE_LX ||
+	    phy_types & I40E_CAP_PHY_TYPE_1000BASE_T_OPTICAL) {
+		ethtool_link_ksettings_add_link_mode(ks, supported,
+						     1000baseT_Full);
+		if (hw_link_info->requested_speeds & I40E_LINK_SPEED_1GB)
+			ethtool_link_ksettings_add_link_mode(ks, advertising,
+							     1000baseT_Full);
+	}
+#endif /* HAVE_ETHTOOL_NEW_10G_BITS */
 	/* Autoneg PHY types */
 	if (phy_types & I40E_CAP_PHY_TYPE_SGMII ||
 	    phy_types & I40E_CAP_PHY_TYPE_40GBASE_KR4 ||
@@ -710,9 +617,10 @@
 	}
 }
 
+#ifdef ETHTOOL_GFECPARAM
 /**
  * i40e_get_settings_link_up_fec - Get the FEC mode encoding from mask
- * @req_fec_info: mask request FEC info
+ * @req_fec_info: mask request fec info
  * @ks: ethtool ksettings to fill in
  **/
 static void i40e_get_settings_link_up_fec(u8 req_fec_info,
@@ -739,9 +647,10 @@
 						     FEC_NONE);
 	}
 }
+#endif /* ETHTOOL_GFECPARAM */
 
 /**
- * i40e_get_settings_link_up - Get the Link settings for when link is up
+ * i40e_get_settings_link_up - Get Link settings for when link is up
  * @hw: hw structure
  * @ks: ethtool ksettings to fill in
  * @netdev: network interface device structure
@@ -795,11 +704,16 @@
 	case I40E_PHY_TYPE_1000BASE_LX:
 		ethtool_link_ksettings_add_link_mode(ks, supported, Autoneg);
 		ethtool_link_ksettings_add_link_mode(ks, advertising, Autoneg);
+#ifdef HAVE_ETHTOOL_25G_BITS
 		ethtool_link_ksettings_add_link_mode(ks, supported,
 						     25000baseSR_Full);
 		ethtool_link_ksettings_add_link_mode(ks, advertising,
 						     25000baseSR_Full);
+#ifdef ETHTOOL_GFECPARAM
 		i40e_get_settings_link_up_fec(hw_link_info->req_fec_info, ks);
+#endif /* ETHTOOL_GFECPARAM */
+#endif /* HAVE_ETHTOOL_25G_BITS */
+#ifdef HAVE_ETHTOOL_NEW_10G_BITS
 		ethtool_link_ksettings_add_link_mode(ks, supported,
 						     10000baseSR_Full);
 		ethtool_link_ksettings_add_link_mode(ks, advertising,
@@ -812,6 +726,7 @@
 						     1000baseX_Full);
 		ethtool_link_ksettings_add_link_mode(ks, advertising,
 						     1000baseX_Full);
+#endif /* HAVE_ETHTOOL_NEW_10G_BITS */
 		ethtool_link_ksettings_add_link_mode(ks, supported,
 						     10000baseT_Full);
 		if (hw_link_info->module_type[2] &
@@ -830,17 +745,21 @@
 							     10000baseT_Full);
 		break;
 	case I40E_PHY_TYPE_10GBASE_T:
-	case I40E_PHY_TYPE_5GBASE_T:
-	case I40E_PHY_TYPE_2_5GBASE_T:
+	case I40E_PHY_TYPE_5GBASE_T_LINK_STATUS:
+	case I40E_PHY_TYPE_2_5GBASE_T_LINK_STATUS:
 	case I40E_PHY_TYPE_1000BASE_T:
 	case I40E_PHY_TYPE_100BASE_TX:
 		ethtool_link_ksettings_add_link_mode(ks, supported, Autoneg);
 		ethtool_link_ksettings_add_link_mode(ks, supported,
 						     10000baseT_Full);
+#ifdef HAVE_ETHTOOL_5G_BITS
 		ethtool_link_ksettings_add_link_mode(ks, supported,
 						     5000baseT_Full);
+#endif /* HAVE_ETHTOOL_5G_BITS */
+#ifdef HAVE_ETHTOOL_NEW_2500MB_BITS
 		ethtool_link_ksettings_add_link_mode(ks, supported,
 						     2500baseT_Full);
+#endif /* HAVE_ETHTOOL_NEW_2500MB_BITS */
 		ethtool_link_ksettings_add_link_mode(ks, supported,
 						     1000baseT_Full);
 		ethtool_link_ksettings_add_link_mode(ks, supported,
@@ -849,12 +768,16 @@
 		if (hw_link_info->requested_speeds & I40E_LINK_SPEED_10GB)
 			ethtool_link_ksettings_add_link_mode(ks, advertising,
 							     10000baseT_Full);
+#ifdef HAVE_ETHTOOL_5G_BITS
 		if (hw_link_info->requested_speeds & I40E_LINK_SPEED_5GB)
 			ethtool_link_ksettings_add_link_mode(ks, advertising,
 							     5000baseT_Full);
+#endif /* HAVE_ETHTOOL_5G_BITS */
+#ifdef HAVE_ETHTOOL_NEW_2500MB_BITS
 		if (hw_link_info->requested_speeds & I40E_LINK_SPEED_2_5GB)
 			ethtool_link_ksettings_add_link_mode(ks, advertising,
 							     2500baseT_Full);
+#endif /* HAVE_ETHTOOL_NEW_2500MB_BITS */
 		if (hw_link_info->requested_speeds & I40E_LINK_SPEED_1GB)
 			ethtool_link_ksettings_add_link_mode(ks, advertising,
 							     1000baseT_Full);
@@ -889,6 +812,9 @@
 		if (hw_link_info->requested_speeds & I40E_LINK_SPEED_10GB)
 			ethtool_link_ksettings_add_link_mode(ks, advertising,
 							     10000baseT_Full);
+#ifdef ETHTOOL_GFECPARAM
+		i40e_get_settings_link_up_fec(hw_link_info->req_fec_info, ks);
+#endif /* ETHTOOL_GFECPARAM */
 		break;
 	case I40E_PHY_TYPE_SGMII:
 		ethtool_link_ksettings_add_link_mode(ks, supported, Autoneg);
@@ -914,8 +840,10 @@
 	case I40E_PHY_TYPE_1000BASE_KX:
 		ethtool_link_ksettings_add_link_mode(ks, supported,
 						     40000baseKR4_Full);
+#ifdef HAVE_ETHTOOL_25G_BITS
 		ethtool_link_ksettings_add_link_mode(ks, supported,
 						     25000baseKR_Full);
+#endif /* HAVE_ETHTOOL_25G_BITS */
 		ethtool_link_ksettings_add_link_mode(ks, supported,
 						     20000baseKR2_Full);
 		ethtool_link_ksettings_add_link_mode(ks, supported,
@@ -927,9 +855,13 @@
 		ethtool_link_ksettings_add_link_mode(ks, supported, Autoneg);
 		ethtool_link_ksettings_add_link_mode(ks, advertising,
 						     40000baseKR4_Full);
+#ifdef HAVE_ETHTOOL_25G_BITS
 		ethtool_link_ksettings_add_link_mode(ks, advertising,
 						     25000baseKR_Full);
+#ifdef ETHTOOL_GFECPARAM
 		i40e_get_settings_link_up_fec(hw_link_info->req_fec_info, ks);
+#endif /* ETHTOOL_GFECPARAM */
+#endif /* HAVE_ETHTOOL_25G_BITS */
 		ethtool_link_ksettings_add_link_mode(ks, advertising,
 						     20000baseKR2_Full);
 		ethtool_link_ksettings_add_link_mode(ks, advertising,
@@ -943,32 +875,42 @@
 	case I40E_PHY_TYPE_25GBASE_CR:
 		ethtool_link_ksettings_add_link_mode(ks, supported, Autoneg);
 		ethtool_link_ksettings_add_link_mode(ks, advertising, Autoneg);
+#ifdef HAVE_ETHTOOL_25G_BITS
 		ethtool_link_ksettings_add_link_mode(ks, supported,
 						     25000baseCR_Full);
 		ethtool_link_ksettings_add_link_mode(ks, advertising,
 						     25000baseCR_Full);
+#ifdef ETHTOOL_GFECPARAM
 		i40e_get_settings_link_up_fec(hw_link_info->req_fec_info, ks);
+#endif /* ETHTOOL_GFECPARAM */
 
+#endif /* HAVE_ETHTOOL_25G_BITS */
 		break;
 	case I40E_PHY_TYPE_25GBASE_AOC:
 	case I40E_PHY_TYPE_25GBASE_ACC:
 		ethtool_link_ksettings_add_link_mode(ks, supported, Autoneg);
 		ethtool_link_ksettings_add_link_mode(ks, advertising, Autoneg);
+#ifdef HAVE_ETHTOOL_25G_BITS
 		ethtool_link_ksettings_add_link_mode(ks, supported,
 						     25000baseCR_Full);
 		ethtool_link_ksettings_add_link_mode(ks, advertising,
 						     25000baseCR_Full);
+#ifdef ETHTOOL_GFECPARAM
 		i40e_get_settings_link_up_fec(hw_link_info->req_fec_info, ks);
+#endif /* ETHTOOL_GFECPARAM */
 
+#endif /* HAVE_ETHTOOL_25G_BITS */
+#ifdef HAVE_ETHTOOL_NEW_10G_BITS
 		ethtool_link_ksettings_add_link_mode(ks, supported,
 						     10000baseCR_Full);
 		ethtool_link_ksettings_add_link_mode(ks, advertising,
 						     10000baseCR_Full);
+#endif /* HAVE_ETHTOOL_NEW_10G_BITS */
 		break;
 	default:
 		/* if we got here and link is up something bad is afoot */
 		netdev_info(netdev,
-			    "WARNING: Link is up but PHY type 0x%x is not recognized.\n",
+			    "WARNING: Link is up but PHY type 0x%x is not recognized, or incorrect cable is in use\n",
 			    hw_link_info->phy_type);
 	}
 
@@ -1014,7 +956,7 @@
 }
 
 /**
- * i40e_get_settings_link_down - Get the Link settings for when link is down
+ * i40e_get_settings_link_down - Get the Link settings when link is down
  * @hw: hw structure
  * @ks: ethtool ksettings to fill in
  * @pf: pointer to physical function struct
@@ -1029,7 +971,6 @@
 	 * supported phy types to figure out what info to display
 	 */
 	i40e_phy_type_to_ethtool(pf, ks);
-
 	/* With no link speed and duplex are unknown */
 	ks->base.speed = SPEED_UNKNOWN;
 	ks->base.duplex = DUPLEX_UNKNOWN;
@@ -1061,7 +1002,7 @@
 
 	/* Now set the settings that don't rely on link being up/down */
 	/* Set autoneg settings */
-	ks->base.autoneg = ((hw_link_info->an_info & I40E_AQ_AN_COMPLETED) ?
+	ks->base.autoneg = (hw_link_info->an_info & I40E_AQ_AN_COMPLETED ?
 			    AUTONEG_ENABLE : AUTONEG_DISABLE);
 
 	/* Set media type settings */
@@ -1098,6 +1039,7 @@
 
 	/* Set flow control settings */
 	ethtool_link_ksettings_add_link_mode(ks, supported, Pause);
+	ethtool_link_ksettings_add_link_mode(ks, supported, Asym_Pause);
 
 	switch (hw->fc.requested_mode) {
 	case I40E_FC_FULL:
@@ -1118,10 +1060,10 @@
 						     Asym_Pause);
 		break;
 	}
-
 	return 0;
 }
 
+#ifdef ETHTOOL_GLINKSETTINGS
 /**
  * i40e_set_link_ksettings - Set Speed and Duplex
  * @netdev: network interface device structure
@@ -1141,7 +1083,7 @@
 	struct i40e_vsi *vsi = np->vsi;
 	struct i40e_hw *hw = &pf->hw;
 	bool autoneg_changed = false;
-	i40e_status status = 0;
+	i40e_status status = I40E_SUCCESS;
 	int timeout = 50;
 	int err = 0;
 	u8 autoneg;
@@ -1185,7 +1127,7 @@
 	i40e_get_link_ksettings(netdev, &safe_ks);
 
 	/* Get link modes supported by hardware and check against modes
-	 * requested by the user.  Return an error if unsupported mode was set.
+	 * requested by user.  Return an error if unsupported mode was set.
 	 */
 	if (!bitmap_subset(copy_ks.link_modes.advertising,
 			   safe_ks.link_modes.supported,
@@ -1228,9 +1170,8 @@
 		/* If autoneg was not already enabled */
 		if (!(hw->phy.link_info.an_info & I40E_AQ_AN_COMPLETED)) {
 			/* If autoneg is not supported, return error */
-			if (!ethtool_link_ksettings_test_link_mode(&safe_ks,
-								   supported,
-								   Autoneg)) {
+			if (!ethtool_link_ksettings_test_link_mode(
+				      &safe_ks, supported, Autoneg)) {
 				netdev_info(netdev, "Autoneg not supported on this phy\n");
 				err = -EINVAL;
 				goto done;
@@ -1246,11 +1187,9 @@
 			/* If autoneg is supported 10GBASE_T is the only PHY
 			 * that can disable it, so otherwise return error
 			 */
-			if (ethtool_link_ksettings_test_link_mode(&safe_ks,
-								  supported,
-								  Autoneg) &&
-			    hw->phy.link_info.phy_type !=
-			    I40E_PHY_TYPE_10GBASE_T) {
+			if (ethtool_link_ksettings_test_link_mode(
+				     &safe_ks, supported, Autoneg) &&
+			    hw->phy.media_type != I40E_MEDIA_TYPE_BASET) {
 				netdev_info(netdev, "Autoneg cannot be disabled on this phy\n");
 				err = -EINVAL;
 				goto done;
@@ -1267,8 +1206,10 @@
 		config.link_speed |= I40E_LINK_SPEED_100MB;
 	if (ethtool_link_ksettings_test_link_mode(ks, advertising,
 						  1000baseT_Full) ||
+#ifdef HAVE_ETHTOOL_NEW_10G_BITS
 	    ethtool_link_ksettings_test_link_mode(ks, advertising,
 						  1000baseX_Full) ||
+#endif
 	    ethtool_link_ksettings_test_link_mode(ks, advertising,
 						  1000baseKX_Full))
 		config.link_speed |= I40E_LINK_SPEED_1GB;
@@ -1278,22 +1219,31 @@
 						  10000baseKX4_Full) ||
 	    ethtool_link_ksettings_test_link_mode(ks, advertising,
 						  10000baseKR_Full) ||
+#ifdef HAVE_ETHTOOL_NEW_10G_BITS
 	    ethtool_link_ksettings_test_link_mode(ks, advertising,
 						  10000baseCR_Full) ||
 	    ethtool_link_ksettings_test_link_mode(ks, advertising,
 						  10000baseSR_Full) ||
 	    ethtool_link_ksettings_test_link_mode(ks, advertising,
 						  10000baseLR_Full))
+#else
+	    0)
+#endif /* HAVE_ETHTOOL_NEW_10G_BITS */
 		config.link_speed |= I40E_LINK_SPEED_10GB;
+#ifdef HAVE_ETHTOOL_NEW_2500MB_BITS
 	if (ethtool_link_ksettings_test_link_mode(ks, advertising,
 						  2500baseT_Full))
 		config.link_speed |= I40E_LINK_SPEED_2_5GB;
+#endif /* HAVE_ETHTOOL_NEW_2500MB_BITS */
+#ifdef HAVE_ETHTOOL_5G_BITS
 	if (ethtool_link_ksettings_test_link_mode(ks, advertising,
 						  5000baseT_Full))
 		config.link_speed |= I40E_LINK_SPEED_5GB;
+#endif /* HAVE_ETHTOOL_5G_BITS */
 	if (ethtool_link_ksettings_test_link_mode(ks, advertising,
 						  20000baseKR2_Full))
 		config.link_speed |= I40E_LINK_SPEED_20GB;
+#ifdef HAVE_ETHTOOL_25G_BITS
 	if (ethtool_link_ksettings_test_link_mode(ks, advertising,
 						  25000baseCR_Full) ||
 	    ethtool_link_ksettings_test_link_mode(ks, advertising,
@@ -1301,6 +1251,7 @@
 	    ethtool_link_ksettings_test_link_mode(ks, advertising,
 						  25000baseSR_Full))
 		config.link_speed |= I40E_LINK_SPEED_25GB;
+#endif /* HAVE_ETHTOOL_25G_BITS */
 	if (ethtool_link_ksettings_test_link_mode(ks, advertising,
 						  40000baseKR4_Full) ||
 	    ethtool_link_ksettings_test_link_mode(ks, advertising,
@@ -1317,7 +1268,7 @@
 	 */
 	if (!config.link_speed)
 		config.link_speed = abilities.link_speed;
-	if (autoneg_changed || abilities.link_speed != config.link_speed) {
+	if (autoneg_changed || (abilities.link_speed != config.link_speed)) {
 		/* copy over the rest of the abilities */
 		config.phy_type = abilities.phy_type;
 		config.phy_type_ext = abilities.phy_type_ext;
@@ -1369,14 +1320,259 @@
 	return err;
 }
 
+#else /* ETHTOOL_GLINKSETTINGS */
+/**
+ * i40e_get_settings - Get Link Speed and Duplex settings
+ * @netdev: network interface device structure
+ * @ecmd: ethtool command
+ *
+ * Reports speed/duplex settings based on media_type.  Since we've backported
+ * the new API constructs to use in the old API, this ends up just being a
+ * wrapper to i40e_get_link_ksettings.
+ **/
+static int i40e_get_settings(struct net_device *netdev,
+			     struct ethtool_cmd *ecmd)
+{
+	struct ethtool_link_ksettings ks;
+
+	i40e_get_link_ksettings(netdev, &ks);
+	_kc_ethtool_ksettings_to_cmd(&ks, ecmd);
+	ecmd->transceiver = XCVR_EXTERNAL;
+	return 0;
+}
+
+/**
+ * i40e_set_settings - Set Speed and Duplex
+ * @netdev: network interface device structure
+ * @ecmd: ethtool command
+ *
+ * Set speed/duplex per media_types advertised/forced
+ **/
+static int i40e_set_settings(struct net_device *netdev,
+			     struct ethtool_cmd *ecmd)
+{
+	struct i40e_netdev_priv *np = netdev_priv(netdev);
+	struct i40e_aq_get_phy_abilities_resp abilities;
+	struct i40e_aq_set_phy_config config;
+	struct i40e_pf *pf = np->vsi->back;
+	struct i40e_vsi *vsi = np->vsi;
+	struct i40e_hw *hw = &pf->hw;
+	struct ethtool_cmd safe_ecmd;
+	i40e_status status = I40E_SUCCESS;
+	bool change = false;
+	int timeout = 50;
+	int err = 0;
+	u8 autoneg;
+	u32 advertise;
+	u32 old_ethtool_advertising = 0;
+
+	/* Changing port settings is not supported if this isn't the
+	 * port's controlling PF
+	 */
+	if (hw->partition_id != 1) {
+		i40e_partition_setting_complaint(pf);
+		return -EOPNOTSUPP;
+	}
+
+	if (vsi != pf->vsi[pf->lan_vsi])
+		return -EOPNOTSUPP;
+
+	if (hw->phy.media_type != I40E_MEDIA_TYPE_BASET &&
+	    hw->phy.media_type != I40E_MEDIA_TYPE_FIBER &&
+	    hw->phy.media_type != I40E_MEDIA_TYPE_BACKPLANE &&
+	    hw->phy.media_type != I40E_MEDIA_TYPE_DA &&
+	    hw->phy.link_info.link_info & I40E_AQ_LINK_UP)
+		return -EOPNOTSUPP;
+
+	if (hw->device_id == I40E_DEV_ID_KX_B ||
+	    hw->device_id == I40E_DEV_ID_KX_C ||
+	    hw->device_id == I40E_DEV_ID_20G_KR2 ||
+	    hw->device_id == I40E_DEV_ID_20G_KR2_A) {
+		netdev_info(netdev, "Changing settings is not supported on backplane.\n");
+		return -EOPNOTSUPP;
+	}
+
+	/* get our own copy of the bits to check against */
+	memset(&safe_ecmd, 0, sizeof(struct ethtool_cmd));
+	i40e_get_settings(netdev, &safe_ecmd);
+
+	/* save autoneg and speed out of ecmd */
+	autoneg = ecmd->autoneg;
+	advertise = ecmd->advertising;
+
+	/* set autoneg and speed back to what they currently are */
+	ecmd->autoneg = safe_ecmd.autoneg;
+	ecmd->advertising = safe_ecmd.advertising;
+
+	/* Due to a bug in ethtool versions < 3.6 this check is necessary */
+	old_ethtool_advertising = ecmd->supported &
+				  (ADVERTISED_10baseT_Half |
+				   ADVERTISED_10baseT_Full |
+				   ADVERTISED_100baseT_Half |
+				   ADVERTISED_100baseT_Full |
+				   ADVERTISED_1000baseT_Half |
+				   ADVERTISED_1000baseT_Full |
+				   ADVERTISED_2500baseX_Full |
+				   ADVERTISED_10000baseT_Full);
+	old_ethtool_advertising |= (old_ethtool_advertising |
+				   ADVERTISED_20000baseMLD2_Full |
+				   ADVERTISED_20000baseKR2_Full);
+
+	if (advertise == old_ethtool_advertising)
+		netdev_info(netdev, "If you are not setting advertising to %x then you may have an old version of ethtool. Please update.\n",
+			    advertise);
+	ecmd->cmd = safe_ecmd.cmd;
+	/* If ecmd and safe_ecmd are not the same now, then they are
+	 * trying to set something that we do not support
+	 */
+	if (memcmp(ecmd, &safe_ecmd, sizeof(struct ethtool_cmd)))
+		return -EOPNOTSUPP;
+
+	while (test_and_set_bit(__I40E_CONFIG_BUSY, pf->state)) {
+		timeout--;
+		if (!timeout)
+			return -EBUSY;
+		usleep_range(1000, 2000);
+	}
+
+	/* Get the current phy config */
+	status = i40e_aq_get_phy_capabilities(hw, false, false, &abilities,
+					      NULL);
+	if (status) {
+		err = -EAGAIN;
+		goto done;
+	}
+
+	/* Copy abilities to config in case autoneg is not
+	 * set below
+	 */
+	memset(&config, 0, sizeof(struct i40e_aq_set_phy_config));
+	config.abilities = abilities.abilities;
+
+	/* Check autoneg */
+	if (autoneg == AUTONEG_ENABLE) {
+		/* If autoneg was not already enabled */
+		if (!(hw->phy.link_info.an_info & I40E_AQ_AN_COMPLETED)) {
+			/* If autoneg is not supported, return error */
+			if (!(safe_ecmd.supported & SUPPORTED_Autoneg)) {
+				netdev_info(netdev, "Autoneg not supported on this phy\n");
+				err = -EINVAL;
+				goto done;
+			}
+			/* Autoneg is allowed to change */
+			config.abilities = abilities.abilities |
+					   I40E_AQ_PHY_ENABLE_AN;
+			change = true;
+		}
+	} else {
+		/* If autoneg is currently enabled */
+		if (hw->phy.link_info.an_info & I40E_AQ_AN_COMPLETED) {
+			/* If autoneg is supported 10GBASE_T is the only phy
+			 * that can disable it, so otherwise return error
+			 */
+			if (safe_ecmd.supported & SUPPORTED_Autoneg &&
+			    hw->phy.link_info.phy_type !=
+			    I40E_PHY_TYPE_10GBASE_T) {
+				netdev_info(netdev, "Autoneg cannot be disabled on this phy\n");
+				err = -EINVAL;
+				goto done;
+			}
+			/* Autoneg is allowed to change */
+			config.abilities = abilities.abilities &
+					   ~I40E_AQ_PHY_ENABLE_AN;
+			change = true;
+		}
+	}
+
+	if (advertise & ~safe_ecmd.supported) {
+		err = -EINVAL;
+		goto done;
+	}
+
+	if (advertise & ADVERTISED_100baseT_Full)
+		config.link_speed |= I40E_LINK_SPEED_100MB;
+	if (advertise & ADVERTISED_1000baseT_Full ||
+	    advertise & ADVERTISED_1000baseKX_Full)
+		config.link_speed |= I40E_LINK_SPEED_1GB;
+	if (advertise & ADVERTISED_10000baseT_Full ||
+	    advertise & ADVERTISED_10000baseKX4_Full ||
+	    advertise & ADVERTISED_10000baseKR_Full)
+		config.link_speed |= I40E_LINK_SPEED_10GB;
+	if (advertise & ADVERTISED_20000baseKR2_Full)
+		config.link_speed |= I40E_LINK_SPEED_20GB;
+	if (advertise & ADVERTISED_40000baseKR4_Full ||
+	    advertise & ADVERTISED_40000baseCR4_Full ||
+	    advertise & ADVERTISED_40000baseSR4_Full ||
+	    advertise & ADVERTISED_40000baseLR4_Full)
+		config.link_speed |= I40E_LINK_SPEED_40GB;
+
+	/* If speed didn't get set, set it to what it currently is.
+	 * This is needed because if advertise is 0 (as it is when autoneg
+	 * is disabled) then speed won't get set.
+	 */
+	if (!config.link_speed)
+		config.link_speed = abilities.link_speed;
+
+	if (change || abilities.link_speed != config.link_speed) {
+		/* copy over the rest of the abilities */
+		config.phy_type = abilities.phy_type;
+		config.phy_type_ext = abilities.phy_type_ext;
+		config.eee_capability = abilities.eee_capability;
+		config.eeer = abilities.eeer_val;
+		config.low_power_ctrl = abilities.d3_lpan;
+		config.fec_config = abilities.fec_cfg_curr_mod_ext_info &
+				    I40E_AQ_PHY_FEC_CONFIG_MASK;
+
+		/* save the requested speeds */
+		hw->phy.link_info.requested_speeds = config.link_speed;
+		/* set link and auto negotiation so changes take effect */
+		config.abilities |= I40E_AQ_PHY_ENABLE_ATOMIC_LINK;
+		/* If link is up put link down */
+		if (hw->phy.link_info.link_info & I40E_AQ_LINK_UP) {
+			/* Tell the OS link is going down, the link will go
+			 * back up when fw says it is ready asynchronously
+			 */
+			i40e_print_link_message(vsi, false);
+			netif_carrier_off(netdev);
+			netif_tx_stop_all_queues(netdev);
+		}
+
+		/* make the aq call */
+		status = i40e_aq_set_phy_config(hw, &config, NULL);
+		if (status) {
+			netdev_info(netdev, "Set phy config failed, err %s aq_err %s\n",
+				    i40e_stat_str(hw, status),
+				    i40e_aq_str(hw, hw->aq.asq_last_status));
+			err = -EAGAIN;
+			goto done;
+		}
+
+		status = i40e_update_link_info(hw);
+		if (status)
+			netdev_dbg(netdev, "Updating link info failed with err %s aq_err %s\n",
+				   i40e_stat_str(hw, status),
+				   i40e_aq_str(hw, hw->aq.asq_last_status));
+
+	} else {
+		netdev_info(netdev, "Nothing changed, exiting without setting anything.\n");
+	}
+
+done:
+	clear_bit(__I40E_CONFIG_BUSY, pf->state);
+
+	return err;
+}
+
+#endif /* ETHTOOL_GLINKSETTINGS */
+
 static int i40e_set_fec_cfg(struct net_device *netdev, u8 fec_cfg)
 {
 	struct i40e_netdev_priv *np = netdev_priv(netdev);
 	struct i40e_aq_get_phy_abilities_resp abilities;
 	struct i40e_pf *pf = np->vsi->back;
 	struct i40e_hw *hw = &pf->hw;
-	i40e_status status = 0;
-	u32 flags = 0;
+	i40e_status status = I40E_SUCCESS;
+	u64 flags = 0;
 	int err = 0;
 
 	flags = READ_ONCE(pf->flags);
@@ -1396,7 +1592,8 @@
 
 		memset(&config, 0, sizeof(config));
 		config.phy_type = abilities.phy_type;
-		config.abilities = abilities.abilities;
+		config.abilities = abilities.abilities |
+				   I40E_AQ_PHY_ENABLE_ATOMIC_LINK;
 		config.phy_type_ext = abilities.phy_type_ext;
 		config.link_speed = abilities.link_speed;
 		config.eee_capability = abilities.eee_capability;
@@ -1429,6 +1626,7 @@
 	return err;
 }
 
+#ifdef ETHTOOL_GFECPARAM
 static int i40e_get_fec_param(struct net_device *netdev,
 			      struct ethtool_fecparam *fecparam)
 {
@@ -1436,7 +1634,7 @@
 	struct i40e_aq_get_phy_abilities_resp abilities;
 	struct i40e_pf *pf = np->vsi->back;
 	struct i40e_hw *hw = &pf->hw;
-	i40e_status status = 0;
+	i40e_status status = I40E_SUCCESS;
 	int err = 0;
 	u8 fec_cfg;
 
@@ -1482,18 +1680,37 @@
 	int err = 0;
 
 	if (hw->device_id != I40E_DEV_ID_25G_SFP28 &&
-	    hw->device_id != I40E_DEV_ID_25G_B) {
+	    hw->device_id != I40E_DEV_ID_25G_B &&
+	    hw->device_id != I40E_DEV_ID_KX_X722) {
 		err = -EPERM;
 		goto done;
 	}
 
+	if (hw->mac.type == I40E_MAC_X722 &&
+	    !(hw->flags & I40E_HW_FLAG_X722_FEC_REQUEST_CAPABLE)) {
+		netdev_err(netdev, "Setting FEC encoding not supported by firmware. Please update the NVM image.\n");
+		return -EOPNOTSUPP;
+	}
+
 	switch (fecparam->fec) {
 	case ETHTOOL_FEC_AUTO:
-		fec_cfg = I40E_AQ_SET_FEC_AUTO;
+		if (hw->mac.type == I40E_MAC_X722) {
+			dev_warn(&pf->pdev->dev, "Unsupported FEC mode: AUTO");
+			err = -EINVAL;
+			goto done;
+		} else {
+			fec_cfg = I40E_AQ_SET_FEC_AUTO;
+		}
 		break;
 	case ETHTOOL_FEC_RS:
-		fec_cfg = (I40E_AQ_SET_FEC_REQUEST_RS |
-			     I40E_AQ_SET_FEC_ABILITY_RS);
+		if (hw->mac.type == I40E_MAC_X722) {
+			dev_warn(&pf->pdev->dev, "Unsupported FEC mode: RS");
+			err = -EINVAL;
+			goto done;
+		} else {
+			fec_cfg = (I40E_AQ_SET_FEC_REQUEST_RS |
+				   I40E_AQ_SET_FEC_ABILITY_RS);
+		}
 		break;
 	case ETHTOOL_FEC_BASER:
 		fec_cfg = (I40E_AQ_SET_FEC_REQUEST_KR |
@@ -1515,6 +1732,7 @@
 done:
 	return err;
 }
+#endif /* ETHTOOL_GFECPARAM */
 
 static int i40e_nway_reset(struct net_device *netdev)
 {
@@ -1523,7 +1741,7 @@
 	struct i40e_pf *pf = np->vsi->back;
 	struct i40e_hw *hw = &pf->hw;
 	bool link_up = hw->phy.link_info.link_info & I40E_AQ_LINK_UP;
-	i40e_status ret = 0;
+	i40e_status ret = I40E_SUCCESS;
 
 	ret = i40e_aq_set_link_restart_an(hw, link_up, NULL);
 	if (ret) {
@@ -1549,12 +1767,9 @@
 	struct i40e_netdev_priv *np = netdev_priv(netdev);
 	struct i40e_pf *pf = np->vsi->back;
 	struct i40e_hw *hw = &pf->hw;
-	struct i40e_link_status *hw_link_info = &hw->phy.link_info;
 	struct i40e_dcbx_config *dcbx_cfg = &hw->local_dcbx_config;
 
-	pause->autoneg =
-		((hw_link_info->an_info & I40E_AQ_AN_COMPLETED) ?
-		  AUTONEG_ENABLE : AUTONEG_DISABLE);
+	pause->autoneg = hw->phy.link_info.an_info & I40E_AQ_AN_COMPLETED;
 
 	/* PFC enabled so report LFC as off */
 	if (dcbx_cfg->pfc.pfcenable) {
@@ -1618,7 +1833,7 @@
 
 	if (dcbx_cfg->pfc.pfcenable) {
 		netdev_info(netdev,
-			    "Priority flow control enabled. Cannot set link flow control.\n");
+			 "Priority flow control enabled. Cannot set link flow control.\n");
 		return -EOPNOTSUPP;
 	}
 
@@ -1672,14 +1887,133 @@
 	return err;
 }
 
-static u32 i40e_get_msglevel(struct net_device *netdev)
+#ifndef HAVE_NDO_SET_FEATURES
+static u32 i40e_get_rx_csum(struct net_device *netdev)
 {
 	struct i40e_netdev_priv *np = netdev_priv(netdev);
 	struct i40e_pf *pf = np->vsi->back;
-	u32 debug_mask = pf->hw.debug_mask;
 
-	if (debug_mask)
-		netdev_info(netdev, "i40e debug_mask: 0x%08X\n", debug_mask);
+	return pf->flags & I40E_FLAG_RX_CSUM_ENABLED;
+}
+
+static int i40e_set_rx_csum(struct net_device *netdev, u32 data)
+{
+	struct i40e_netdev_priv *np = netdev_priv(netdev);
+	struct i40e_pf *pf = np->vsi->back;
+
+	if (data)
+		pf->flags |= I40E_FLAG_RX_CSUM_ENABLED;
+	else
+		pf->flags &= ~I40E_FLAG_RX_CSUM_ENABLED;
+
+	return 0;
+}
+
+static u32 i40e_get_tx_csum(struct net_device *netdev)
+{
+	return (netdev->features & NETIF_F_IP_CSUM) != 0;
+}
+
+static int i40e_set_tx_csum(struct net_device *netdev, u32 data)
+{
+	if (data) {
+#ifdef NETIF_F_IPV6_CSUM
+		netdev->features |= NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM;
+#else
+		netdev->features |= NETIF_F_IP_CSUM;
+#endif
+		netdev->features |= NETIF_F_SCTP_CRC;
+	} else {
+#ifdef NETIF_F_IPV6_CSUM
+		netdev->features &= ~(NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM |
+				      NETIF_F_SCTP_CRC);
+#else
+		netdev->features &= ~(NETIF_F_IP_CSUM | NETIF_F_SCTP_CRC);
+#endif
+	}
+
+	return 0;
+}
+
+static int i40e_set_tso(struct net_device *netdev, u32 data)
+{
+	if (data) {
+#ifndef HAVE_NDO_FEATURES_CHECK
+		if (netdev->mtu >= 576) {
+			netdev->features |= NETIF_F_TSO;
+			netdev->features |= NETIF_F_TSO6;
+		} else {
+			netdev_info(netdev, "MTU setting is too low to enable TSO\n");
+		}
+#else
+		netdev->features |= NETIF_F_TSO;
+		netdev->features |= NETIF_F_TSO6;
+#endif
+	} else {
+#ifndef HAVE_NETDEV_VLAN_FEATURES
+		struct i40e_netdev_priv *np = netdev_priv(netdev);
+		/* disable TSO on all VLANs if they're present */
+		if (np->vsi->vlgrp) {
+			int i;
+			struct net_device *v_netdev;
+
+			for (i = 0; i < VLAN_GROUP_ARRAY_LEN; i++) {
+				v_netdev =
+				       vlan_group_get_device(np->vsi->vlgrp, i);
+				if (v_netdev) {
+					v_netdev->features &= ~NETIF_F_TSO;
+					v_netdev->features &= ~NETIF_F_TSO6;
+					vlan_group_set_device(np->vsi->vlgrp, i,
+							      v_netdev);
+				}
+			}
+		}
+#endif /* HAVE_NETDEV_VLAN_FEATURES */
+		netdev->features &= ~NETIF_F_TSO;
+		netdev->features &= ~NETIF_F_TSO6;
+	}
+
+	return 0;
+}
+#ifdef ETHTOOL_GFLAGS
+static int i40e_set_flags(struct net_device *netdev, u32 data)
+{
+#ifdef ETHTOOL_GRXRINGS
+	struct i40e_netdev_priv *np = netdev_priv(netdev);
+	struct i40e_pf *pf = np->vsi->back;
+#endif
+	u32 supported_flags = 0;
+	bool need_reset = false;
+	int rc;
+
+#ifdef NETIF_F_RXHASH
+	supported_flags |= ETH_FLAG_RXHASH;
+
+#endif
+#ifdef ETHTOOL_GRXRINGS
+	if (!(pf->flags & I40E_FLAG_MFP_ENABLED))
+		supported_flags |= ETH_FLAG_NTUPLE;
+#endif
+	rc = ethtool_op_set_flags(netdev, data, supported_flags);
+	if (rc)
+		return rc;
+
+	/* if state changes we need to update pf->flags and maybe reset */
+#ifdef ETHTOOL_GRXRINGS
+	need_reset = i40e_set_ntuple(pf, netdev->features);
+#endif /* ETHTOOL_GRXRINGS */
+	if (need_reset)
+		i40e_do_reset(pf, BIT(__I40E_PF_RESET_REQUESTED), true);
+
+	return 0;
+}
+#endif /* ETHTOOL_GFLAGS */
+
+#endif /* HAVE_NDO_SET_FEATURES */
+static u32 i40e_get_msglevel(struct net_device *netdev)
+{
+	struct i40e_netdev_priv *np = netdev_priv(netdev);
+	struct i40e_pf *pf = np->vsi->back;
 
 	return pf->msg_enable;
 }
@@ -1691,8 +2025,7 @@
 
 	if (I40E_DEBUG_USER & data)
 		pf->hw.debug_mask = data;
-	else
-		pf->msg_enable = data;
+	pf->msg_enable = data;
 }
 
 static int i40e_get_regs_len(struct net_device *netdev)
@@ -1757,25 +2090,25 @@
 	magic = hw->vendor_id | (hw->device_id << 16);
 	if (eeprom->magic && eeprom->magic != magic) {
 		struct i40e_nvm_access *cmd = (struct i40e_nvm_access *)eeprom;
-		int errno = 0;
+		int err = 0;
 
 		/* make sure it is the right magic for NVMUpdate */
 		if ((eeprom->magic >> 16) != hw->device_id)
-			errno = -EINVAL;
+			err = -EINVAL;
 		else if (test_bit(__I40E_RESET_RECOVERY_PENDING, pf->state) ||
 			 test_bit(__I40E_RESET_INTR_RECEIVED, pf->state))
-			errno = -EBUSY;
+			err = -EBUSY;
 		else
-			ret_val = i40e_nvmupd_command(hw, cmd, bytes, &errno);
+			ret_val = i40e_nvmupd_command(hw, cmd, bytes, &err);
 
-		if ((errno || ret_val) && (hw->debug_mask & I40E_DEBUG_NVM))
+		if ((err || ret_val) && (hw->debug_mask & I40E_DEBUG_NVM))
 			dev_info(&pf->pdev->dev,
 				 "NVMUpdate read failed err=%d status=0x%x errno=%d module=%d offset=0x%x size=%d\n",
-				 ret_val, hw->aq.asq_last_status, errno,
+				 ret_val, hw->aq.asq_last_status, err,
 				 (u8)(cmd->config & I40E_NVM_MOD_PNT_MASK),
 				 cmd->offset, cmd->data_size);
 
-		return errno;
+		return err;
 	}
 
 	/* normal ethtool get_eeprom support */
@@ -1859,30 +2192,30 @@
 	struct i40e_pf *pf = np->vsi->back;
 	struct i40e_nvm_access *cmd = (struct i40e_nvm_access *)eeprom;
 	int ret_val = 0;
-	int errno = 0;
+	int err = 0;
 	u32 magic;
 
 	/* normal ethtool set_eeprom is not supported */
 	magic = hw->vendor_id | (hw->device_id << 16);
 	if (eeprom->magic == magic)
-		errno = -EOPNOTSUPP;
+		err = -EOPNOTSUPP;
 	/* check for NVMUpdate access method */
 	else if (!eeprom->magic || (eeprom->magic >> 16) != hw->device_id)
-		errno = -EINVAL;
+		err = -EINVAL;
 	else if (test_bit(__I40E_RESET_RECOVERY_PENDING, pf->state) ||
 		 test_bit(__I40E_RESET_INTR_RECEIVED, pf->state))
-		errno = -EBUSY;
+		err = -EBUSY;
 	else
-		ret_val = i40e_nvmupd_command(hw, cmd, bytes, &errno);
+		ret_val = i40e_nvmupd_command(hw, cmd, bytes, &err);
 
-	if ((errno || ret_val) && (hw->debug_mask & I40E_DEBUG_NVM))
+	if ((err || ret_val) && (hw->debug_mask & I40E_DEBUG_NVM))
 		dev_info(&pf->pdev->dev,
 			 "NVMUpdate write failed err=%d status=0x%x errno=%d module=%d offset=0x%x size=%d\n",
-			 ret_val, hw->aq.asq_last_status, errno,
+			 ret_val, hw->aq.asq_last_status, err,
 			 (u8)(cmd->config & I40E_NVM_MOD_PNT_MASK),
 			 cmd->offset, cmd->data_size);
 
-	return errno;
+	return err;
 }
 
 static void i40e_get_drvinfo(struct net_device *netdev,
@@ -1899,9 +2232,11 @@
 		sizeof(drvinfo->fw_version));
 	strlcpy(drvinfo->bus_info, pci_name(pf->pdev),
 		sizeof(drvinfo->bus_info));
+#ifdef HAVE_ETHTOOL_GET_SSET_COUNT
 	drvinfo->n_priv_flags = I40E_PRIV_FLAGS_STR_LEN;
 	if (pf->hw.pf_id == 0)
 		drvinfo->n_priv_flags += I40E_GL_PRIV_FLAGS_STR_LEN;
+#endif
 }
 
 static void i40e_get_ringparam(struct net_device *netdev,
@@ -1967,13 +2302,6 @@
 	    (new_rx_count == vsi->rx_rings[0]->count))
 		return 0;
 
-	/* If there is a AF_XDP UMEM attached to any of Rx rings,
-	 * disallow changing the number of descriptors -- regardless
-	 * if the netdev is running or not.
-	 */
-	if (i40e_xsk_any_rx_ring_enabled(vsi))
-		return -EBUSY;
-
 	while (test_and_set_bit(__I40E_CONFIG_BUSY, pf->state)) {
 		timeout--;
 		if (!timeout)
@@ -2004,7 +2332,7 @@
 			       (i40e_enabled_xdp_vsi(vsi) ? 2 : 1);
 	if (new_tx_count != vsi->tx_rings[0]->count) {
 		netdev_info(netdev,
-			    "Changing Tx descriptor count from %d to %d.\n",
+			    "Changing Tx descriptor count from %d to %d\n",
 			    vsi->tx_rings[0]->count, new_tx_count);
 		tx_rings = kcalloc(tx_alloc_queue_pairs,
 				   sizeof(struct i40e_ring), GFP_KERNEL);
@@ -2016,7 +2344,7 @@
 		for (i = 0; i < tx_alloc_queue_pairs; i++) {
 			if (!i40e_active_tx_ring_index(vsi, i))
 				continue;
-
+			/* clone ring and setup updated count */
 			tx_rings[i] = *vsi->tx_rings[i];
 			tx_rings[i].count = new_tx_count;
 			/* the desc and bi pointers will be reallocated in the
@@ -2063,8 +2391,11 @@
 			 */
 			rx_rings[i].desc = NULL;
 			rx_rings[i].rx_bi = NULL;
+#ifdef HAVE_XDP_BUFF_RXQ
 			/* Clear cloned XDP RX-queue info before setup call */
-			memset(&rx_rings[i].xdp_rxq, 0, sizeof(rx_rings[i].xdp_rxq));
+			memset(&rx_rings[i].xdp_rxq, 0,
+			       sizeof(rx_rings[i].xdp_rxq));
+#endif
 			/* this is to allow wr32 to have something to write to
 			 * during early allocation of Rx buffers
 			 */
@@ -2188,11 +2519,27 @@
 	 * works because the num_tx_queues is set at device creation and never
 	 * changes.
 	 */
-	stats_len += I40E_QUEUE_STATS_LEN * 2 * netdev->num_tx_queues;
+#ifndef I40E_PF_EXTRA_STATS_OFF
+	/* The same applies to additional stats showing here the network usage
+	 * counters for VFs. In order to handle it in a safe way, we also
+	 * report here, similarly as in the queues case described above,
+	 * the maximum possible, fixed number of these extra stats items.
+	 */
+#endif /* !I40E_PF_EXTRA_STATS_OFF */
+	stats_len += I40E_QUEUE_STATS_LEN * 2 * netdev->real_num_tx_queues;
+#ifdef HAVE_XDP_SUPPORT
+	stats_len += I40E_QUEUE_STATS_XDP_LEN * netdev->real_num_tx_queues;
+#endif
 
+#ifndef I40E_PF_EXTRA_STATS_OFF
+	if (vsi == pf->vsi[pf->lan_vsi] && pf->hw.partition_id == 1)
+		stats_len += I40E_PF_STATS_EXTRA_LEN;
+
+#endif /* !I40E_PF_EXTRA_STATS_OFF */
 	return stats_len;
 }
 
+#ifdef HAVE_ETHTOOL_GET_SSET_COUNT
 static int i40e_get_sset_count(struct net_device *netdev, int sset)
 {
 	struct i40e_netdev_priv *np = netdev_priv(netdev);
@@ -2211,6 +2558,29 @@
 		return -EOPNOTSUPP;
 	}
 }
+#endif /* HAVE_ETHTOOL_GET_SSET_COUNT */
+
+/**
+ * i40e_get_veb_tc_stats - copy VEB TC statistics to formatted structure
+ * @tc: the TC statistics in VEB structure (veb->tc_stats)
+ * @i: the index of traffic class in (veb->tc_stats) structure to copy
+ *
+ * Copy VEB TC statistics from structure of arrays (veb->tc_stats) to
+ * one dimensional structure i40e_cp_veb_tc_stats.
+ * Produce o formatted i40e_cp_veb_tc_stats structure of the VEB TC
+ * statistics for the given TC.
+ **/
+static inline struct i40e_cp_veb_tc_stats
+i40e_get_veb_tc_stats(struct i40e_veb_tc_stats *tc, unsigned int i)
+{
+	struct i40e_cp_veb_tc_stats veb_tc = {
+		.tc_rx_packets = tc->tc_rx_packets[i],
+		.tc_rx_bytes = tc->tc_rx_bytes[i],
+		.tc_tx_packets = tc->tc_tx_packets[i],
+		.tc_tx_bytes = tc->tc_tx_bytes[i],
+	};
+	return veb_tc;
+}
 
 /**
  * i40e_get_pfc_stats - copy HW PFC statistics to formatted structure
@@ -2258,6 +2628,12 @@
 	struct i40e_vsi *vsi = np->vsi;
 	struct i40e_pf *pf = vsi->back;
 	struct i40e_veb *veb = NULL;
+#ifndef I40E_PF_EXTRA_STATS_OFF
+	unsigned int vsi_idx;
+	unsigned int vf_idx;
+	unsigned int vf_id;
+	bool is_vf_valid;
+#endif /* !I40E_PF_EXTRA_STATS_OFF */
 	unsigned int i;
 	bool veb_stats;
 	u64 *p = data;
@@ -2270,9 +2646,12 @@
 	i40e_add_ethtool_stats(&data, vsi, i40e_gstrings_misc_stats);
 
 	rcu_read_lock();
-	for (i = 0; i < netdev->num_tx_queues; i++) {
+	for (i = 0; i < netdev->real_num_tx_queues; i++) {
 		i40e_add_queue_stats(&data, READ_ONCE(vsi->tx_rings[i]));
 		i40e_add_queue_stats(&data, READ_ONCE(vsi->rx_rings[i]));
+#ifdef HAVE_XDP_SUPPORT
+		i40e_add_rx_queue_xdp_stats(&data, READ_ONCE(vsi->rx_rings[i]));
+#endif
 	}
 	rcu_read_unlock();
 
@@ -2296,8 +2675,15 @@
 			       i40e_gstrings_veb_stats);
 
 	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++)
-		i40e_add_ethtool_stats(&data, veb_stats ? veb : NULL,
-				       i40e_gstrings_veb_tc_stats);
+		if (veb_stats) {
+			struct i40e_cp_veb_tc_stats veb_tc =
+				i40e_get_veb_tc_stats(&veb->tc_stats, i);
+			i40e_add_ethtool_stats(&data, &veb_tc,
+					       i40e_gstrings_veb_tc_stats);
+		} else {
+			i40e_add_ethtool_stats(&data, NULL,
+					       i40e_gstrings_veb_tc_stats);
+		}
 
 	i40e_add_ethtool_stats(&data, pf, i40e_gstrings_stats);
 
@@ -2307,11 +2693,94 @@
 		i40e_add_ethtool_stats(&data, &pfc, i40e_gstrings_pfc_stats);
 	}
 
+#ifndef I40E_PF_EXTRA_STATS_OFF
+	/* As for now, we only process the SRIOV type VSIs (as extra stats to
+	 * PF core stats) which are correlated with VF LAN VSI (hence below,
+	 * in this for-loop instruction block, only VF's LAN VSIs are currently
+	 * processed).
+	 */
+	for (vf_id = 0; vf_id < pf->num_alloc_vfs; vf_id++) {
+		is_vf_valid = true;
+		for (vf_idx = 0; vf_idx < pf->num_alloc_vfs; vf_idx++)
+			if (pf->vf[vf_idx].vf_id == vf_id)
+				break;
+		if (vf_idx >= pf->num_alloc_vfs) {
+			dev_info(&pf->pdev->dev,
+				 "In the PF's array, there is no VF instance with VF_ID identifier %d or it is not set/initialized correctly yet\n",
+				 vf_id);
+			is_vf_valid = false;
+			goto check_vf;
+		}
+		vsi_idx = pf->vf[vf_idx].lan_vsi_idx;
+
+		vsi = pf->vsi[vsi_idx];
+		if (!vsi) {
+			/* It means empty field in the PF VSI array... */
+			dev_info(&pf->pdev->dev,
+				 "No LAN VSI instance referenced by VF %d or it is not set/initialized correctly yet\n",
+				 vf_id);
+			is_vf_valid = false;
+			goto check_vf;
+		}
+		if (vsi->vf_id != vf_id) {
+			dev_info(&pf->pdev->dev,
+				 "In the PF's array, there is incorrectly set/initialized LAN VSI or reference to it from VF %d is not set/initialized correctly yet\n",
+				 vf_id);
+			is_vf_valid = false;
+			goto check_vf;
+		}
+		if (vsi->vf_id != pf->vf[vf_idx].vf_id ||
+		    !i40e_find_vsi_from_id(pf, pf->vf[vsi->vf_id].lan_vsi_id)) {
+			/* Disjointed identifiers or broken references VF-VSI */
+			dev_warn(&pf->pdev->dev,
+				 "SRIOV LAN VSI (index %d in PF VSI array) with invalid VF Identifier %d (referenced by VF %d, ordered as %d in VF array)\n",
+				 vsi_idx, pf->vsi[vsi_idx]->vf_id,
+				 pf->vf[vf_idx].vf_id, vf_idx);
+			is_vf_valid = false;
+		}
+check_vf:
+		if (!is_vf_valid) {
+			i40e_add_ethtool_stats(&data, NULL,
+					       i40e_gstrings_eth_stats_extra);
+		} else {
+			i40e_update_eth_stats(vsi);
+			i40e_add_ethtool_stats(&data, vsi,
+					       i40e_gstrings_eth_stats_extra);
+		}
+	}
+	for (; vf_id < I40E_STATS_EXTRA_COUNT; vf_id++)
+		i40e_add_ethtool_stats(&data, NULL,
+				       i40e_gstrings_eth_stats_extra);
+
+#endif /* !I40E_PF_EXTRA_STATS_OFF */
 check_data_pointer:
 	WARN_ONCE(data - p != i40e_get_stats_count(netdev),
 		  "ethtool stats count mismatch!");
 }
 
+#ifndef I40E_PF_EXTRA_STATS_OFF
+/**
+ * i40e_update_vfid_in_stats - print VF num to stats names
+ * @stats_extra: array of stats structs with stats name strings
+ * @strings_num: number of stats name strings in array above (length)
+ * @vf_id: VF number to update stats name strings with
+ *
+ * Helper function to i40e_get_stat_strings() in case of extra stats.
+ **/
+static inline void
+i40e_update_vfid_in_stats(struct i40e_stats stats_extra[],
+			  int strings_num, int vf_id)
+{
+	int i;
+
+	for (i = 0; i < strings_num; i++) {
+		snprintf(stats_extra[i].stat_string,
+			 I40E_STATS_NAME_VFID_EXTRA_LEN, "vf%03d", vf_id);
+		stats_extra[i].stat_string[I40E_STATS_NAME_VFID_EXTRA_LEN -
+								       1] = '.';
+	}
+}
+#endif /* !I40E_PF_EXTRA_STATS_OFF */
 /**
  * i40e_get_stat_strings - copy stat strings into supplied buffer
  * @netdev: the netdev to collect strings for
@@ -2334,11 +2803,15 @@
 
 	i40e_add_stat_strings(&data, i40e_gstrings_misc_stats);
 
-	for (i = 0; i < netdev->num_tx_queues; i++) {
+	for (i = 0; i < netdev->real_num_tx_queues; i++) {
 		i40e_add_stat_strings(&data, i40e_gstrings_queue_stats,
 				      "tx", i);
 		i40e_add_stat_strings(&data, i40e_gstrings_queue_stats,
 				      "rx", i);
+#ifdef HAVE_XDP_SUPPORT
+		i40e_add_stat_strings(&data, i40e_gstrings_rx_queue_xdp_stats,
+				      "rx", i);
+#endif
 	}
 
 	if (vsi != pf->vsi[pf->lan_vsi] || pf->hw.partition_id != 1)
@@ -2354,11 +2827,21 @@
 	for (i = 0; i < I40E_MAX_USER_PRIORITY; i++)
 		i40e_add_stat_strings(&data, i40e_gstrings_pfc_stats, i);
 
+#ifndef I40E_PF_EXTRA_STATS_OFF
+	for (i = 0; i < I40E_STATS_EXTRA_COUNT; i++) {
+		i40e_update_vfid_in_stats
+			(i40e_gstrings_eth_stats_extra,
+			 ARRAY_SIZE(i40e_gstrings_eth_stats_extra), i);
+		i40e_add_stat_strings(&data, i40e_gstrings_eth_stats_extra);
+	}
+
+#endif /* !I40E_PF_EXTRA_STATS_OFF */
 check_data_pointer:
 	WARN_ONCE(data - p != i40e_get_stats_count(netdev) * ETH_GSTRING_LEN,
 		  "stat strings count mismatch!");
 }
 
+#ifdef HAVE_ETHTOOL_GET_SSET_COUNT
 static void i40e_get_priv_flag_strings(struct net_device *netdev, u8 *data)
 {
 	struct i40e_netdev_priv *np = netdev_priv(netdev);
@@ -2380,6 +2863,7 @@
 		p += ETH_GSTRING_LEN;
 	}
 }
+#endif
 
 static void i40e_get_strings(struct net_device *netdev, u32 stringset,
 			     u8 *data)
@@ -2392,17 +2876,21 @@
 	case ETH_SS_STATS:
 		i40e_get_stat_strings(netdev, data);
 		break;
+#ifdef HAVE_ETHTOOL_GET_SSET_COUNT
 	case ETH_SS_PRIV_FLAGS:
 		i40e_get_priv_flag_strings(netdev, data);
 		break;
+#endif
 	default:
 		break;
 	}
 }
 
+#ifdef HAVE_ETHTOOL_GET_TS_INFO
 static int i40e_get_ts_info(struct net_device *dev,
 			    struct ethtool_ts_info *info)
 {
+#ifdef HAVE_PTP_1588_CLOCK
 	struct i40e_pf *pf = i40e_netdev_to_pf(dev);
 
 	/* only report HW timestamping if PTP is enabled */
@@ -2439,8 +2927,12 @@
 				    BIT(HWTSTAMP_FILTER_PTP_V2_L4_DELAY_REQ);
 
 	return 0;
+#else /* HAVE_PTP_1588_CLOCK */
+	return ethtool_op_get_ts_info(dev, info);
+#endif /* HAVE_PTP_1588_CLOCK */
 }
 
+#endif /* HAVE_ETHTOOL_GET_TS_INFO */
 static u64 i40e_link_test(struct net_device *netdev, u64 *data)
 {
 	struct i40e_netdev_priv *np = netdev_priv(netdev);
@@ -2450,7 +2942,7 @@
 
 	netif_info(pf, hw, netdev, "link test\n");
 	status = i40e_get_link_status(&pf->hw, &link_up);
-	if (status) {
+	if (status != I40E_SUCCESS) {
 		netif_err(pf, drv, netdev, "link query timed out, please retry test\n");
 		*data = 1;
 		return *data;
@@ -2508,6 +3000,13 @@
 	return *data;
 }
 
+#ifndef HAVE_ETHTOOL_GET_SSET_COUNT
+static int i40e_diag_test_count(struct net_device *netdev)
+{
+	return I40E_TEST_LEN;
+}
+
+#endif /* HAVE_ETHTOOL_GET_SSET_COUNT */
 static inline bool i40e_active_vfs(struct i40e_pf *pf)
 {
 	struct i40e_vf *vfs = pf->vf;
@@ -2644,7 +3143,7 @@
 		return -EOPNOTSUPP;
 
 	/* only magic packet is supported */
-	if (wol->wolopts & ~WAKE_MAGIC)
+	if (wol->wolopts && (wol->wolopts != WAKE_MAGIC))
 		return -EOPNOTSUPP;
 
 	/* is this a new value? */
@@ -2656,11 +3155,12 @@
 	return 0;
 }
 
+#ifdef HAVE_ETHTOOL_SET_PHYS_ID
 static int i40e_set_phys_id(struct net_device *netdev,
 			    enum ethtool_phys_id_state state)
 {
 	struct i40e_netdev_priv *np = netdev_priv(netdev);
-	i40e_status ret = 0;
+	i40e_status ret = I40E_SUCCESS;
 	struct i40e_pf *pf = np->vsi->back;
 	struct i40e_hw *hw = &pf->hw;
 	int blink_freq = 2;
@@ -2705,11 +3205,58 @@
 	default:
 		break;
 	}
+		if (ret)
+			return -ENOENT;
+		else
+			return 0;
+}
+#else /* HAVE_ETHTOOL_SET_PHYS_ID */
+static int i40e_phys_id(struct net_device *netdev, u32 data)
+{
+	struct i40e_netdev_priv *np = netdev_priv(netdev);
+	struct i40e_pf *pf = np->vsi->back;
+	struct i40e_hw *hw = &pf->hw;
+	i40e_status ret = I40E_SUCCESS;
+	u16 temp_status;
+	int i;
+
+	if (!(pf->hw_features & I40E_HW_PHY_CONTROLS_LEDS)) {
+		pf->led_status = i40e_led_get(hw);
+	} else {
+		ret = i40e_led_get_phy(hw, &temp_status,
+				       &pf->phy_led_val);
+		pf->led_status = temp_status;
+	}
+
+	if (!data || data > 300)
+		data = 300;
+
+	/* 10GBaseT PHY controls led's through PHY, not MAC */
+	for (i = 0; i < (data * 1000); i += 400) {
+		if (!(pf->hw_features & I40E_HW_PHY_CONTROLS_LEDS))
+			i40e_led_set(hw, 0xF, false);
+		else
+			ret = i40e_led_set_phy(hw, true, pf->led_status, 0);
+		msleep_interruptible(200);
+		if (!(pf->hw_features & I40E_HW_PHY_CONTROLS_LEDS))
+			i40e_led_set(hw, 0x0, false);
+		else
+			ret = i40e_led_set_phy(hw, false, pf->led_status, 0);
+		msleep_interruptible(200);
+	}
+	if (!(pf->hw_features & I40E_HW_PHY_CONTROLS_LEDS))
+		i40e_led_set(hw, pf->led_status, false);
+	else
+		ret = i40e_led_set_phy(hw, false, pf->led_status,
+				       (pf->led_status |
+				       I40E_PHY_LED_MODE_ORIG));
+
 	if (ret)
 		return -ENOENT;
 	else
 		return 0;
 }
+#endif /* HAVE_ETHTOOL_SET_PHYS_ID */
 
 /* NOTE: i40e hardware uses a conversion factor of 2 for Interrupt
  * Throttle Rate (ITR) ie. ITR(1) = 2us ITR(10) = 20 us, and also
@@ -2784,6 +3331,7 @@
 	return __i40e_get_coalesce(netdev, ec, -1);
 }
 
+#ifdef ETHTOOL_PERQUEUE
 /**
  * i40e_get_per_queue_coalesce - gets coalesce settings for particular queue
  * @netdev: netdev structure
@@ -2798,6 +3346,7 @@
 	return __i40e_get_coalesce(netdev, ec, queue);
 }
 
+#endif /* ETHTOOL_PERQUEUE */
 /**
  * i40e_set_itr_per_queue - set ITR values for specific queue
  * @vsi: the VSI to set values for
@@ -2848,6 +3397,52 @@
 }
 
 /**
+ * i40e_is_coalesce_param_invalid - check for unsupported coalesce parameters
+ * @netdev: pointer to the netdev associated with this query
+ * @ec: ethtool structure to fill with driver's coalesce settings
+ *
+ * Print netdev info if driver doesn't support one of the parameters
+ * and return error. When any parameters will be implemented, remove only
+ * this parameter from param array.
+ */
+static
+int i40e_is_coalesce_param_invalid(struct net_device *netdev,
+				   struct ethtool_coalesce *ec)
+{
+	struct i40e_ethtool_not_used {
+		u32 value;
+		const char *name;
+	} param[] = {
+		{ec->stats_block_coalesce_usecs, "stats-block-usecs"},
+		{ec->rate_sample_interval, "sample-interval"},
+		{ec->pkt_rate_low, "pkt-rate-low"},
+		{ec->pkt_rate_high, "pkt-rate-high"},
+		{ec->rx_max_coalesced_frames, "rx-frames"},
+		{ec->rx_coalesce_usecs_irq, "rx-usecs-irq"},
+		{ec->tx_max_coalesced_frames, "tx-frames"},
+		{ec->tx_coalesce_usecs_irq, "tx-usecs-irq"},
+		{ec->rx_coalesce_usecs_low, "rx-usecs-low"},
+		{ec->rx_max_coalesced_frames_low, "rx-frames-low"},
+		{ec->tx_coalesce_usecs_low, "tx-usecs-low"},
+		{ec->tx_max_coalesced_frames_low, "tx-frames-low"},
+		{ec->rx_max_coalesced_frames_high, "rx-frames-high"},
+		{ec->tx_max_coalesced_frames_high, "tx-frames-high"}
+	};
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(param); i++) {
+		if (param[i].value) {
+			netdev_info(netdev,
+				    "Setting %s not supported\n",
+				    param[i].name);
+			return -EOPNOTSUPP;
+		}
+	}
+
+	return 0;
+}
+
+/**
  * __i40e_set_coalesce - set coalesce settings for particular queue
  * @netdev: the netdev to change
  * @ec: ethtool coalesce settings
@@ -2865,6 +3460,9 @@
 	struct i40e_pf *pf = vsi->back;
 	int i;
 
+	if (i40e_is_coalesce_param_invalid(netdev, ec))
+		return -EOPNOTSUPP;
+
 	if (ec->tx_max_coalesced_frames_irq || ec->rx_max_coalesced_frames_irq)
 		vsi->work_limit = ec->tx_max_coalesced_frames_irq;
 
@@ -2956,6 +3554,20 @@
 	return __i40e_set_coalesce(netdev, ec, -1);
 }
 
+#ifdef ETHTOOL_SRXNTUPLE
+/* We need to keep this around for kernels 2.6.33 - 2.6.39 in order to avoid
+ * a null pointer dereference as it was assumend if the NETIF_F_NTUPLE flag
+ * was defined that this function was present.
+ */
+static int i40e_set_rx_ntuple(struct net_device *dev,
+			      struct ethtool_rx_ntuple *cmd)
+{
+	return -EOPNOTSUPP;
+}
+
+#endif /* ETHTOOL_SRXNTUPLE */
+
+#ifdef ETHTOOL_PERQUEUE
 /**
  * i40e_set_per_queue_coalesce - set specific queue's coalesce settings
  * @netdev: the netdev to change
@@ -2969,7 +3581,9 @@
 {
 	return __i40e_set_coalesce(netdev, ec, queue);
 }
+#endif /* ETHTOOL_PERQUEUE */
 
+#ifdef ETHTOOL_GRXRINGS
 /**
  * i40e_get_rss_hash_opts - Get RSS hash Input Set for each flow type
  * @pf: pointer to the physical function struct
@@ -3068,8 +3682,44 @@
 		return -1;
 }
 
+#define I40E_CUSTOM_CF_DDP_PACKAGE 0x8000000e
+
+/**
+ * i40e_check_custom_cf_package - Check presence of DDP package
+ * @pf: pointer to physical function struct
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_check_custom_cf_package(struct i40e_pf *pf)
+{
+	struct i40e_ddp_profile_list *profile_list = NULL;
+	struct i40e_profile_info *profile_info = NULL;
+	u8 buf[I40E_PROFILE_LIST_SIZE];
+	int i = 0, aq_ret;
+
+	aq_ret = i40e_aq_get_ddp_list(&pf->hw, buf, I40E_PROFILE_LIST_SIZE, 0,
+				      NULL);
+	if (aq_ret != I40E_SUCCESS) {
+		dev_info(&pf->pdev->dev,
+			 "Failed to query loaded DDP packages, err %s aq_err %s\n",
+			 i40e_stat_str(&pf->hw, aq_ret),
+			 i40e_aq_str(&pf->hw, pf->hw.aq.asq_last_status));
+		goto err;
+	}
+
+	profile_list = (struct i40e_ddp_profile_list *)buf;
+	for (i = 0; i < profile_list->p_count; i++) {
+		profile_info = &profile_list->p_info[i];
+		if (profile_info->track_id == I40E_CUSTOM_CF_DDP_PACKAGE)
+			return 0;
+	}
+err:
+	return -ENOENT;
+}
+
 /**
  * i40e_parse_rx_flow_user_data - Deconstruct user-defined data
+ * @pf: pointer to physical function struct
  * @fsp: pointer to rx flow specification
  * @data: pointer to userdef data structure for storage
  *
@@ -3087,7 +3737,8 @@
  * modified even if FLOW_EXT is not set.
  *
  **/
-static int i40e_parse_rx_flow_user_data(struct ethtool_rx_flow_spec *fsp,
+static int i40e_parse_rx_flow_user_data(struct i40e_pf *pf,
+					struct ethtool_rx_flow_spec *fsp,
 					struct i40e_rx_flow_userdef *data)
 {
 	u64 value, mask;
@@ -3099,21 +3750,88 @@
 	if (!(fsp->flow_type & FLOW_EXT))
 		return 0;
 
-	value = be64_to_cpu(*((__be64 *)fsp->h_ext.data));
-	mask = be64_to_cpu(*((__be64 *)fsp->m_ext.data));
+	value = be64_to_cpu(*((__force __be64 *)fsp->h_ext.data));
+	mask = be64_to_cpu(*((__force __be64 *)fsp->m_ext.data));
 
-#define I40E_USERDEF_FLEX_WORD		GENMASK_ULL(15, 0)
-#define I40E_USERDEF_FLEX_OFFSET	GENMASK_ULL(31, 16)
+#define I40E_USERDEF_CLOUD_FILTER	BIT_ULL(63)
+#define I40E_USERDEF_CLOUD_OUTERIP	BIT_ULL(62)
+
+#define I40E_USERDEF_CLOUD_RESERVED	GENMASK_ULL(62, 32)
+#define I40E_USERDEF_TUNNEL_TYPE	GENMASK_ULL(31, 24)
+#define I40E_USERDEF_TENANT_ID		GENMASK_ULL(23, 0)
+
+#define I40E_USERDEF_RESERVED		GENMASK_ULL(62, 32)
 #define I40E_USERDEF_FLEX_FILTER	GENMASK_ULL(31, 0)
 
-	valid = i40e_check_mask(mask, I40E_USERDEF_FLEX_FILTER);
-	if (valid < 0) {
-		return -EINVAL;
-	} else if (valid) {
-		data->flex_word = value & I40E_USERDEF_FLEX_WORD;
-		data->flex_offset =
-			(value & I40E_USERDEF_FLEX_OFFSET) >> 16;
-		data->flex_filter = true;
+#define I40E_USERDEF_FLEX_OFFSET	GENMASK_ULL(31, 16)
+#define I40E_USERDEF_FLEX_WORD		GENMASK_ULL(15, 0)
+
+	if ((mask & I40E_USERDEF_CLOUD_FILTER) &&
+	    (value & I40E_USERDEF_CLOUD_FILTER))
+		data->cloud_filter = true;
+
+	if (data->cloud_filter) {
+		/* Reject OUTER IP cloud filters if bit 62 is set and
+		 * DDP package 0x80000000e is not loaded
+		 */
+		if ((mask & I40E_USERDEF_CLOUD_OUTERIP) &&
+		    (value & I40E_USERDEF_CLOUD_OUTERIP)) {
+			if (!i40e_check_custom_cf_package(pf)) {
+				data->outer_ip = true;
+				return 0;
+			} else {
+				dev_warn(&pf->pdev->dev,
+					 "Load the appropriate DDP package for Outer IP cloud filters\n");
+				return -ENOPKG;
+			}
+		}
+
+		/* Make sure that the reserved bits are not set */
+		valid = i40e_check_mask(mask, I40E_USERDEF_CLOUD_RESERVED);
+		if (valid < 0) {
+			return -EINVAL;
+		} else if (valid) {
+			if ((value & I40E_USERDEF_CLOUD_RESERVED) != 0)
+				return -EINVAL;
+		}
+
+		/* These fields are only valid if this is a cloud filter */
+		valid = i40e_check_mask(mask, I40E_USERDEF_TENANT_ID);
+		if (valid < 0) {
+			return -EINVAL;
+		} else if (valid) {
+			data->tenant_id = value & I40E_USERDEF_TENANT_ID;
+			data->tenant_id_valid = true;
+		}
+
+		valid = i40e_check_mask(mask, I40E_USERDEF_TUNNEL_TYPE);
+		if (valid < 0) {
+			return -EINVAL;
+		} else if (valid) {
+			data->tunnel_type =
+				(value & I40E_USERDEF_TUNNEL_TYPE) >> 24;
+			data->tunnel_type_valid = true;
+		}
+	} else {
+		/* Make sure that the reserved bits are not set */
+		valid = i40e_check_mask(mask, I40E_USERDEF_RESERVED);
+		if (valid < 0) {
+			return -EINVAL;
+		} else if (valid) {
+			if ((value & I40E_USERDEF_RESERVED) != 0)
+				return -EINVAL;
+		}
+
+		/* These fields are only valid if this isn't a cloud filter */
+		valid = i40e_check_mask(mask, I40E_USERDEF_FLEX_FILTER);
+		if (valid < 0) {
+			return -EINVAL;
+		} else if (valid) {
+			data->flex_word = value & I40E_USERDEF_FLEX_WORD;
+			data->flex_offset =
+				(value & I40E_USERDEF_FLEX_OFFSET) >> 16;
+			data->flex_filter = true;
+		}
 	}
 
 	return 0;
@@ -3132,21 +3850,41 @@
 {
 	u64 value = 0, mask = 0;
 
-	if (data->flex_filter) {
-		value |= data->flex_word;
-		value |= (u64)data->flex_offset << 16;
-		mask |= I40E_USERDEF_FLEX_FILTER;
+	if (data->cloud_filter) {
+		value |= I40E_USERDEF_CLOUD_FILTER;
+		mask |= I40E_USERDEF_CLOUD_FILTER;
+
+		if (data->tenant_id_valid) {
+			value |= data->tenant_id;
+			mask |= I40E_USERDEF_TENANT_ID;
+		}
+
+		if (data->tunnel_type_valid) {
+			value |= (u64)data->tunnel_type << 24;
+			mask |= I40E_USERDEF_TUNNEL_TYPE;
+		}
+
+		if (data->outer_ip) {
+			value |= I40E_USERDEF_CLOUD_OUTERIP;
+			mask |= I40E_USERDEF_CLOUD_OUTERIP;
+		}
+	} else {
+		if (data->flex_filter) {
+			value |= data->flex_word;
+			value |= (u64)data->flex_offset << 16;
+			mask |= I40E_USERDEF_FLEX_FILTER;
+		}
 	}
 
 	if (value || mask)
 		fsp->flow_type |= FLOW_EXT;
 
-	*((__be64 *)fsp->h_ext.data) = cpu_to_be64(value);
-	*((__be64 *)fsp->m_ext.data) = cpu_to_be64(mask);
+	*((__force __be64 *)fsp->h_ext.data) = cpu_to_be64(value);
+	*((__force __be64 *)fsp->m_ext.data) = cpu_to_be64(mask);
 }
 
 /**
- * i40e_get_ethtool_fdir_all - Populates the rule count of a command
+ * i40e_get_rx_filter_ids - Populates the rule count of a command
  * @pf: Pointer to the physical function struct
  * @cmd: The command to get or set Rx flow classification rules
  * @rule_locs: Array of used rule locations
@@ -3156,23 +3894,34 @@
  *
  * Returns 0 on success or -EMSGSIZE if entry not found
  **/
-static int i40e_get_ethtool_fdir_all(struct i40e_pf *pf,
-				     struct ethtool_rxnfc *cmd,
-				     u32 *rule_locs)
+static int i40e_get_rx_filter_ids(struct i40e_pf *pf,
+				  struct ethtool_rxnfc *cmd,
+				  u32 *rule_locs)
 {
-	struct i40e_fdir_filter *rule;
+	struct i40e_fdir_filter *f_rule;
+	struct i40e_cloud_filter *c_rule;
 	struct hlist_node *node2;
-	int cnt = 0;
+	unsigned int cnt = 0;
 
 	/* report total rule count */
 	cmd->data = i40e_get_fd_cnt_all(pf);
 
-	hlist_for_each_entry_safe(rule, node2,
+	hlist_for_each_entry_safe(f_rule, node2,
 				  &pf->fdir_filter_list, fdir_node) {
 		if (cnt == cmd->rule_cnt)
 			return -EMSGSIZE;
 
-		rule_locs[cnt] = rule->fd_id;
+		rule_locs[cnt] = f_rule->fd_id;
+		cnt++;
+	}
+
+	/* find the cloud filter rule ids */
+	hlist_for_each_entry_safe(c_rule, node2,
+				  &pf->cloud_filter_list, cloud_node) {
+		if (cnt == cmd->rule_cnt)
+			return -EMSGSIZE;
+
+		rule_locs[cnt] = c_rule->id;
 		cnt++;
 	}
 
@@ -3217,14 +3966,41 @@
 		fsp->h_u.usr_ip4_spec.proto = 0;
 		fsp->m_u.usr_ip4_spec.proto = 0;
 	}
-
-	/* Reverse the src and dest notion, since the HW views them from
-	 * Tx perspective where as the user expects it from Rx filter view.
+#ifdef HAVE_ETHTOOL_FLOW_UNION_IP6_SPEC
+	if (fsp->flow_type == IPV6_USER_FLOW ||
+	    fsp->flow_type == UDP_V6_FLOW ||
+	    fsp->flow_type == TCP_V6_FLOW ||
+	    fsp->flow_type == SCTP_V6_FLOW) {
+		/* Reverse the src and dest notion, since the HW views them
+		 * from Tx perspective where as the user expects it from
+		 * Rx filter view.
+		 */
+		fsp->h_u.tcp_ip6_spec.psrc = rule->dst_port;
+		fsp->h_u.tcp_ip6_spec.pdst = rule->src_port;
+		memcpy(fsp->h_u.tcp_ip6_spec.ip6dst, rule->src_ip6,
+		       sizeof(__be32) * 4);
+		memcpy(fsp->h_u.tcp_ip6_spec.ip6src, rule->dst_ip6,
+		       sizeof(__be32) * 4);
+	} else {
+		/* Reverse the src and dest notion, since the HW views them
+		 * from Tx perspective where as the user expects it from
+		 * Rx filter view.
+		 */
+		fsp->h_u.tcp_ip4_spec.psrc = rule->dst_port;
+		fsp->h_u.tcp_ip4_spec.pdst = rule->src_port;
+		fsp->h_u.tcp_ip4_spec.ip4src = rule->dst_ip;
+		fsp->h_u.tcp_ip4_spec.ip4dst = rule->src_ip;
+	}
+#else /* !HAVE_ETHTOOL_FLOW_UNION_IP6_SPEC */
+	/* Reverse the src and dest notion, since the HW views them
+	 * from Tx perspective where as the user expects it from
+	 * Rx filter view.
 	 */
 	fsp->h_u.tcp_ip4_spec.psrc = rule->dst_port;
 	fsp->h_u.tcp_ip4_spec.pdst = rule->src_port;
 	fsp->h_u.tcp_ip4_spec.ip4src = rule->dst_ip;
 	fsp->h_u.tcp_ip4_spec.ip4dst = rule->src_ip;
+#endif /* HAVE_ETHTOOL_FLOW_UNION_IP6_SPEC */
 
 	switch (rule->flow_type) {
 	case SCTP_V4_FLOW:
@@ -3236,9 +4012,24 @@
 	case UDP_V4_FLOW:
 		index = I40E_FILTER_PCTYPE_NONF_IPV4_UDP;
 		break;
+	case SCTP_V6_FLOW:
+		index = I40E_FILTER_PCTYPE_NONF_IPV6_SCTP;
+		break;
+	case TCP_V6_FLOW:
+		index = I40E_FILTER_PCTYPE_NONF_IPV6_TCP;
+		break;
+	case UDP_V6_FLOW:
+		index = I40E_FILTER_PCTYPE_NONF_IPV6_UDP;
+		break;
+
 	case IP_USER_FLOW:
 		index = I40E_FILTER_PCTYPE_NONF_IPV4_OTHER;
 		break;
+#ifdef HAVE_ETHTOOL_FLOW_UNION_IP6_SPEC
+	case IPV6_USER_FLOW:
+		index = I40E_FILTER_PCTYPE_NONF_IPV6_OTHER;
+		break;
+#endif /* HAVE_ETHTOOL_FLOW_UNION_IP6_SPEC */
 	default:
 		/* If we have stored a filter with a flow type not listed here
 		 * it is almost certainly a driver bug. WARN(), and then
@@ -3254,6 +4045,21 @@
 	input_set = i40e_read_fd_input_set(pf, index);
 
 no_input_set:
+#ifdef HAVE_ETHTOOL_FLOW_UNION_IP6_SPEC
+	if (input_set & I40E_L3_V6_SRC_MASK) {
+		fsp->m_u.tcp_ip6_spec.ip6src[0] = htonl(0xFFFFFFFF);
+		fsp->m_u.tcp_ip6_spec.ip6src[1] = htonl(0xFFFFFFFF);
+		fsp->m_u.tcp_ip6_spec.ip6src[2] = htonl(0xFFFFFFFF);
+		fsp->m_u.tcp_ip6_spec.ip6src[3] = htonl(0xFFFFFFFF);
+	}
+
+	if (input_set & I40E_L3_V6_DST_MASK) {
+		fsp->m_u.tcp_ip6_spec.ip6dst[0] = htonl(0xFFFFFFFF);
+		fsp->m_u.tcp_ip6_spec.ip6dst[1] = htonl(0xFFFFFFFF);
+		fsp->m_u.tcp_ip6_spec.ip6dst[2] = htonl(0xFFFFFFFF);
+		fsp->m_u.tcp_ip6_spec.ip6dst[3] = htonl(0xFFFFFFFF);
+	}
+#endif /* HAVE_ETHTOOL_FLOW_UNION_IP6_SPEC */
 	if (input_set & I40E_L3_SRC_MASK)
 		fsp->m_u.tcp_ip4_spec.ip4src = htonl(0xFFFFFFFF);
 
@@ -3271,6 +4077,14 @@
 	else
 		fsp->ring_cookie = rule->q_index;
 
+	if (rule->vlan_tag) {
+		fsp->h_ext.vlan_etype = rule->vlan_etype;
+		fsp->m_ext.vlan_etype = htons(0xFFFF);
+		fsp->h_ext.vlan_tci = rule->vlan_tag;
+		fsp->m_ext.vlan_tci = htons(0xFFFF);
+		fsp->flow_type |= FLOW_EXT;
+	}
+
 	if (rule->dest_vsi != pf->vsi[pf->lan_vsi]->id) {
 		struct i40e_vsi *vsi;
 
@@ -3297,6 +4111,108 @@
 	return 0;
 }
 
+#define VXLAN_PORT	8472
+
+/**
+ * i40e_get_cloud_filter_entry - get a cloud filter by loc
+ * @pf: pointer to the physical function struct
+ * @cmd: The command to get or set Rx flow classification rules
+ *
+ * get cloud filter by loc.
+ * Returns 0 if success.
+ **/
+static int i40e_get_cloud_filter_entry(struct i40e_pf *pf,
+				       struct ethtool_rxnfc *cmd)
+{
+	struct ethtool_rx_flow_spec *fsp =
+			(struct ethtool_rx_flow_spec *)&cmd->fs;
+	static const u8 mac_broadcast[] = {0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF};
+	struct i40e_cloud_filter *rule, *filter = NULL;
+	struct i40e_rx_flow_userdef userdef = {0};
+	struct hlist_node *node2;
+
+	hlist_for_each_entry_safe(rule, node2,
+				  &pf->cloud_filter_list, cloud_node) {
+		/* filter found */
+		if (rule->id == fsp->location)
+			filter = rule;
+
+		/* bail out if we've passed the likely location in the list */
+		if (rule->id >= fsp->location)
+			break;
+
+	}
+	if (!filter) {
+		dev_info(&pf->pdev->dev, "No cloud filter with loc %d\n",
+			fsp->location);
+		return -ENOENT;
+	}
+
+	userdef.cloud_filter = true;
+
+	fsp->ring_cookie = filter->queue_id;
+	if (filter->seid != pf->vsi[pf->lan_vsi]->seid) {
+		struct i40e_vsi *vsi;
+
+		vsi = i40e_find_vsi_from_seid(pf, filter->seid);
+		if (vsi && vsi->type == I40E_VSI_SRIOV) {
+			/* VFs are zero-indexed by the driver, but ethtool
+			 * expects them to be one-indexed, so add one here
+			 */
+			u64 ring_vf = vsi->vf_id + 1;
+
+			ring_vf <<= ETHTOOL_RX_FLOW_SPEC_RING_VF_OFF;
+			fsp->ring_cookie |= ring_vf;
+		}
+	}
+
+	ether_addr_copy(fsp->h_u.ether_spec.h_dest, filter->outer_mac);
+	ether_addr_copy(fsp->h_u.ether_spec.h_source, filter->inner_mac);
+
+	if (filter->flags & I40E_CLOUD_FIELD_OMAC)
+		ether_addr_copy(fsp->m_u.ether_spec.h_dest, mac_broadcast);
+	if (filter->flags & I40E_CLOUD_FIELD_IMAC)
+		ether_addr_copy(fsp->m_u.ether_spec.h_source, mac_broadcast);
+	if (filter->flags & I40E_CLOUD_FIELD_IVLAN)
+		fsp->h_ext.vlan_tci = filter->inner_vlan;
+	if (filter->flags & I40E_CLOUD_FIELD_TEN_ID) {
+		userdef.tenant_id_valid = true;
+		userdef.tenant_id = filter->tenant_id;
+	}
+	if (filter->tunnel_type != I40E_CLOUD_TNL_TYPE_NONE) {
+		userdef.tunnel_type_valid = true;
+		userdef.tunnel_type = filter->tunnel_type;
+	}
+
+	if (filter->flags & I40E_CLOUD_FIELD_IIP) {
+		if (i40e_is_l4mode_enabled())  {
+			fsp->flow_type = UDP_V4_FLOW;
+			fsp->h_u.udp_ip4_spec.pdst = filter->dst_port;
+		} else {
+			fsp->flow_type = IP_USER_FLOW;
+		}
+
+		fsp->h_u.usr_ip4_spec.ip4dst = filter->dst_ipv4;
+		fsp->h_u.usr_ip4_spec.ip_ver = ETH_RX_NFC_IP4;
+	} else {
+		fsp->flow_type = ETHER_FLOW;
+	}
+
+	if (filter->flags & I40E_CLOUD_FIELD_OIP1 ||
+	    filter->flags & I40E_CLOUD_FIELD_OIP2) {
+		fsp->flow_type = IP_USER_FLOW;
+		fsp->h_u.usr_ip4_spec.ip_ver = ETH_RX_NFC_IP4;
+		fsp->h_u.usr_ip4_spec.ip4dst = filter->dst_ipv4;
+		userdef.outer_ip = true;
+	}
+
+	i40e_fill_rx_flow_user_data(fsp, &userdef);
+
+	fsp->flow_type |= FLOW_EXT;
+
+	return 0;
+}
+
 /**
  * i40e_get_rxnfc - command to get RX flow classification rules
  * @netdev: network interface device structure
@@ -3306,7 +4222,11 @@
  * Returns Success if the command is supported.
  **/
 static int i40e_get_rxnfc(struct net_device *netdev, struct ethtool_rxnfc *cmd,
+#ifdef HAVE_ETHTOOL_GET_RXNFC_VOID_RULE_LOCS
+			  void *rule_locs)
+#else
 			  u32 *rule_locs)
+#endif
 {
 	struct i40e_netdev_priv *np = netdev_priv(netdev);
 	struct i40e_vsi *vsi = np->vsi;
@@ -3323,15 +4243,23 @@
 		break;
 	case ETHTOOL_GRXCLSRLCNT:
 		cmd->rule_cnt = pf->fdir_pf_active_filters;
+		cmd->rule_cnt += pf->num_cloud_filters;
 		/* report total rule count */
 		cmd->data = i40e_get_fd_cnt_all(pf);
 		ret = 0;
 		break;
 	case ETHTOOL_GRXCLSRULE:
 		ret = i40e_get_ethtool_fdir_entry(pf, cmd);
+		/* if no such fdir filter then try the cloud list */
+		if (ret)
+			ret = i40e_get_cloud_filter_entry(pf, cmd);
 		break;
 	case ETHTOOL_GRXCLSRLALL:
-		ret = i40e_get_ethtool_fdir_all(pf, cmd, rule_locs);
+#ifdef HAVE_ETHTOOL_GET_RXNFC_VOID_RULE_LOCS
+		ret = i40e_get_rx_filter_ids(pf, cmd, (u32 *)rule_locs);
+#else
+		ret = i40e_get_rx_filter_ids(pf, cmd, rule_locs);
+#endif
 		break;
 	default:
 		break;
@@ -3498,11 +4426,491 @@
 }
 
 /**
+ * i40e_get_cloud_ip_field - Identify the IP cloud filter flag
+ * @pf: pointer to the physical function struct
+ * @userdef: pointer to the userdef field data
+ *
+ * This function identifies which IP cloud filter to set when the
+ * flow type if IP.
+ *
+ * Returns a flag of type u8
+ **/
+static u8 i40e_get_cloud_ip_field(struct i40e_pf *pf,
+				  struct i40e_rx_flow_userdef *userdef)
+{
+	u8 flag;
+
+	if (userdef->outer_ip)
+		flag = I40E_CLOUD_FIELD_OIP1;
+	else
+		flag = I40E_CLOUD_FIELD_IIP;
+	return flag;
+}
+
+/**
+ * i40e_cloud_filter_mask2flags- Convert cloud filter details to filter type
+ * @pf: pointer to the physical function struct
+ * @fsp: RX flow classification rules
+ * @userdef: pointer to userdef field data
+ * @flags: Resultant combination of all the fields to decide the tuple
+ *
+ * The general trick in setting these flags is that if the mask field for
+ * a value is non-zero, then the field itself was set to something, so we
+ * use this to tell us what has been selected.
+ *
+ * Returns 0 if a valid filter type was identified.
+ **/
+static int i40e_cloud_filter_mask2flags(struct i40e_pf *pf,
+					struct ethtool_rx_flow_spec *fsp,
+					struct i40e_rx_flow_userdef *userdef,
+					u8 *flags)
+{
+	u8 i = 0;
+	int ret;
+
+	*flags = 0;
+
+	switch (fsp->flow_type & ~FLOW_EXT) {
+	case ETHER_FLOW:
+		/* use is_broadcast and is_zero to check for all 0xf or 0 */
+		if (is_broadcast_ether_addr(fsp->m_u.ether_spec.h_dest)) {
+			i |= I40E_CLOUD_FIELD_OMAC;
+		} else if (is_zero_ether_addr(fsp->m_u.ether_spec.h_dest)) {
+			i &= ~I40E_CLOUD_FIELD_OMAC;
+		} else {
+			dev_info(&pf->pdev->dev, "Bad ether dest mask %pM\n",
+				 fsp->m_u.ether_spec.h_dest);
+			return I40E_ERR_CONFIG;
+		}
+
+		if (is_broadcast_ether_addr(fsp->m_u.ether_spec.h_source)) {
+			i |= I40E_CLOUD_FIELD_IMAC;
+		} else if (is_zero_ether_addr(fsp->m_u.ether_spec.h_source)) {
+			if (userdef->tunnel_type_valid &&
+			    userdef->tunnel_type ==
+			    I40E_CLOUD_FILTER_TUNNEL_TYPE_NVGRE)
+				i |= I40E_CLOUD_FIELD_IMAC;
+			else
+				i &= ~I40E_CLOUD_FIELD_IMAC;
+		} else {
+			dev_info(&pf->pdev->dev, "Bad ether source mask %pM\n",
+				 fsp->m_u.ether_spec.h_source);
+			return I40E_ERR_CONFIG;
+		}
+		break;
+
+	case IP_USER_FLOW:
+		if (pf->hw.mac.type == I40E_MAC_X722) {
+			dev_info(&pf->pdev->dev, "Failed to set filter. Destination IP filters are not supported for this device.\n");
+			return I40E_ERR_CONFIG;
+		}
+		if (fsp->m_u.usr_ip4_spec.ip4dst == cpu_to_be32(0xffffffff)) {
+			i |= i40e_get_cloud_ip_field(pf, userdef);
+		} else if (!fsp->m_u.usr_ip4_spec.ip4dst) {
+			i &= ~(I40E_CLOUD_FIELD_IIP | I40E_CLOUD_FIELD_OIP1 |
+			       I40E_CLOUD_FIELD_OIP2);
+		} else {
+			dev_info(&pf->pdev->dev, "Bad ip dst mask 0x%08x\n",
+				 be32_to_cpu(fsp->m_u.usr_ip4_spec.ip4dst));
+			return I40E_ERR_CONFIG;
+		}
+		break;
+
+	case UDP_V4_FLOW:
+		if (fsp->m_u.udp_ip4_spec.pdst == cpu_to_be16(0xffff)) {
+			i |= I40E_CLOUD_FIELD_IIP;
+		} else {
+			dev_info(&pf->pdev->dev, "Bad UDP dst mask 0x%04x\n",
+				 be16_to_cpu(fsp->m_u.udp_ip4_spec.pdst));
+			return I40E_ERR_CONFIG;
+		}
+		break;
+
+	default:
+		return I40E_ERR_CONFIG;
+	}
+
+	switch (be16_to_cpu(fsp->m_ext.vlan_tci)) {
+	case 0xffff:
+		if (fsp->h_ext.vlan_tci & cpu_to_be16(~0x7fff)) {
+			dev_info(&pf->pdev->dev, "Bad vlan %u\n",
+				 be16_to_cpu(fsp->h_ext.vlan_tci));
+			return I40E_ERR_CONFIG;
+		}
+		i |= I40E_CLOUD_FIELD_IVLAN;
+		break;
+	case 0:
+		i &= ~I40E_CLOUD_FIELD_IVLAN;
+		break;
+	default:
+		dev_info(&pf->pdev->dev, "Bad vlan mask %u\n",
+			 be16_to_cpu(fsp->m_ext.vlan_tci));
+		return I40E_ERR_CONFIG;
+	}
+
+	/* We already know that we're a cloud filter, so we don't need to
+	 * re-check that.
+	 */
+	if (userdef->tenant_id_valid) {
+		if (userdef->tenant_id == 0)
+			if (userdef->tunnel_type_valid &&
+			    userdef->tunnel_type ==
+			    I40E_CLOUD_FILTER_TUNNEL_TYPE_NVGRE)
+				i |= I40E_CLOUD_FIELD_TEN_ID;
+			else
+				i &= ~I40E_CLOUD_FIELD_TEN_ID;
+		else
+			i |= I40E_CLOUD_FIELD_TEN_ID;
+	}
+
+	/* Make sure the flags produce a valid type */
+	if (userdef->outer_ip)
+		ret = i40e_get_custom_cloud_filter_type(i, NULL);
+	else
+		ret = i40e_get_cloud_filter_type(i, NULL);
+
+	if (ret) {
+		dev_info(&pf->pdev->dev, "Invalid mask config, flags = %d\n",
+			 i);
+		return I40E_ERR_CONFIG;
+	}
+
+	*flags = i;
+	return I40E_SUCCESS;
+}
+
+/* i40e_add_cloud_filter_ethtool needs i40e_del_fdir_ethtool() */
+static int i40e_del_fdir_entry(struct i40e_vsi *vsi,
+			       struct ethtool_rxnfc *cmd);
+
+/**
+ * i40e_add_del_custom_cloud_filter - Add custom cloud filter
+ * @vsi: pointer to the VSI structure
+ * @filter: cloud filter rule
+ * @add: if true, add, if false, delete
+ *
+ * Returns 0 on success, negative on failure
+ **/
+int i40e_add_del_custom_cloud_filter(struct i40e_vsi *vsi,
+				     struct i40e_cloud_filter *filter,
+				     bool add)
+{
+	struct i40e_aqc_cloud_filters_element_bb cld_filter;
+	struct i40e_pf *pf = vsi->back;
+	u32 ip_addr;
+	int ret = 0;
+	static const u16 flag_table[128] = {
+		[I40E_CLOUD_FILTER_FLAGS_OIP1] =
+			I40E_AQC_ADD_CLOUD_FILTER_OIP1,
+		[I40E_CLOUD_FILTER_FLAGS_OIP2] =
+			I40E_AQC_ADD_CLOUD_FILTER_OIP2,
+	};
+
+	if (filter->flags >= ARRAY_SIZE(flag_table))
+		return I40E_ERR_CONFIG;
+
+	memset(&cld_filter, 0, sizeof(cld_filter));
+
+	cld_filter.element.flags |= cpu_to_le16(flag_table[filter->flags] |
+				I40E_AQC_ADD_CLOUD_FLAGS_IPV4);
+	ip_addr = be32_to_cpu(filter->dst_ipv4);
+
+	if (filter->flags == I40E_CLOUD_FILTER_FLAGS_OIP1) {
+		/* Copy the ip addr to bytes 64-67, 0 to 68-69 */
+		memcpy(&cld_filter.general_fields
+				[I40E_AQC_ADD_CLOUD_FV_FLU_0X10_WORD0],
+		       &ip_addr, sizeof(ip_addr));
+	} else if (filter->flags == I40E_CLOUD_FILTER_FLAGS_OIP2) {
+		/* Copy the ip addr to bytes 76-79, 0 to 80-81 */
+		memcpy(&cld_filter.general_fields
+				[I40E_AQC_ADD_CLOUD_FV_FLU_0X12_WORD0],
+		       &ip_addr, sizeof(ip_addr));
+	}
+
+	if (add)
+		ret = i40e_aq_add_cloud_filters_bb(&pf->hw, filter->seid,
+						   &cld_filter, 1);
+	else
+		ret = i40e_aq_rem_cloud_filters_bb(&pf->hw, filter->seid,
+						   &cld_filter, 1);
+	if (ret) {
+		dev_info(&pf->pdev->dev,
+			 "Failed to %s cloud filters(big buffer) err %d aq_err %d\n",
+			 add ? "add" : "delete", ret,
+			 pf->hw.aq.asq_last_status);
+	} else {
+		/* Filter add/remove successful */
+		dev_dbg(&pf->pdev->dev,
+			"%s cloud filter for VSI: %d, dst IP: %pI4\n",
+			add ? "add" : "delete",
+			vsi->seid, &filter->dst_ipv4);
+		if (filter->flags == I40E_CLOUD_FILTER_FLAGS_OIP1)
+			pf->outerip_filters[0] =
+			add ? (pf->outerip_filters[0] + 1) :
+			(pf->outerip_filters[0] - 1);
+		else if (filter->flags == I40E_CLOUD_FILTER_FLAGS_OIP2)
+			pf->outerip_filters[1] =
+			add ? (pf->outerip_filters[1] + 1) :
+			(pf->outerip_filters[1] - 1);
+	}
+	return ret;
+}
+
+/**
+ * i40e_add_cloud_filter_ethtool - Add cloud filter
+ * @vsi: pointer to the VSI structure
+ * @cmd: The command to get or set Rx flow classification rules
+ * @userdef: pointer to userdef field data
+ *
+ * Add cloud filter for a specific flow spec.
+ * Returns 0 if the filter were successfully added.
+ **/
+static int i40e_add_cloud_filter_ethtool(struct i40e_vsi *vsi,
+					 struct ethtool_rxnfc *cmd,
+					 struct i40e_rx_flow_userdef *userdef)
+{
+	struct i40e_cloud_filter *rule, *parent, *filter = NULL;
+	struct ethtool_rx_flow_spec *fsp;
+	u16 dest_seid = 0, q_index = 0;
+	struct i40e_pf *pf = vsi->back;
+	struct hlist_node *node2;
+	u32 ring, vf;
+	u8 flags = 0;
+	int ret;
+
+	if (!(pf->flags & I40E_FLAG_FD_SB_ENABLED))
+		return -EOPNOTSUPP;
+
+	if (pf->flags & I40E_FLAG_MFP_ENABLED)
+		return -EOPNOTSUPP;
+
+	if (test_bit(__I40E_RESET_RECOVERY_PENDING, pf->state) ||
+	    test_bit(__I40E_RESET_INTR_RECEIVED, pf->state))
+		return -EBUSY;
+
+	fsp = (struct ethtool_rx_flow_spec *)&cmd->fs;
+
+	/* The ring_cookie is a mask of queue index and VF id we wish to
+	 * target. This is the same for regular flow director filters.
+	 */
+	if (fsp->ring_cookie == RX_CLS_FLOW_DISC) {
+		dev_warn(&pf->pdev->dev, "Cloud filters do not support the drop action.\n");
+		return -EOPNOTSUPP;
+	}
+
+	ring = ethtool_get_flow_spec_ring(fsp->ring_cookie);
+	vf = ethtool_get_flow_spec_ring_vf(fsp->ring_cookie);
+
+	if (!vf) {
+		if (ring >= vsi->num_queue_pairs)
+			return -EINVAL;
+		dest_seid = vsi->seid;
+	} else {
+		/* VFs are zero-indexed, so we subtract one here */
+		vf--;
+
+		if (vf >= pf->num_alloc_vfs)
+			return -EINVAL;
+
+		if (ring >= pf->vf[vf].num_queue_pairs &&
+		    ring != I40E_CLOUD_FILTER_ANY_QUEUE)
+			return -EINVAL;
+
+		/* l4mode disallows any filter, do not apply this check if
+		 * custom ddp package was applied.
+		 */
+		if (!i40e_is_l4mode_enabled() &&
+		    ring >= pf->vf[vf].num_queue_pairs &&
+		    !userdef->outer_ip)
+			return -EINVAL;
+
+		dest_seid = pf->vsi[pf->vf[vf].lan_vsi_idx]->seid;
+	}
+	q_index = ring;
+
+	ret = i40e_cloud_filter_mask2flags(pf, fsp, userdef, &flags);
+	if (ret)
+		return -EINVAL;
+
+	/* if filter exists with same id, delete the old one */
+	parent = NULL;
+	hlist_for_each_entry_safe(rule, node2,
+				  &pf->cloud_filter_list, cloud_node) {
+		/* filter exists with the id */
+		if (rule->id == fsp->location)
+			filter = rule;
+
+		/* Abort now if we're trying to add an outer IP filter and it
+		 * already exists in the device. We must detect this condition
+		 * here since we can't rely on the firmware return code to tell
+		 * us this later.
+		 */
+		if (userdef->outer_ip && fsp->h_u.usr_ip4_spec.ip4dst ==
+		    rule->dst_ipv4)
+			return -EEXIST;
+
+		/* bail out if we've passed the likely location in the list */
+		if (rule->id >= fsp->location)
+			break;
+
+		/* track where we left off */
+		parent = rule;
+	}
+
+	/* Continue searching for duplicates if we're dealing with outer IP
+	 * filters and haven't reached the end of the list yet.
+	 */
+	if (rule && userdef->outer_ip)
+		hlist_for_each_entry_continue(rule, cloud_node)
+			if (fsp->h_u.usr_ip4_spec.ip4dst == rule->dst_ipv4)
+				return -EEXIST;
+
+	if (filter && (filter->id == fsp->location)) {
+		/* found it in the cloud list, so remove it */
+		if (filter->flags & I40E_CLOUD_FIELD_OIP1 ||
+		    filter->flags & I40E_CLOUD_FIELD_OIP2)
+			ret = i40e_add_del_custom_cloud_filter(vsi, filter,
+							       false);
+		else
+			ret = i40e_add_del_cloud_filter_ex(pf, filter, false);
+		if (ret && pf->hw.aq.asq_last_status != I40E_AQ_RC_ENOENT)
+			return ret;
+		hlist_del(&filter->cloud_node);
+		kfree(filter);
+		pf->num_cloud_filters--;
+	} else {
+		/* not in the cloud list, so check the PF's fdir list */
+		(void)i40e_del_fdir_entry(pf->vsi[pf->lan_vsi], cmd);
+	}
+
+	filter = kzalloc(sizeof(*filter), GFP_KERNEL);
+	if (!filter)
+		return -ENOMEM;
+
+	switch (fsp->flow_type & ~FLOW_EXT) {
+	case ETHER_FLOW:
+		ether_addr_copy(filter->outer_mac,
+				fsp->h_u.ether_spec.h_dest);
+		ether_addr_copy(filter->inner_mac,
+				fsp->h_u.ether_spec.h_source);
+		break;
+
+	case IP_USER_FLOW:
+		if (flags & I40E_CLOUD_FIELD_TEN_ID) {
+			dev_info(&pf->pdev->dev, "Tenant id not allowed for ip filter\n");
+			kfree(filter);
+			return I40E_ERR_CONFIG;
+		}
+		filter->dst_ipv4 = fsp->h_u.usr_ip4_spec.ip4dst;
+		filter->n_proto = ETH_P_IP;
+		break;
+
+	case UDP_V4_FLOW:
+		filter->dst_port = fsp->h_u.udp_ip4_spec.pdst;
+		break;
+
+	default:
+		dev_info(&pf->pdev->dev, "unknown flow type 0x%x\n",
+			 (fsp->flow_type & ~FLOW_EXT));
+		kfree(filter);
+		return I40E_ERR_CONFIG;
+	}
+
+	if (userdef->tenant_id_valid)
+		filter->tenant_id = userdef->tenant_id;
+	else
+		filter->tenant_id = 0;
+	if (userdef->tunnel_type_valid)
+		filter->tunnel_type = userdef->tunnel_type;
+	else
+		filter->tunnel_type = I40E_CLOUD_TNL_TYPE_NONE;
+
+	filter->id = fsp->location;
+	filter->seid = dest_seid;
+	filter->queue_id = q_index;
+	filter->flags = flags;
+	filter->inner_vlan = fsp->h_ext.vlan_tci;
+
+	if (userdef->outer_ip) {
+		/* Assign the filter to a specific table and program it */
+		if (pf->outerip_filters[0] < 512) {
+			filter->flags = I40E_CLOUD_FIELD_OIP1;
+		} else if (pf->outerip_filters[1] < 1024) {
+			filter->flags = I40E_CLOUD_FIELD_OIP2;
+		} else {
+			dev_info(&pf->pdev->dev, "Too many Outer IP filters in device\n");
+			kfree(filter);
+			return -ENOSPC;
+		}
+
+		ret = i40e_add_del_custom_cloud_filter(vsi, filter, true);
+	} else {
+		ret = i40e_add_del_cloud_filter_ex(pf, filter, true);
+	}
+
+	if (ret) {
+		kfree(filter);
+		return ret;
+	}
+
+	/* add filter to the ordered list */
+	INIT_HLIST_NODE(&filter->cloud_node);
+	if (parent)
+		hlist_add_behind(&filter->cloud_node, &parent->cloud_node);
+	else
+		hlist_add_head(&filter->cloud_node, &pf->cloud_filter_list);
+	pf->num_cloud_filters++;
+
+	return 0;
+}
+
+/**
+ * i40e_del_cloud_filter_ethtool - del vxlan filter
+ * @pf: pointer to the physical function struct
+ * @cmd: RX flow classification rules
+ *
+ * Delete vxlan filter for a specific flow spec.
+ * Returns 0 if the filter was successfully deleted.
+ **/
+static int i40e_del_cloud_filter_ethtool(struct i40e_pf *pf,
+					 struct ethtool_rxnfc *cmd)
+{
+	struct i40e_cloud_filter *rule, *filter = NULL;
+	struct ethtool_rx_flow_spec *fsp;
+	struct hlist_node *node2;
+	struct i40e_vsi *vsi = pf->vsi[pf->lan_vsi];
+
+	fsp = (struct ethtool_rx_flow_spec *)&cmd->fs;
+	hlist_for_each_entry_safe(rule, node2,
+				  &pf->cloud_filter_list, cloud_node) {
+		/* filter found */
+		if (rule->id == fsp->location)
+			filter = rule;
+
+		/* bail out if we've passed the likely location in the list */
+		if (rule->id >= fsp->location)
+			break;
+	}
+	if (!filter)
+		return -ENOENT;
+
+	/* remove filter from the list even if failed to remove from device */
+	if (filter->flags & I40E_CLOUD_FIELD_OIP1 ||
+	    filter->flags & I40E_CLOUD_FIELD_OIP2)
+		(void)i40e_add_del_custom_cloud_filter(vsi, filter, false);
+	else
+		(void)i40e_add_del_cloud_filter_ex(pf, filter, false);
+	hlist_del(&filter->cloud_node);
+	kfree(filter);
+	pf->num_cloud_filters--;
+
+	return 0;
+}
+/**
  * i40e_update_ethtool_fdir_entry - Updates the fdir filter entry
  * @vsi: Pointer to the targeted VSI
  * @input: The filter to update or NULL to indicate deletion
  * @sw_idx: Software index to the filter
- * @cmd: The command to get or set Rx flow classification rules
  *
  * This function updates (or deletes) a Flow Director entry from
  * the hlist of the corresponding PF
@@ -3511,38 +4919,38 @@
  **/
 static int i40e_update_ethtool_fdir_entry(struct i40e_vsi *vsi,
 					  struct i40e_fdir_filter *input,
-					  u16 sw_idx,
-					  struct ethtool_rxnfc *cmd)
+					  u16 sw_idx)
 {
 	struct i40e_fdir_filter *rule, *parent;
 	struct i40e_pf *pf = vsi->back;
 	struct hlist_node *node2;
-	int err = -EINVAL;
+	int err = -ENOENT;
 
 	parent = NULL;
 	rule = NULL;
 
 	hlist_for_each_entry_safe(rule, node2,
 				  &pf->fdir_filter_list, fdir_node) {
-		/* hash found, or no matching entry */
+		/* rule id found, or passed its spot in the list */
 		if (rule->fd_id >= sw_idx)
 			break;
 		parent = rule;
 	}
 
-	/* if there is an old rule occupying our place remove it */
+	/* is there is an old rule occupying our target filter slot? */
 	if (rule && (rule->fd_id == sw_idx)) {
 		/* Remove this rule, since we're either deleting it, or
 		 * replacing it.
 		 */
 		err = i40e_add_del_fdir(vsi, rule, false);
 		hlist_del(&rule->fdir_node);
-		kfree(rule);
 		pf->fdir_pf_active_filters--;
+
+		kfree(rule);
 	}
 
 	/* If we weren't given an input, this is a delete, so just return the
-	 * error code indicating if there was an entry at the requested slot
+	 * error code indicating if there was an entry at the requested slot.
 	 */
 	if (!input)
 		return err;
@@ -3550,12 +4958,11 @@
 	/* Otherwise, install the new rule as requested */
 	INIT_HLIST_NODE(&input->fdir_node);
 
-	/* add filter to the list */
+	/* add filter to the ordered list */
 	if (parent)
 		hlist_add_behind(&input->fdir_node, &parent->fdir_node);
 	else
-		hlist_add_head(&input->fdir_node,
-			       &pf->fdir_filter_list);
+		hlist_add_head(&input->fdir_node, &pf->fdir_filter_list);
 
 	/* update counts */
 	pf->fdir_pf_active_filters++;
@@ -3651,7 +5058,7 @@
 	if (test_bit(__I40E_FD_FLUSH_REQUESTED, pf->state))
 		return -EBUSY;
 
-	ret = i40e_update_ethtool_fdir_entry(vsi, NULL, fsp->location, cmd);
+	ret = i40e_update_ethtool_fdir_entry(vsi, NULL, fsp->location);
 
 	i40e_prune_flex_pit_list(pf);
 
@@ -3719,7 +5126,6 @@
 	 */
 	if (size >= I40E_FLEX_PIT_TABLE_SIZE)
 		return ERR_PTR(-ENOSPC);
-
 	return NULL;
 }
 
@@ -3917,6 +5323,16 @@
 		return "sctp4";
 	case IP_USER_FLOW:
 		return "ip4";
+	case TCP_V6_FLOW:
+		return "tcp6";
+	case UDP_V6_FLOW:
+		return "udp6";
+	case SCTP_V6_FLOW:
+		return "sctp6";
+#ifdef HAVE_ETHTOOL_FLOW_UNION_IP6_SPEC
+	case IPV6_USER_FLOW:
+		return "ip6";
+#endif /* HAVE_ETHTOOL_FLOW_UNION_IP6_SPEC */
 	default:
 		return "unknown";
 	}
@@ -3956,49 +5372,50 @@
 /**
  * i40e_print_input_set - Show changes between two input sets
  * @vsi: the vsi being configured
- * @old: the old input set
- * @new: the new input set
+ * @old_input: the old input set
+ * @new_input: the new input set
  *
  * Print the difference between old and new input sets by showing which series
  * of words are toggled on or off. Only displays the bits we actually support
  * changing.
  **/
-static void i40e_print_input_set(struct i40e_vsi *vsi, u64 old, u64 new)
+static void i40e_print_input_set(struct i40e_vsi *vsi,
+				 u64 old_input, u64 new_input)
 {
 	struct i40e_pf *pf = vsi->back;
 	bool old_value, new_value;
 	int i;
 
-	old_value = !!(old & I40E_L3_SRC_MASK);
-	new_value = !!(new & I40E_L3_SRC_MASK);
+	old_value = !!(old_input & I40E_L3_SRC_MASK);
+	new_value = !!(new_input & I40E_L3_SRC_MASK);
 	if (old_value != new_value)
 		netif_info(pf, drv, vsi->netdev, "L3 source address: %s -> %s\n",
 			   old_value ? "ON" : "OFF",
 			   new_value ? "ON" : "OFF");
 
-	old_value = !!(old & I40E_L3_DST_MASK);
-	new_value = !!(new & I40E_L3_DST_MASK);
+	old_value = !!(old_input & I40E_L3_DST_MASK);
+	new_value = !!(new_input & I40E_L3_DST_MASK);
 	if (old_value != new_value)
 		netif_info(pf, drv, vsi->netdev, "L3 destination address: %s -> %s\n",
 			   old_value ? "ON" : "OFF",
 			   new_value ? "ON" : "OFF");
 
-	old_value = !!(old & I40E_L4_SRC_MASK);
-	new_value = !!(new & I40E_L4_SRC_MASK);
+	old_value = !!(old_input & I40E_L4_SRC_MASK);
+	new_value = !!(new_input & I40E_L4_SRC_MASK);
 	if (old_value != new_value)
 		netif_info(pf, drv, vsi->netdev, "L4 source port: %s -> %s\n",
 			   old_value ? "ON" : "OFF",
 			   new_value ? "ON" : "OFF");
 
-	old_value = !!(old & I40E_L4_DST_MASK);
-	new_value = !!(new & I40E_L4_DST_MASK);
+	old_value = !!(old_input & I40E_L4_DST_MASK);
+	new_value = !!(new_input & I40E_L4_DST_MASK);
 	if (old_value != new_value)
 		netif_info(pf, drv, vsi->netdev, "L4 destination port: %s -> %s\n",
 			   old_value ? "ON" : "OFF",
 			   new_value ? "ON" : "OFF");
 
-	old_value = !!(old & I40E_VERIFY_TAG_MASK);
-	new_value = !!(new & I40E_VERIFY_TAG_MASK);
+	old_value = !!(old_input & I40E_VERIFY_TAG_MASK);
+	new_value = !!(new_input & I40E_VERIFY_TAG_MASK);
 	if (old_value != new_value)
 		netif_info(pf, drv, vsi->netdev, "SCTP verification tag: %s -> %s\n",
 			   old_value ? "ON" : "OFF",
@@ -4008,8 +5425,8 @@
 	for (i = 0; i < I40E_FLEX_INDEX_ENTRIES; i++) {
 		u64 flex_mask = i40e_pit_index_to_mask(i);
 
-		old_value = !!(old & flex_mask);
-		new_value = !!(new & flex_mask);
+		old_value = !!(old_input & flex_mask);
+		new_value = !!(new_input & flex_mask);
 		if (old_value != new_value)
 			netif_info(pf, drv, vsi->netdev, "FLEX index %d: %s -> %s\n",
 				   i,
@@ -4018,9 +5435,9 @@
 	}
 
 	netif_info(pf, drv, vsi->netdev, "  Current input set: %0llx\n",
-		   old);
+		   old_input);
 	netif_info(pf, drv, vsi->netdev, "Requested input set: %0llx\n",
-		   new);
+		   new_input);
 }
 
 /**
@@ -4052,9 +5469,16 @@
 				     struct ethtool_rx_flow_spec *fsp,
 				     struct i40e_rx_flow_userdef *userdef)
 {
-	struct i40e_pf *pf = vsi->back;
+#ifdef HAVE_ETHTOOL_FLOW_UNION_IP6_SPEC
+	static const __be32 ipv6_full_mask[4] = {
+		cpu_to_be32(0xffffffff), cpu_to_be32(0xffffffff),
+		cpu_to_be32(0xffffffff), cpu_to_be32(0xffffffff)};
+	struct ethtool_tcpip6_spec *tcp_ip6_spec;
+	struct ethtool_usrip6_spec *usr_ip6_spec;
+#endif /* HAVE_ETHTOOL_FLOW_UNION_IP6_SPEC */
 	struct ethtool_tcpip4_spec *tcp_ip4_spec;
 	struct ethtool_usrip4_spec *usr_ip4_spec;
+	struct i40e_pf *pf = vsi->back;
 	u64 current_mask, new_mask;
 	bool new_flex_offset = false;
 	bool flex_l3 = false;
@@ -4076,11 +5500,30 @@
 		index = I40E_FILTER_PCTYPE_NONF_IPV4_UDP;
 		fdir_filter_count = &pf->fd_udp4_filter_cnt;
 		break;
+	case SCTP_V6_FLOW:
+		index = I40E_FILTER_PCTYPE_NONF_IPV6_SCTP;
+		fdir_filter_count = &pf->fd_sctp6_filter_cnt;
+		break;
+	case TCP_V6_FLOW:
+		index = I40E_FILTER_PCTYPE_NONF_IPV6_TCP;
+		fdir_filter_count = &pf->fd_tcp6_filter_cnt;
+		break;
+	case UDP_V6_FLOW:
+		index = I40E_FILTER_PCTYPE_NONF_IPV6_UDP;
+		fdir_filter_count = &pf->fd_udp6_filter_cnt;
+		break;
 	case IP_USER_FLOW:
 		index = I40E_FILTER_PCTYPE_NONF_IPV4_OTHER;
 		fdir_filter_count = &pf->fd_ip4_filter_cnt;
 		flex_l3 = true;
 		break;
+#ifdef HAVE_ETHTOOL_FLOW_UNION_IP6_SPEC
+	case IPV6_USER_FLOW:
+		index = I40E_FILTER_PCTYPE_NONF_IPV6_OTHER;
+		fdir_filter_count = &pf->fd_ip6_filter_cnt;
+		flex_l3 = true;
+		break;
+#endif /* HAVE_ETHTOOL_FLOW_UNION_IP6_SPEC */
 	default:
 		return -EOPNOTSUPP;
 	}
@@ -4143,6 +5586,57 @@
 			return -EOPNOTSUPP;
 
 		break;
+	case SCTP_V6_FLOW:
+		new_mask &= ~I40E_VERIFY_TAG_MASK;
+		/* Fall through */
+	case TCP_V6_FLOW:
+	case UDP_V6_FLOW:
+#ifdef HAVE_ETHTOOL_FLOW_UNION_IP6_SPEC
+		tcp_ip6_spec = &fsp->m_u.tcp_ip6_spec;
+
+		/* Check if user provided IPv6 source address. */
+		if (ipv6_addr_equal((struct in6_addr *)&tcp_ip6_spec->ip6src,
+				    (struct in6_addr *)&ipv6_full_mask))
+			new_mask |= I40E_L3_V6_SRC_MASK;
+		else if (ipv6_addr_any((struct in6_addr *)
+				       &tcp_ip6_spec->ip6src))
+			new_mask &= ~I40E_L3_V6_SRC_MASK;
+		else
+			return -EOPNOTSUPP;
+
+		/* Check if user provided destination address. */
+		if (ipv6_addr_equal((struct in6_addr *)&tcp_ip6_spec->ip6dst,
+				    (struct in6_addr *)&ipv6_full_mask))
+			new_mask |= I40E_L3_V6_DST_MASK;
+		else if (ipv6_addr_any((struct in6_addr *)
+				       &tcp_ip6_spec->ip6dst))
+			new_mask &= ~I40E_L3_V6_DST_MASK;
+		else
+			return -EOPNOTSUPP;
+
+		/* L4 source port */
+		if (tcp_ip6_spec->psrc == htons(0xFFFF))
+			new_mask |= I40E_L4_SRC_MASK;
+		else if (!tcp_ip6_spec->psrc)
+			new_mask &= ~I40E_L4_SRC_MASK;
+		else
+			return -EOPNOTSUPP;
+
+		/* L4 destination port */
+		if (tcp_ip6_spec->pdst == htons(0xFFFF))
+			new_mask |= I40E_L4_DST_MASK;
+		else if (!tcp_ip6_spec->pdst)
+			new_mask &= ~I40E_L4_DST_MASK;
+		else
+			return -EOPNOTSUPP;
+
+		/* Filtering on Traffic Classes is not supported. */
+		if (tcp_ip6_spec->tclass)
+			return -EOPNOTSUPP;
+#else /* !HAVE_ETHTOOL_FLOW_UNION_IP6_SPEC */
+		return -EOPNOTSUPP;
+#endif /* HAVE_ETHTOOL_FLOW_UNION_IP6_SPEC */
+		break;
 	case IP_USER_FLOW:
 		usr_ip4_spec = &fsp->m_u.usr_ip4_spec;
 
@@ -4183,10 +5677,64 @@
 			return -EINVAL;
 
 		break;
+#ifdef HAVE_ETHTOOL_FLOW_UNION_IP6_SPEC
+	case IPV6_USER_FLOW:
+		usr_ip6_spec = &fsp->m_u.usr_ip6_spec;
+
+		/* Check if user provided IPv6 source address. */
+		if (ipv6_addr_equal((struct in6_addr *)&usr_ip6_spec->ip6src,
+				    (struct in6_addr *)&ipv6_full_mask))
+			new_mask |= I40E_L3_V6_SRC_MASK;
+		else if (ipv6_addr_any((struct in6_addr *)
+				       &usr_ip6_spec->ip6src))
+			new_mask &= ~I40E_L3_V6_SRC_MASK;
+		else
+			return -EOPNOTSUPP;
+
+		/* Check if user provided destination address. */
+		if (ipv6_addr_equal((struct in6_addr *)&usr_ip6_spec->ip6dst,
+				    (struct in6_addr *)&ipv6_full_mask))
+			new_mask |= I40E_L3_V6_DST_MASK;
+		else if (ipv6_addr_any((struct in6_addr *)
+				       &usr_ip6_spec->ip6src))
+			new_mask &= ~I40E_L3_V6_DST_MASK;
+		else
+			return -EOPNOTSUPP;
+
+		if (usr_ip6_spec->l4_4_bytes == htonl(0xFFFFFFFF))
+			new_mask |= I40E_L4_SRC_MASK | I40E_L4_DST_MASK;
+		else if (!usr_ip6_spec->l4_4_bytes)
+			new_mask &= ~(I40E_L4_SRC_MASK | I40E_L4_DST_MASK);
+		else
+			return -EOPNOTSUPP;
+
+		/* Filtering on Traffic class is not supported. */
+		if (usr_ip6_spec->tclass)
+			return -EOPNOTSUPP;
+
+		/* Filtering on L4 protocol is not supported */
+		if (usr_ip6_spec->l4_proto)
+			return -EINVAL;
+
+		break;
+#endif /* HAVE_ETHTOOL_FLOW_UNION_IP6_SPEC */
 	default:
 		return -EOPNOTSUPP;
 	}
 
+	if (fsp->flow_type & FLOW_EXT) {
+	/* Allow only 802.1Q and no etype defined, as
+	 * later it's modified to 0x8100
+	 */
+		if (fsp->h_ext.vlan_etype != htons(ETH_P_8021Q) &&
+		    fsp->h_ext.vlan_etype != 0)
+			return -EOPNOTSUPP;
+		if (fsp->m_ext.vlan_tci == htons(0xFFFF))
+			new_mask |= I40E_VLAN_SRC_MASK;
+		else
+			new_mask &= ~I40E_VLAN_SRC_MASK;
+	}
+
 	/* First, clear all flexible filter entries */
 	new_mask &= ~I40E_FLEX_INPUT_MASK;
 
@@ -4360,13 +5908,15 @@
 static bool i40e_match_fdir_filter(struct i40e_fdir_filter *a,
 				   struct i40e_fdir_filter *b)
 {
-	/* The filters do not much if any of these criteria differ. */
+	/* The filters do not match if any of these criteria differ. */
 	if (a->dst_ip != b->dst_ip ||
 	    a->src_ip != b->src_ip ||
 	    a->dst_port != b->dst_port ||
 	    a->src_port != b->src_port ||
 	    a->flow_type != b->flow_type ||
-	    a->ip4_proto != b->ip4_proto)
+	    a->ipl4_proto != b->ipl4_proto ||
+	    a->vlan_tag != b->vlan_tag ||
+	    a->vlan_etype != b->vlan_etype)
 		return false;
 
 	return true;
@@ -4467,8 +6017,12 @@
 	fsp = (struct ethtool_rx_flow_spec *)&cmd->fs;
 
 	/* Parse the user-defined field */
-	if (i40e_parse_rx_flow_user_data(fsp, &userdef))
-		return -EINVAL;
+	ret = i40e_parse_rx_flow_user_data(pf, fsp, &userdef);
+	if (ret)
+		return ret;
+
+	if (userdef.cloud_filter)
+		return i40e_add_cloud_filter_ethtool(vsi, cmd, &userdef);
 
 	/* Extended MAC field is not supported */
 	if (fsp->flow_type & FLOW_MAC_EXT)
@@ -4511,7 +6065,6 @@
 	}
 
 	input = kzalloc(sizeof(*input), GFP_KERNEL);
-
 	if (!input)
 		return -ENOMEM;
 
@@ -4521,18 +6074,46 @@
 	input->dest_ctl = dest_ctl;
 	input->fd_status = I40E_FILTER_PROGRAM_DESC_FD_STATUS_FD_ID;
 	input->cnt_index  = I40E_FD_SB_STAT_IDX(pf->hw.pf_id);
-	input->dst_ip = fsp->h_u.tcp_ip4_spec.ip4src;
-	input->src_ip = fsp->h_u.tcp_ip4_spec.ip4dst;
 	input->flow_type = fsp->flow_type & ~FLOW_EXT;
-	input->ip4_proto = fsp->h_u.usr_ip4_spec.proto;
-
-	/* Reverse the src and dest notion, since the HW expects them to be from
-	 * Tx perspective where as the input from user is from Rx filter view.
-	 */
+	input->vlan_etype = fsp->h_ext.vlan_etype;
+	if (!fsp->m_ext.vlan_etype && fsp->h_ext.vlan_tci)
+		input->vlan_etype = cpu_to_be16(ETH_P_8021Q);
+	if (fsp->m_ext.vlan_tci && input->vlan_etype)
+		input->vlan_tag = fsp->h_ext.vlan_tci;
+#ifdef HAVE_ETHTOOL_FLOW_UNION_IP6_SPEC
+	if (input->flow_type == IPV6_USER_FLOW ||
+	    input->flow_type == UDP_V6_FLOW ||
+	    input->flow_type == TCP_V6_FLOW ||
+	    input->flow_type == SCTP_V6_FLOW) {
+		/* Reverse the src and dest notion, since the HW expects them
+		 * to be from Tx perspective where as the input from user is
+		 * from Rx filter view.
+		 */
+		input->ipl4_proto = fsp->h_u.usr_ip6_spec.l4_proto;
+		input->dst_port = fsp->h_u.tcp_ip6_spec.psrc;
+		input->src_port = fsp->h_u.tcp_ip6_spec.pdst;
+		memcpy(input->dst_ip6, fsp->h_u.ah_ip6_spec.ip6src,
+		       sizeof(__be32) * 4);
+		memcpy(input->src_ip6, fsp->h_u.ah_ip6_spec.ip6dst,
+		       sizeof(__be32) * 4);
+	} else {
+		/* Reverse the src and dest notion, since the HW expects them
+		 * to be from Tx perspective where as the input from user is
+		 * from Rx filter view.
+		 */
+		input->ipl4_proto = fsp->h_u.usr_ip4_spec.proto;
+		input->dst_port = fsp->h_u.tcp_ip4_spec.psrc;
+		input->src_port = fsp->h_u.tcp_ip4_spec.pdst;
+		input->dst_ip = fsp->h_u.tcp_ip4_spec.ip4src;
+		input->src_ip = fsp->h_u.tcp_ip4_spec.ip4dst;
+	}
+#else /* !HAVE_ETHTOOL_FLOW_UNION_IP6_SPEC */
+	input->ipl4_proto = fsp->h_u.usr_ip4_spec.proto;
 	input->dst_port = fsp->h_u.tcp_ip4_spec.psrc;
 	input->src_port = fsp->h_u.tcp_ip4_spec.pdst;
 	input->dst_ip = fsp->h_u.tcp_ip4_spec.ip4src;
 	input->src_ip = fsp->h_u.tcp_ip4_spec.ip4dst;
+#endif /* HAVE_ETHTOOL_FLOW_UNION_IP6_SPEC */
 
 	if (userdef.flex_filter) {
 		input->flex_filter = true;
@@ -4545,14 +6126,18 @@
 	if (ret)
 		goto free_filter_memory;
 
-	/* Add the input filter to the fdir_input_list, possibly replacing
+	/* Add the input filter to the fdir_filter_list, possibly replacing
 	 * a previous filter. Do not free the input structure after adding it
-	 * to the list as this would cause a use-after-free bug.
+	 * to the list as this would cause a use after free bug.
 	 */
-	i40e_update_ethtool_fdir_entry(vsi, input, fsp->location, NULL);
+	i40e_update_ethtool_fdir_entry(vsi, input, fsp->location);
+
+	(void)i40e_del_cloud_filter_ethtool(pf, cmd);
 	ret = i40e_add_del_fdir(vsi, input, true);
+
 	if (ret)
 		goto remove_sw_rule;
+
 	return 0;
 
 remove_sw_rule:
@@ -4581,12 +6166,17 @@
 	case ETHTOOL_SRXFH:
 		ret = i40e_set_rss_hash_opt(pf, cmd);
 		break;
+
 	case ETHTOOL_SRXCLSRLINS:
 		ret = i40e_add_fdir_ethtool(vsi, cmd);
 		break;
+
 	case ETHTOOL_SRXCLSRLDEL:
 		ret = i40e_del_fdir_entry(vsi, cmd);
+		if (ret == -ENOENT)
+			ret = i40e_del_cloud_filter_ethtool(pf, cmd);
 		break;
+
 	default:
 		break;
 	}
@@ -4594,6 +6184,8 @@
 	return ret;
 }
 
+#endif /* ETHTOOL_GRXRINGS */
+#ifdef ETHTOOL_SCHANNELS
 /**
  * i40e_max_channels - get Max number of combined channels supported
  * @vsi: vsi pointer
@@ -4708,6 +6300,9 @@
 		return -EINVAL;
 }
 
+#endif /* ETHTOOL_SCHANNELS */
+
+#if defined(ETHTOOL_GRSSH) && defined(ETHTOOL_SRSSH)
 /**
  * i40e_get_rxfh_key_size - get the RSS hash key size
  * @netdev: network interface device structure
@@ -4740,8 +6335,12 @@
  * Reads the indirection table directly from the hardware. Returns 0 on
  * success.
  **/
+#ifdef HAVE_RXFH_HASHFUNC
 static int i40e_get_rxfh(struct net_device *netdev, u32 *indir, u8 *key,
 			 u8 *hfunc)
+#else
+static int i40e_get_rxfh(struct net_device *netdev, u32 *indir, u8 *key)
+#endif
 {
 	struct i40e_netdev_priv *np = netdev_priv(netdev);
 	struct i40e_vsi *vsi = np->vsi;
@@ -4749,9 +6348,11 @@
 	int ret;
 	u16 i;
 
+#ifdef HAVE_RXFH_HASHFUNC
 	if (hfunc)
 		*hfunc = ETH_RSS_HASH_TOP;
 
+#endif
 	if (!indir)
 		return 0;
 
@@ -4781,8 +6382,17 @@
  * Returns -EINVAL if the table specifies an invalid queue id, otherwise
  * returns 0 after programming the table.
  **/
+#ifdef HAVE_RXFH_HASHFUNC
 static int i40e_set_rxfh(struct net_device *netdev, const u32 *indir,
 			 const u8 *key, const u8 hfunc)
+#else
+#ifdef HAVE_RXFH_NONCONST
+static int i40e_set_rxfh(struct net_device *netdev, u32 *indir, u8 *key)
+#else
+static int i40e_set_rxfh(struct net_device *netdev, const u32 *indir,
+			 const u8 *key)
+#endif /* HAVE_RXFH_NONCONST */
+#endif /* HAVE_RXFH_HASHFUNC */
 {
 	struct i40e_netdev_priv *np = netdev_priv(netdev);
 	struct i40e_vsi *vsi = np->vsi;
@@ -4790,8 +6400,18 @@
 	u8 *seed = NULL;
 	u16 i;
 
+#ifdef HAVE_RXFH_HASHFUNC
 	if (hfunc != ETH_RSS_HASH_NO_CHANGE && hfunc != ETH_RSS_HASH_TOP)
 		return -EOPNOTSUPP;
+#endif
+
+	/* Verify user input. */
+	if (indir) {
+		for (i = 0; i < I40E_HLUT_ARRAY_SIZE; i++) {
+			if (indir[i] >= vsi->rss_size)
+				return -EINVAL;
+		}
+	}
 
 	if (key) {
 		if (!vsi->rss_hkey_user) {
@@ -4820,7 +6440,9 @@
 	return i40e_config_rss(vsi, seed, vsi->rss_lut_user,
 			       I40E_HLUT_ARRAY_SIZE);
 }
+#endif /* ETHTOOL_GRSSH && ETHTOOL_SRSSH */
 
+#ifdef HAVE_ETHTOOL_GET_SSET_COUNT
 /**
  * i40e_get_priv_flags - report device private flags
  * @dev: network interface device structure
@@ -4874,7 +6496,7 @@
 	enum i40e_admin_queue_err adq_err;
 	struct i40e_vsi *vsi = np->vsi;
 	struct i40e_pf *pf = vsi->back;
-	bool is_reset_needed;
+	u32 reset_needed = 0;
 	i40e_status status;
 	u32 i, j;
 
@@ -4919,9 +6541,12 @@
 flags_complete:
 	changed_flags = orig_flags ^ new_flags;
 
-	is_reset_needed = !!(changed_flags & (I40E_FLAG_VEB_STATS_ENABLED |
-		I40E_FLAG_LEGACY_RX | I40E_FLAG_SOURCE_PRUNING_DISABLED |
-		I40E_FLAG_DISABLE_FW_LLDP));
+	if (changed_flags & (I40E_FLAG_DISABLE_FW_LLDP |
+	    I40E_FLAG_MULTIPLE_TRAFFIC_CLASSES))
+		reset_needed = I40E_PF_RESET_AND_REBUILD_FLAG;
+	if (changed_flags & (I40E_FLAG_VEB_STATS_ENABLED |
+	    I40E_FLAG_LEGACY_RX | I40E_FLAG_SOURCE_PRUNING_DISABLED))
+		reset_needed = BIT(__I40E_PF_RESET_REQUESTED);
 
 	/* Before we finalize any flag changes, we need to perform some
 	 * checks to ensure that the changes are supported and safe.
@@ -4940,7 +6565,8 @@
 	 * disable LLDP, however we _must_ not allow the user to enable/disable
 	 * LLDP with this flag on unsupported FW versions.
 	 */
-	if (changed_flags & I40E_FLAG_DISABLE_FW_LLDP) {
+	if (changed_flags & (I40E_FLAG_DISABLE_FW_LLDP |
+	    I40E_FLAG_MULTIPLE_TRAFFIC_CLASSES)) {
 		if (!(pf->hw.flags & I40E_HW_FLAG_FW_LLDP_STOPPABLE)) {
 			dev_warn(&pf->pdev->dev,
 				 "Device does not support changing FW LLDP\n");
@@ -4948,8 +6574,7 @@
 		}
 	}
 
-	if (((changed_flags & I40E_FLAG_RS_FEC) ||
-	     (changed_flags & I40E_FLAG_BASE_R_FEC)) &&
+	if (changed_flags & I40E_FLAG_RS_FEC &&
 	    pf->hw.device_id != I40E_DEV_ID_25G_SFP28 &&
 	    pf->hw.device_id != I40E_DEV_ID_25G_B) {
 		dev_warn(&pf->pdev->dev,
@@ -4957,6 +6582,14 @@
 		return -EOPNOTSUPP;
 	}
 
+	if (changed_flags & I40E_FLAG_BASE_R_FEC &&
+	    pf->hw.device_id != I40E_DEV_ID_25G_SFP28 &&
+	    pf->hw.device_id != I40E_DEV_ID_25G_B &&
+	    pf->hw.device_id != I40E_DEV_ID_KX_X722) {
+		dev_warn(&pf->pdev->dev,
+			 "Device does not support changing FEC configuration\n");
+		return -EOPNOTSUPP;
+	}
 	/* Process any additional changes needed as a result of flag changes.
 	 * The changed_flags value reflects the list of bits that were
 	 * changed in the code above.
@@ -5006,52 +6639,73 @@
 			dev_warn(&pf->pdev->dev, "Cannot change FEC config\n");
 	}
 
+	if ((changed_flags & I40E_FLAG_LINK_DOWN_ON_CLOSE_ENABLED) &&
+	    (orig_flags & I40E_FLAG_TOTAL_PORT_SHUTDOWN)) {
+		dev_err(&pf->pdev->dev,
+			"Setting link-down-on-close not supported on this port\n");
+		return -EOPNOTSUPP;
+	}
+
 	if ((changed_flags & new_flags &
 	     I40E_FLAG_LINK_DOWN_ON_CLOSE_ENABLED) &&
 	    (new_flags & I40E_FLAG_MFP_ENABLED))
 		dev_warn(&pf->pdev->dev,
 			 "Turning on link-down-on-close flag may affect other partitions\n");
 
-	if (changed_flags & I40E_FLAG_DISABLE_FW_LLDP) {
-		if (new_flags & I40E_FLAG_DISABLE_FW_LLDP) {
-			struct i40e_dcbx_config *dcbcfg;
+	if ((changed_flags & I40E_FLAG_DISABLE_FW_LLDP) &&
+	    orig_flags & I40E_FLAG_MULTIPLE_TRAFFIC_CLASSES) {
+		dev_warn(&pf->pdev->dev,
+			 "Cannot change FW LLDP setting, disable multiple-traffic-class to change this setting\n");
+		return -EINVAL;
+	}
 
+	if ((changed_flags & I40E_FLAG_VF_VLAN_PRUNE_DISABLE) &&
+	    pf->num_alloc_vfs) {
+		dev_warn(&pf->pdev->dev,
+			 "Changing vf-vlan-prune-disable flag while VF(s) are active is not supported");
+		return -EOPNOTSUPP;
+	}
+
+	if ((changed_flags & I40E_FLAG_DISABLE_FW_LLDP) ||
+	    (changed_flags & I40E_FLAG_MULTIPLE_TRAFFIC_CLASSES)) {
+		if ((new_flags & I40E_FLAG_DISABLE_FW_LLDP) &&
+		    !(new_flags & I40E_FLAG_MULTIPLE_TRAFFIC_CLASSES)) {
+#ifdef CONFIG_DCB
+			i40e_dcb_sw_default_config(pf, I40E_ETS_WILLING_MODE);
+#endif /* CONFIG_DCB */
+			i40e_aq_cfg_lldp_mib_change_event(&pf->hw, false, NULL);
+			i40e_aq_stop_lldp(&pf->hw, true, false, NULL);
+		} else if (new_flags &
+			   I40E_FLAG_MULTIPLE_TRAFFIC_CLASSES) {
+#ifdef CONFIG_DCB
+			i40e_dcb_sw_default_config(pf,
+						   I40E_ETS_NON_WILLING_MODE);
+#endif /* CONFIG_DCB */
+			i40e_aq_cfg_lldp_mib_change_event(&pf->hw, false, NULL);
 			i40e_aq_stop_lldp(&pf->hw, true, false, NULL);
-			i40e_aq_set_dcb_parameters(&pf->hw, true, NULL);
-			/* reset local_dcbx_config to default */
-			dcbcfg = &pf->hw.local_dcbx_config;
-			dcbcfg->etscfg.willing = 1;
-			dcbcfg->etscfg.maxtcs = 0;
-			dcbcfg->etscfg.tcbwtable[0] = 100;
-			for (i = 1; i < I40E_MAX_TRAFFIC_CLASS; i++)
-				dcbcfg->etscfg.tcbwtable[i] = 0;
-			for (i = 0; i < I40E_MAX_USER_PRIORITY; i++)
-				dcbcfg->etscfg.prioritytable[i] = 0;
-			dcbcfg->etscfg.tsatable[0] = I40E_IEEE_TSA_ETS;
-			dcbcfg->pfc.willing = 1;
-			dcbcfg->pfc.pfccap = I40E_MAX_TRAFFIC_CLASS;
+			new_flags &= ~(I40E_FLAG_DISABLE_FW_LLDP);
 		} else {
 			status = i40e_aq_start_lldp(&pf->hw, false, NULL);
-			if (status) {
+			if (status != I40E_SUCCESS) {
 				adq_err = pf->hw.aq.asq_last_status;
 				switch (adq_err) {
 				case I40E_AQ_RC_EEXIST:
 					dev_warn(&pf->pdev->dev,
 						 "FW LLDP agent is already running\n");
-					is_reset_needed = false;
+					reset_needed = 0;
 					break;
 				case I40E_AQ_RC_EPERM:
 					dev_warn(&pf->pdev->dev,
 						 "Device configuration forbids SW from starting the LLDP agent.\n");
-					return -EINVAL;
+					return (-EINVAL);
 				default:
 					dev_warn(&pf->pdev->dev,
-						 "Starting FW LLDP agent failed: error: %s, %s\n",
+						 "Starting FW LLDP agent failed with error: %s, %s\n",
 						 i40e_stat_str(&pf->hw,
-							       status),
+						 status),
 						 i40e_aq_str(&pf->hw,
-							     adq_err));
-					return -EINVAL;
+						 adq_err));
+					return (-EINVAL);
 				}
 			}
 		}
@@ -5067,11 +6721,14 @@
 	/* Issue reset to cause things to take effect, as additional bits
 	 * are added we will need to create a mask of bits requiring reset
 	 */
-	if (is_reset_needed)
-		i40e_do_reset(pf, BIT(__I40E_PF_RESET_REQUESTED), true);
+	if (reset_needed)
+		i40e_do_reset(pf, reset_needed, true);
 
 	return 0;
 }
+#endif /* HAVE_ETHTOOL_GET_SSET_COUNT */
+
+#ifdef ETHTOOL_GMODULEINFO
 
 /**
  * i40e_get_module_info - get (Q)SFP+ module type info
@@ -5081,6 +6738,7 @@
 static int i40e_get_module_info(struct net_device *netdev,
 				struct ethtool_modinfo *modinfo)
 {
+	i40e_status status;
 	struct i40e_netdev_priv *np = netdev_priv(netdev);
 	struct i40e_vsi *vsi = np->vsi;
 	struct i40e_pf *pf = vsi->back;
@@ -5088,8 +6746,7 @@
 	u32 sff8472_comp = 0;
 	u32 sff8472_swap = 0;
 	u32 sff8636_rev = 0;
-	i40e_status status;
-	u32 type = 0;
+	u8 type;
 
 	/* Check if firmware supports reading module EEPROM. */
 	if (!(hw->flags & I40E_HW_FLAG_AQ_PHY_ACCESS_CAPABLE)) {
@@ -5112,7 +6769,7 @@
 	case I40E_MODULE_TYPE_SFP:
 		status = i40e_aq_get_phy_register(hw,
 				I40E_AQ_PHY_REG_ACCESS_EXTERNAL_MODULE,
-				I40E_I2C_EEPROM_DEV_ADDR,
+				I40E_I2C_EEPROM_DEV_ADDR, true,
 				I40E_MODULE_SFF_8472_COMP,
 				&sff8472_comp, NULL);
 		if (status)
@@ -5120,7 +6777,7 @@
 
 		status = i40e_aq_get_phy_register(hw,
 				I40E_AQ_PHY_REG_ACCESS_EXTERNAL_MODULE,
-				I40E_I2C_EEPROM_DEV_ADDR,
+				I40E_I2C_EEPROM_DEV_ADDR, true,
 				I40E_MODULE_SFF_8472_SWAP,
 				&sff8472_swap, NULL);
 		if (status)
@@ -5133,26 +6790,21 @@
 			netdev_warn(vsi->netdev, "Module address swap to access page 0xA2 is not supported.\n");
 			modinfo->type = ETH_MODULE_SFF_8079;
 			modinfo->eeprom_len = ETH_MODULE_SFF_8079_LEN;
-		} else if (sff8472_comp == 0x00) {
+		} else if (sff8472_comp &&
+			   (sff8472_swap & I40E_MODULE_SFF_DIAG_CAPAB)) {
+			modinfo->type = ETH_MODULE_SFF_8472;
+			modinfo->eeprom_len = ETH_MODULE_SFF_8472_LEN;
+		} else {
 			/* Module is not SFF-8472 compliant */
 			modinfo->type = ETH_MODULE_SFF_8079;
 			modinfo->eeprom_len = ETH_MODULE_SFF_8079_LEN;
-		} else if (!(sff8472_swap & I40E_MODULE_SFF_DDM_IMPLEMENTED)) {
-			/* Module is SFF-8472 compliant but doesn't implement
-			 * Digital Diagnostic Monitoring (DDM).
-			 */
-			modinfo->type = ETH_MODULE_SFF_8079;
-			modinfo->eeprom_len = ETH_MODULE_SFF_8079_LEN;
-		} else {
-			modinfo->type = ETH_MODULE_SFF_8472;
-			modinfo->eeprom_len = ETH_MODULE_SFF_8472_LEN;
 		}
 		break;
 	case I40E_MODULE_TYPE_QSFP_PLUS:
 		/* Read from memory page 0. */
 		status = i40e_aq_get_phy_register(hw,
 				I40E_AQ_PHY_REG_ACCESS_EXTERNAL_MODULE,
-				0,
+				0, true,
 				I40E_MODULE_REVISION_ADDR,
 				&sff8636_rev, NULL);
 		if (status)
@@ -5172,7 +6824,7 @@
 		modinfo->eeprom_len = I40E_MODULE_QSFP_MAX_LEN;
 		break;
 	default:
-		netdev_err(vsi->netdev, "Module type unrecognized\n");
+		netdev_err(vsi->netdev, "SFP module type unrecognized or no SFP connector.\n");
 		return -EINVAL;
 	}
 	return 0;
@@ -5188,12 +6840,12 @@
 				  struct ethtool_eeprom *ee,
 				  u8 *data)
 {
+	i40e_status status;
 	struct i40e_netdev_priv *np = netdev_priv(netdev);
 	struct i40e_vsi *vsi = np->vsi;
 	struct i40e_pf *pf = vsi->back;
 	struct i40e_hw *hw = &pf->hw;
 	bool is_sfp = false;
-	i40e_status status;
 	u32 value = 0;
 	int i;
 
@@ -5223,31 +6875,160 @@
 
 		status = i40e_aq_get_phy_register(hw,
 				I40E_AQ_PHY_REG_ACCESS_EXTERNAL_MODULE,
-				addr, offset, &value, NULL);
+				addr, true, offset, &value, NULL);
 		if (status)
 			return -EIO;
 		data[i] = value;
 	}
 	return 0;
 }
+#endif /* ETHTOOL_GMODULEINFO */
 
+#ifdef ETHTOOL_GEEE
 static int i40e_get_eee(struct net_device *netdev, struct ethtool_eee *edata)
 {
-	return -EOPNOTSUPP;
+	struct i40e_netdev_priv *np = netdev_priv(netdev);
+	struct i40e_aq_get_phy_abilities_resp phy_cfg;
+	i40e_status status = I40E_SUCCESS;
+	struct i40e_vsi *vsi = np->vsi;
+	struct i40e_pf *pf = vsi->back;
+	struct i40e_hw *hw = &pf->hw;
+
+	/* Get initial PHY capabilities */
+	status = i40e_aq_get_phy_capabilities(hw, false, true, &phy_cfg, NULL);
+	if (status)
+		return -EAGAIN;
+
+	/* Check whether NIC configuration is compatible with Energy Efficient
+	 * Ethernet (EEE) mode.
+	 */
+	if (phy_cfg.eee_capability == 0)
+		return -EOPNOTSUPP;
+
+	edata->supported = SUPPORTED_Autoneg;
+	edata->lp_advertised = edata->supported;
+
+	/* Get current configuration */
+	status = i40e_aq_get_phy_capabilities(hw, false, false, &phy_cfg, NULL);
+	if (status)
+		return -EAGAIN;
+
+	edata->advertised = phy_cfg.eee_capability ? SUPPORTED_Autoneg : 0U;
+	edata->eee_enabled = !!edata->advertised;
+	edata->tx_lpi_enabled = pf->stats.tx_lpi_status;
+
+	edata->eee_active = pf->stats.tx_lpi_status && pf->stats.rx_lpi_status;
+
+	return 0;
+}
+#endif /* ETHTOOL_GEEE */
+
+#ifdef ETHTOOL_SEEE
+static int i40e_is_eee_param_supported(struct net_device *netdev,
+				       struct ethtool_eee *edata)
+{
+	struct i40e_netdev_priv *np = netdev_priv(netdev);
+	struct i40e_vsi *vsi = np->vsi;
+	struct i40e_pf *pf = vsi->back;
+	struct i40e_ethtool_not_used {
+		u32 value;
+		const char *name;
+	} param[] = {
+		{edata->advertised & ~SUPPORTED_Autoneg, "advertise"},
+		{edata->tx_lpi_timer, "tx-timer"},
+		{edata->tx_lpi_enabled != pf->stats.tx_lpi_status, "tx-lpi"}
+	};
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(param); i++) {
+		if (param[i].value) {
+			netdev_info(netdev,
+				    "EEE setting %s not supported\n",
+				    param[i].name);
+			return -EOPNOTSUPP;
+		}
+	}
+
+	return 0;
 }
 
 static int i40e_set_eee(struct net_device *netdev, struct ethtool_eee *edata)
 {
-	return -EOPNOTSUPP;
+	struct i40e_netdev_priv *np = netdev_priv(netdev);
+	struct i40e_aq_get_phy_abilities_resp abilities;
+	i40e_status status = I40E_SUCCESS;
+	struct i40e_aq_set_phy_config config;
+	struct i40e_vsi *vsi = np->vsi;
+	struct i40e_pf *pf = vsi->back;
+	struct i40e_hw *hw = &pf->hw;
+	__le16 eee_capability;
+
+	/* Deny parameters we don't support */
+	if (i40e_is_eee_param_supported(netdev, edata))
+		return -EOPNOTSUPP;
+
+	/* Get initial PHY capabilities */
+	status = i40e_aq_get_phy_capabilities(hw, false, true, &abilities,
+					      NULL);
+	if (status)
+		return -EAGAIN;
+
+	/* Check whether NIC configuration is compatible with Energy Efficient
+	 * Ethernet (EEE) mode.
+	 */
+	if (abilities.eee_capability == 0)
+		return -EOPNOTSUPP;
+
+	/* Cache initial EEE capability */
+	eee_capability = abilities.eee_capability;
+
+	/* Get current PHY configuration */
+	status = i40e_aq_get_phy_capabilities(hw, false, false, &abilities,
+					      NULL);
+	if (status)
+		return -EAGAIN;
+
+	/* Cache current PHY configuration */
+	config.phy_type = abilities.phy_type;
+	config.phy_type_ext = abilities.phy_type_ext;
+	config.link_speed = abilities.link_speed;
+	config.abilities = abilities.abilities |
+			   I40E_AQ_PHY_ENABLE_ATOMIC_LINK;
+	config.eeer = abilities.eeer_val;
+	config.low_power_ctrl = abilities.d3_lpan;
+	config.fec_config = abilities.fec_cfg_curr_mod_ext_info &
+			    I40E_AQ_PHY_FEC_CONFIG_MASK;
+
+	/* Set desired EEE state */
+	if (edata->eee_enabled) {
+		config.eee_capability = eee_capability;
+		config.eeer |= cpu_to_le32(I40E_PRTPM_EEER_TX_LPI_EN_MASK);
+	} else {
+		config.eee_capability = 0;
+		config.eeer &= ~cpu_to_le32(I40E_PRTPM_EEER_TX_LPI_EN_MASK);
+	}
+
+	/* Apply modified PHY configuration */
+	status = i40e_aq_set_phy_config(hw, &config, NULL);
+	if (status)
+		return -EAGAIN;
+
+	return 0;
 }
+#endif /* ETHTOOL_SEEE */
 
 static const struct ethtool_ops i40e_ethtool_recovery_mode_ops = {
+	.get_drvinfo		= i40e_get_drvinfo,
 	.set_eeprom		= i40e_set_eeprom,
 	.get_eeprom_len		= i40e_get_eeprom_len,
 	.get_eeprom		= i40e_get_eeprom,
 };
 
 static const struct ethtool_ops i40e_ethtool_ops = {
+#ifndef ETHTOOL_GLINKSETTINGS
+	.get_settings		= i40e_get_settings,
+	.set_settings		= i40e_set_settings,
+#endif
 	.get_drvinfo		= i40e_get_drvinfo,
 	.get_regs_len		= i40e_get_regs_len,
 	.get_regs		= i40e_get_regs,
@@ -5264,35 +7045,124 @@
 	.set_pauseparam		= i40e_set_pauseparam,
 	.get_msglevel		= i40e_get_msglevel,
 	.set_msglevel		= i40e_set_msglevel,
+#ifndef HAVE_NDO_SET_FEATURES
+	.get_rx_csum		= i40e_get_rx_csum,
+	.set_rx_csum		= i40e_set_rx_csum,
+	.get_tx_csum		= i40e_get_tx_csum,
+	.set_tx_csum		= i40e_set_tx_csum,
+	.get_sg			= ethtool_op_get_sg,
+	.set_sg			= ethtool_op_set_sg,
+	.get_tso		= ethtool_op_get_tso,
+	.set_tso		= i40e_set_tso,
+#ifdef ETHTOOL_GFLAGS
+	.get_flags		= ethtool_op_get_flags,
+	.set_flags		= i40e_set_flags,
+#endif
+#endif /* HAVE_NDO_SET_FEATURES */
+#ifdef ETHTOOL_GRXRINGS
 	.get_rxnfc		= i40e_get_rxnfc,
 	.set_rxnfc		= i40e_set_rxnfc,
+#ifdef ETHTOOL_SRXNTUPLE
+	.set_rx_ntuple		= i40e_set_rx_ntuple,
+#endif
+#endif
+#ifndef HAVE_ETHTOOL_GET_SSET_COUNT
+	.self_test_count	= i40e_diag_test_count,
+#endif /* HAVE_ETHTOOL_GET_SSET_COUNT */
 	.self_test		= i40e_diag_test,
 	.get_strings		= i40e_get_strings,
+#ifndef HAVE_RHEL6_ETHTOOL_OPS_EXT_STRUCT
+#ifdef ETHTOOL_GEEE
 	.get_eee		= i40e_get_eee,
+#endif /* ETHTOOL_GEEE */
+#ifdef ETHTOOL_SEEE
 	.set_eee		= i40e_set_eee,
+#endif /* ETHTOOL_SEEE */
+#ifdef HAVE_ETHTOOL_SET_PHYS_ID
 	.set_phys_id		= i40e_set_phys_id,
+#else
+	.phys_id		= i40e_phys_id,
+#endif /* HAVE_ETHTOOL_SET_PHYS_ID */
+#endif /* HAVE_RHEL6_ETHTOOL_OPS_EXT_STRUCT */
+#ifndef HAVE_ETHTOOL_GET_SSET_COUNT
+	.get_stats_count	= i40e_get_stats_count,
+#else /* HAVE_ETHTOOL_GET_SSET_COUNT */
 	.get_sset_count		= i40e_get_sset_count,
+	.get_priv_flags		= i40e_get_priv_flags,
+	.set_priv_flags		= i40e_set_priv_flags,
+#endif /* HAVE_ETHTOOL_GET_SSET_COUNT */
 	.get_ethtool_stats	= i40e_get_ethtool_stats,
+#ifdef HAVE_ETHTOOL_GET_PERM_ADDR
+	.get_perm_addr		= ethtool_op_get_perm_addr,
+#endif
+#ifdef HAVE_ETHTOOL_COALESCE_PARAMS_SUPPORT
+	.supported_coalesce_params = ETHTOOL_COALESCE_USECS |
+				     ETHTOOL_COALESCE_MAX_FRAMES_IRQ |
+				     ETHTOOL_COALESCE_USE_ADAPTIVE |
+				     ETHTOOL_COALESCE_RX_USECS_HIGH |
+				     ETHTOOL_COALESCE_TX_USECS_HIGH,
+#endif
 	.get_coalesce		= i40e_get_coalesce,
 	.set_coalesce		= i40e_set_coalesce,
+#ifndef HAVE_RHEL6_ETHTOOL_OPS_EXT_STRUCT
+#if defined(ETHTOOL_GRSSH) && defined(ETHTOOL_SRSSH)
 	.get_rxfh_key_size	= i40e_get_rxfh_key_size,
 	.get_rxfh_indir_size	= i40e_get_rxfh_indir_size,
 	.get_rxfh		= i40e_get_rxfh,
 	.set_rxfh		= i40e_set_rxfh,
+#endif /* ETHTOOL_GRSSH && ETHTOOL_SRSSH */
+#ifdef ETHTOOL_SCHANNELS
 	.get_channels		= i40e_get_channels,
 	.set_channels		= i40e_set_channels,
+#endif
+#ifdef ETHTOOL_GMODULEINFO
 	.get_module_info	= i40e_get_module_info,
 	.get_module_eeprom	= i40e_get_module_eeprom,
+#endif
+#ifdef HAVE_ETHTOOL_GET_TS_INFO
 	.get_ts_info		= i40e_get_ts_info,
-	.get_priv_flags		= i40e_get_priv_flags,
-	.set_priv_flags		= i40e_set_priv_flags,
+#endif /* HAVE_ETHTOOL_GET_TS_INFO */
+#ifdef ETHTOOL_PERQUEUE
 	.get_per_queue_coalesce	= i40e_get_per_queue_coalesce,
 	.set_per_queue_coalesce	= i40e_set_per_queue_coalesce,
-	.get_link_ksettings	= i40e_get_link_ksettings,
-	.set_link_ksettings	= i40e_set_link_ksettings,
+#endif /* ETHTOOL_PERQUEUE */
+#endif /* HAVE_RHEL6_ETHTOOL_OPS_EXT_STRUCT */
+#ifdef ETHTOOL_GLINKSETTINGS
+	.get_link_ksettings = i40e_get_link_ksettings,
+	.set_link_ksettings = i40e_set_link_ksettings,
+#endif /* ETHTOOL_GLINKSETTINGS */
+#ifdef ETHTOOL_GFECPARAM
 	.get_fecparam = i40e_get_fec_param,
 	.set_fecparam = i40e_set_fec_param,
+#endif /* ETHTOOL_GFECPARAM */
+#ifdef HAVE_DDP_PROFILE_UPLOAD_SUPPORT
 	.flash_device = i40e_ddp_flash,
+#endif /* DDP_PROFILE_UPLOAD_SUPPORT */
+};
+
+#ifdef HAVE_RHEL6_ETHTOOL_OPS_EXT_STRUCT
+static const struct ethtool_ops_ext i40e_ethtool_ops_ext = {
+	.size			= sizeof(struct ethtool_ops_ext),
+	.get_ts_info		= i40e_get_ts_info,
+	.set_phys_id		= i40e_set_phys_id,
+	.get_channels		= i40e_get_channels,
+	.set_channels		= i40e_set_channels,
+#if defined(ETHTOOL_GRSSH) && defined(ETHTOOL_SRSSH)
+	.get_rxfh_key_size	= i40e_get_rxfh_key_size,
+	.get_rxfh_indir_size	= i40e_get_rxfh_indir_size,
+	.get_rxfh		= i40e_get_rxfh,
+	.set_rxfh		= i40e_set_rxfh,
+#endif /* ETHTOOL_GRSSH && ETHTOOL_SRSSH */
+#ifdef ETHTOOL_GEEE
+	.get_eee		= i40e_get_eee,
+#endif /* ETHTOOL_GEEE */
+#ifdef ETHTOOL_SEEE
+	.set_eee		= i40e_set_eee,
+#endif /* ETHTOOL_SEEE */
+#ifdef ETHTOOL_GMODULEINFO
+	.get_module_info	= i40e_get_module_info,
+	.get_module_eeprom	= i40e_get_module_eeprom,
+#endif
 };
 
 void i40e_set_ethtool_ops(struct net_device *netdev)
@@ -5300,8 +7170,23 @@
 	struct i40e_netdev_priv *np = netdev_priv(netdev);
 	struct i40e_pf		*pf = np->vsi->back;
 
-	if (!test_bit(__I40E_RECOVERY_MODE, pf->state))
+	if (test_bit(__I40E_RECOVERY_MODE, pf->state)) {
+		netdev->ethtool_ops = &i40e_ethtool_recovery_mode_ops;
+	} else {
 		netdev->ethtool_ops = &i40e_ethtool_ops;
-	else
+		set_ethtool_ops_ext(netdev, &i40e_ethtool_ops_ext);
+	}
+}
+#else
+void i40e_set_ethtool_ops(struct net_device *netdev)
+{
+	struct i40e_netdev_priv *np = netdev_priv(netdev);
+	struct i40e_pf		*pf = np->vsi->back;
+
+	if (test_bit(__I40E_RECOVERY_MODE, pf->state))
 		netdev->ethtool_ops = &i40e_ethtool_recovery_mode_ops;
+	else
+		netdev->ethtool_ops = &i40e_ethtool_ops;
 }
+#endif /* HAVE_RHEL6_ETHTOOL_OPS_EXT_STRUCT */
+#endif /* SIOCETHTOOL */
Binary files linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_ethtool.o and linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_ethtool.o differ
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_ethtool_stats.h linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_ethtool_stats.h
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_ethtool_stats.h	1969-12-31 19:00:00.000000000 -0500
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_ethtool_stats.h	2024-05-13 03:58:25.372491940 -0400
@@ -0,0 +1,292 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
+
+/* ethtool statistics helpers */
+
+/**
+ * struct i40e_stats - definition for an ethtool statistic
+ * @stat_string: statistic name to display in ethtool -S output
+ * @sizeof_stat: the sizeof() the stat, must be no greater than sizeof(u64)
+ * @stat_offset: offsetof() the stat from a base pointer
+ *
+ * This structure defines a statistic to be added to the ethtool stats buffer.
+ * It defines a statistic as offset from a common base pointer. Stats should
+ * be defined in constant arrays using the I40E_STAT macro, with every element
+ * of the array using the same _type for calculating the sizeof_stat and
+ * stat_offset.
+ *
+ * The sizeof_stat is expected to be sizeof(u8), sizeof(u16), sizeof(u32) or
+ * sizeof(u64). Other sizes are not expected and will produce a WARN_ONCE from
+ * the i40e_add_ethtool_stat() helper function.
+ *
+ * The stat_string is interpreted as a format string, allowing formatted
+ * values to be inserted while looping over multiple structures for a given
+ * statistics array. Thus, every statistic string in an array should have the
+ * same type and number of format specifiers, to be formatted by variadic
+ * arguments to the i40e_add_stat_string() helper function.
+ **/
+struct i40e_stats {
+	char stat_string[ETH_GSTRING_LEN];
+	int sizeof_stat;
+	int stat_offset;
+};
+
+/* Helper macro to define an i40e_stat structure with proper size and type.
+ * Use this when defining constant statistics arrays. Note that @_type expects
+ * only a type name and is used multiple times.
+ */
+#define I40E_STAT(_type, _name, _stat) { \
+	.stat_string = _name, \
+	.sizeof_stat = sizeof_field(_type, _stat), \
+	.stat_offset = offsetof(_type, _stat) \
+}
+
+/* Helper macro for defining some statistics directly copied from the netdev
+ * stats structure.
+ */
+#ifdef HAVE_NDO_GET_STATS64
+#define I40E_NETDEV_STAT(_net_stat) \
+	I40E_STAT(struct rtnl_link_stats64, #_net_stat, _net_stat)
+#else
+#define I40E_NETDEV_STAT(_net_stat) \
+	I40E_STAT(struct net_device_stats, #_net_stat, _net_stat)
+#endif
+
+/* Helper macro for defining some statistics related to queues */
+#define I40E_QUEUE_STAT(_name, _stat) \
+	I40E_STAT(struct i40e_ring, _name, _stat)
+
+/* Stats associated with a Tx or Rx ring */
+static const struct i40e_stats i40e_gstrings_queue_stats[] = {
+	I40E_QUEUE_STAT("%s-%u.packets", stats.packets),
+	I40E_QUEUE_STAT("%s-%u.bytes", stats.bytes),
+};
+
+#ifdef HAVE_XDP_SUPPORT
+/* Stats associated with Rx ring's XDP prog */
+static const struct i40e_stats i40e_gstrings_rx_queue_xdp_stats[] = {
+	I40E_QUEUE_STAT("%s-%u.xdp.pass", xdp_stats.xdp_pass),
+	I40E_QUEUE_STAT("%s-%u.xdp.drop", xdp_stats.xdp_drop),
+	I40E_QUEUE_STAT("%s-%u.xdp.tx", xdp_stats.xdp_tx),
+	I40E_QUEUE_STAT("%s-%u.xdp.unknown", xdp_stats.xdp_unknown),
+	I40E_QUEUE_STAT("%s-%u.xdp.redirect", xdp_stats.xdp_redirect),
+	I40E_QUEUE_STAT("%s-%u.xdp.redirect_fail", xdp_stats.xdp_redirect_fail),
+};
+#endif
+
+/**
+ * i40e_add_one_ethtool_stat - copy the stat into the supplied buffer
+ * @data: location to store the stat value
+ * @pointer: basis for where to copy from
+ * @stat: the stat definition
+ *
+ * Copies the stat data defined by the pointer and stat structure pair into
+ * the memory supplied as data. Used to implement i40e_add_ethtool_stats and
+ * i40e_add_queue_stats. If the pointer is null, data will be zero'd.
+ */
+static void
+i40e_add_one_ethtool_stat(u64 *data, void *pointer,
+			  const struct i40e_stats *stat)
+{
+	char *p;
+
+	if (!pointer) {
+		/* ensure that the ethtool data buffer is zero'd for any stats
+		 * which don't have a valid pointer.
+		 */
+		*data = 0;
+		return;
+	}
+
+	p = (char *)pointer + stat->stat_offset;
+	switch (stat->sizeof_stat) {
+	case sizeof(u64):
+		*data = *((u64 *)p);
+		break;
+	case sizeof(u32):
+		*data = *((u32 *)p);
+		break;
+	case sizeof(u16):
+		*data = *((u16 *)p);
+		break;
+	case sizeof(u8):
+		*data = *((u8 *)p);
+		break;
+	default:
+		WARN_ONCE(1, "unexpected stat size for %s",
+			  stat->stat_string);
+		*data = 0;
+	}
+}
+
+/**
+ * __i40e_add_ethtool_stats - copy stats into the ethtool supplied buffer
+ * @data: ethtool stats buffer
+ * @pointer: location to copy stats from
+ * @stats: array of stats to copy
+ * @size: the size of the stats definition
+ *
+ * Copy the stats defined by the stats array using the pointer as a base into
+ * the data buffer supplied by ethtool. Updates the data pointer to point to
+ * the next empty location for successive calls to __i40e_add_ethtool_stats.
+ * If pointer is null, set the data values to zero and update the pointer to
+ * skip these stats.
+ **/
+static void
+__i40e_add_ethtool_stats(u64 **data, void *pointer,
+			 const struct i40e_stats stats[],
+			 const unsigned int size)
+{
+	unsigned int i;
+
+	for (i = 0; i < size; i++)
+		i40e_add_one_ethtool_stat((*data)++, pointer, &stats[i]);
+}
+
+/**
+ * i40e_add_ethtool_stats - copy stats into ethtool supplied buffer
+ * @data: ethtool stats buffer
+ * @pointer: location where stats are stored
+ * @stats: static const array of stat definitions
+ *
+ * Macro to ease the use of __i40e_add_ethtool_stats by taking a static
+ * constant stats array and passing the ARRAY_SIZE(). This avoids typos by
+ * ensuring that we pass the size associated with the given stats array.
+ *
+ * The parameter stats is evaluated twice, so parameters with side effects
+ * should be avoided.
+ **/
+#define i40e_add_ethtool_stats(data, pointer, stats) \
+	__i40e_add_ethtool_stats(data, pointer, stats, ARRAY_SIZE(stats))
+
+/**
+ * i40e_add_queue_stats - copy queue statistics into supplied buffer
+ * @data: ethtool stats buffer
+ * @ring: the ring to copy
+ *
+ * Queue statistics must be copied while protected by
+ * u64_stats_fetch_begin_irq, so we can't directly use i40e_add_ethtool_stats.
+ * Assumes that queue stats are defined in i40e_gstrings_queue_stats. If the
+ * ring pointer is null, zero out the queue stat values and update the data
+ * pointer. Otherwise safely copy the stats from the ring into the supplied
+ * buffer and update the data pointer when finished.
+ *
+ * This function expects to be called while under rcu_read_lock().
+ **/
+static void
+i40e_add_queue_stats(u64 **data, struct i40e_ring *ring)
+{
+	const unsigned int size = ARRAY_SIZE(i40e_gstrings_queue_stats);
+	const struct i40e_stats *stats = i40e_gstrings_queue_stats;
+#ifdef HAVE_NDO_GET_STATS64
+	unsigned int start;
+#endif
+	unsigned int i;
+
+	/* To avoid invalid statistics values, ensure that we keep retrying
+	 * the copy until we get a consistent value according to
+	 * u64_stats_fetch_retry_irq. But first, make sure our ring is
+	 * non-null before attempting to access its syncp.
+	 */
+#ifdef HAVE_NDO_GET_STATS64
+	do {
+		start = !ring ? 0 : u64_stats_fetch_begin_irq(&ring->syncp);
+		for (i = 0; i < size; i++)
+			i40e_add_one_ethtool_stat(&(*data)[i], ring, &stats[i]);
+	} while (ring && u64_stats_fetch_retry_irq(&ring->syncp, start));
+#else
+	for (i = 0; i < size; i++)
+		i40e_add_one_ethtool_stat(&(*data)[i], ring, &stats[i]);
+#endif
+
+	/* Once we successfully copy the stats in, update the data pointer */
+	*data += size;
+}
+
+#ifdef HAVE_XDP_SUPPORT
+/**
+ * i40e_add_rx_queue_xdp_stats - copy XDP statistics into supplied buffer
+ * @data: ethtool stats buffer
+ * @rx_ring: the rx ring to copy
+ *
+ * RX queue XDP statistics must be copied while protected by
+ * u64_stats_fetch_begin_irq, so we can't directly use i40e_add_ethtool_stats.
+ * Assumes that queue stats are defined in i40e_gstrings_rx_queue_xdp_stats. If
+ * the ring pointer is null, zero out the queue stat values and update the data
+ * pointer. Otherwise safely copy the stats from the ring into the supplied
+ * buffer and update the data pointer when finished.
+ *
+ * This function expects to be called while under rcu_read_lock().
+ **/
+static void
+i40e_add_rx_queue_xdp_stats(u64 **data, struct i40e_ring *rx_ring)
+{
+	const unsigned int xdp_size =
+		ARRAY_SIZE(i40e_gstrings_rx_queue_xdp_stats);
+	const struct i40e_stats *xdp_stats = i40e_gstrings_rx_queue_xdp_stats;
+#ifdef HAVE_NDO_GET_STATS64
+	unsigned int start;
+#endif
+	unsigned int i;
+
+	/* To avoid invalid statistics values, ensure that we keep retrying
+	 * the copy until we get a consistent value according to
+	 * u64_stats_fetch_retry_irq. But first, make sure our ring is
+	 * non-null before attempting to access its syncp.
+	 */
+#ifdef HAVE_NDO_GET_STATS64
+	do {
+		start = !rx_ring ? 0 :
+			u64_stats_fetch_begin_irq(&rx_ring->syncp);
+#endif
+	for (i = 0; i < xdp_size; i++) {
+		i40e_add_one_ethtool_stat(&(*data)[i], rx_ring,
+					  &xdp_stats[i]);
+	}
+#ifdef HAVE_NDO_GET_STATS64
+	} while (rx_ring && u64_stats_fetch_retry_irq(&rx_ring->syncp, start));
+#endif
+
+	/* Once we successfully copy the stats in, update the data pointer */
+	*data += xdp_size;
+}
+#endif
+
+/**
+ * __i40e_add_stat_strings - copy stat strings into ethtool buffer
+ * @p: ethtool supplied buffer
+ * @stats: stat definitions array
+ * @size: size of the stats array
+ *
+ * Format and copy the strings described by stats into the buffer pointed at
+ * by p.
+ **/
+static void __i40e_add_stat_strings(u8 **p, const struct i40e_stats stats[],
+				    const unsigned int size, ...)
+{
+	unsigned int i;
+
+	for (i = 0; i < size; i++) {
+		va_list args;
+
+		va_start(args, size);
+		vsnprintf((char *)*p, ETH_GSTRING_LEN, stats[i].stat_string, args);
+		*p += ETH_GSTRING_LEN;
+		va_end(args);
+	}
+}
+
+/**
+ * i40e_add_stat_strings - copy stat strings into ethtool buffer
+ * @p: ethtool supplied buffer
+ * @stats: stat definitions array
+ *
+ * Format and copy the strings described by the const static stats value into
+ * the buffer pointed at by p.
+ *
+ * The parameter stats is evaluated twice, so parameters with side effects
+ * should be avoided. Additionally, stats must be an array such that
+ * ARRAY_SIZE can be called on it.
+ **/
+#define i40e_add_stat_strings(p, stats, ...) \
+	__i40e_add_stat_strings(p, stats, ARRAY_SIZE(stats), ## __VA_ARGS__)
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_filters.c linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_filters.c
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_filters.c	1969-12-31 19:00:00.000000000 -0500
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_filters.c	2024-05-13 03:58:25.372491940 -0400
@@ -0,0 +1,40 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
+
+#include "i40e_filters.h"
+
+/**
+ * __i40e_del_filter - Remove a specific filter from the VSI
+ * @vsi: VSI to remove from
+ * @f: the filter to remove from the list
+ *
+ * This function should be called instead of i40e_del_filter only if you know
+ * the exact filter you will remove already, such as via i40e_find_filter or
+ * i40e_find_mac.
+ *
+ * NOTE: This function is expected to be called with mac_filter_hash_lock
+ * being held.
+ * ANOTHER NOTE: This function MUST be called from within the context of
+ * the "safe" variants of any list iterators, e.g. list_for_each_entry_safe()
+ * instead of list_for_each_entry().
+ **/
+void __i40e_del_filter(struct i40e_vsi *vsi, struct i40e_mac_filter *f)
+{
+	if (!f)
+		return;
+
+	/* If the filter was never added to firmware then we can just delete it
+	 * directly and we don't want to set the status to remove or else an
+	 * admin queue command will unnecessarily fire.
+	 */
+	if (f->state == I40E_FILTER_FAILED || f->state == I40E_FILTER_NEW) {
+		hash_del(&f->hlist);
+		kfree(f);
+	} else {
+		f->state = I40E_FILTER_REMOVE;
+	}
+
+	vsi->flags |= I40E_VSI_FLAG_FILTER_CHANGED;
+	set_bit(__I40E_MACVLAN_SYNC_PENDING, vsi->back->state);
+}
+
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_filters.h linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_filters.h
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_filters.h	1969-12-31 19:00:00.000000000 -0500
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_filters.h	2024-05-13 03:58:25.372491940 -0400
@@ -0,0 +1,11 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
+
+#ifndef _I40E_FILTERS_H_
+#define _I40E_FILTERS_H_
+
+#include "i40e.h"
+
+void __i40e_del_filter(struct i40e_vsi *vsi, struct i40e_mac_filter *f);
+
+#endif /* _I40E_FILTERS_H_ */
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e.h linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e.h
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e.h	2024-05-10 01:26:45.309079402 -0400
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e.h	2024-05-13 03:58:25.372491940 -0400
@@ -1,5 +1,5 @@
 /* SPDX-License-Identifier: GPL-2.0 */
-/* Copyright(c) 2013 - 2018 Intel Corporation. */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
 
 #ifndef _I40E_H_
 #define _I40E_H_
@@ -13,10 +13,10 @@
 #include <linux/aer.h>
 #include <linux/netdevice.h>
 #include <linux/ioport.h>
-#include <linux/iommu.h>
+#include <linux/crash_dump.h>
 #include <linux/slab.h>
 #include <linux/list.h>
-#include <linux/hashtable.h>
+#include <linux/hash.h>
 #include <linux/string.h>
 #include <linux/in.h>
 #include <linux/ip.h>
@@ -24,37 +24,56 @@
 #include <linux/pkt_sched.h>
 #include <linux/ipv6.h>
 #include <net/checksum.h>
+#include <net/ipv6.h>
 #include <net/ip6_checksum.h>
+#ifdef SIOCETHTOOL
 #include <linux/ethtool.h>
+#endif
 #include <linux/if_vlan.h>
-#include <linux/if_macvlan.h>
 #include <linux/if_bridge.h>
+#include "kcompat.h"
+#ifdef HAVE_IOMMU_PRESENT
+#include <linux/iommu.h>
+#endif
+#ifdef HAVE_SCTP
+#include <linux/sctp.h>
+#endif
+#ifdef HAVE_PTP_1588_CLOCK
 #include <linux/clocksource.h>
 #include <linux/net_tstamp.h>
 #include <linux/ptp_clock_kernel.h>
+#endif /* HAVE_PTP_1588_CLOCK */
+#ifdef __TC_MQPRIO_MODE_MAX
 #include <net/pkt_cls.h>
-#include <net/tc_act/tc_gact.h>
-#include <net/tc_act/tc_mirred.h>
-#include <net/xdp_sock.h>
+#endif
 #include "i40e_type.h"
 #include "i40e_prototype.h"
 #include "i40e_client.h"
-#include <linux/avf/virtchnl.h>
+#include "virtchnl.h"
 #include "i40e_virtchnl_pf.h"
 #include "i40e_txrx.h"
 #include "i40e_dcb.h"
 
+#ifdef HAVE_XDP_SUPPORT
+#include <linux/bpf_trace.h>
+#endif
+
 /* Useful i40e defaults */
 #define I40E_MAX_VEB			16
 
 #define I40E_MAX_NUM_DESCRIPTORS	4096
 #define I40E_MAX_CSR_SPACE		(4 * 1024 * 1024 - 64 * 1024)
 #define I40E_DEFAULT_NUM_DESCRIPTORS	512
+#define L4_MODE_UDP 0
+#define L4_MODE_TCP 1
+#define L4_MODE_BOTH 2
+#define L4_MODE_DISABLED -1
+bool i40e_is_l4mode_enabled(void);
 #define I40E_REQ_DESCRIPTOR_MULTIPLE	32
 #define I40E_MIN_NUM_DESCRIPTORS	64
 #define I40E_MIN_MSIX			2
 #define I40E_DEFAULT_NUM_VMDQ_VSI	8 /* max 256 VSIs */
-#define I40E_MIN_VSI_ALLOC		83 /* LAN, ATR, FCOE, 64 VF */
+#define I40E_MIN_VSI_ALLOC		83 /* LAN, ATR, FCOE, 64 VF, 16 VMDQ */
 /* max 16 qps */
 #define i40e_default_queues_per_vmdq(pf) \
 		(((pf)->hw_features & I40E_HW_RSS_AQ_CAPABLE) ? 4 : 1)
@@ -67,7 +86,13 @@
 #define I40E_FDIR_RING_COUNT		32
 #define I40E_MAX_AQ_BUF_SIZE		4096
 #define I40E_AQ_LEN			256
+#define I40E_MIN_ARQ_LEN		1
+#define I40E_MIN_ASQ_LEN		2
 #define I40E_AQ_WORK_LIMIT		66 /* max number of VFs + a little */
+/*
+ * If I40E_MAX_USER_PRIORITY is updated please also update
+ * I40E_CLIENT_MAX_USER_PRIORITY in i40e_client.h and i40evf_client.h
+ */
 #define I40E_MAX_USER_PRIORITY		8
 #define I40E_DEFAULT_TRAFFIC_CLASS	BIT(0)
 #define I40E_DEFAULT_MSG_ENABLE		4
@@ -97,7 +122,7 @@
 #define I40E_CURRENT_NVM_VERSION_LO	0x40
 
 #define I40E_RX_DESC(R, i)	\
-	(&(((union i40e_32byte_rx_desc *)((R)->desc))[i]))
+	(&(((union i40e_rx_desc *)((R)->desc))[i]))
 #define I40E_TX_DESC(R, i)	\
 	(&(((struct i40e_tx_desc *)((R)->desc))[i]))
 #define I40E_TX_CTXTDESC(R, i)	\
@@ -110,8 +135,7 @@
 
 /* BW rate limiting */
 #define I40E_BW_CREDIT_DIVISOR		50 /* 50Mbps per BW credit */
-#define I40E_BW_MBPS_DIVISOR		125000 /* rate / (1000000 / 8) Mbps */
-#define I40E_MAX_BW_INACTIVE_ACCUM	4 /* accumulate 4 credits max */
+#define I40E_MAX_BW_INACTIVE_ACCUM	4  /* accumulate 4 credits max */
 
 /* driver state flags */
 enum i40e_state_t {
@@ -129,19 +153,23 @@
 	__I40E_RESET_INTR_RECEIVED,
 	__I40E_REINIT_REQUESTED,
 	__I40E_PF_RESET_REQUESTED,
+	__I40E_PF_RESET_AND_REBUILD_REQUESTED,
 	__I40E_CORE_RESET_REQUESTED,
 	__I40E_GLOBAL_RESET_REQUESTED,
+	__I40E_EMP_RESET_REQUESTED,
 	__I40E_EMP_RESET_INTR_RECEIVED,
 	__I40E_SUSPENDED,
-	__I40E_PTP_TX_IN_PROGRESS,
 	__I40E_BAD_EEPROM,
+	__I40E_DEBUG_MODE,
 	__I40E_DOWN_REQUESTED,
 	__I40E_FD_FLUSH_REQUESTED,
 	__I40E_FD_ATR_AUTO_DISABLED,
 	__I40E_FD_SB_AUTO_DISABLED,
 	__I40E_RESET_FAILED,
 	__I40E_PORT_SUSPENDED,
+	__I40E_PTP_TX_IN_PROGRESS,
 	__I40E_VF_DISABLE,
+	__I40E_RECOVERY_MODE,
 	__I40E_MACVLAN_SYNC_PENDING,
 	__I40E_UDP_FILTER_SYNC_PENDING,
 	__I40E_TEMP_LINK_POLLING,
@@ -149,13 +177,15 @@
 	__I40E_CLIENT_L2_CHANGE,
 	__I40E_CLIENT_RESET,
 	__I40E_VIRTCHNL_OP_PENDING,
-	__I40E_RECOVERY_MODE,
+	__I40E_VFS_RELEASING,
 	__I40E_VF_RESETS_DISABLED,	/* disable resets during i40e_remove */
 	/* This must be last as it determines the size of the BITMAP */
 	__I40E_STATE_SIZE__,
 };
 
 #define I40E_PF_RESET_FLAG	BIT_ULL(__I40E_PF_RESET_REQUESTED)
+#define I40E_PF_RESET_AND_REBUILD_FLAG	\
+	BIT_ULL(__I40E_PF_RESET_AND_REBUILD_REQUESTED)
 
 /* VSI state flags */
 enum i40e_vsi_state_t {
@@ -165,6 +195,7 @@
 	__I40E_VSI_OVERFLOW_PROMISC,
 	__I40E_VSI_REINIT_REQUESTED,
 	__I40E_VSI_DOWN_REQUESTED,
+	__I40E_VSI_RELEASING,
 	/* This must be last as it determines the size of the BITMAP */
 	__I40E_VSI_STATE_SIZE__,
 };
@@ -177,7 +208,6 @@
 
 struct i40e_lump_tracking {
 	u16 num_entries;
-	u16 search_hint;
 	u16 list[0];
 #define I40E_PILE_VALID_BIT  0x8000
 #define I40E_IWARP_IRQ_PILE_ID  (I40E_PILE_VALID_BIT - 2)
@@ -185,6 +215,14 @@
 
 #define I40E_DEFAULT_ATR_SAMPLE_RATE	20
 #define I40E_FDIR_MAX_RAW_PACKET_SIZE	512
+#define I40E_TCPIP_DUMMY_PACKET_LEN	54
+#define I40E_TCPIP6_DUMMY_PACKET_LEN	74
+#define I40E_SCTPIP_DUMMY_PACKET_LEN	46
+#define I40E_SCTPIP6_DUMMY_PACKET_LEN	66
+#define I40E_UDPIP_DUMMY_PACKET_LEN	42
+#define I40E_UDPIP6_DUMMY_PACKET_LEN	62
+#define I40E_IP_DUMMY_PACKET_LEN	34
+#define I40E_IP6_DUMMY_PACKET_LEN	54
 #define I40E_FDIR_BUFFER_FULL_MARGIN	10
 #define I40E_FDIR_BUFFER_HEAD_ROOM	32
 #define I40E_FDIR_BUFFER_HEAD_ROOM_FOR_ATR (I40E_FDIR_BUFFER_HEAD_ROOM * 4)
@@ -207,27 +245,43 @@
 #define I40E_FD_ATR_TUNNEL_STAT_IDX(pf_id) \
 			(I40E_FD_STAT_PF_IDX(pf_id) + I40E_FD_STAT_ATR_TUNNEL)
 
+/* get PTP pins for ioctl */
+#define SIOCGPINS	(SIOCDEVPRIVATE + 0)
+/* set PTP pins for ioctl */
+#define SIOCSPINS	(SIOCDEVPRIVATE + 1)
+
 /* The following structure contains the data parsed from the user-defined
  * field of the ethtool_rx_flow_spec structure.
  */
 struct i40e_rx_flow_userdef {
+	bool cloud_filter;
+	bool tenant_id_valid;
+	u32 tenant_id;
+	bool tunnel_type_valid;
+	u8 tunnel_type;
 	bool flex_filter;
 	u16 flex_word;
 	u16 flex_offset;
+	bool outer_ip;
 };
 
 struct i40e_fdir_filter {
 	struct hlist_node fdir_node;
-	/* filter ipnut set */
+	/* filter input set */
 	u8 flow_type;
-	u8 ip4_proto;
+	u8 ipl4_proto;
 	/* TX packet view of src and dst */
 	__be32 dst_ip;
 	__be32 src_ip;
+	__be32 dst_ip6[4];
+	__be32 src_ip6[4];
+
 	__be16 src_port;
 	__be16 dst_port;
 	__be32 sctp_v_tag;
 
+	__be16 vlan_etype;
+	__be16 vlan_tag;
 	/* Flexible data to match within the packet payload */
 	__be16 flex_word;
 	u16 flex_offset;
@@ -235,7 +289,6 @@
 
 	/* filter control */
 	u16 q_index;
-	u8  flex_off;
 	u8  pctype;
 	u16 dest_vsi;
 	u8  dest_ctl;
@@ -249,25 +302,42 @@
 #define I40E_CLOUD_FIELD_IVLAN		BIT(2)
 #define I40E_CLOUD_FIELD_TEN_ID		BIT(3)
 #define I40E_CLOUD_FIELD_IIP		BIT(4)
+#define I40E_CLOUD_FIELD_OIP1		BIT(5)
+#define I40E_CLOUD_FIELD_OIP2		BIT(6)
 
-#define I40E_CLOUD_FILTER_FLAGS_OMAC	I40E_CLOUD_FIELD_OMAC
-#define I40E_CLOUD_FILTER_FLAGS_IMAC	I40E_CLOUD_FIELD_IMAC
-#define I40E_CLOUD_FILTER_FLAGS_IMAC_IVLAN	(I40E_CLOUD_FIELD_IMAC | \
-						 I40E_CLOUD_FIELD_IVLAN)
-#define I40E_CLOUD_FILTER_FLAGS_IMAC_TEN_ID	(I40E_CLOUD_FIELD_IMAC | \
-						 I40E_CLOUD_FIELD_TEN_ID)
+#define I40E_CLOUD_FILTER_FLAGS_OMAC I40E_CLOUD_FIELD_OMAC
+#define I40E_CLOUD_FILTER_FLAGS_IMAC I40E_CLOUD_FIELD_IMAC
+#define I40E_CLOUD_FILTER_FLAGS_IMAC_IVLAN (I40E_CLOUD_FIELD_IMAC | \
+					    I40E_CLOUD_FIELD_IVLAN)
+#define I40E_CLOUD_FILTER_FLAGS_IMAC_TEN_ID (I40E_CLOUD_FIELD_IMAC | \
+					     I40E_CLOUD_FIELD_TEN_ID)
 #define I40E_CLOUD_FILTER_FLAGS_OMAC_TEN_ID_IMAC (I40E_CLOUD_FIELD_OMAC | \
 						  I40E_CLOUD_FIELD_IMAC | \
 						  I40E_CLOUD_FIELD_TEN_ID)
 #define I40E_CLOUD_FILTER_FLAGS_IMAC_IVLAN_TEN_ID (I40E_CLOUD_FIELD_IMAC | \
 						   I40E_CLOUD_FIELD_IVLAN | \
 						   I40E_CLOUD_FIELD_TEN_ID)
-#define I40E_CLOUD_FILTER_FLAGS_IIP	I40E_CLOUD_FIELD_IIP
+#define I40E_CLOUD_FILTER_FLAGS_IIP  I40E_CLOUD_FIELD_IIP
+#define I40E_CLOUD_FILTER_FLAGS_OIP1 I40E_CLOUD_FIELD_OIP1
+#define I40E_CLOUD_FILTER_FLAGS_OIP2 I40E_CLOUD_FIELD_OIP2
+
+#define I40E_CLOUD_FILTER_ANY_QUEUE		0xffff
+#define I40E_CLOUD_FILTER_TUNNEL_TYPE_VXLAN	0x0
+#define I40E_CLOUD_FILTER_TUNNEL_TYPE_NVGRE	0x1
+#define I40E_CLOUD_FILTER_TUNNEL_TYPE_GENEVE	0x2
+#define I40E_CLOUD_FILTER_TUNNEL_TYPE_IP_IN_GRE	0x3
 
 struct i40e_cloud_filter {
 	struct hlist_node cloud_node;
 	unsigned long cookie;
 	/* cloud filter input set follows */
+	u8 outer_mac[ETH_ALEN];
+	u8 inner_mac[ETH_ALEN];
+	__be16 inner_vlan;
+	__be32 inner_ip[4];
+	u16 queue_id;
+	u32 id;
+	/* cloud filter input set follows */
 	u8 dst_mac[ETH_ALEN];
 	u8 src_mac[ETH_ALEN];
 	__be16 vlan_id;
@@ -292,10 +362,12 @@
 	u16 n_proto;    /* Ethernet Protocol */
 	u8 ip_proto;    /* IPPROTO value */
 	u8 flags;
-#define I40E_CLOUD_TNL_TYPE_NONE        0xff
+#define I40E_CLOUD_TNL_TYPE_NONE	0xff
 	u8 tunnel_type;
 };
 
+#define I40E_ETH_P_LLDP			0x88cc
+
 #define I40E_DCB_PRIO_TYPE_STRICT	0
 #define I40E_DCB_PRIO_TYPE_ETS		1
 #define I40E_DCB_STRICT_PRIO_CREDITS	127
@@ -304,6 +376,7 @@
 	u16	qoffset;	/* Queue offset from base queue */
 	u16	qcount;		/* Total Queues */
 	u8	netdev_tc;	/* Netdev TC index if netdev associated */
+	u16	tc_bw_credits;	/* Relative credits assigned to TC */
 };
 
 /* TC configuration data structure */
@@ -327,7 +400,6 @@
 #define I40E_PROFILE_LIST_SIZE \
 	(I40E_PROFILE_INFO_SIZE * I40E_MAX_PROFILE_NUM + 4)
 #define I40E_DDP_PROFILE_PATH "intel/i40e/ddp/"
-#define I40E_DDP_PROFILE_NAME_MAX 64
 
 int i40e_ddp_load(struct net_device *netdev, const u8 *data, size_t size,
 		  bool is_add);
@@ -407,17 +479,26 @@
 				 I40E_FLEX_54_MASK | I40E_FLEX_55_MASK | \
 				 I40E_FLEX_56_MASK | I40E_FLEX_57_MASK)
 
+#define I40E_QINT_TQCTL_VAL(qp, vector, nextq_type) \
+	(I40E_QINT_TQCTL_CAUSE_ENA_MASK | \
+	(I40E_TX_ITR << I40E_QINT_TQCTL_ITR_INDX_SHIFT) | \
+	((vector) << I40E_QINT_TQCTL_MSIX_INDX_SHIFT) | \
+	((qp) << I40E_QINT_TQCTL_NEXTQ_INDX_SHIFT) | \
+	(I40E_QUEUE_TYPE_##nextq_type << I40E_QINT_TQCTL_NEXTQ_TYPE_SHIFT))
+
+#define I40E_QINT_RQCTL_VAL(qp, vector, nextq_type) \
+	(I40E_QINT_RQCTL_CAUSE_ENA_MASK | \
+	(I40E_RX_ITR << I40E_QINT_RQCTL_ITR_INDX_SHIFT) | \
+	((vector) << I40E_QINT_RQCTL_MSIX_INDX_SHIFT) | \
+	((qp) << I40E_QINT_RQCTL_NEXTQ_INDX_SHIFT) | \
+	(I40E_QUEUE_TYPE_##nextq_type << I40E_QINT_RQCTL_NEXTQ_TYPE_SHIFT))
+
 struct i40e_flex_pit {
 	struct list_head list;
 	u16 src_offset;
 	u8 pit_index;
 };
 
-struct i40e_fwd_adapter {
-	struct net_device *netdev;
-	int bit_no;
-};
-
 struct i40e_channel {
 	struct list_head list;
 	bool initialized;
@@ -432,24 +513,14 @@
 	struct i40e_aqc_vsi_properties_data info;
 
 	u64 max_tx_rate;
-	struct i40e_fwd_adapter *fwd;
 
 	/* track this channel belongs to which VSI */
 	struct i40e_vsi *parent_vsi;
 };
 
-static inline bool i40e_is_channel_macvlan(struct i40e_channel *ch)
-{
-	return !!ch->fwd;
-}
-
-static inline u8 *i40e_channel_mac(struct i40e_channel *ch)
-{
-	if (i40e_is_channel_macvlan(ch))
-		return ch->fwd->netdev->dev_addr;
-	else
-		return NULL;
-}
+#ifdef HAVE_PTP_1588_CLOCK
+struct i40e_ptp_pins_settings;
+#endif /* HAVE_PTP_1588_CLOCK */
 
 /* struct that defines the Ethernet device */
 struct i40e_pf {
@@ -463,7 +534,7 @@
 	u16 num_vmdq_vsis;         /* num vmdq vsis this PF has set up */
 	u16 num_vmdq_qps;          /* num queue pairs per vmdq pool */
 	u16 num_vmdq_msix;         /* num queue vectors per vmdq pool */
-	u16 num_req_vfs;           /* num VFs requested for this PF */
+	u16 num_req_vfs;           /* num VFs requested for this VF */
 	u16 num_vf_qps;            /* num queue pairs per VF */
 	u16 num_lan_qps;           /* num lan queues this PF has set up */
 	u16 num_lan_msix;          /* num queue vectors for the base PF vsi */
@@ -494,6 +565,11 @@
 	u16 fd_sctp4_filter_cnt;
 	u16 fd_ip4_filter_cnt;
 
+	u16 fd_tcp6_filter_cnt;
+	u16 fd_udp6_filter_cnt;
+	u16 fd_sctp6_filter_cnt;
+	u16 fd_ip6_filter_cnt;
+
 	/* Flexible filter table values that need to be programmed into
 	 * hardware, which expects L3 and L4 to be programmed separately. We
 	 * need to ensure that the values are in ascended order and don't have
@@ -508,6 +584,9 @@
 	struct hlist_head cloud_filter_list;
 	u16 num_cloud_filters;
 
+	/* Array of count of outerip cloud filters */
+	u16 outerip_filters[2];
+
 	enum i40e_interrupt_policy int_policy;
 	u16 rx_itr_default;
 	u16 tx_itr_default;
@@ -539,8 +618,9 @@
 #define I40E_HW_STOP_FW_LLDP			BIT(16)
 #define I40E_HW_PORT_ID_VALID			BIT(17)
 #define I40E_HW_RESTART_AUTONEG			BIT(18)
+#define I40E_HW_OUTER_VLAN_CAPABLE		BIT(19)
 
-	u32 flags;
+	u64 flags;
 #define I40E_FLAG_RX_CSUM_ENABLED		BIT(0)
 #define I40E_FLAG_MSI_ENABLED			BIT(1)
 #define I40E_FLAG_MSIX_ENABLED			BIT(2)
@@ -558,7 +638,9 @@
 #define I40E_FLAG_LINK_POLLING_ENABLED		BIT(14)
 #define I40E_FLAG_TRUE_PROMISC_SUPPORT		BIT(15)
 #define I40E_FLAG_LEGACY_RX			BIT(16)
+#ifdef HAVE_PTP_1588_CLOCK
 #define I40E_FLAG_PTP				BIT(17)
+#endif /* HAVE_PTP_1588_CLOCK */
 #define I40E_FLAG_IWARP_ENABLED			BIT(18)
 #define I40E_FLAG_LINK_DOWN_ON_CLOSE_ENABLED	BIT(19)
 #define I40E_FLAG_SOURCE_PRUNING_DISABLED       BIT(20)
@@ -568,6 +650,13 @@
 #define I40E_FLAG_DISABLE_FW_LLDP		BIT(24)
 #define I40E_FLAG_RS_FEC			BIT(25)
 #define I40E_FLAG_BASE_R_FEC			BIT(26)
+#define I40E_FLAG_TOTAL_PORT_SHUTDOWN		BIT(27)
+#define I40E_FLAG_MULTIPLE_TRAFFIC_CLASSES	BIT(28)
+#define I40E_FLAG_CLS_FLOWER			BIT(29)
+#define I40E_FLAG_VF_VLAN_PRUNE_DISABLE		BIT(30)
+
+	/* flag to enable/disable vf base mode support */
+	bool vf_base_mode_only;
 
 	struct i40e_client_instance *cinst;
 	bool stat_offsets_loaded;
@@ -625,6 +714,76 @@
 	u16 dcbx_cap;
 
 	struct i40e_filter_control_settings filter_settings;
+	struct i40e_rx_pb_config pb_cfg; /* Current Rx packet buffer config */
+	struct i40e_dcbx_config tmp_cfg;
+#ifdef HAVE_PTP_1588_CLOCK
+/* GPIO defines used by PTP */
+#define I40E_SDP3_2			18
+#define I40E_SDP3_3			19
+#define I40E_GPIO_4			20
+#define I40E_LED2_0			26
+#define I40E_LED2_1			27
+#define I40E_LED3_0			28
+#define I40E_LED3_1			29
+#define I40E_GLGEN_GPIO_SET_SDP_DATA_HI \
+	(1 << I40E_GLGEN_GPIO_SET_SDP_DATA_SHIFT)
+#define I40E_GLGEN_GPIO_SET_DRV_SDP_DATA \
+	(1 << I40E_GLGEN_GPIO_SET_DRIVE_SDP_SHIFT)
+#define I40E_GLGEN_GPIO_CTL_PRT_NUM_0 \
+	(0 << I40E_GLGEN_GPIO_CTL_PRT_NUM_SHIFT)
+#define I40E_GLGEN_GPIO_CTL_PRT_NUM_1 \
+	(1 << I40E_GLGEN_GPIO_CTL_PRT_NUM_SHIFT)
+#define I40E_GLGEN_GPIO_CTL_RESERVED	BIT(2)
+#define I40E_GLGEN_GPIO_CTL_PRT_NUM_NA_Z \
+	(1 << I40E_GLGEN_GPIO_CTL_PRT_NUM_NA_SHIFT)
+#define I40E_GLGEN_GPIO_CTL_DIR_OUT \
+	(1 << I40E_GLGEN_GPIO_CTL_PIN_DIR_SHIFT)
+#define I40E_GLGEN_GPIO_CTL_TRI_DRV_HI \
+	(1 << I40E_GLGEN_GPIO_CTL_TRI_CTL_SHIFT)
+#define I40E_GLGEN_GPIO_CTL_OUT_HI_RST \
+	(1 << I40E_GLGEN_GPIO_CTL_OUT_CTL_SHIFT)
+#define I40E_GLGEN_GPIO_CTL_TIMESYNC_0 \
+	(3 << I40E_GLGEN_GPIO_CTL_PIN_FUNC_SHIFT)
+#define I40E_GLGEN_GPIO_CTL_TIMESYNC_1 \
+	(4 << I40E_GLGEN_GPIO_CTL_PIN_FUNC_SHIFT)
+#define I40E_GLGEN_GPIO_CTL_NOT_FOR_PHY_CONN \
+	(0x3F << I40E_GLGEN_GPIO_CTL_PHY_PIN_NAME_SHIFT)
+#define I40E_GLGEN_GPIO_CTL_OUT_DEFAULT \
+	(1 << I40E_GLGEN_GPIO_CTL_OUT_DEFAULT_SHIFT)
+#define I40E_GLGEN_GPIO_CTL_PORT_0_IN_TIMESYNC_0 \
+	(I40E_GLGEN_GPIO_CTL_NOT_FOR_PHY_CONN | \
+	 I40E_GLGEN_GPIO_CTL_TIMESYNC_0 | \
+	 I40E_GLGEN_GPIO_CTL_RESERVED | I40E_GLGEN_GPIO_CTL_PRT_NUM_0)
+#define I40E_GLGEN_GPIO_CTL_PORT_1_IN_TIMESYNC_0 \
+	(I40E_GLGEN_GPIO_CTL_NOT_FOR_PHY_CONN | \
+	 I40E_GLGEN_GPIO_CTL_TIMESYNC_0 | \
+	 I40E_GLGEN_GPIO_CTL_RESERVED | I40E_GLGEN_GPIO_CTL_PRT_NUM_1)
+#define I40E_GLGEN_GPIO_CTL_PORT_0_OUT_TIMESYNC_1 \
+	(I40E_GLGEN_GPIO_CTL_NOT_FOR_PHY_CONN | \
+	 I40E_GLGEN_GPIO_CTL_TIMESYNC_1 | I40E_GLGEN_GPIO_CTL_OUT_HI_RST | \
+	 I40E_GLGEN_GPIO_CTL_TRI_DRV_HI | I40E_GLGEN_GPIO_CTL_DIR_OUT | \
+	 I40E_GLGEN_GPIO_CTL_RESERVED | I40E_GLGEN_GPIO_CTL_PRT_NUM_0)
+#define I40E_GLGEN_GPIO_CTL_PORT_1_OUT_TIMESYNC_1 \
+	(I40E_GLGEN_GPIO_CTL_NOT_FOR_PHY_CONN | \
+	 I40E_GLGEN_GPIO_CTL_TIMESYNC_1 | I40E_GLGEN_GPIO_CTL_OUT_HI_RST | \
+	 I40E_GLGEN_GPIO_CTL_TRI_DRV_HI | I40E_GLGEN_GPIO_CTL_DIR_OUT | \
+	 I40E_GLGEN_GPIO_CTL_RESERVED | I40E_GLGEN_GPIO_CTL_PRT_NUM_1)
+#define I40E_GLGEN_GPIO_CTL_LED_INIT \
+	(I40E_GLGEN_GPIO_CTL_PRT_NUM_NA_Z | \
+	 I40E_GLGEN_GPIO_CTL_DIR_OUT | \
+	 I40E_GLGEN_GPIO_CTL_TRI_DRV_HI | \
+	 I40E_GLGEN_GPIO_CTL_OUT_HI_RST | \
+	 I40E_GLGEN_GPIO_CTL_OUT_DEFAULT | \
+	 I40E_GLGEN_GPIO_CTL_NOT_FOR_PHY_CONN)
+#define I40E_PRTTSYN_AUX_1_INSTNT \
+	(1 << I40E_PRTTSYN_AUX_1_INSTNT_SHIFT)
+#define I40E_PRTTSYN_AUX_0_OUT_ENABLE \
+	(1 << I40E_PRTTSYN_AUX_0_OUT_ENA_SHIFT)
+#define I40E_PRTTSYN_AUX_0_OUT_CLK_MOD	(3 << I40E_PRTTSYN_AUX_0_OUTMOD_SHIFT)
+#define I40E_PRTTSYN_AUX_0_OUT_ENABLE_CLK_MOD \
+	(I40E_PRTTSYN_AUX_0_OUT_ENABLE | I40E_PRTTSYN_AUX_0_OUT_CLK_MOD)
+#define I40E_PTP_HALF_SECOND		500000000LL /* nano seconds */
+#define I40E_PTP_2_SEC_DELAY		2
 
 	struct ptp_clock *ptp_clock;
 	struct ptp_clock_info ptp_caps;
@@ -632,6 +791,9 @@
 	unsigned long ptp_tx_start;
 	struct hwtstamp_config tstamp_config;
 	struct timespec64 ptp_prev_hw_time;
+	struct work_struct ptp_pps_work;
+	struct work_struct ptp_extts0_work;
+	struct work_struct ptp_extts1_work;
 	ktime_t ptp_reset_start;
 	struct mutex tmreg_lock; /* Used to protect the SYSTIME registers. */
 	u32 ptp_adj_mult;
@@ -639,10 +801,32 @@
 	u32 tx_hwtstamp_skipped;
 	u32 rx_hwtstamp_cleared;
 	u32 latch_event_flags;
+	u64 ptp_pps_start;
+	u32 pps_delay;
 	spinlock_t ptp_rx_lock; /* Used to protect Rx timestamp registers. */
+	struct ptp_pin_desc ptp_pin[3];
 	unsigned long latch_events[4];
 	bool ptp_tx;
 	bool ptp_rx;
+	struct i40e_ptp_pins_settings *ptp_pins;
+	struct kobject *ptp_kobj;
+#endif /* HAVE_PTP_1588_CLOCK */
+#ifdef I40E_ADD_PROBES
+	u64 tcp_segs;
+	u64 tx_tcp_cso;
+	u64 tx_udp_cso;
+	u64 tx_sctp_cso;
+	u64 tx_ip4_cso;
+	u64 rx_tcp_cso;
+	u64 rx_udp_cso;
+	u64 rx_sctp_cso;
+	u64 rx_ip4_cso;
+	u64 hw_csum_rx_outer;
+	u64 rx_tcp_cso_err;
+	u64 rx_udp_cso_err;
+	u64 rx_sctp_cso_err;
+	u64 rx_ip4_cso_err;
+#endif
 	u16 rss_table_size; /* HW RSS table size */
 	u32 max_bw;
 	u32 min_bw;
@@ -650,12 +834,32 @@
 	u32 ioremap_len;
 	u32 fd_inv;
 	u16 phy_led_val;
-
-	u16 override_q_count;
 	u16 last_sw_conf_flags;
 	u16 last_sw_conf_valid_flags;
+
+	u16 override_q_count;
+	struct vfd_objects *vfd_obj;
+	u16 ingress_rule_id;
+	int ingress_vlan;
+	u16 egress_rule_id;
+	int egress_vlan;
+	bool vf_bw_applied;
+	struct list_head *mac_list;	/* for backup vfs mac list */
+	/* User priority map provided in sysfs for each TC.
+	 * Defines if priority (index) belongs to TC, 0xff
+	 * means it is free for user to pick.
+	 */
+#define I40E_MULTIPLE_TRAFFIC_CLASS_NO_ENTRY 0xff
+	u8 dcb_user_up_map[I40E_MAX_USER_PRIORITY];
+	u8 dcb_user_lsp_map[I40E_MAX_TRAFFIC_CLASS];
+	u8 dcb_mib_bw_map[I40E_MAX_TRAFFIC_CLASS];
+	u16 dcb_veb_bw_map[I40E_MAX_TRAFFIC_CLASS];
+	bool dcb_user_reconfig;
 	/* List to keep previous DDP profiles to be rolled back in the future */
 	struct list_head ddp_old_prof;
+#if IS_ENABLED(CONFIG_MFD_CORE)
+	int peer_idx;
+#endif
 };
 
 /**
@@ -678,6 +882,7 @@
 	I40E_FILTER_ACTIVE,		/* Added to switch by FW */
 	I40E_FILTER_FAILED,		/* Rejected by FW */
 	I40E_FILTER_REMOVE,		/* To be removed */
+	I40E_FILTER_INACTIVE,		/* Removed from FW, only for vlan 0 */
 /* There is no 'removed' state; the filter struct is freed */
 };
 struct i40e_mac_filter {
@@ -731,7 +936,11 @@
 /* struct that defines a VSI, associated with a dev */
 struct i40e_vsi {
 	struct net_device *netdev;
+#ifdef HAVE_VLAN_RX_REGISTER
+	struct vlan_group *vlgrp;
+#else
 	unsigned long active_vlans[BITS_TO_LONGS(VLAN_N_VID)];
+#endif
 	bool netdev_registered;
 	bool stat_offsets_loaded;
 
@@ -739,7 +948,7 @@
 	DECLARE_BITMAP(state, __I40E_VSI_STATE_SIZE__);
 #define I40E_VSI_FLAG_FILTER_CHANGED	BIT(0)
 #define I40E_VSI_FLAG_VEB_OWNER		BIT(1)
-	unsigned long flags;
+	u64 flags;
 
 	/* Per VSI lock to protect elements/hash (MAC filter) */
 	spinlock_t mac_filter_hash_lock;
@@ -748,8 +957,13 @@
 	bool has_vlan_filter;
 
 	/* VSI stats */
+#ifdef HAVE_NDO_GET_STATS64
 	struct rtnl_link_stats64 net_stats;
 	struct rtnl_link_stats64 net_stats_offsets;
+#else
+	struct net_device_stats net_stats;
+	struct net_device_stats net_stats_offsets;
+#endif
 	struct i40e_eth_stats eth_stats;
 	struct i40e_eth_stats eth_stats_offsets;
 	u32 tx_restart;
@@ -775,7 +989,6 @@
 	u8  *rss_hkey_user;	/* User configured hash keys */
 	u8  *rss_lut_user;	/* User configured lookup table entries */
 
-
 	u16 max_frame;
 	u16 rx_buf_len;
 
@@ -799,8 +1012,9 @@
 	u16 num_rx_desc;
 	enum i40e_vsi_type type;  /* VSI type, e.g., LAN, FCoE, etc */
 	s16 vf_id;		/* Virtual function ID for SRIOV VSIs */
-
+#ifdef __TC_MQPRIO_MODE_MAX
 	struct tc_mqprio_qopt_offload mqprio_qopt; /* queue parameters */
+#endif
 	struct i40e_tc_configuration tc_config;
 	struct i40e_aqc_vsi_properties_data info;
 
@@ -821,7 +1035,6 @@
 	struct kobject *kobj;	/* sysfs object */
 	bool current_isup;	/* Sync 'link up' logging */
 	enum i40e_aq_link_speed current_speed;	/* Sync link speed logging */
-
 	/* channel specific fields */
 	u16 cnt_q_avail;	/* num of queues available for channel usage */
 	u16 orig_rss_size;
@@ -833,19 +1046,14 @@
 	struct list_head ch_list;
 	u16 tc_seid_map[I40E_MAX_TRAFFIC_CLASS];
 
-	/* macvlan fields */
-#define I40E_MAX_MACVLANS		128 /* Max HW vectors - 1 on FVL */
-#define I40E_MIN_MACVLAN_VECTORS	2   /* Min vectors to enable macvlans */
-	DECLARE_BITMAP(fwd_bitmask, I40E_MAX_MACVLANS);
-	struct list_head macvlan_list;
-	int macvlan_cnt;
-
 	void *priv;	/* client driver data reference. */
+	bool block_tx_timeout;
 
 	/* VSI specific handlers */
 	irqreturn_t (*irq_handler)(int irq, void *data);
+#ifdef ETHTOOL_GRXRINGS
+#endif
 
-	unsigned long *af_xdp_zc_qps; /* tracks AF_XDP ZC enabled qps */
 } ____cacheline_internodealigned_in_smp;
 
 struct i40e_netdev_priv {
@@ -867,8 +1075,10 @@
 	u8 itr_countdown;	/* when 0 should adjust adaptive ITR */
 	u8 num_ringpairs;	/* total number of ring pairs in vector */
 
+#ifdef HAVE_IRQ_AFFINITY_NOTIFY
 	cpumask_t affinity_mask;
 	struct irq_affinity_notify affinity_notify;
+#endif
 
 	struct rcu_head rcu;	/* to avoid race with update stats on free */
 	char name[I40E_INT_NAME_STR_LEN];
@@ -992,7 +1202,7 @@
 /* needed by i40e_ethtool.c */
 int i40e_up(struct i40e_vsi *vsi);
 void i40e_down(struct i40e_vsi *vsi);
-extern const char i40e_driver_name[];
+extern char i40e_driver_name[];
 extern const char i40e_driver_version_str[];
 void i40e_do_reset_safe(struct i40e_pf *pf, u32 reset_flags);
 void i40e_do_reset(struct i40e_pf *pf, u32 reset_flags, bool lock_acquired);
@@ -1001,6 +1211,7 @@
 void i40e_fill_rss_lut(struct i40e_pf *pf, u8 *lut,
 		       u16 rss_table_size, u16 rss_size);
 struct i40e_vsi *i40e_find_vsi_from_id(struct i40e_pf *pf, u16 id);
+struct i40e_vsi *i40e_find_vsi_from_seid(struct i40e_pf *pf, u16 seid);
 /**
  * i40e_find_vsi_by_type - Find and return Flow Director VSI
  * @pf: PF to search for VSI
@@ -1023,7 +1234,11 @@
 void i40e_update_stats(struct i40e_vsi *vsi);
 void i40e_update_veb_stats(struct i40e_veb *veb);
 void i40e_update_eth_stats(struct i40e_vsi *vsi);
+#ifdef HAVE_NDO_GET_STATS64
 struct rtnl_link_stats64 *i40e_get_vsi_stats_struct(struct i40e_vsi *vsi);
+#else
+struct net_device_stats *i40e_get_vsi_stats_struct(struct i40e_vsi *vsi);
+#endif
 int i40e_fetch_switch_configuration(struct i40e_pf *pf,
 				    bool printconfig);
 
@@ -1036,6 +1251,8 @@
 u32 i40e_get_global_fd_count(struct i40e_pf *pf);
 bool i40e_set_ntuple(struct i40e_pf *pf, netdev_features_t features);
 void i40e_set_ethtool_ops(struct net_device *netdev);
+struct i40e_mac_filter *i40e_find_filter(struct i40e_vsi *vsi,
+					 const u8 *macaddr, s16 vlan);
 struct i40e_mac_filter *i40e_add_filter(struct i40e_vsi *vsi,
 					const u8 *macaddr, s16 vlan);
 void __i40e_del_filter(struct i40e_vsi *vsi, struct i40e_mac_filter *f);
@@ -1044,6 +1261,11 @@
 struct i40e_vsi *i40e_vsi_setup(struct i40e_pf *pf, u8 type,
 				u16 uplink, u32 param1);
 int i40e_vsi_release(struct i40e_vsi *vsi);
+int i40e_vsi_mem_alloc(struct i40e_pf *pf, enum i40e_vsi_type type);
+int i40e_vsi_setup_rx_resources(struct i40e_vsi *vsi);
+int i40e_vsi_setup_tx_resources(struct i40e_vsi *vsi);
+int i40e_vsi_config_tc(struct i40e_vsi *vsi, u8 enabled_tc);
+int i40e_vsi_request_irq_msix(struct i40e_vsi *vsi, char *basename);
 void i40e_service_event_schedule(struct i40e_pf *pf);
 void i40e_notify_client_of_vf_msg(struct i40e_vsi *vsi, u32 vf_id,
 				  u8 *msg, u16 len);
@@ -1055,14 +1277,27 @@
 void i40e_vsi_stop_rings(struct i40e_vsi *vsi);
 void i40e_vsi_stop_rings_no_wait(struct  i40e_vsi *vsi);
 int i40e_vsi_wait_queues_disabled(struct i40e_vsi *vsi);
+void i40e_quiesce_vsi(struct i40e_vsi *vsi);
+void i40e_unquiesce_vsi(struct i40e_vsi *vsi);
+void i40e_pf_quiesce_all_vsi(struct i40e_pf *pf);
+void i40e_pf_unquiesce_all_vsi(struct i40e_pf *pf);
 int i40e_reconfig_rss_queues(struct i40e_pf *pf, int queue_count);
 struct i40e_veb *i40e_veb_setup(struct i40e_pf *pf, u16 flags, u16 uplink_seid,
 				u16 downlink_seid, u8 enabled_tc);
 void i40e_veb_release(struct i40e_veb *veb);
+int i40e_max_lump_qp(struct i40e_pf *pf);
 
 int i40e_veb_config_tc(struct i40e_veb *veb, u8 enabled_tc);
 int i40e_vsi_add_pvid(struct i40e_vsi *vsi, u16 vid);
 void i40e_vsi_remove_pvid(struct i40e_vsi *vsi);
+bool i40e_is_double_vlan(struct i40e_hw *hw);
+bool i40e_is_vid(struct i40e_aqc_vsi_properties_data *info);
+__le16 *i40e_get_current_vid(struct i40e_vsi *vsi);
+int i40e_get_custom_cloud_filter_type(u8 flags, u16 *type);
+int i40e_add_del_custom_cloud_filter(struct i40e_vsi *vsi,
+				     struct i40e_cloud_filter *filter,
+				     bool add);
+int i40e_get_cloud_filter_type(u8 flags, u16 *type);
 void i40e_vsi_reset_stats(struct i40e_vsi *vsi);
 void i40e_pf_reset_stats(struct i40e_pf *pf);
 #ifdef CONFIG_DEBUG_FS
@@ -1107,10 +1342,11 @@
 void i40e_irq_dynamic_disable_icr0(struct i40e_pf *pf);
 void i40e_irq_dynamic_enable_icr0(struct i40e_pf *pf);
 int i40e_ioctl(struct net_device *netdev, struct ifreq *ifr, int cmd);
+int i40e_ptp_ioctl(struct net_device *netdev, struct ifreq *ifr, int cmd);
 int i40e_open(struct net_device *netdev);
 int i40e_close(struct net_device *netdev);
 int i40e_vsi_open(struct i40e_vsi *vsi);
-void i40e_vlan_stripping_disable(struct i40e_vsi *vsi);
+int i40e_vlan_stripping_disable(struct i40e_vsi *vsi);
 int i40e_add_vlan_all_mac(struct i40e_vsi *vsi, s16 vid);
 int i40e_vsi_add_vlan(struct i40e_vsi *vsi, u16 vid);
 void i40e_rm_vlan_all_mac(struct i40e_vsi *vsi, s16 vid);
@@ -1119,18 +1355,32 @@
 					    const u8 *macaddr);
 int i40e_del_mac_filter(struct i40e_vsi *vsi, const u8 *macaddr);
 bool i40e_is_vsi_in_vlan(struct i40e_vsi *vsi);
+int i40e_count_filters(struct i40e_vsi *vsi);
 struct i40e_mac_filter *i40e_find_mac(struct i40e_vsi *vsi, const u8 *macaddr);
-void i40e_vlan_stripping_enable(struct i40e_vsi *vsi);
-#ifdef CONFIG_I40E_DCB
+int i40e_vlan_stripping_enable(struct i40e_vsi *vsi);
+static inline bool i40e_is_sw_dcb(struct i40e_pf *pf)
+{
+	return !!(pf->flags & I40E_FLAG_DISABLE_FW_LLDP);
+}
+
+#ifdef CONFIG_DCB
+#ifdef HAVE_DCBNL_IEEE
 void i40e_dcbnl_flush_apps(struct i40e_pf *pf,
 			   struct i40e_dcbx_config *old_cfg,
 			   struct i40e_dcbx_config *new_cfg);
 void i40e_dcbnl_set_all(struct i40e_vsi *vsi);
 void i40e_dcbnl_setup(struct i40e_vsi *vsi);
+#endif /* HAVE_DCBNL_IEEE */
+int i40e_update_ets(struct i40e_pf *pf);
 bool i40e_dcb_need_reconfig(struct i40e_pf *pf,
 			    struct i40e_dcbx_config *old_cfg,
 			    struct i40e_dcbx_config *new_cfg);
-#endif /* CONFIG_I40E_DCB */
+int i40e_hw_dcb_config(struct i40e_pf *pf, struct i40e_dcbx_config *new_cfg);
+#define I40E_ETS_NON_WILLING_MODE	0
+#define I40E_ETS_WILLING_MODE		1
+int i40e_dcb_sw_default_config(struct i40e_pf *pf, u8 ets_willing);
+#endif /* CONFIG_DCB */
+#ifdef HAVE_PTP_1588_CLOCK
 void i40e_ptp_rx_hang(struct i40e_pf *pf);
 void i40e_ptp_tx_hang(struct i40e_pf *pf);
 void i40e_ptp_tx_hwtstamp(struct i40e_pf *pf);
@@ -1138,29 +1388,50 @@
 void i40e_ptp_set_increment(struct i40e_pf *pf);
 int i40e_ptp_set_ts_config(struct i40e_pf *pf, struct ifreq *ifr);
 int i40e_ptp_get_ts_config(struct i40e_pf *pf, struct ifreq *ifr);
+int i40e_ptp_set_pins_ioctl(struct i40e_pf *pf, struct ifreq *ifr);
+int i40e_ptp_get_pins(struct i40e_pf *pf, struct ifreq *ifr);
 void i40e_ptp_save_hw_time(struct i40e_pf *pf);
 void i40e_ptp_restore_hw_time(struct i40e_pf *pf);
 void i40e_ptp_init(struct i40e_pf *pf);
 void i40e_ptp_stop(struct i40e_pf *pf);
+int i40e_ptp_alloc_pins(struct i40e_pf *pf);
+#endif /* HAVE_PTP_1588_CLOCK */
+u8 i40e_pf_get_num_tc(struct i40e_pf *pf);
+int i40e_update_adq_vsi_queues(struct i40e_vsi *vsi, int vsi_offset);
+int i40e_vsi_get_bw_info(struct i40e_vsi *vsi);
 int i40e_is_vsi_uplink_mode_veb(struct i40e_vsi *vsi);
 i40e_status i40e_get_partition_bw_setting(struct i40e_pf *pf);
 i40e_status i40e_set_partition_bw_setting(struct i40e_pf *pf);
 i40e_status i40e_commit_partition_bw_setting(struct i40e_pf *pf);
-void i40e_print_link_message(struct i40e_vsi *vsi, bool isup);
-
-void i40e_set_fec_in_flags(u8 fec_cfg, u32 *flags);
-
-static inline bool i40e_enabled_xdp_vsi(struct i40e_vsi *vsi)
-{
-	return !!READ_ONCE(vsi->xdp_prog);
-}
-
-int i40e_create_queue_channel(struct i40e_vsi *vsi, struct i40e_channel *ch);
 int i40e_set_bw_limit(struct i40e_vsi *vsi, u16 seid, u64 max_tx_rate);
+int i40e_add_del_cloud_filter_ex(struct i40e_pf *pf,
+				 struct i40e_cloud_filter *filter,
+				 bool add);
 int i40e_add_del_cloud_filter(struct i40e_vsi *vsi,
 			      struct i40e_cloud_filter *filter,
 			      bool add);
 int i40e_add_del_cloud_filter_big_buf(struct i40e_vsi *vsi,
 				      struct i40e_cloud_filter *filter,
 				      bool add);
+void i40e_print_link_message(struct i40e_vsi *vsi, bool isup);
+int i40e_create_queue_channel(struct i40e_vsi *vsi, struct i40e_channel *ch);
+int i40e_get_link_speed(struct i40e_vsi *vsi);
+
+void i40e_set_fec_in_flags(u8 fec_cfg, u64 *flags);
+
+#ifdef HAVE_XDP_SUPPORT
+int i40e_queue_pair_disable(struct i40e_vsi *vsi, int queue_pair);
+int i40e_queue_pair_enable(struct i40e_vsi *vsi, int queue_pair);
+#endif
+
+static inline bool i40e_enabled_xdp_vsi(struct i40e_vsi *vsi)
+{
+	return !!vsi->xdp_prog;
+}
+
+int i40e_restore_ingress_egress_mirror(struct i40e_vsi *src_vsi, int mirror,
+				       u16 rule_type, u16 *rule_id);
+int i40e_vsi_configure_tc_max_bw(struct i40e_vsi *vsi);
+int i40e_veb_configure_tc_max_bw(struct i40e_veb *veb, u8 enabled_tc);
+
 #endif /* _I40E_H_ */
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_helper.h linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_helper.h
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_helper.h	1969-12-31 19:00:00.000000000 -0500
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_helper.h	2024-05-13 03:58:25.372491940 -0400
@@ -0,0 +1,129 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
+
+#ifndef _I40E_HELPER_H_
+#define _I40E_HELPER_H_
+
+#include "i40e_alloc.h"
+
+/**
+ * i40e_allocate_dma_mem_d - OS specific memory alloc for shared code
+ * @hw:   pointer to the HW structure
+ * @mem:  ptr to mem struct to fill out
+ * @mtype: memory type identifier (unused)
+ * @size: size of memory requested
+ * @alignment: what to align the allocation to
+ **/
+inline int i40e_allocate_dma_mem_d(struct i40e_hw *hw,
+				   struct i40e_dma_mem *mem,
+				   __always_unused enum i40e_memory_type mtype,
+				   u64 size, u32 alignment)
+{
+	struct i40e_pf *nf = (struct i40e_pf *)hw->back;
+
+	mem->size = ALIGN(size, alignment);
+#ifdef HAVE_DMA_ALLOC_COHERENT_ZEROES_MEM
+	mem->va = dma_alloc_coherent(&nf->pdev->dev, mem->size,
+				     &mem->pa, GFP_KERNEL);
+#else
+	mem->va = dma_zalloc_coherent(&nf->pdev->dev, mem->size,
+				      &mem->pa, GFP_KERNEL);
+#endif
+	if (!mem->va)
+		return -ENOMEM;
+
+	return 0;
+}
+
+/**
+ * i40e_free_dma_mem_d - OS specific memory free for shared code
+ * @hw:   pointer to the HW structure
+ * @mem:  ptr to mem struct to free
+ **/
+inline int i40e_free_dma_mem_d(struct i40e_hw *hw, struct i40e_dma_mem *mem)
+{
+	struct i40e_pf *nf = (struct i40e_pf *)hw->back;
+
+	dma_free_coherent(&nf->pdev->dev, mem->size, mem->va, mem->pa);
+	mem->va = NULL;
+	mem->pa = 0;
+	mem->size = 0;
+
+	return 0;
+}
+
+/**
+ * i40e_allocate_virt_mem_d - OS specific memory alloc for shared code
+ * @hw:   pointer to the HW structure
+ * @mem:  ptr to mem struct to fill out
+ * @size: size of memory requested
+ **/
+inline int i40e_allocate_virt_mem_d(struct i40e_hw *hw,
+				    struct i40e_virt_mem *mem,
+				    u32 size)
+{
+	mem->size = size;
+	mem->va = kzalloc(size, GFP_KERNEL);
+
+	if (!mem->va)
+		return -ENOMEM;
+
+	return 0;
+}
+
+/**
+ * i40e_free_virt_mem_d - OS specific memory free for shared code
+ * @hw:   pointer to the HW structure
+ * @mem:  ptr to mem struct to free
+ **/
+inline int i40e_free_virt_mem_d(struct i40e_hw *hw, struct i40e_virt_mem *mem)
+{
+	/* it's ok to kfree a NULL pointer */
+	kfree(mem->va);
+	mem->va = NULL;
+	mem->size = 0;
+
+	return 0;
+}
+
+/* prototype */
+inline void i40e_destroy_spinlock_d(struct i40e_spinlock *sp);
+inline void i40e_acquire_spinlock_d(struct i40e_spinlock *sp);
+inline void i40e_release_spinlock_d(struct i40e_spinlock *sp);
+
+/**
+ * i40e_init_spinlock_d - OS specific spinlock init for shared code
+ * @sp: pointer to a spinlock declared in driver space
+ **/
+static inline void i40e_init_spinlock_d(struct i40e_spinlock *sp)
+{
+	mutex_init((struct mutex *)sp);
+}
+
+/**
+ * i40e_acquire_spinlock_d - OS specific spinlock acquire for shared code
+ * @sp: pointer to a spinlock declared in driver space
+ **/
+inline void i40e_acquire_spinlock_d(struct i40e_spinlock *sp)
+{
+	mutex_lock((struct mutex *)sp);
+}
+
+/**
+ * i40e_release_spinlock_d - OS specific spinlock release for shared code
+ * @sp: pointer to a spinlock declared in driver space
+ **/
+inline void i40e_release_spinlock_d(struct i40e_spinlock *sp)
+{
+	mutex_unlock((struct mutex *)sp);
+}
+
+/**
+ * i40e_destroy_spinlock_d - OS specific spinlock destroy for shared code
+ * @sp: pointer to a spinlock declared in driver space
+ **/
+inline void i40e_destroy_spinlock_d(struct i40e_spinlock *sp)
+{
+	mutex_destroy((struct mutex *)sp);
+}
+#endif /* _I40E_HELPER_H_ */
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_hmc.c linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_hmc.c
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_hmc.c	2024-05-10 01:26:45.341079560 -0400
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_hmc.c	2024-05-13 03:58:25.372491940 -0400
@@ -1,13 +1,14 @@
 // SPDX-License-Identifier: GPL-2.0
-/* Copyright(c) 2013 - 2018 Intel Corporation. */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
 
-#include "i40e.h"
 #include "i40e_osdep.h"
 #include "i40e_register.h"
 #include "i40e_status.h"
 #include "i40e_alloc.h"
 #include "i40e_hmc.h"
+#ifndef I40E_NO_TYPE_HEADER
 #include "i40e_type.h"
+#endif
 
 /**
  * i40e_add_sd_table_entry - Adds a segment descriptor to the table
@@ -23,11 +24,11 @@
 					      enum i40e_sd_entry_type type,
 					      u64 direct_mode_sz)
 {
-	enum i40e_memory_type mem_type __attribute__((unused));
+	i40e_status ret_code = I40E_SUCCESS;
 	struct i40e_hmc_sd_entry *sd_entry;
+	enum   i40e_memory_type mem_type;
 	bool dma_mem_alloc_done = false;
 	struct i40e_dma_mem mem;
-	i40e_status ret_code = I40E_SUCCESS;
 	u64 alloc_len;
 
 	if (NULL == hmc_info->sd_table.sd_entry) {
@@ -67,9 +68,13 @@
 			sd_entry->u.pd_table.pd_entry =
 				(struct i40e_hmc_pd_entry *)
 				sd_entry->u.pd_table.pd_entry_virt_mem.va;
-			sd_entry->u.pd_table.pd_page_addr = mem;
+			i40e_memcpy(&sd_entry->u.pd_table.pd_page_addr,
+				    &mem, sizeof(struct i40e_dma_mem),
+				    I40E_NONDMA_TO_NONDMA);
 		} else {
-			sd_entry->u.bp.addr = mem;
+			i40e_memcpy(&sd_entry->u.bp.addr,
+				    &mem, sizeof(struct i40e_dma_mem),
+				    I40E_NONDMA_TO_NONDMA);
 			sd_entry->u.bp.sd_pd_index = sd_index;
 		}
 		/* initialize the sd entry */
@@ -82,7 +87,7 @@
 	if (I40E_SD_TYPE_DIRECT == sd_entry->entry_type)
 		I40E_INC_BP_REFCNT(&sd_entry->u.bp);
 exit:
-	if (ret_code)
+	if (I40E_SUCCESS != ret_code)
 		if (dma_mem_alloc_done)
 			i40e_free_dma_mem(hw, &mem);
 
@@ -111,7 +116,7 @@
 					      u32 pd_index,
 					      struct i40e_dma_mem *rsrc_pg)
 {
-	i40e_status ret_code = 0;
+	i40e_status ret_code = I40E_SUCCESS;
 	struct i40e_hmc_pd_table *pd_table;
 	struct i40e_hmc_pd_entry *pd_entry;
 	struct i40e_dma_mem mem;
@@ -149,7 +154,8 @@
 			pd_entry->rsrc_pg = false;
 		}
 
-		pd_entry->bp.addr = *page;
+		i40e_memcpy(&pd_entry->bp.addr, page,
+			    sizeof(struct i40e_dma_mem), I40E_NONDMA_TO_NONDMA);
 		pd_entry->bp.sd_pd_index = pd_index;
 		pd_entry->bp.entry_type = I40E_SD_TYPE_PAGED;
 		/* Set page address and valid bit */
@@ -159,7 +165,8 @@
 		pd_addr += rel_pd_idx;
 
 		/* Add the backing page physical address in the pd entry */
-		memcpy(pd_addr, &page_desc, sizeof(u64));
+		i40e_memcpy(pd_addr, &page_desc, sizeof(u64),
+			    I40E_NONDMA_TO_DMA);
 
 		pd_entry->sd_index = sd_idx;
 		pd_entry->valid = true;
@@ -189,7 +196,7 @@
 					struct i40e_hmc_info *hmc_info,
 					u32 idx)
 {
-	i40e_status ret_code = 0;
+	i40e_status ret_code = I40E_SUCCESS;
 	struct i40e_hmc_pd_entry *pd_entry;
 	struct i40e_hmc_pd_table *pd_table;
 	struct i40e_hmc_sd_entry *sd_entry;
@@ -222,13 +229,13 @@
 	I40E_DEC_PD_REFCNT(pd_table);
 	pd_addr = (u64 *)pd_table->pd_page_addr.va;
 	pd_addr += rel_pd_idx;
-	memset(pd_addr, 0, sizeof(u64));
+	i40e_memset(pd_addr, 0, sizeof(u64), I40E_DMA_MEM);
 	I40E_INVALIDATE_PF_HMC_PD(hw, sd_idx, idx);
 
 	/* free memory here */
 	if (!pd_entry->rsrc_pg)
-		ret_code = i40e_free_dma_mem(hw, &pd_entry->bp.addr);
-	if (ret_code)
+		ret_code = i40e_free_dma_mem(hw, &(pd_entry->bp.addr));
+	if (I40E_SUCCESS != ret_code)
 		goto exit;
 	if (!pd_table->ref_cnt)
 		i40e_free_virt_mem(hw, &pd_table->pd_entry_virt_mem);
@@ -244,7 +251,7 @@
 i40e_status i40e_prep_remove_sd_bp(struct i40e_hmc_info *hmc_info,
 					     u32 idx)
 {
-	i40e_status ret_code = 0;
+	i40e_status ret_code = I40E_SUCCESS;
 	struct i40e_hmc_sd_entry *sd_entry;
 
 	/* get the entry and decrease its ref counter */
@@ -282,7 +289,7 @@
 	sd_entry = &hmc_info->sd_table.sd_entry[idx];
 	I40E_CLEAR_PF_SD_ENTRY(hw, idx, I40E_SD_TYPE_DIRECT);
 
-	return i40e_free_dma_mem(hw, &sd_entry->u.bp.addr);
+	return i40e_free_dma_mem(hw, &(sd_entry->u.bp.addr));
 }
 
 /**
@@ -293,7 +300,7 @@
 i40e_status i40e_prep_remove_pd_page(struct i40e_hmc_info *hmc_info,
 					       u32 idx)
 {
-	i40e_status ret_code = 0;
+	i40e_status ret_code = I40E_SUCCESS;
 	struct i40e_hmc_sd_entry *sd_entry;
 
 	sd_entry = &hmc_info->sd_table.sd_entry[idx];
@@ -330,5 +337,5 @@
 	sd_entry = &hmc_info->sd_table.sd_entry[idx];
 	I40E_CLEAR_PF_SD_ENTRY(hw, idx, I40E_SD_TYPE_PAGED);
 
-	return  i40e_free_dma_mem(hw, &sd_entry->u.pd_table.pd_page_addr);
+	return i40e_free_dma_mem(hw, &(sd_entry->u.pd_table.pd_page_addr));
 }
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_hmc.h linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_hmc.h
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_hmc.h	2024-05-10 01:26:45.341079560 -0400
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_hmc.h	2024-05-13 03:58:25.372491940 -0400
@@ -1,5 +1,5 @@
 /* SPDX-License-Identifier: GPL-2.0 */
-/* Copyright(c) 2013 - 2018 Intel Corporation. */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
 
 #ifndef _I40E_HMC_H_
 #define _I40E_HMC_H_
Binary files linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_hmc.o and linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_hmc.o differ
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_lan_hmc.c linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_lan_hmc.c
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_lan_hmc.c	2024-05-10 01:26:45.341079560 -0400
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_lan_hmc.c	2024-05-13 03:58:25.372491940 -0400
@@ -1,7 +1,6 @@
 // SPDX-License-Identifier: GPL-2.0
-/* Copyright(c) 2013 - 2018 Intel Corporation. */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
 
-#include "i40e.h"
 #include "i40e_osdep.h"
 #include "i40e_register.h"
 #include "i40e_type.h"
@@ -79,7 +78,7 @@
 					u32 fcoe_filt_num)
 {
 	struct i40e_hmc_obj_info *obj, *full_obj;
-	i40e_status ret_code = 0;
+	i40e_status ret_code = I40E_SUCCESS;
 	u64 l2fpm_size;
 	u32 size_exp;
 
@@ -114,7 +113,7 @@
 		ret_code = I40E_ERR_INVALID_HMC_OBJ_COUNT;
 		hw_dbg(hw, "i40e_init_lan_hmc: Tx context: asks for 0x%x but max allowed is 0x%x, returns error %d\n",
 			  txq_num, obj->max_cnt, ret_code);
-		goto init_lan_hmc_out;
+		goto free_hmc_out;
 	}
 
 	/* aggregate values into the full LAN object for later */
@@ -137,7 +136,7 @@
 		ret_code = I40E_ERR_INVALID_HMC_OBJ_COUNT;
 		hw_dbg(hw, "i40e_init_lan_hmc: Rx context: asks for 0x%x but max allowed is 0x%x, returns error %d\n",
 			  rxq_num, obj->max_cnt, ret_code);
-		goto init_lan_hmc_out;
+		goto free_hmc_out;
 	}
 
 	/* aggregate values into the full LAN object for later */
@@ -160,7 +159,7 @@
 		ret_code = I40E_ERR_INVALID_HMC_OBJ_COUNT;
 		hw_dbg(hw, "i40e_init_lan_hmc: FCoE context: asks for 0x%x but max allowed is 0x%x, returns error %d\n",
 			  fcoe_cntx_num, obj->max_cnt, ret_code);
-		goto init_lan_hmc_out;
+		goto free_hmc_out;
 	}
 
 	/* aggregate values into the full LAN object for later */
@@ -183,7 +182,7 @@
 		ret_code = I40E_ERR_INVALID_HMC_OBJ_COUNT;
 		hw_dbg(hw, "i40e_init_lan_hmc: FCoE filter: asks for 0x%x but max allowed is 0x%x, returns error %d\n",
 			  fcoe_filt_num, obj->max_cnt, ret_code);
-		goto init_lan_hmc_out;
+		goto free_hmc_out;
 	}
 
 	/* aggregate values into the full LAN object for later */
@@ -204,7 +203,7 @@
 					  (sizeof(struct i40e_hmc_sd_entry) *
 					  hw->hmc.sd_table.sd_cnt));
 		if (ret_code)
-			goto init_lan_hmc_out;
+			goto free_hmc_out;
 		hw->hmc.sd_table.sd_entry =
 			(struct i40e_hmc_sd_entry *)hw->hmc.sd_table.addr.va;
 	}
@@ -213,6 +212,11 @@
 
 init_lan_hmc_out:
 	return ret_code;
+free_hmc_out:
+	if (hw->hmc.hmc_obj_virt_mem.va)
+		i40e_free_virt_mem(hw, &hw->hmc.hmc_obj_virt_mem);
+
+	return ret_code;
 }
 
 /**
@@ -233,9 +237,9 @@
 						 struct i40e_hmc_info *hmc_info,
 						 u32 idx)
 {
-	i40e_status ret_code = 0;
+	i40e_status ret_code = I40E_SUCCESS;
 
-	if (!i40e_prep_remove_pd_page(hmc_info, idx))
+	if (i40e_prep_remove_pd_page(hmc_info, idx) == I40E_SUCCESS)
 		ret_code = i40e_remove_pd_page_new(hw, hmc_info, idx, true);
 
 	return ret_code;
@@ -260,9 +264,9 @@
 					       struct i40e_hmc_info *hmc_info,
 					       u32 idx)
 {
-	i40e_status ret_code = 0;
+	i40e_status ret_code = I40E_SUCCESS;
 
-	if (!i40e_prep_remove_sd_bp(hmc_info, idx))
+	if (i40e_prep_remove_sd_bp(hmc_info, idx) == I40E_SUCCESS)
 		ret_code = i40e_remove_sd_bp_new(hw, hmc_info, idx, true);
 
 	return ret_code;
@@ -279,7 +283,7 @@
 static i40e_status i40e_create_lan_hmc_object(struct i40e_hw *hw,
 				struct i40e_hmc_lan_create_obj_info *info)
 {
-	i40e_status ret_code = 0;
+	i40e_status ret_code = I40E_SUCCESS;
 	struct i40e_hmc_sd_entry *sd_entry;
 	u32 pd_idx1 = 0, pd_lmt1 = 0;
 	u32 pd_idx = 0, pd_lmt = 0;
@@ -349,7 +353,7 @@
 		ret_code = i40e_add_sd_table_entry(hw, info->hmc_info, j,
 						   info->entry_type,
 						   sd_size);
-		if (ret_code)
+		if (I40E_SUCCESS != ret_code)
 			goto exit_sd_error;
 		sd_entry = &info->hmc_info->sd_table.sd_entry[j];
 		if (I40E_SD_TYPE_PAGED == sd_entry->entry_type) {
@@ -366,7 +370,7 @@
 				ret_code = i40e_add_pd_table_entry(hw,
 								info->hmc_info,
 								i, NULL);
-				if (ret_code) {
+				if (I40E_SUCCESS != ret_code) {
 					pd_error = true;
 					break;
 				}
@@ -439,9 +443,9 @@
 					     enum i40e_hmc_model model)
 {
 	struct i40e_hmc_lan_create_obj_info info;
-	i40e_status ret_code = 0;
 	u8 hmc_fn_id = hw->hmc.hmc_fn_id;
 	struct i40e_hmc_obj_info *obj;
+	i40e_status ret_code = I40E_SUCCESS;
 
 	/* Initialize part of the create object info struct */
 	info.hmc_info = &hw->hmc;
@@ -457,9 +461,9 @@
 		/* Make one big object, a single SD */
 		info.count = 1;
 		ret_code = i40e_create_lan_hmc_object(hw, &info);
-		if (ret_code && (model == I40E_HMC_MODEL_DIRECT_PREFERRED))
+		if ((ret_code != I40E_SUCCESS) && (model == I40E_HMC_MODEL_DIRECT_PREFERRED))
 			goto try_type_paged;
-		else if (ret_code)
+		else if (ret_code != I40E_SUCCESS)
 			goto configure_lan_hmc_out;
 		/* else clause falls through the break */
 		break;
@@ -469,7 +473,7 @@
 		/* Make one big object in the PD table */
 		info.count = 1;
 		ret_code = i40e_create_lan_hmc_object(hw, &info);
-		if (ret_code)
+		if (ret_code != I40E_SUCCESS)
 			goto configure_lan_hmc_out;
 		break;
 	default:
@@ -511,7 +515,7 @@
 }
 
 /**
- * i40e_delete_hmc_object - remove hmc objects
+ * i40e_delete_lan_hmc_object - remove hmc objects
  * @hw: pointer to the HW structure
  * @info: pointer to i40e_hmc_delete_obj_info struct
  *
@@ -523,7 +527,7 @@
 static i40e_status i40e_delete_lan_hmc_object(struct i40e_hw *hw,
 				struct i40e_hmc_lan_delete_obj_info *info)
 {
-	i40e_status ret_code = 0;
+	i40e_status ret_code = I40E_SUCCESS;
 	struct i40e_hmc_pd_table *pd_table;
 	u32 pd_idx, pd_lmt, rel_pd_idx;
 	u32 sd_idx, sd_lmt;
@@ -588,7 +592,7 @@
 			&info->hmc_info->sd_table.sd_entry[sd_idx].u.pd_table;
 		if (pd_table->pd_entry[rel_pd_idx].valid) {
 			ret_code = i40e_remove_pd_bp(hw, info->hmc_info, j);
-			if (ret_code)
+			if (I40E_SUCCESS != ret_code)
 				goto exit;
 		}
 	}
@@ -609,12 +613,12 @@
 		switch (info->hmc_info->sd_table.sd_entry[i].entry_type) {
 		case I40E_SD_TYPE_DIRECT:
 			ret_code = i40e_remove_sd_bp(hw, info->hmc_info, i);
-			if (ret_code)
+			if (I40E_SUCCESS != ret_code)
 				goto exit;
 			break;
 		case I40E_SD_TYPE_PAGED:
 			ret_code = i40e_remove_pd_page(hw, info->hmc_info, i);
-			if (ret_code)
+			if (I40E_SUCCESS != ret_code)
 				goto exit;
 			break;
 		default:
@@ -659,7 +663,7 @@
 
 #define I40E_HMC_STORE(_struct, _ele)		\
 	offsetof(struct _struct, _ele),		\
-	FIELD_SIZEOF(struct _struct, _ele)
+	sizeof_field(struct _struct, _ele)
 
 struct i40e_context_ele {
 	u16 offset;
@@ -752,13 +756,13 @@
 	/* get the current bits from the target bit string */
 	dest = hmc_bits + (ce_info->lsb / 8);
 
-	memcpy(&dest_byte, dest, sizeof(dest_byte));
+	i40e_memcpy(&dest_byte, dest, sizeof(dest_byte), I40E_DMA_TO_NONDMA);
 
 	dest_byte &= ~mask;	/* get the bits not changing */
 	dest_byte |= src_byte;	/* add in the new bits */
 
 	/* put it all back */
-	memcpy(dest, &dest_byte, sizeof(dest_byte));
+	i40e_memcpy(dest, &dest_byte, sizeof(dest_byte), I40E_NONDMA_TO_DMA);
 }
 
 /**
@@ -796,13 +800,13 @@
 	/* get the current bits from the target bit string */
 	dest = hmc_bits + (ce_info->lsb / 8);
 
-	memcpy(&dest_word, dest, sizeof(dest_word));
+	i40e_memcpy(&dest_word, dest, sizeof(dest_word), I40E_DMA_TO_NONDMA);
 
-	dest_word &= ~(cpu_to_le16(mask));	/* get the bits not changing */
-	dest_word |= cpu_to_le16(src_word);	/* add in the new bits */
+	dest_word &= ~(CPU_TO_LE16(mask));	/* get the bits not changing */
+	dest_word |= CPU_TO_LE16(src_word);	/* add in the new bits */
 
 	/* put it all back */
-	memcpy(dest, &dest_word, sizeof(dest_word));
+	i40e_memcpy(dest, &dest_word, sizeof(dest_word), I40E_NONDMA_TO_DMA);
 }
 
 /**
@@ -848,13 +852,13 @@
 	/* get the current bits from the target bit string */
 	dest = hmc_bits + (ce_info->lsb / 8);
 
-	memcpy(&dest_dword, dest, sizeof(dest_dword));
+	i40e_memcpy(&dest_dword, dest, sizeof(dest_dword), I40E_DMA_TO_NONDMA);
 
-	dest_dword &= ~(cpu_to_le32(mask));	/* get the bits not changing */
-	dest_dword |= cpu_to_le32(src_dword);	/* add in the new bits */
+	dest_dword &= ~(CPU_TO_LE32(mask));	/* get the bits not changing */
+	dest_dword |= CPU_TO_LE32(src_dword);	/* add in the new bits */
 
 	/* put it all back */
-	memcpy(dest, &dest_dword, sizeof(dest_dword));
+	i40e_memcpy(dest, &dest_dword, sizeof(dest_dword), I40E_NONDMA_TO_DMA);
 }
 
 /**
@@ -900,13 +904,13 @@
 	/* get the current bits from the target bit string */
 	dest = hmc_bits + (ce_info->lsb / 8);
 
-	memcpy(&dest_qword, dest, sizeof(dest_qword));
+	i40e_memcpy(&dest_qword, dest, sizeof(dest_qword), I40E_DMA_TO_NONDMA);
 
-	dest_qword &= ~(cpu_to_le64(mask));	/* get the bits not changing */
-	dest_qword |= cpu_to_le64(src_qword);	/* add in the new bits */
+	dest_qword &= ~(CPU_TO_LE64(mask));	/* get the bits not changing */
+	dest_qword |= CPU_TO_LE64(src_qword);	/* add in the new bits */
 
 	/* put it all back */
-	memcpy(dest, &dest_qword, sizeof(dest_qword));
+	i40e_memcpy(dest, &dest_qword, sizeof(dest_qword), I40E_NONDMA_TO_DMA);
 }
 
 /**
@@ -920,9 +924,10 @@
 					enum i40e_hmc_lan_rsrc_type hmc_type)
 {
 	/* clean the bit array */
-	memset(context_bytes, 0, (u32)hw->hmc.hmc_obj[hmc_type].size);
+	i40e_memset(context_bytes, 0, (u32)hw->hmc.hmc_obj[hmc_type].size,
+		    I40E_DMA_MEM);
 
-	return 0;
+	return I40E_SUCCESS;
 }
 
 /**
@@ -959,12 +964,12 @@
 		}
 	}
 
-	return 0;
+	return I40E_SUCCESS;
 }
 
 /**
  * i40e_hmc_get_object_va - retrieves an object's virtual address
- * @hw: the hardware struct, from which we obtain the i40e_hmc_info pointer
+ * @hw: pointer to the hw structure
  * @object_base: pointer to u64 to get the va
  * @rsrc_type: the hmc resource type
  * @obj_idx: hmc object index
@@ -973,24 +978,20 @@
  * base pointer.  This function is used for LAN Queue contexts.
  **/
 static
-i40e_status i40e_hmc_get_object_va(struct i40e_hw *hw, u8 **object_base,
-				   enum i40e_hmc_lan_rsrc_type rsrc_type,
-				   u32 obj_idx)
+i40e_status i40e_hmc_get_object_va(struct i40e_hw *hw,
+					u8 **object_base,
+					enum i40e_hmc_lan_rsrc_type rsrc_type,
+					u32 obj_idx)
 {
-	struct i40e_hmc_info *hmc_info = &hw->hmc;
 	u32 obj_offset_in_sd, obj_offset_in_pd;
+	struct i40e_hmc_info     *hmc_info = &hw->hmc;
 	struct i40e_hmc_sd_entry *sd_entry;
 	struct i40e_hmc_pd_entry *pd_entry;
 	u32 pd_idx, pd_lmt, rel_pd_idx;
-	i40e_status ret_code = 0;
+	i40e_status ret_code = I40E_SUCCESS;
 	u64 obj_offset_in_fpm;
 	u32 sd_idx, sd_lmt;
 
-	if (NULL == hmc_info) {
-		ret_code = I40E_ERR_BAD_PTR;
-		hw_dbg(hw, "i40e_hmc_get_object_va: bad hmc_info ptr\n");
-		goto exit;
-	}
 	if (NULL == hmc_info->hmc_obj) {
 		ret_code = I40E_ERR_BAD_PTR;
 		hw_dbg(hw, "i40e_hmc_get_object_va: bad hmc_info->hmc_obj ptr\n");
@@ -1048,8 +1049,7 @@
 	i40e_status err;
 	u8 *context_bytes;
 
-	err = i40e_hmc_get_object_va(hw, &context_bytes,
-				     I40E_HMC_LAN_TX, queue);
+	err = i40e_hmc_get_object_va(hw, &context_bytes, I40E_HMC_LAN_TX, queue);
 	if (err < 0)
 		return err;
 
@@ -1069,8 +1069,7 @@
 	i40e_status err;
 	u8 *context_bytes;
 
-	err = i40e_hmc_get_object_va(hw, &context_bytes,
-				     I40E_HMC_LAN_TX, queue);
+	err = i40e_hmc_get_object_va(hw, &context_bytes, I40E_HMC_LAN_TX, queue);
 	if (err < 0)
 		return err;
 
@@ -1089,8 +1088,7 @@
 	i40e_status err;
 	u8 *context_bytes;
 
-	err = i40e_hmc_get_object_va(hw, &context_bytes,
-				     I40E_HMC_LAN_RX, queue);
+	err = i40e_hmc_get_object_va(hw, &context_bytes, I40E_HMC_LAN_RX, queue);
 	if (err < 0)
 		return err;
 
@@ -1110,8 +1108,7 @@
 	i40e_status err;
 	u8 *context_bytes;
 
-	err = i40e_hmc_get_object_va(hw, &context_bytes,
-				     I40E_HMC_LAN_RX, queue);
+	err = i40e_hmc_get_object_va(hw, &context_bytes, I40E_HMC_LAN_RX, queue);
 	if (err < 0)
 		return err;
 
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_lan_hmc.h linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_lan_hmc.h
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_lan_hmc.h	2024-05-10 01:26:45.341079560 -0400
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_lan_hmc.h	2024-05-13 03:58:25.372491940 -0400
@@ -1,5 +1,5 @@
 /* SPDX-License-Identifier: GPL-2.0 */
-/* Copyright(c) 2013 - 2018 Intel Corporation. */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
 
 #ifndef _I40E_LAN_HMC_H_
 #define _I40E_LAN_HMC_H_
Binary files linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_lan_hmc.o and linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_lan_hmc.o differ
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_main.c linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_main.c
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_main.c	2024-05-10 01:26:45.357079639 -0400
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_main.c	2024-05-13 03:58:25.372491940 -0400
@@ -1,17 +1,29 @@
 // SPDX-License-Identifier: GPL-2.0
-/* Copyright(c) 2013 - 2018 Intel Corporation. */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
 
-#include <linux/etherdevice.h>
-#include <linux/of_net.h>
-#include <linux/pci.h>
+#ifdef HAVE_XDP_SUPPORT
 #include <linux/bpf.h>
-
+#endif
 /* Local includes */
 #include "i40e.h"
+#include "i40e_helper.h"
 #include "i40e_diag.h"
-#include "i40e_xsk.h"
+#ifdef HAVE_VXLAN_RX_OFFLOAD
+#if IS_ENABLED(CONFIG_VXLAN)
+#include <net/vxlan.h>
+#endif
+#endif /* HAVE_VXLAN_RX_OFFLOAD */
+#ifdef HAVE_GRE_ENCAP_OFFLOAD
+#include <net/gre.h>
+#endif /* HAVE_GRE_ENCAP_OFFLOAD */
+#ifdef HAVE_GENEVE_RX_OFFLOAD
+#if IS_ENABLED(CONFIG_GENEVE)
+#include <net/geneve.h>
+#endif
+#endif /* HAVE_GENEVE_RX_OFFLOAD */
+#ifdef HAVE_UDP_ENC_RX_OFFLOAD
 #include <net/udp_tunnel.h>
-#include <net/xdp_sock.h>
+#endif
 /* All i40e tracepoints are defined by the include below, which
  * must be included exactly once across the whole kernel with
  * CREATE_TRACE_POINTS defined
@@ -19,20 +31,26 @@
 #define CREATE_TRACE_POINTS
 #include "i40e_trace.h"
 
-const char i40e_driver_name[] = "i40e";
+char i40e_driver_name[] = "i40e";
 static const char i40e_driver_string[] =
-			"Intel(R) Ethernet Connection XL710 Network Driver";
+		"Intel(R) 40-10 Gigabit Ethernet Connection Network Driver";
 
-#define DRV_KERN "-k"
+#ifndef DRV_VERSION_LOCAL
+#define DRV_VERSION_LOCAL
+#endif /* DRV_VERSION_LOCAL */
+
+#define DRV_VERSION_DESC ""
 
 #define DRV_VERSION_MAJOR 2
-#define DRV_VERSION_MINOR 8
-#define DRV_VERSION_BUILD 20
+#define DRV_VERSION_MINOR 17
+#define DRV_VERSION_BUILD 4
+#define DRV_VERSION_SUBBUILD 0
 #define DRV_VERSION __stringify(DRV_VERSION_MAJOR) "." \
-	     __stringify(DRV_VERSION_MINOR) "." \
-	     __stringify(DRV_VERSION_BUILD)    DRV_KERN
+	__stringify(DRV_VERSION_MINOR) "." \
+	__stringify(DRV_VERSION_BUILD) \
+	DRV_VERSION_DESC __stringify(DRV_VERSION_LOCAL)
 const char i40e_driver_version_str[] = DRV_VERSION;
-static const char i40e_copyright[] = "Copyright (c) 2013 - 2019 Intel Corporation.";
+static const char i40e_copyright[] = "Copyright(c) 2013 - 2021 Intel Corporation.";
 
 /* a bit of forward declarations */
 static void i40e_vsi_reinit_locked(struct i40e_vsi *vsi);
@@ -43,19 +61,24 @@
 static int i40e_setup_misc_vector(struct i40e_pf *pf);
 static void i40e_determine_queue_usage(struct i40e_pf *pf);
 static int i40e_setup_pf_filter_control(struct i40e_pf *pf);
-static void i40e_prep_for_reset(struct i40e_pf *pf, bool lock_acquired);
+static void i40e_clear_rss_config_user(struct i40e_vsi *vsi);
+static void i40e_prep_for_reset(struct i40e_pf *pf);
+static void i40e_reset_and_rebuild(struct i40e_pf *pf, bool reinit,
+				   bool lock_acquired);
 static int i40e_reset(struct i40e_pf *pf);
 static void i40e_rebuild(struct i40e_pf *pf, bool reinit, bool lock_acquired);
 static int i40e_setup_misc_vector_for_recovery_mode(struct i40e_pf *pf);
 static int i40e_restore_interrupt_scheme(struct i40e_pf *pf);
 static bool i40e_check_recovery_mode(struct i40e_pf *pf);
 static int i40e_init_recovery_mode(struct i40e_pf *pf, struct i40e_hw *hw);
+static i40e_status i40e_force_link_state(struct i40e_pf *pf, bool is_up);
+static bool i40e_is_total_port_shutdown_enabled(struct i40e_pf *pf);
 static void i40e_fdir_sb_setup(struct i40e_pf *pf);
+
 static int i40e_veb_get_bw_info(struct i40e_veb *veb);
 static int i40e_get_capabilities(struct i40e_pf *pf,
 				 enum i40e_admin_queue_opc list_type);
 
-
 /* i40e_pci_tbl - PCI Device ID Table
  *
  * Last entry must be all 0s
@@ -71,21 +94,22 @@
 	{PCI_VDEVICE(INTEL, I40E_DEV_ID_QSFP_A), 0},
 	{PCI_VDEVICE(INTEL, I40E_DEV_ID_QSFP_B), 0},
 	{PCI_VDEVICE(INTEL, I40E_DEV_ID_QSFP_C), 0},
+	{PCI_VDEVICE(INTEL, I40E_DEV_ID_5G_BASE_T_BC), 0},
 	{PCI_VDEVICE(INTEL, I40E_DEV_ID_10G_BASE_T), 0},
 	{PCI_VDEVICE(INTEL, I40E_DEV_ID_10G_BASE_T4), 0},
 	{PCI_VDEVICE(INTEL, I40E_DEV_ID_10G_BASE_T_BC), 0},
 	{PCI_VDEVICE(INTEL, I40E_DEV_ID_10G_SFP), 0},
 	{PCI_VDEVICE(INTEL, I40E_DEV_ID_10G_B), 0},
+	{PCI_VDEVICE(INTEL, I40E_DEV_ID_20G_KR2), 0},
+	{PCI_VDEVICE(INTEL, I40E_DEV_ID_20G_KR2_A), 0},
+	{PCI_VDEVICE(INTEL, I40E_DEV_ID_X710_N3000), 0},
+	{PCI_VDEVICE(INTEL, I40E_DEV_ID_XXV710_N3000), 0},
 	{PCI_VDEVICE(INTEL, I40E_DEV_ID_KX_X722), 0},
 	{PCI_VDEVICE(INTEL, I40E_DEV_ID_QSFP_X722), 0},
 	{PCI_VDEVICE(INTEL, I40E_DEV_ID_SFP_X722), 0},
 	{PCI_VDEVICE(INTEL, I40E_DEV_ID_1G_BASE_T_X722), 0},
 	{PCI_VDEVICE(INTEL, I40E_DEV_ID_10G_BASE_T_X722), 0},
 	{PCI_VDEVICE(INTEL, I40E_DEV_ID_SFP_I_X722), 0},
-	{PCI_VDEVICE(INTEL, I40E_DEV_ID_20G_KR2), 0},
-	{PCI_VDEVICE(INTEL, I40E_DEV_ID_20G_KR2_A), 0},
-	{PCI_VDEVICE(INTEL, I40E_DEV_ID_X710_N3000), 0},
-	{PCI_VDEVICE(INTEL, I40E_DEV_ID_XXV710_N3000), 0},
 	{PCI_VDEVICE(INTEL, I40E_DEV_ID_25G_B), 0},
 	{PCI_VDEVICE(INTEL, I40E_DEV_ID_25G_SFP28), 0},
 	/* required last entry */
@@ -94,86 +118,38 @@
 MODULE_DEVICE_TABLE(pci, i40e_pci_tbl);
 
 #define I40E_MAX_VF_COUNT 128
+#define OPTION_UNSET    -1
+#define I40E_PARAM_INIT { [0 ... I40E_MAX_NIC] = OPTION_UNSET}
+#define I40E_MAX_NIC 64
+#if !defined(HAVE_SRIOV_CONFIGURE) && !defined(HAVE_RHEL6_SRIOV_CONFIGURE)
+#ifdef CONFIG_PCI_IOV
+static int max_vfs[I40E_MAX_NIC+1] = I40E_PARAM_INIT;
+module_param_array_named(max_vfs, max_vfs, int, NULL, 0);
+MODULE_PARM_DESC(max_vfs,
+	"Number of Virtual Functions: 0 = disable (default), 1-"
+	__stringify(I40E_MAX_VF_COUNT) " = enable "
+	"this many VFs");
+#endif /* CONFIG_PCI_IOV */
+#endif /* HAVE_SRIOV_CONFIGURE */
+
 static int debug = -1;
-module_param(debug, uint, 0);
-MODULE_PARM_DESC(debug, "Debug level (0=none,...,16=all), Debug mask (0x8XXXXXXX)");
+module_param(debug, int, 0);
+MODULE_PARM_DESC(debug, "Debug level (0=none,...,16=all)");
+static int l4mode = L4_MODE_DISABLED;
+module_param(l4mode, int, 0000);
+MODULE_PARM_DESC(l4mode, "L4 cloud filter mode: 0=UDP,1=TCP,2=Both,-1=Disabled(default)");
+
 
 MODULE_AUTHOR("Intel Corporation, <e1000-devel@lists.sourceforge.net>");
-MODULE_DESCRIPTION("Intel(R) Ethernet Connection XL710 Network Driver");
-MODULE_LICENSE("GPL v2");
+MODULE_DESCRIPTION("Intel(R) 40-10 Gigabit Ethernet Connection Network Driver");
+MODULE_LICENSE("GPL");
 MODULE_VERSION(DRV_VERSION);
 
 static struct workqueue_struct *i40e_wq;
 
-/**
- * i40e_allocate_dma_mem_d - OS specific memory alloc for shared code
- * @hw:   pointer to the HW structure
- * @mem:  ptr to mem struct to fill out
- * @size: size of memory requested
- * @alignment: what to align the allocation to
- **/
-int i40e_allocate_dma_mem_d(struct i40e_hw *hw, struct i40e_dma_mem *mem,
-			    u64 size, u32 alignment)
-{
-	struct i40e_pf *pf = (struct i40e_pf *)hw->back;
-
-	mem->size = ALIGN(size, alignment);
-	mem->va = dma_alloc_coherent(&pf->pdev->dev, mem->size, &mem->pa,
-				     GFP_KERNEL);
-	if (!mem->va)
-		return -ENOMEM;
-
-	return 0;
-}
-
-/**
- * i40e_free_dma_mem_d - OS specific memory free for shared code
- * @hw:   pointer to the HW structure
- * @mem:  ptr to mem struct to free
- **/
-int i40e_free_dma_mem_d(struct i40e_hw *hw, struct i40e_dma_mem *mem)
+bool i40e_is_l4mode_enabled(void)
 {
-	struct i40e_pf *pf = (struct i40e_pf *)hw->back;
-
-	dma_free_coherent(&pf->pdev->dev, mem->size, mem->va, mem->pa);
-	mem->va = NULL;
-	mem->pa = 0;
-	mem->size = 0;
-
-	return 0;
-}
-
-/**
- * i40e_allocate_virt_mem_d - OS specific memory alloc for shared code
- * @hw:   pointer to the HW structure
- * @mem:  ptr to mem struct to fill out
- * @size: size of memory requested
- **/
-int i40e_allocate_virt_mem_d(struct i40e_hw *hw, struct i40e_virt_mem *mem,
-			     u32 size)
-{
-	mem->size = size;
-	mem->va = kzalloc(size, GFP_KERNEL);
-
-	if (!mem->va)
-		return -ENOMEM;
-
-	return 0;
-}
-
-/**
- * i40e_free_virt_mem_d - OS specific memory free for shared code
- * @hw:   pointer to the HW structure
- * @mem:  ptr to mem struct to free
- **/
-int i40e_free_virt_mem_d(struct i40e_hw *hw, struct i40e_virt_mem *mem)
-{
-	/* it's ok to kfree a NULL pointer */
-	kfree(mem->va);
-	mem->va = NULL;
-	mem->size = 0;
-
-	return 0;
+	return l4mode > L4_MODE_DISABLED;
 }
 
 /**
@@ -184,16 +160,12 @@
  * @id: an owner id to stick on the items assigned
  *
  * Returns the base item index of the lump, or negative for error
- *
- * The search_hint trick and lack of advanced fit-finding only work
- * because we're highly likely to have all the same size lump requests.
- * Linear search time and any fragmentation should be minimal.
  **/
 static int i40e_get_lump(struct i40e_pf *pf, struct i40e_lump_tracking *pile,
 			 u16 needed, u16 id)
 {
 	int ret = -ENOMEM;
-	int i, j;
+	u16 i, j;
 
 	if (!pile || needed == 0 || id >= I40E_PILE_VALID_BIT) {
 		dev_info(&pf->pdev->dev,
@@ -202,8 +174,8 @@
 		return -EINVAL;
 	}
 
-	/* start the linear search with an imperfect hint */
-	i = pile->search_hint;
+	/* start from beginning because earlier areas may have been freed */
+	i = 0;
 	while (i < pile->num_entries) {
 		/* skip already allocated entries */
 		if (pile->list[i] & I40E_PILE_VALID_BIT) {
@@ -222,7 +194,6 @@
 			for (j = 0; j < needed; j++)
 				pile->list[i+j] = id | I40E_PILE_VALID_BIT;
 			ret = i;
-			pile->search_hint = i + j;
 			break;
 		}
 
@@ -245,7 +216,7 @@
 {
 	int valid_id = (id | I40E_PILE_VALID_BIT);
 	int count = 0;
-	int i;
+	u16 i;
 
 	if (!pile || index >= pile->num_entries)
 		return -EINVAL;
@@ -257,13 +228,43 @@
 		count++;
 	}
 
-	if (count && index < pile->search_hint)
-		pile->search_hint = index;
-
 	return count;
 }
 
 /**
+ * i40e_max_lump_qp - find a biggest size of lump available in qp_pile
+ * @pf: pointer to private device data structure
+ *
+ * Returns the max size of lump in a qp_pile, or negative for error
+ */
+int i40e_max_lump_qp(struct i40e_pf *pf)
+{
+	struct i40e_lump_tracking *pile = pf->qp_pile;
+	int pool_size, max_size;
+	u16 i;
+
+	if (!pile) {
+		dev_info(&pf->pdev->dev,
+			 "param err: pile=%s\n",
+			 pile ? "<valid>" : "<null>");
+		return -EINVAL;
+	}
+
+	pool_size = 0;
+	max_size = 0;
+	for (i = 0; i < pile->num_entries; i++) {
+		if (pile->list[i] & I40E_PILE_VALID_BIT) {
+			pool_size = 0;
+			continue;
+		}
+		if (max_size < ++pool_size)
+			max_size = pool_size;
+	}
+
+	return max_size;
+}
+
+/**
  * i40e_find_vsi_from_id - searches for the vsi with the given id
  * @pf: the pf structure to search for the vsi
  * @id: id of the vsi it is searching for
@@ -280,6 +281,22 @@
 }
 
 /**
+ * i40e_find_vsi_from_seid - searches for the vsi with the given seid
+ * @pf: the pf structure to search for the vsi
+ * @seid: seid of the vsi it is searching for
+ **/
+struct i40e_vsi *i40e_find_vsi_from_seid(struct i40e_pf *pf, u16 seid)
+{
+	int i;
+
+	for (i = 0; i < pf->num_alloc_vsi; i++)
+		if (pf->vsi[i] && (pf->vsi[i]->seid == seid))
+			return pf->vsi[i];
+
+	return NULL;
+}
+
+/**
  * i40e_service_event_schedule - Schedule the service task to wake up
  * @pf: board private structure
  *
@@ -296,12 +313,18 @@
 /**
  * i40e_tx_timeout - Respond to a Tx Hang
  * @netdev: network interface device structure
+ * @txqueue: stuck queue
  *
  * If any port has noticed a Tx timeout, it is likely that the whole
  * device is munged, not just the one netdev port, so go for the full
  * reset.
  **/
+#ifdef HAVE_TX_TIMEOUT_TXQUEUE
+static void
+i40e_tx_timeout(struct net_device *netdev, __always_unused unsigned int txqueue)
+#else
 static void i40e_tx_timeout(struct net_device *netdev)
+#endif
 {
 	struct i40e_netdev_priv *np = netdev_priv(netdev);
 	struct i40e_vsi *vsi = np->vsi;
@@ -342,6 +365,13 @@
 		}
 	}
 
+#ifdef CONFIG_DEBUG_FS
+	if (vsi->block_tx_timeout) {
+		netdev_info(netdev, "tx_timeout recovery disabled\n");
+		return;
+	}
+#endif
+
 	if (time_after(jiffies, (pf->tx_timeout_last_recovery + HZ*20)))
 		pf->tx_timeout_recovery_level = 1;  /* reset after some time */
 	else if (time_before(jiffies,
@@ -383,7 +413,9 @@
 		set_bit(__I40E_GLOBAL_RESET_REQUESTED, pf->state);
 		break;
 	default:
-		netdev_err(netdev, "tx_timeout recovery unsuccessful\n");
+		netdev_err(netdev, "tx_timeout recovery unsuccessful, device is in non-recoverable state.\n");
+		set_bit(__I40E_DOWN_REQUESTED, pf->state);
+		set_bit(__I40E_VSI_DOWN_REQUESTED, vsi->state);
 		break;
 	}
 
@@ -398,11 +430,23 @@
  * Returns the address of the device statistics structure.
  * The statistics are actually updated from the service task.
  **/
+#ifdef HAVE_NDO_GET_STATS64
 struct rtnl_link_stats64 *i40e_get_vsi_stats_struct(struct i40e_vsi *vsi)
 {
 	return &vsi->net_stats;
 }
+#else
+struct net_device_stats *i40e_get_vsi_stats_struct(struct i40e_vsi *vsi)
+{
+	/* It is possible for a VSIs to not have a netdev */
+	if (vsi->netdev)
+		return &vsi->netdev->stats;
+	else
+		return &vsi->net_stats;
+}
+#endif
 
+#ifdef HAVE_NDO_GET_STATS64
 /**
  * i40e_get_netdev_stats_struct_tx - populate stats from a Tx ring
  * @ring: Tx ring to get statistics from
@@ -432,50 +476,58 @@
  * Returns the address of the device statistics structure.
  * The statistics are actually updated from the service task.
  **/
+#ifdef HAVE_VOID_NDO_GET_STATS64
 static void i40e_get_netdev_stats_struct(struct net_device *netdev,
-				  struct rtnl_link_stats64 *stats)
+					 struct rtnl_link_stats64 *stats)
+#else
+static struct rtnl_link_stats64 *i40e_get_netdev_stats_struct(
+					     struct net_device *netdev,
+					     struct rtnl_link_stats64 *stats)
+#endif /* HAVE_VOID_NDO_GET_STATS64 */
 {
 	struct i40e_netdev_priv *np = netdev_priv(netdev);
+	struct i40e_ring *tx_ring, *rx_ring;
 	struct i40e_vsi *vsi = np->vsi;
 	struct rtnl_link_stats64 *vsi_stats = i40e_get_vsi_stats_struct(vsi);
-	struct i40e_ring *ring;
 	int i;
 
 	if (test_bit(__I40E_VSI_DOWN, vsi->state))
+#ifdef HAVE_VOID_NDO_GET_STATS64
 		return;
+#else
+		return stats;
+#endif /* HAVE_VOID_NDO_GET_STATS_64 */
 
 	if (!vsi->tx_rings)
+#ifdef HAVE_VOID_NDO_GET_STATS64
 		return;
+#else
+		return stats;
+#endif /* HAVE_VOID_NDO_GET_STATS_64 */
 
 	rcu_read_lock();
 	for (i = 0; i < vsi->num_queue_pairs; i++) {
 		u64 bytes, packets;
 		unsigned int start;
 
-		ring = READ_ONCE(vsi->tx_rings[i]);
-		if (!ring)
+		tx_ring = READ_ONCE(vsi->tx_rings[i]);
+		if (!tx_ring)
 			continue;
-		i40e_get_netdev_stats_struct_tx(ring, stats);
 
-		if (i40e_enabled_xdp_vsi(vsi)) {
-			ring = READ_ONCE(vsi->xdp_rings[i]);
-			if (!ring)
-				continue;
-			i40e_get_netdev_stats_struct_tx(ring, stats);
-		}
+		i40e_get_netdev_stats_struct_tx(tx_ring, stats);
+		rx_ring = &tx_ring[1];
 
-		ring = READ_ONCE(vsi->rx_rings[i]);
-		if (!ring)
-			continue;
 		do {
-			start   = u64_stats_fetch_begin_irq(&ring->syncp);
-			packets = ring->stats.packets;
-			bytes   = ring->stats.bytes;
-		} while (u64_stats_fetch_retry_irq(&ring->syncp, start));
+			start = u64_stats_fetch_begin_irq(&rx_ring->syncp);
+			packets = rx_ring->stats.packets;
+			bytes   = rx_ring->stats.bytes;
+		} while (u64_stats_fetch_retry_irq(&rx_ring->syncp, start));
 
 		stats->rx_packets += packets;
 		stats->rx_bytes   += bytes;
 
+		if (i40e_enabled_xdp_vsi(vsi))
+			i40e_get_netdev_stats_struct_tx(&rx_ring[1], stats);
 	}
 	rcu_read_unlock();
 
@@ -487,7 +539,21 @@
 	stats->rx_dropped	= vsi_stats->rx_dropped;
 	stats->rx_crc_errors	= vsi_stats->rx_crc_errors;
 	stats->rx_length_errors	= vsi_stats->rx_length_errors;
+#ifndef HAVE_VOID_NDO_GET_STATS64
+
+		return stats;
+#endif
+}
+#else
+static struct net_device_stats *i40e_get_netdev_stats_struct(
+						      struct net_device *netdev)
+{
+	struct i40e_netdev_priv *np = netdev_priv(netdev);
+	struct i40e_vsi *vsi = np->vsi;
+
+	return i40e_get_vsi_stats_struct(vsi);
 }
+#endif /* HAVE_NDO_GET_STATS64 */
 
 /**
  * i40e_vsi_reset_stats - Resets all stats of the given vsi
@@ -495,7 +561,11 @@
  **/
 void i40e_vsi_reset_stats(struct i40e_vsi *vsi)
 {
+#ifdef HAVE_NDO_GET_STATS64
 	struct rtnl_link_stats64 *ns;
+#else
+	struct net_device_stats *ns;
+#endif
 	int i;
 
 	if (!vsi)
@@ -512,6 +582,10 @@
 			       sizeof(vsi->rx_rings[i]->stats));
 			memset(&vsi->rx_rings[i]->rx_stats, 0,
 			       sizeof(vsi->rx_rings[i]->rx_stats));
+#ifdef HAVE_XDP_SUPPORT
+			memset(&vsi->rx_rings[i]->xdp_stats, 0,
+			       sizeof(vsi->rx_rings[i]->xdp_stats));
+#endif
 			memset(&vsi->tx_rings[i]->stats, 0,
 			       sizeof(vsi->tx_rings[i]->stats));
 			memset(&vsi->tx_rings[i]->tx_stats, 0,
@@ -547,6 +621,22 @@
 		}
 	}
 	pf->hw_csum_rx_error = 0;
+#ifdef I40E_ADD_PROBES
+	pf->tcp_segs = 0;
+	pf->tx_tcp_cso = 0;
+	pf->tx_udp_cso = 0;
+	pf->tx_sctp_cso = 0;
+	pf->tx_ip4_cso = 0;
+	pf->rx_tcp_cso = 0;
+	pf->rx_udp_cso = 0;
+	pf->rx_sctp_cso = 0;
+	pf->rx_ip4_cso = 0;
+	pf->rx_tcp_cso_err = 0;
+	pf->rx_udp_cso_err = 0;
+	pf->rx_sctp_cso_err = 0;
+	pf->rx_ip4_cso_err = 0;
+	pf->hw_csum_rx_outer = 0;
+#endif
 }
 
 /**
@@ -706,11 +796,10 @@
 	i40e_stat_update32(hw, I40E_GLSW_TDPC(idx),
 			   veb->stat_offsets_loaded,
 			   &oes->tx_discards, &es->tx_discards);
-	if (hw->revision_id > 0)
-		i40e_stat_update32(hw, I40E_GLSW_RUPP(idx),
-				   veb->stat_offsets_loaded,
-				   &oes->rx_unknown_protocol,
-				   &es->rx_unknown_protocol);
+	i40e_stat_update32(hw, I40E_GLSW_RUPP(idx),
+			   veb->stat_offsets_loaded,
+			   &oes->rx_unknown_protocol, &es->rx_unknown_protocol);
+
 	i40e_stat_update48(hw, I40E_GLSW_GORCH(idx), I40E_GLSW_GORCL(idx),
 			   veb->stat_offsets_loaded,
 			   &oes->rx_bytes, &es->rx_bytes);
@@ -774,15 +863,22 @@
 static void i40e_update_vsi_stats(struct i40e_vsi *vsi)
 {
 	struct i40e_pf *pf = vsi->back;
+#ifdef HAVE_NDO_GET_STATS64
 	struct rtnl_link_stats64 *ons;
 	struct rtnl_link_stats64 *ns;   /* netdev stats */
+#else
+	struct net_device_stats *ons;
+	struct net_device_stats *ns;   /* netdev stats */
+#endif
 	struct i40e_eth_stats *oes;
 	struct i40e_eth_stats *es;     /* device's eth stats */
 	u32 tx_restart, tx_busy;
 	struct i40e_ring *p;
 	u32 rx_page, rx_buf;
 	u64 bytes, packets;
+#ifdef HAVE_NDO_GET_STATS64
 	unsigned int start;
+#endif
 	u64 tx_linearize;
 	u64 tx_force_wb;
 	u64 rx_p, rx_b;
@@ -810,14 +906,16 @@
 	for (q = 0; q < vsi->num_queue_pairs; q++) {
 		/* locate Tx ring */
 		p = READ_ONCE(vsi->tx_rings[q]);
-		if (!p)
-			continue;
 
+#ifdef HAVE_NDO_GET_STATS64
 		do {
 			start = u64_stats_fetch_begin_irq(&p->syncp);
+#endif
 			packets = p->stats.packets;
 			bytes = p->stats.bytes;
+#ifdef HAVE_NDO_GET_STATS64
 		} while (u64_stats_fetch_retry_irq(&p->syncp, start));
+#endif
 		tx_b += bytes;
 		tx_p += packets;
 		tx_restart += p->tx_stats.restart_queue;
@@ -825,16 +923,17 @@
 		tx_linearize += p->tx_stats.tx_linearize;
 		tx_force_wb += p->tx_stats.tx_force_wb;
 
-		/* locate Rx ring */
-		p = READ_ONCE(vsi->rx_rings[q]);
-		if (!p)
-			continue;
-
+		/* Rx queue is part of the same block as Tx queue */
+		p = &p[1];
+#ifdef HAVE_NDO_GET_STATS64
 		do {
 			start = u64_stats_fetch_begin_irq(&p->syncp);
+#endif
 			packets = p->stats.packets;
 			bytes = p->stats.bytes;
+#ifdef HAVE_NDO_GET_STATS64
 		} while (u64_stats_fetch_retry_irq(&p->syncp, start));
+#endif
 		rx_b += bytes;
 		rx_p += packets;
 		rx_buf += p->rx_stats.alloc_buff_failed;
@@ -867,7 +966,12 @@
 	/* pull in a couple PF stats if this is the main vsi */
 	if (vsi == pf->vsi[pf->lan_vsi]) {
 		ns->rx_crc_errors = pf->stats.crc_errors;
-		ns->rx_errors = pf->stats.crc_errors + pf->stats.illegal_bytes;
+		ns->rx_errors = pf->stats.crc_errors +
+				pf->stats.illegal_bytes +
+				pf->hw_csum_rx_error +
+				pf->stats.rx_length_errors +
+				pf->stats.rx_undersize +
+				pf->stats.rx_oversize;
 		ns->rx_length_errors = pf->stats.rx_length_errors;
 	}
 }
@@ -881,7 +985,7 @@
 	struct i40e_hw_port_stats *osd = &pf->stats_offsets;
 	struct i40e_hw_port_stats *nsd = &pf->stats;
 	struct i40e_hw *hw = &pf->hw;
-	u32 val;
+
 	int i;
 
 	i40e_stat_update48(hw, I40E_GLPRT_GORCH(hw->port),
@@ -1073,19 +1177,12 @@
 			I40E_GLQF_PCNT(I40E_FD_ATR_TUNNEL_STAT_IDX(hw->pf_id)),
 			&nsd->fd_atr_tunnel_match);
 
-	val = rd32(hw, I40E_PRTPM_EEE_STAT);
-	nsd->tx_lpi_status =
-		       (val & I40E_PRTPM_EEE_STAT_TX_LPI_STATUS_MASK) >>
-			I40E_PRTPM_EEE_STAT_TX_LPI_STATUS_SHIFT;
-	nsd->rx_lpi_status =
-		       (val & I40E_PRTPM_EEE_STAT_RX_LPI_STATUS_MASK) >>
-			I40E_PRTPM_EEE_STAT_RX_LPI_STATUS_SHIFT;
-	i40e_stat_update32(hw, I40E_PRTPM_TLPIC,
-			   pf->stat_offsets_loaded,
-			   &osd->tx_lpi_count, &nsd->tx_lpi_count);
-	i40e_stat_update32(hw, I40E_PRTPM_RLPIC,
-			   pf->stat_offsets_loaded,
-			   &osd->rx_lpi_count, &nsd->rx_lpi_count);
+	i40e_get_phy_lpi_status(hw, nsd);
+	i40e_lpi_stat_update(hw, pf->stat_offsets_loaded,
+			     &osd->tx_lpi_count, &nsd->tx_lpi_count,
+			     &osd->rx_lpi_count, &nsd->rx_lpi_count);
+	i40e_get_lpi_duration(hw, nsd,
+			      &nsd->tx_lpi_duration, &nsd->rx_lpi_duration);
 
 	if (pf->flags & I40E_FLAG_FD_SB_ENABLED &&
 	    !test_bit(__I40E_FD_SB_AUTO_DISABLED, pf->state))
@@ -1119,6 +1216,25 @@
 }
 
 /**
+ * i40e_count_filters - counts VSI mac filters
+ * @vsi: the VSI to be searched
+ *
+ * Returns count of mac filters
+ **/
+int i40e_count_filters(struct i40e_vsi *vsi)
+{
+	struct i40e_mac_filter *f;
+	struct hlist_node *h;
+	int bkt;
+	int cnt = 0;
+
+	hash_for_each_safe(vsi->mac_filter_hash, bkt, h, f, hlist)
+		++cnt;
+
+	return cnt;
+}
+
+/**
  * i40e_find_filter - Search VSI filter list for specific mac/vlan filter
  * @vsi: the VSI to be searched
  * @macaddr: the MAC address
@@ -1126,8 +1242,8 @@
  *
  * Returns ptr to the filter object or NULL
  **/
-static struct i40e_mac_filter *i40e_find_filter(struct i40e_vsi *vsi,
-						const u8 *macaddr, s16 vlan)
+struct i40e_mac_filter *i40e_find_filter(struct i40e_vsi *vsi,
+					 const u8 *macaddr, s16 vlan)
 {
 	struct i40e_mac_filter *f;
 	u64 key;
@@ -1162,13 +1278,36 @@
 
 	key = i40e_addr_to_hkey(macaddr);
 	hash_for_each_possible(vsi->mac_filter_hash, f, hlist, key) {
-		if ((ether_addr_equal(macaddr, f->macaddr)))
+		if (ether_addr_equal(macaddr, f->macaddr))
 			return f;
 	}
 	return NULL;
 }
 
 /**
+ * i40e_is_vid - Check if VSI is tagging/stripping VLAN
+ * @info: info field of VSI
+ *
+ * Returns true if VSI has ovid/pvid configured or false otherwise
+ **/
+bool i40e_is_vid(struct i40e_aqc_vsi_properties_data *info)
+{
+	return info->pvid || info->outer_vlan;
+}
+
+/**
+ * i40e_is_double_vlan - Check if HW is processing double VLAN
+ * @hw: pointer to the hardware structure
+ *
+ * Returns true if HW is processing on outer vlan and has configured
+ * double VLAN or false otherwise
+ **/
+bool i40e_is_double_vlan(struct i40e_hw *hw)
+{
+	return hw->is_double_vlan && hw->is_outer_vlan_processing;
+}
+
+/**
  * i40e_is_vsi_in_vlan - Check if VSI is in vlan mode
  * @vsi: the VSI to be searched
  *
@@ -1176,11 +1315,8 @@
  **/
 bool i40e_is_vsi_in_vlan(struct i40e_vsi *vsi)
 {
-	/* If we have a PVID, always operate in VLAN mode */
-	if (vsi->info.pvid)
-		return true;
-
-	/* We need to operate in VLAN mode whenever we have any filters with
+	/* If we have a PVID or OVID, always operate in VLAN mode.
+	 * We need to operate in VLAN mode whenever we have any filters with
 	 * a VLAN other than I40E_VLAN_ALL. We could check the table each
 	 * time, incurring search cost repeatedly. However, we can notice two
 	 * things:
@@ -1192,20 +1328,20 @@
 	 *    i40e_sync_filters_subtask.
 	 *
 	 * Thus, we can simply use a boolean value, has_vlan_filters which we
-	 * will set to true when we add a VLAN filter in i40e_add_filter. Then
+	 * will set to true when we add a vlan filter in i40e_add_filter. Then
 	 * we have to perform the full search after deleting filters in
 	 * i40e_sync_filters_subtask, but we already have to search
 	 * filters here and can perform the check at the same time. This
-	 * results in avoiding embedding a loop for VLAN mode inside another
+	 * results in avoiding embedding a loop for vlan mode inside another
 	 * loop over all the filters, and should maintain correctness as noted
 	 * above.
 	 */
-	return vsi->has_vlan_filter;
+	return i40e_is_vid(&vsi->info) || vsi->has_vlan_filter;
 }
 
 /**
  * i40e_correct_mac_vlan_filters - Correct non-VLAN filters if necessary
- * @vsi: the VSI to configure
+ * @vsi: the vsi to configure
  * @tmp_add_list: list of filters ready to be added
  * @tmp_del_list: list of filters ready to be deleted
  * @vlan_filters: the number of active VLAN filters
@@ -1237,9 +1373,8 @@
 					 struct hlist_head *tmp_del_list,
 					 int vlan_filters)
 {
-	s16 pvid = le16_to_cpu(vsi->info.pvid);
 	struct i40e_mac_filter *f, *add_head;
-	struct i40e_new_mac_filter *new;
+	struct i40e_new_mac_filter *new_mac;
 	struct hlist_node *h;
 	int bkt, new_vlan;
 
@@ -1258,48 +1393,42 @@
 	 */
 
 	/* Update the filters about to be added in place */
-	hlist_for_each_entry(new, tmp_add_list, hlist) {
-		if (pvid && new->f->vlan != pvid)
-			new->f->vlan = pvid;
-		else if (vlan_filters && new->f->vlan == I40E_VLAN_ANY)
-			new->f->vlan = 0;
-		else if (!vlan_filters && new->f->vlan == 0)
-			new->f->vlan = I40E_VLAN_ANY;
+	hlist_for_each_entry(new_mac, tmp_add_list, hlist) {
+		if (vlan_filters && new_mac->f->vlan == I40E_VLAN_ANY)
+			new_mac->f->vlan = 0;
+		else if (!vlan_filters && new_mac->f->vlan == 0)
+			new_mac->f->vlan = I40E_VLAN_ANY;
 	}
 
 	/* Update the remaining active filters */
 	hash_for_each_safe(vsi->mac_filter_hash, bkt, h, f, hlist) {
 		/* Combine the checks for whether a filter needs to be changed
-		 * and then determine the new VLAN inside the if block, in
+		 * and then determine the new vlan inside the if block, in
 		 * order to avoid duplicating code for adding the new filter
 		 * then deleting the old filter.
 		 */
-		if ((pvid && f->vlan != pvid) ||
-		    (vlan_filters && f->vlan == I40E_VLAN_ANY) ||
+		if ((vlan_filters && f->vlan == I40E_VLAN_ANY) ||
 		    (!vlan_filters && f->vlan == 0)) {
-			/* Determine the new vlan we will be adding */
-			if (pvid)
-				new_vlan = pvid;
-			else if (vlan_filters)
+			/* Determine the new vlan */
+			if (vlan_filters)
 				new_vlan = 0;
 			else
 				new_vlan = I40E_VLAN_ANY;
-
 			/* Create the new filter */
 			add_head = i40e_add_filter(vsi, f->macaddr, new_vlan);
 			if (!add_head)
 				return -ENOMEM;
 
 			/* Create a temporary i40e_new_mac_filter */
-			new = kzalloc(sizeof(*new), GFP_ATOMIC);
-			if (!new)
+			new_mac = kzalloc(sizeof(*new_mac), GFP_ATOMIC);
+			if (!new_mac)
 				return -ENOMEM;
 
-			new->f = add_head;
-			new->state = add_head->state;
+			new_mac->f = add_head;
+			new_mac->state = add_head->state;
 
 			/* Add the new filter to the tmp list */
-			hlist_add_head(&new->hlist, tmp_add_list);
+			hlist_add_head(&new_mac->hlist, tmp_add_list);
 
 			/* Put the original filter into the delete list */
 			f->state = I40E_FILTER_REMOVE;
@@ -1314,6 +1443,162 @@
 }
 
 /**
+ * i40e_get_vf_new_vlan - Get new vlan id on a vf
+ * @vsi: the vsi to configure
+ * @new_mac: new mac filter to be added
+ * @f: existing mac filter, replaced with new_mac->f if new_mac is not NULL
+ * @vlan_filters: the number of active VLAN filters
+ * @trusted: flag if the VF is trusted
+ *
+ * If VF somehow has VLAN=-1 filter, update them to VLAN=0. VF allowed only
+ * to listen to untagged traffic, unless it has VLAN filter present.
+ *
+ * Finally, in a similar fashion, this function will return PVID when
+ * there is an active PVID assigned to this VSI.
+ *
+ * Returns the value of the new vlan filter or
+ * the old value if no new filter is needed.
+ */
+static s16 i40e_get_vf_new_vlan(struct i40e_vsi *vsi,
+				struct i40e_new_mac_filter *new_mac,
+				struct i40e_mac_filter *f,
+				int vlan_filters,
+				bool trusted)
+{
+	struct i40e_pf *pf = vsi->back;
+	bool is_any;
+
+	if (new_mac)
+		f = new_mac->f;
+
+	is_any = (trusted ||
+		  (pf->flags & I40E_FLAG_VF_VLAN_PRUNE_DISABLE));
+
+	if ((vlan_filters && f->vlan == I40E_VLAN_ANY) ||
+	    (!is_any && !vlan_filters && f->vlan == I40E_VLAN_ANY) ||
+	    (is_any && !vlan_filters && f->vlan == 0)) {
+		if (is_any)
+			return I40E_VLAN_ANY;
+		else
+			return 0;
+	}
+
+	return f->vlan;
+}
+
+/**
+ * i40e_correct_vf_mac_vlan_filters - Correct non-VLAN VF filters if necessary
+ * @vsi: the vsi to configure
+ * @tmp_add_list: list of filters ready to be added
+ * @tmp_del_list: list of filters ready to be deleted
+ * @vlan_filters: the number of active VLAN filters
+ * @trusted: flag if the VF is trusted
+ * @allow_untagged: flag if the VF allows untagged VLAN packets
+ *
+ * Correct VF VLAN filters based on current VLAN filters, trust, PVID
+ * and vf-vlan-prune-disable flag.
+ *
+ * In case of memory allocation failure return -ENOMEM. Otherwise, return 0.
+ *
+ * This function is only expected to be called from within
+ * i40e_sync_vsi_filters.
+ *
+ * NOTE: This function expects to be called while under the
+ * mac_filter_hash_lock
+ */
+static int i40e_correct_vf_mac_vlan_filters(struct i40e_vsi *vsi,
+					    struct hlist_head *tmp_add_list,
+					    struct hlist_head *tmp_del_list,
+					    int vlan_filters,
+					    bool allow_untagged,
+					    bool trusted)
+{
+	enum i40e_filter_state new_state = I40E_FILTER_INVALID;
+	struct i40e_mac_filter *f, *add_head;
+	struct i40e_new_mac_filter *new_mac;
+	struct hlist_node *h;
+	int bkt, new_vlan;
+
+	hlist_for_each_entry(new_mac, tmp_add_list, hlist) {
+		new_mac->f->vlan = i40e_get_vf_new_vlan(vsi, new_mac, NULL,
+							vlan_filters, trusted);
+	}
+
+	hash_for_each_safe(vsi->mac_filter_hash, bkt, h, f, hlist) {
+		new_vlan = i40e_get_vf_new_vlan(vsi, NULL, f, vlan_filters,
+						trusted);
+		if (new_vlan != f->vlan) {
+			add_head = i40e_add_filter(vsi, f->macaddr, new_vlan);
+			if (!add_head)
+				return -ENOMEM;
+			/* Create a temporary i40e_new_mac_filter */
+			new_mac = kzalloc(sizeof(*new_mac), GFP_ATOMIC);
+			if (!new_mac)
+				return -ENOMEM;
+			new_mac->f = add_head;
+			new_mac->state = add_head->state;
+
+			/* Add the new filter to the tmp list */
+			hlist_add_head(&new_mac->hlist, tmp_add_list);
+
+			/* Put the original filter into the delete list */
+			f->state = I40E_FILTER_REMOVE;
+			hash_del(&f->hlist);
+			hlist_add_head(&f->hlist, tmp_del_list);
+		}
+	}
+
+	hash_for_each_safe(vsi->mac_filter_hash, bkt, h, f, hlist) {
+		new_state = f->state;
+		if (!allow_untagged &&
+		    (f->state == I40E_FILTER_ACTIVE ||
+		     f->state == I40E_FILTER_NEW) &&
+		    f->vlan == 0)
+			new_state = I40E_FILTER_INACTIVE;
+		if (allow_untagged && f->state == I40E_FILTER_INACTIVE &&
+		    f->vlan == 0)
+			new_state = I40E_FILTER_ACTIVE;
+		if (new_state != f->state) {
+			f->state = new_state;
+			if (new_state == I40E_FILTER_INACTIVE) {
+				add_head = kzalloc(sizeof(*f), GFP_ATOMIC);
+				if (!add_head)
+					return -ENOMEM;
+				ether_addr_copy(add_head->macaddr, f->macaddr);
+				add_head->vlan = 0;
+				add_head->state = I40E_FILTER_REMOVE;
+				INIT_HLIST_NODE(&add_head->hlist);
+				/* Add the existing filter to the tmp del list,
+				 * it will be removed from FW in
+				 * sync_vsi_filters
+				 */
+				hlist_add_head(&add_head->hlist, tmp_del_list);
+			} else {
+				add_head = i40e_add_filter(vsi, f->macaddr, 0);
+				if (!add_head)
+					return -ENOMEM;
+				/* As this filter exists already, driver must
+				 * update it's state to new
+				 */
+				add_head->state = I40E_FILTER_NEW;
+				/* Create a temporary i40e_new_mac_filter */
+				new_mac = kzalloc(sizeof(*new_mac), GFP_ATOMIC);
+				if (!new_mac)
+					return -ENOMEM;
+				new_mac->f = add_head;
+				new_mac->state = I40E_FILTER_NEW;
+				/* Add the new filter to the tmp add list,
+				 * it will be added to FW in sync_vsi_filters
+				 */
+				hlist_add_head(&new_mac->hlist, tmp_add_list);
+			}
+		}
+	}
+	vsi->has_vlan_filter = !!vlan_filters;
+	return 0;
+}
+
+/**
  * i40e_rm_default_mac_filter - Remove the default MAC filter set by NVM
  * @vsi: the PF Main VSI - inappropriate for any other VSI
  * @macaddr: the MAC address
@@ -1373,7 +1658,7 @@
 			return NULL;
 
 		/* Update the boolean indicating if we need to function in
-		 * VLAN mode.
+		 * vlan mode.
 		 */
 		if (vlan >= 0)
 			vsi->has_vlan_filter = true;
@@ -1381,6 +1666,7 @@
 		ether_addr_copy(f->macaddr, macaddr);
 		f->vlan = vlan;
 		f->state = I40E_FILTER_NEW;
+
 		INIT_HLIST_NODE(&f->hlist);
 
 		key = i40e_addr_to_hkey(macaddr);
@@ -1405,46 +1691,10 @@
 }
 
 /**
- * __i40e_del_filter - Remove a specific filter from the VSI
- * @vsi: VSI to remove from
- * @f: the filter to remove from the list
- *
- * This function should be called instead of i40e_del_filter only if you know
- * the exact filter you will remove already, such as via i40e_find_filter or
- * i40e_find_mac.
- *
- * NOTE: This function is expected to be called with mac_filter_hash_lock
- * being held.
- * ANOTHER NOTE: This function MUST be called from within the context of
- * the "safe" variants of any list iterators, e.g. list_for_each_entry_safe()
- * instead of list_for_each_entry().
- **/
-void __i40e_del_filter(struct i40e_vsi *vsi, struct i40e_mac_filter *f)
-{
-	if (!f)
-		return;
-
-	/* If the filter was never added to firmware then we can just delete it
-	 * directly and we don't want to set the status to remove or else an
-	 * admin queue command will unnecessarily fire.
-	 */
-	if ((f->state == I40E_FILTER_FAILED) ||
-	    (f->state == I40E_FILTER_NEW)) {
-		hash_del(&f->hlist);
-		kfree(f);
-	} else {
-		f->state = I40E_FILTER_REMOVE;
-	}
-
-	vsi->flags |= I40E_VSI_FLAG_FILTER_CHANGED;
-	set_bit(__I40E_MACVLAN_SYNC_PENDING, vsi->back->state);
-}
-
-/**
- * i40e_del_filter - Remove a MAC/VLAN filter from the VSI
+ * i40e_del_filter - Remove a mac/vlan filter from the VSI
  * @vsi: the VSI to be searched
  * @macaddr: the MAC address
- * @vlan: the VLAN
+ * @vlan: the vlan
  *
  * NOTE: This function is expected to be called with mac_filter_hash_lock
  * being held.
@@ -1482,12 +1732,21 @@
 	struct hlist_node *h;
 	int bkt;
 
-	if (vsi->info.pvid)
-		return i40e_add_filter(vsi, macaddr,
-				       le16_to_cpu(vsi->info.pvid));
+	if (i40e_is_vid(&vsi->info)) {
+		__le16 vid = i40e_is_double_vlan(&vsi->back->hw) ?
+			      vsi->info.outer_vlan :
+			      vsi->info.pvid;
+		i40e_add_filter(vsi, macaddr, 0);
+		/* Driver must add filter 0, in case that port vlan is
+		 * configured after reset and does not have filters 0.
+		 * VLAN 0 will be corrected to active or inactive state based
+		 * on allow_untagged flag
+		 */
+		return i40e_add_filter(vsi, macaddr, le16_to_cpu(vid));
+	}
 
 	if (!i40e_is_vsi_in_vlan(vsi))
-		return i40e_add_filter(vsi, macaddr, I40E_VLAN_ANY);
+		return i40e_add_filter(vsi, macaddr, 0);
 
 	hash_for_each_safe(vsi->mac_filter_hash, bkt, h, f, hlist) {
 		if (f->state == I40E_FILTER_REMOVE)
@@ -1517,7 +1776,6 @@
 	bool found = false;
 	int bkt;
 
-	lockdep_assert_held(&vsi->mac_filter_hash_lock);
 	hash_for_each_safe(vsi->mac_filter_hash, bkt, h, f, hlist) {
 		if (ether_addr_equal(macaddr, f->macaddr)) {
 			__i40e_del_filter(vsi, f);
@@ -1541,10 +1799,10 @@
 static int i40e_set_mac(struct net_device *netdev, void *p)
 {
 	struct i40e_netdev_priv *np = netdev_priv(netdev);
+	struct sockaddr *addr = p;
 	struct i40e_vsi *vsi = np->vsi;
 	struct i40e_pf *pf = vsi->back;
 	struct i40e_hw *hw = &pf->hw;
-	struct sockaddr *addr = p;
 
 	if (!is_valid_ether_addr(addr->sa_data))
 		return -EADDRNOTAVAIL;
@@ -1571,6 +1829,7 @@
 	 * - Copy new address
 	 * - Add new address to MAC filter
 	 */
+
 	spin_lock_bh(&vsi->mac_filter_hash_lock);
 	i40e_del_mac_filter(vsi, netdev->dev_addr);
 	ether_addr_copy(netdev->dev_addr, addr->sa_data);
@@ -1599,13 +1858,15 @@
  * i40e_config_rss_aq - Prepare for RSS using AQ commands
  * @vsi: vsi structure
  * @seed: RSS hash seed
+ * @lut: Buffer to store the lookup table entries
+ * @lut_size: Size of buffer to store the lookup table entries
  **/
 static int i40e_config_rss_aq(struct i40e_vsi *vsi, const u8 *seed,
 			      u8 *lut, u16 lut_size)
 {
+	i40e_status ret = I40E_SUCCESS;
 	struct i40e_pf *pf = vsi->back;
 	struct i40e_hw *hw = &pf->hw;
-	int ret = 0;
 
 	if (seed) {
 		struct i40e_aqc_get_set_rss_key_data *seed_dw =
@@ -1672,6 +1933,7 @@
 	return ret;
 }
 
+#ifdef __TC_MQPRIO_MODE_MAX
 /**
  * i40e_vsi_setup_queue_map_mqprio - Prepares mqprio based tc_config
  * @vsi: the VSI being configured,
@@ -1724,6 +1986,7 @@
 			vsi->tc_config.tc_info[i].qcount = 1;
 			vsi->tc_config.tc_info[i].netdev_tc = 0;
 		}
+		vsi->tc_config.tc_info[i].tc_bw_credits = 0;
 	}
 
 	/* Set actual Tx/Rx queue pairs */
@@ -1758,6 +2021,7 @@
 	}
 	return 0;
 }
+#endif
 
 /**
  * i40e_vsi_setup_queue_map - Setup a VSI queue map based on enabled_tc
@@ -1774,20 +2038,41 @@
 				     bool is_add)
 {
 	struct i40e_pf *pf = vsi->back;
+	u16 num_tc_qps = 0;
 	u16 sections = 0;
 	u8 netdev_tc = 0;
+	u16 qcount = 0;
 	u16 numtc = 1;
-	u16 qcount;
 	u8 offset;
 	u16 qmap;
 	int i;
-	u16 num_tc_qps = 0;
 
 	sections = I40E_AQ_VSI_PROP_QUEUE_MAP_VALID;
 	offset = 0;
+	/* zero out queue mapping, it will get updated on the end of the function */
+	memset(ctxt->info.queue_mapping, '\0', sizeof(ctxt->info.queue_mapping));
+
+	if (vsi->type == I40E_VSI_MAIN) {
+		/* This code helps add more queue to the VSI if we have
+		 * more cores than RSS can support, the higher cores will
+		 * be served by ATR or other filters. Furthermore, the
+		 * non-zero req_queue_pairs says that user requested a new
+		 * queue count via ethtool's set_channels, so use this
+		 * value for queues distribution across traffic classes
+		 */
+		if (vsi->req_queue_pairs > 0)
+			vsi->num_queue_pairs = vsi->req_queue_pairs;
+		else if (pf->flags & I40E_FLAG_MSIX_ENABLED)
+			vsi->num_queue_pairs = pf->num_lan_msix;
+	}
 
 	/* Number of queues per enabled TC */
-	num_tc_qps = vsi->alloc_queue_pairs;
+	if (vsi->type == I40E_VSI_MAIN ||
+	    (vsi->type == I40E_VSI_SRIOV && vsi->num_queue_pairs != 0))
+		num_tc_qps = vsi->num_queue_pairs;
+	else
+		num_tc_qps = vsi->alloc_queue_pairs;
+
 	if (enabled_tc && (vsi->back->flags & I40E_FLAG_DCB_ENABLED)) {
 		/* Find numtc from enabled TC bitmap */
 		for (i = 0, numtc = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
@@ -1814,7 +2099,6 @@
 	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
 		/* See if the given TC is enabled for the given VSI */
 		if (vsi->tc_config.enabled_tc & BIT(i)) {
-			/* TC is enabled */
 			int pow, num_qps;
 
 			switch (vsi->type) {
@@ -1827,8 +2111,14 @@
 					break;
 				}
 				/* fall through */
-			case I40E_VSI_FDIR:
 			case I40E_VSI_SRIOV:
+				/* Do not warn on SRIOV VSIs, map
+				 * queues anyway
+				 */
+				qcount = num_tc_qps;
+				break;
+				/* fall through */
+			case I40E_VSI_FDIR:
 			case I40E_VSI_VMDQ2:
 			default:
 				qcount = num_tc_qps;
@@ -1863,17 +2153,15 @@
 
 			qmap = 0;
 		}
+		vsi->tc_config.tc_info[i].tc_bw_credits = 0;
 		ctxt->info.tc_mapping[i] = cpu_to_le16(qmap);
 	}
 
-	/* Set actual Tx/Rx queue pairs */
-	vsi->num_queue_pairs = offset;
-	if ((vsi->type == I40E_VSI_MAIN) && (numtc == 1)) {
-		if (vsi->req_queue_pairs > 0)
-			vsi->num_queue_pairs = vsi->req_queue_pairs;
-		else if (pf->flags & I40E_FLAG_MSIX_ENABLED)
-			vsi->num_queue_pairs = pf->num_lan_msix;
-	}
+	/* Do not change previously set num_queue_pairs for PFs and VFs*/
+	if ((vsi->type == I40E_VSI_MAIN && numtc != 1) ||
+	    (vsi->type == I40E_VSI_SRIOV && vsi->num_queue_pairs == 0) ||
+	    (vsi->type != I40E_VSI_MAIN && vsi->type != I40E_VSI_SRIOV))
+		vsi->num_queue_pairs = offset;
 
 	/* Scheduler section valid can only be set for ADD VSI */
 	if (is_add) {
@@ -1961,11 +2249,16 @@
 		vsi->flags |= I40E_VSI_FLAG_FILTER_CHANGED;
 		set_bit(__I40E_MACVLAN_SYNC_PENDING, vsi->back->state);
 	}
+
+	/* schedule our worker thread which will take care of
+	 * applying the new filter changes
+	 */
+	i40e_service_event_schedule(vsi->back);
 }
 
 /**
  * i40e_undo_del_filter_entries - Undo the changes made to MAC filter entries
- * @vsi: Pointer to VSI struct
+ * @vsi: Pointer to vsi struct
  * @from: Pointer to list which contains MAC filter entries - changes to
  *        those entries needs to be undone.
  *
@@ -1997,18 +2290,18 @@
 static void i40e_undo_add_filter_entries(struct i40e_vsi *vsi,
 					 struct hlist_head *from)
 {
-	struct i40e_new_mac_filter *new;
+	struct i40e_new_mac_filter *new_mac;
 	struct hlist_node *h;
 
-	hlist_for_each_entry_safe(new, h, from, hlist) {
+	hlist_for_each_entry_safe(new_mac, h, from, hlist) {
 		/* We can simply free the wrapper structure */
-		hlist_del(&new->hlist);
-		kfree(new);
+		hlist_del(&new_mac->hlist);
+		kfree(new_mac);
 	}
 }
 
 /**
- * i40e_next_entry - Get the next non-broadcast filter from a list
+ * i40e_next_filter - Get the next non-broadcast filter from a list
  * @next: pointer to filter in list
  *
  * Returns the next non-broadcast filter in the list. Required so that we
@@ -2085,19 +2378,19 @@
 			  int num_del, int *retval)
 {
 	struct i40e_hw *hw = &vsi->back->hw;
+	enum i40e_admin_queue_err aq_status;
 	i40e_status aq_ret;
-	int aq_err;
 
-	aq_ret = i40e_aq_remove_macvlan(hw, vsi->seid, list, num_del, NULL);
-	aq_err = hw->aq.asq_last_status;
+	aq_ret = i40e_aq_remove_macvlan_v2(hw, vsi->seid, list, num_del, NULL,
+					   &aq_status);
 
 	/* Explicitly ignore and do not report when firmware returns ENOENT */
-	if (aq_ret && !(aq_err == I40E_AQ_RC_ENOENT)) {
+	if (aq_ret && !(aq_status == I40E_AQ_RC_ENOENT)) {
 		*retval = -EIO;
 		dev_info(&vsi->back->pdev->dev,
 			 "ignoring delete macvlan error on %s, err %s, aq_err %s\n",
 			 vsi_name, i40e_stat_str(hw, aq_ret),
-			 i40e_aq_str(hw, aq_err));
+			 i40e_aq_str(hw, aq_status));
 	}
 }
 
@@ -2120,10 +2413,10 @@
 			  int num_add)
 {
 	struct i40e_hw *hw = &vsi->back->hw;
-	int aq_err, fcnt;
+	enum i40e_admin_queue_err aq_status;
+	int fcnt;
 
-	i40e_aq_add_macvlan(hw, vsi->seid, list, num_add, NULL);
-	aq_err = hw->aq.asq_last_status;
+	i40e_aq_add_macvlan_v2(hw, vsi->seid, list, num_add, NULL, &aq_status);
 	fcnt = i40e_update_filter_state(num_add, list, add_head);
 
 	if (fcnt != num_add) {
@@ -2131,17 +2424,17 @@
 			set_bit(__I40E_VSI_OVERFLOW_PROMISC, vsi->state);
 			dev_warn(&vsi->back->pdev->dev,
 				 "Error %s adding RX filters on %s, promiscuous mode forced on\n",
-				 i40e_aq_str(hw, aq_err), vsi_name);
+				 i40e_aq_str(hw, aq_status), vsi_name);
 		} else if (vsi->type == I40E_VSI_SRIOV ||
 			   vsi->type == I40E_VSI_VMDQ1 ||
 			   vsi->type == I40E_VSI_VMDQ2) {
 			dev_warn(&vsi->back->pdev->dev,
 				 "Error %s adding RX filters on %s, please set promiscuous on manually for %s\n",
-				 i40e_aq_str(hw, aq_err), vsi_name, vsi_name);
+				 i40e_aq_str(hw, aq_status), vsi_name, vsi_name);
 		} else {
 			dev_warn(&vsi->back->pdev->dev,
 				 "Error %s adding RX filters on %s, incorrect VSI type: %i.\n",
-				 i40e_aq_str(hw, aq_err), vsi_name, vsi->type);
+				 i40e_aq_str(hw, aq_status), vsi_name, vsi->type);
 		}
 	}
 }
@@ -2197,7 +2490,7 @@
  *
  * There are different ways of setting promiscuous mode on a PF depending on
  * what state/environment we're in.  This identifies and sets it appropriately.
- * Returns 0 on success.
+ * Returns I40E_SUCCESS on success.
  **/
 static int i40e_set_promiscuous(struct i40e_pf *pf, bool promisc)
 {
@@ -2257,6 +2550,56 @@
 	return aq_ret;
 }
 
+ /**
+  * i40e_set_switch_mode - sets up switch mode correctly
+  * @pf: working PF
+  * @l4type: TCP, UDP, or both
+  *
+  * Sets up switch mode correctly
+  **/
+static void i40e_set_switch_mode(struct i40e_pf *pf, u8 l4type)
+{
+	struct i40e_hw *hw;
+	i40e_status ret;
+	u8 mode;
+
+	if (!pf)
+		return;
+
+	hw = &pf->hw;
+
+	/* Set Bit 7 to be valid */
+	mode = I40E_AQ_SET_SWITCH_BIT7_VALID;
+
+	/* We only support destination port filters, so don't set the
+	 * source port bit here.
+	 */
+	if (l4type > I40E_AQ_SET_SWITCH_L4_TYPE_BOTH ||
+	    l4type < I40E_AQ_SET_SWITCH_L4_TYPE_TCP) {
+		dev_warn(&pf->pdev->dev,
+			 "invalid L4 type 0x%x, unable to set switch mode\n",
+			 l4type);
+		return;
+	}
+
+	mode |= l4type;
+
+	/* Set cloud filter mode */
+	mode |= I40E_AQ_SET_SWITCH_MODE_L4_PORT;
+
+	dev_dbg(&pf->pdev->dev, "setting switch mode to 0x%x\n", mode);
+	/* Prep mode field for set_switch_config */
+	ret = i40e_aq_set_switch_config(hw, 0, 0, mode, NULL);
+	/* If the driver is reloaded, the AQ call will fail. So don't make a
+	 * big deal about it.
+	 */
+	if (ret && hw->aq.asq_last_status != I40E_AQ_RC_ESRCH)
+		dev_dbg(&pf->pdev->dev,
+			"couldn't set switch config bits, err %s aq_err %s\n",
+			i40e_stat_str(hw, ret),
+			i40e_aq_str(hw, hw->aq.asq_last_status));
+}
+
 /**
  * i40e_sync_vsi_filters - Update the VSI filter list to the HW
  * @vsi: ptr to the VSI
@@ -2267,16 +2610,16 @@
  **/
 int i40e_sync_vsi_filters(struct i40e_vsi *vsi)
 {
+	struct i40e_new_mac_filter *new_mac, *add_head = NULL;
 	struct hlist_head tmp_add_list, tmp_del_list;
-	struct i40e_mac_filter *f;
-	struct i40e_new_mac_filter *new, *add_head = NULL;
 	struct i40e_hw *hw = &vsi->back->hw;
+	i40e_status aq_ret = I40E_SUCCESS;
 	bool old_overflow, new_overflow;
 	unsigned int failed_filters = 0;
 	unsigned int vlan_filters = 0;
+	struct i40e_mac_filter *f;
 	char vsi_name[16] = "PF";
 	int filter_list_len = 0;
-	i40e_status aq_ret = 0;
 	u32 changed_flags = 0;
 	struct hlist_node *h;
 	struct i40e_pf *pf;
@@ -2326,16 +2669,17 @@
 			}
 			if (f->state == I40E_FILTER_NEW) {
 				/* Create a temporary i40e_new_mac_filter */
-				new = kzalloc(sizeof(*new), GFP_ATOMIC);
-				if (!new)
+				new_mac = kzalloc(sizeof(*new_mac),
+						  GFP_ATOMIC);
+				if (!new_mac)
 					goto err_no_memory_locked;
 
 				/* Store pointer to the real filter */
-				new->f = f;
-				new->state = f->state;
+				new_mac->f = f;
+				new_mac->state = f->state;
 
 				/* Add it to the hash list */
-				hlist_add_head(&new->hlist, &tmp_add_list);
+				hlist_add_head(&new_mac->hlist, &tmp_add_list);
 			}
 
 			/* Count the number of active (current and new) VLAN
@@ -2346,10 +2690,16 @@
 				vlan_filters++;
 		}
 
-		retval = i40e_correct_mac_vlan_filters(vsi,
-						       &tmp_add_list,
-						       &tmp_del_list,
-						       vlan_filters);
+		if (vsi->type != I40E_VSI_SRIOV)
+			retval = i40e_correct_mac_vlan_filters
+				(vsi, &tmp_add_list, &tmp_del_list,
+				 vlan_filters);
+		else
+			retval = i40e_correct_vf_mac_vlan_filters
+				(vsi, &tmp_add_list, &tmp_del_list,
+				 vlan_filters,
+				 pf->vf[vsi->vf_id].allow_untagged,
+				 pf->vf[vsi->vf_id].trusted);
 		if (retval)
 			goto err_no_memory_locked;
 
@@ -2374,7 +2724,6 @@
 			 */
 			if (is_broadcast_ether_addr(f->macaddr)) {
 				i40e_aqc_broadcast_filter(vsi, vsi_name, f);
-
 				hlist_del(&f->hlist);
 				kfree(f);
 				continue;
@@ -2387,7 +2736,7 @@
 				cmd_flags |= I40E_AQC_MACVLAN_DEL_IGNORE_VLAN;
 			} else {
 				del_list[num_del].vlan_tag =
-					cpu_to_le16((u16)(f->vlan));
+					CPU_TO_LE16((u16)(f->vlan));
 			}
 
 			cmd_flags |= I40E_AQC_MACVLAN_DEL_PERFECT_MATCH;
@@ -2428,37 +2777,39 @@
 			goto err_no_memory;
 
 		num_add = 0;
-		hlist_for_each_entry_safe(new, h, &tmp_add_list, hlist) {
+		hlist_for_each_entry_safe(new_mac, h, &tmp_add_list, hlist) {
+			if (unlikely(new_mac->state == I40E_FILTER_INACTIVE))
+				continue;
 			/* handle broadcast filters by updating the broadcast
 			 * promiscuous flag instead of adding a MAC filter.
 			 */
-			if (is_broadcast_ether_addr(new->f->macaddr)) {
+			if (is_broadcast_ether_addr(new_mac->f->macaddr)) {
 				if (i40e_aqc_broadcast_filter(vsi, vsi_name,
-							      new->f))
-					new->state = I40E_FILTER_FAILED;
+							      new_mac->f))
+					new_mac->state = I40E_FILTER_FAILED;
 				else
-					new->state = I40E_FILTER_ACTIVE;
+					new_mac->state = I40E_FILTER_ACTIVE;
 				continue;
 			}
 
 			/* add to add array */
 			if (num_add == 0)
-				add_head = new;
+				add_head = new_mac;
 			cmd_flags = 0;
 			ether_addr_copy(add_list[num_add].mac_addr,
-					new->f->macaddr);
-			if (new->f->vlan == I40E_VLAN_ANY) {
+					new_mac->f->macaddr);
+			if (new_mac->f->vlan == I40E_VLAN_ANY) {
 				add_list[num_add].vlan_tag = 0;
 				cmd_flags |= I40E_AQC_MACVLAN_ADD_IGNORE_VLAN;
 			} else {
 				add_list[num_add].vlan_tag =
-					cpu_to_le16((u16)(new->f->vlan));
+					CPU_TO_LE16((u16)(new_mac->f->vlan));
 			}
 			add_list[num_add].queue_number = 0;
 			/* set invalid match method for later detection */
 			add_list[num_add].match_method = I40E_AQC_MM_ERR_NO_RES;
 			cmd_flags |= I40E_AQC_MACVLAN_ADD_PERFECT_MATCH;
-			add_list[num_add].flags = cpu_to_le16(cmd_flags);
+			add_list[num_add].flags = CPU_TO_LE16(cmd_flags);
 			num_add++;
 
 			/* flush a full buffer */
@@ -2477,12 +2828,15 @@
 		 * the VSI's list.
 		 */
 		spin_lock_bh(&vsi->mac_filter_hash_lock);
-		hlist_for_each_entry_safe(new, h, &tmp_add_list, hlist) {
-			/* Only update the state if we're still NEW */
-			if (new->f->state == I40E_FILTER_NEW)
-				new->f->state = new->state;
-			hlist_del(&new->hlist);
-			kfree(new);
+		hlist_for_each_entry_safe(new_mac, h, &tmp_add_list, hlist) {
+			/* Only update the state if we're still NEW or
+			 * INACTIVE
+			 */
+			if (new_mac->f->state == I40E_FILTER_NEW ||
+			    new_mac->state == I40E_FILTER_INACTIVE)
+				new_mac->f->state = new_mac->state;
+			hlist_del(&new_mac->hlist);
+			kfree(new_mac);
 		}
 		spin_unlock_bh(&vsi->mac_filter_hash_lock);
 		kfree(add_list);
@@ -2512,7 +2866,6 @@
 		clear_bit(__I40E_VSI_OVERFLOW_PROMISC, vsi->state);
 		vsi->promisc_threshold = 0;
 	}
-
 	/* if the VF is not trusted do not do promisc */
 	if ((vsi->type == I40E_VSI_SRIOV) && !pf->vf[vsi->vf_id].trusted) {
 		clear_bit(__I40E_VSI_OVERFLOW_PROMISC, vsi->state);
@@ -2545,8 +2898,7 @@
 				 i40e_stat_str(hw, aq_ret),
 				 i40e_aq_str(hw, hw->aq.asq_last_status));
 		} else {
-			dev_info(&pf->pdev->dev, "%s is %s allmulti mode.\n",
-				 vsi->netdev->name,
+			dev_info(&pf->pdev->dev, "%s allmulti mode.\n",
 				 cur_multipromisc ? "entering" : "leaving");
 		}
 	}
@@ -2599,18 +2951,15 @@
 
 	if (!pf)
 		return;
+
 	if (!test_and_clear_bit(__I40E_MACVLAN_SYNC_PENDING, pf->state))
 		return;
-	if (test_and_set_bit(__I40E_VF_DISABLE, pf->state)) {
-		set_bit(__I40E_MACVLAN_SYNC_PENDING, pf->state);
-		return;
-	}
 
 	for (v = 0; v < pf->num_alloc_vsi; v++) {
 		if (pf->vsi[v] &&
-		    (pf->vsi[v]->flags & I40E_VSI_FLAG_FILTER_CHANGED)) {
+		    (pf->vsi[v]->flags & I40E_VSI_FLAG_FILTER_CHANGED) &&
+		    !test_bit(__I40E_VSI_RELEASING, pf->vsi[v]->state)) {
 			int ret = i40e_sync_vsi_filters(pf->vsi[v]);
-
 			if (ret) {
 				/* come back and try again later */
 				set_bit(__I40E_MACVLAN_SYNC_PENDING,
@@ -2619,7 +2968,6 @@
 			}
 		}
 	}
-	clear_bit(__I40E_VF_DISABLE, pf->state);
 }
 
 /**
@@ -2644,16 +2992,41 @@
 static int i40e_change_mtu(struct net_device *netdev, int new_mtu)
 {
 	struct i40e_netdev_priv *np = netdev_priv(netdev);
+	int max_frame = new_mtu + I40E_PACKET_HDR_PAD;
 	struct i40e_vsi *vsi = np->vsi;
 	struct i40e_pf *pf = vsi->back;
 
-	if (i40e_enabled_xdp_vsi(vsi)) {
-		int frame_size = new_mtu + ETH_HLEN + ETH_FCS_LEN + VLAN_HLEN;
+	/* MTU < 68 is an error and causes problems on some kernels */
+	if ((new_mtu < 68) || (max_frame > I40E_MAX_RXBUFFER))
+		return -EINVAL;
 
-		if (frame_size > i40e_max_xdp_frame_size(vsi))
+	if (i40e_enabled_xdp_vsi(vsi)) {
+		if (max_frame > i40e_max_xdp_frame_size(vsi))
 			return -EINVAL;
 	}
 
+#ifndef HAVE_NDO_FEATURES_CHECK
+
+	/* MTU < 576 causes problems with TSO */
+	if (new_mtu < 576) {
+		netdev->features &= ~NETIF_F_TSO;
+		netdev->features &= ~NETIF_F_TSO6;
+#ifdef HAVE_NDO_SET_FEATURES
+	} else {
+#ifndef HAVE_RHEL6_NET_DEVICE_OPS_EXT
+		if (netdev->wanted_features & NETIF_F_TSO)
+			netdev->features |= NETIF_F_TSO;
+		if (netdev->wanted_features & NETIF_F_TSO6)
+			netdev->features |= NETIF_F_TSO6;
+#else
+		if (netdev_extended(netdev)->wanted_features & NETIF_F_TSO)
+			netdev->features |= NETIF_F_TSO;
+		if (netdev_extended(netdev)->wanted_features & NETIF_F_TSO6)
+			netdev->features |= NETIF_F_TSO6;
+#endif /* HAVE_RHEL6_NET_DEVICE_OPS_EXT */
+#endif /* HAVE_NDO_SET_FEATURES */
+	}
+#endif /* ! HAVE_NDO_FEATURES_CHECK */
 	netdev_info(netdev, "changing MTU from %d to %d\n",
 		    netdev->mtu, new_mtu);
 	netdev->mtu = new_mtu;
@@ -2664,6 +3037,7 @@
 	return 0;
 }
 
+#if defined(HAVE_PTP_1588_CLOCK) || defined(HAVE_I40E_INTELCIM_IOCTL)
 /**
  * i40e_ioctl - Access the hwtstamp interface
  * @netdev: network interface device structure
@@ -2672,36 +3046,100 @@
  **/
 int i40e_ioctl(struct net_device *netdev, struct ifreq *ifr, int cmd)
 {
+#ifdef HAVE_PTP_1588_CLOCK
 	struct i40e_netdev_priv *np = netdev_priv(netdev);
 	struct i40e_pf *pf = np->vsi->back;
 
+#endif /* HAVE_PTP_1588_CLOCK */
 	switch (cmd) {
+#ifdef HAVE_PTP_1588_CLOCK
+#ifdef SIOCGHWTSTAMP
 	case SIOCGHWTSTAMP:
 		return i40e_ptp_get_ts_config(pf, ifr);
+#endif
 	case SIOCSHWTSTAMP:
+		if (!capable(CAP_SYS_ADMIN))
+			return -EACCES;
 		return i40e_ptp_set_ts_config(pf, ifr);
+	case SIOCSPINS:
+		if (!capable(CAP_SYS_ADMIN))
+			return -EACCES;
+		return i40e_ptp_set_pins_ioctl(pf, ifr);
+	case SIOCGPINS:
+		return i40e_ptp_get_pins(pf, ifr);
+#endif /* HAVE_PTP_1588_CLOCK */
 	default:
 		return -EOPNOTSUPP;
 	}
 }
 
+#endif
+#define I40E_OVLAN_EMOD_SHIFT(x) ((x) << I40E_AQ_VSI_OVLAN_EMOD_SHIFT)
+
 /**
- * i40e_vlan_stripping_enable - Turn on vlan stripping for the VSI
+ * i40e_outer_vlan_stripping_enable - Turn on vlan stripping for the VSI
  * @vsi: the vsi being adjusted
+ *
+ * Returns 0 on success, negative on failure
  **/
-void i40e_vlan_stripping_enable(struct i40e_vsi *vsi)
+static int i40e_outer_vlan_stripping_enable(struct i40e_vsi *vsi)
 {
+	i40e_status ret = I40E_SUCCESS;
 	struct i40e_vsi_context ctxt;
-	i40e_status ret;
 
-	/* Don't modify stripping options if a port VLAN is active */
-	if (vsi->info.pvid)
-		return;
+	/* Don't modify stripping options if a outer vlan is active */
+	if (vsi->info.outer_vlan) {
+		dev_warn(&vsi->back->pdev->dev,
+			 "Cannot enable vlan stripping when port VLAN is set\n");
+		ret = I40E_ERR_PARAM;
+		goto err_out;
+	}
+	if ((vsi->info.valid_sections &
+	     cpu_to_le16(I40E_AQ_VSI_PROP_VLAN_VALID)) &&
+	    ((vsi->info.outer_vlan_flags & I40E_AQ_VSI_OVLAN_EMOD_MASK) == 0))
+		goto err_out;  /* already enabled */
+
+	vsi->info.valid_sections = cpu_to_le16(I40E_AQ_VSI_PROP_VLAN_VALID);
+	vsi->info.outer_vlan_flags = I40E_AQ_VSI_OVLAN_MODE_ALL |
+		I40E_OVLAN_EMOD_SHIFT(I40E_AQ_VSI_OVLAN_EMOD_SHOW_ALL) |
+		I40E_OVLAN_EMOD_SHIFT(I40E_AQ_VSI_OVLAN_CTRL_ENA);
+
+	ctxt.seid = vsi->seid;
+	ctxt.info = vsi->info;
+	ret = i40e_aq_update_vsi_params(&vsi->back->hw, &ctxt, NULL);
+	if (ret) {
+		dev_info(&vsi->back->pdev->dev,
+			 "update vlan stripping failed, err %s\n",
+			 i40e_stat_str(&vsi->back->hw, ret));
+	}
+err_out:
+	return ret;
+}
+
+/**
+ * i40e_vlan_stripping_enable - Turn on outer vlan stripping for the VSI
+ * @vsi: the vsi being adjusted
+ **/
+int i40e_vlan_stripping_enable(struct i40e_vsi *vsi)
+{
+	i40e_status ret = I40E_SUCCESS;
+	struct i40e_vsi_context ctxt;
+
+	if (i40e_is_double_vlan(&vsi->back->hw))
+		return i40e_outer_vlan_stripping_enable(vsi);
+
+	/* Don't modify stripping options if a port vlan is active */
+	if (vsi->info.pvid) {
+		dev_warn(&vsi->back->pdev->dev,
+			 "Cannot enable vlan stripping when port VLAN is set\n");
+		ret = I40E_ERR_PARAM;
+		goto err_out;
+	}
 
 	if ((vsi->info.valid_sections &
 	     cpu_to_le16(I40E_AQ_VSI_PROP_VLAN_VALID)) &&
 	    ((vsi->info.port_vlan_flags & I40E_AQ_VSI_PVLAN_MODE_MASK) == 0))
-		return;  /* already enabled */
+		goto err_out;  /* already enabled */
 
 	vsi->info.valid_sections = cpu_to_le16(I40E_AQ_VSI_PROP_VLAN_VALID);
 	vsi->info.port_vlan_flags = I40E_AQ_VSI_PVLAN_MODE_ALL |
@@ -2717,26 +3155,77 @@
 			 i40e_aq_str(&vsi->back->hw,
 				     vsi->back->hw.aq.asq_last_status));
 	}
+err_out:
+	return ret;
+}
+
+/**
+ * i40e_outer_vlan_stripping_disable - Turn off vlan stripping for the VSI
+ * @vsi: the vsi being adjusted
+ **/
+static int i40e_outer_vlan_stripping_disable(struct i40e_vsi *vsi)
+{
+	i40e_status ret = I40E_SUCCESS;
+	struct i40e_vsi_context ctxt;
+
+	/* Don't modify stripping options if a outer vlan is active */
+	if (vsi->info.outer_vlan) {
+		dev_warn(&vsi->back->pdev->dev,
+			 "Cannot disable vlan stripping when port VLAN is set\n");
+		ret = I40E_ERR_PARAM;
+		goto err_out;
+	}
+
+	if ((vsi->info.valid_sections &
+	     cpu_to_le16(I40E_AQ_VSI_PROP_VLAN_VALID)) &&
+	    ((vsi->info.outer_vlan_flags & I40E_AQ_VSI_OVLAN_EMOD_MASK) ==
+	     I40E_AQ_VSI_OVLAN_EMOD_MASK))
+		goto err_out;  /* already disabled */
+
+	vsi->info.valid_sections = cpu_to_le16(I40E_AQ_VSI_PROP_VLAN_VALID);
+	vsi->info.outer_vlan_flags = I40E_AQ_VSI_OVLAN_MODE_ALL |
+		I40E_OVLAN_EMOD_SHIFT(I40E_AQ_VSI_OVLAN_EMOD_NOTHING) |
+		I40E_OVLAN_EMOD_SHIFT(I40E_AQ_VSI_OVLAN_CTRL_ENA);
+
+	ctxt.seid = vsi->seid;
+	ctxt.info = vsi->info;
+	ret = i40e_aq_update_vsi_params(&vsi->back->hw, &ctxt, NULL);
+	if (ret) {
+		dev_info(&vsi->back->pdev->dev,
+			 "update vlan stripping failed, err %s\n",
+			 i40e_stat_str(&vsi->back->hw, ret));
+	}
+err_out:
+	return ret;
 }
 
 /**
  * i40e_vlan_stripping_disable - Turn off vlan stripping for the VSI
  * @vsi: the vsi being adjusted
+ *
+ * Returns 0 on success, negative on failure
  **/
-void i40e_vlan_stripping_disable(struct i40e_vsi *vsi)
+int i40e_vlan_stripping_disable(struct i40e_vsi *vsi)
 {
+	i40e_status ret = I40E_SUCCESS;
 	struct i40e_vsi_context ctxt;
-	i40e_status ret;
 
-	/* Don't modify stripping options if a port VLAN is active */
-	if (vsi->info.pvid)
-		return;
+	if (i40e_is_double_vlan(&vsi->back->hw))
+		return i40e_outer_vlan_stripping_disable(vsi);
+
+	/* Don't modify stripping options if a port vlan is active */
+	if (vsi->info.pvid) {
+		dev_warn(&vsi->back->pdev->dev,
+			 "Cannot disable vlan stripping when port VLAN is set\n");
+		ret = I40E_ERR_PARAM;
+		goto err_out;
+	}
 
 	if ((vsi->info.valid_sections &
 	     cpu_to_le16(I40E_AQ_VSI_PROP_VLAN_VALID)) &&
 	    ((vsi->info.port_vlan_flags & I40E_AQ_VSI_PVLAN_EMOD_MASK) ==
 	     I40E_AQ_VSI_PVLAN_EMOD_MASK))
-		return;  /* already disabled */
+		goto err_out;  /* already disabled */
 
 	vsi->info.valid_sections = cpu_to_le16(I40E_AQ_VSI_PROP_VLAN_VALID);
 	vsi->info.port_vlan_flags = I40E_AQ_VSI_PVLAN_MODE_ALL |
@@ -2750,9 +3239,33 @@
 			 "update vlan stripping failed, err %s aq_err %s\n",
 			 i40e_stat_str(&vsi->back->hw, ret),
 			 i40e_aq_str(&vsi->back->hw,
-				     vsi->back->hw.aq.asq_last_status));
+				      vsi->back->hw.aq.asq_last_status));
 	}
+err_out:
+	return ret;
+}
+
+#ifdef HAVE_VLAN_RX_REGISTER
+/**
+ * i40e_vlan_rx_register - Setup or shutdown vlan offload
+ * @netdev: network interface to be adjusted
+ * @grp: new vlan group list, NULL if disabling
+ **/
+static void i40e_vlan_rx_register(struct net_device *netdev,
+				  struct vlan_group *grp)
+{
+	struct i40e_netdev_priv *np = netdev_priv(netdev);
+	struct i40e_vsi *vsi = np->vsi;
+	bool enable;
+
+	vsi->vlgrp = grp;
+	enable = (grp || (vsi->back->flags & I40E_FLAG_DCB_ENABLED));
+	if (enable)
+		i40e_vlan_stripping_enable(vsi);
+	else
+		i40e_vlan_stripping_disable(vsi);
 }
+#endif /* HAVE_VLAN_RX_REGISTER */
 
 /**
  * i40e_add_vlan_all_mac - Add a MAC/VLAN filter for each existing MAC address
@@ -2762,7 +3275,7 @@
  * This is a helper function for adding a new MAC/VLAN filter with the
  * specified VLAN for each existing MAC address already in the hash table.
  * This function does *not* perform any accounting to update filters based on
- * VLAN mode.
+ * vlan mode.
  *
  * NOTE: this function expects to be called while under the
  * mac_filter_hash_lock
@@ -2774,8 +3287,21 @@
 	int bkt;
 
 	hash_for_each_safe(vsi->mac_filter_hash, bkt, h, f, hlist) {
-		if (f->state == I40E_FILTER_REMOVE)
+		/* If we're asked to add a filter that has been marked for
+		 * removal, it is safe to simply restore it to active state.
+		 * __i40e_del_filter will have simply deleted any filters which
+		 * were previously marked NEW or FAILED, so if it is currently
+		 * marked REMOVE it must have previously been ACTIVE. Since we
+		 * haven't yet run the sync filters task, just restore this
+		 * filter to the ACTIVE state so that the sync task leaves it
+		 * in place.
+		 */
+		if (f->state == I40E_FILTER_REMOVE && f->vlan == vid) {
+			f->state = I40E_FILTER_ACTIVE;
 			continue;
+		} else if (f->state == I40E_FILTER_REMOVE) {
+			continue;
+		}
 		add_f = i40e_add_filter(vsi, f->macaddr, vid);
 		if (!add_f) {
 			dev_info(&vsi->back->pdev->dev,
@@ -2789,17 +3315,14 @@
 }
 
 /**
- * i40e_vsi_add_vlan - Add VSI membership for given VLAN
- * @vsi: the VSI being configured
- * @vid: VLAN id to be added
+ * i40e_vsi_add_vlan - Add vsi membership for given vlan
+ * @vsi: the vsi being configured
+ * @vid: vlan id to be added
  **/
 int i40e_vsi_add_vlan(struct i40e_vsi *vsi, u16 vid)
 {
 	int err;
 
-	if (vsi->info.pvid)
-		return -EINVAL;
-
 	/* The network stack will attempt to add VID=0, with the intention to
 	 * receive priority tagged packets with a VLAN of 0. Our HW receives
 	 * these packets by default when configured to receive untagged
@@ -2851,13 +3374,13 @@
 }
 
 /**
- * i40e_vsi_kill_vlan - Remove VSI membership for given VLAN
- * @vsi: the VSI being configured
- * @vid: VLAN id to be removed
+ * i40e_vsi_kill_vlan - Remove vsi membership for given vlan
+ * @vsi: the vsi being configured
+ * @vid: vlan id to be removed
  **/
 void i40e_vsi_kill_vlan(struct i40e_vsi *vsi, u16 vid)
 {
-	if (!vid || vsi->info.pvid)
+	if (!vid || i40e_is_vid(&vsi->info))
 		return;
 
 	spin_lock_bh(&vsi->mac_filter_hash_lock);
@@ -2878,21 +3401,56 @@
  *
  * net_device_ops implementation for adding vlan ids
  **/
+#ifdef HAVE_INT_NDO_VLAN_RX_ADD_VID
+#ifdef NETIF_F_HW_VLAN_CTAG_RX
 static int i40e_vlan_rx_add_vid(struct net_device *netdev,
 				__always_unused __be16 proto, u16 vid)
+#else
+static int i40e_vlan_rx_add_vid(struct net_device *netdev, u16 vid)
+#endif
+#else
+static void i40e_vlan_rx_add_vid(struct net_device *netdev, u16 vid)
+#endif
 {
 	struct i40e_netdev_priv *np = netdev_priv(netdev);
 	struct i40e_vsi *vsi = np->vsi;
 	int ret = 0;
 
 	if (vid >= VLAN_N_VID)
+#ifdef HAVE_INT_NDO_VLAN_RX_ADD_VID
 		return -EINVAL;
+#else
+		return;
+#endif
 
 	ret = i40e_vsi_add_vlan(vsi, vid);
+#ifndef HAVE_VLAN_RX_REGISTER
 	if (!ret)
 		set_bit(vid, vsi->active_vlans);
+#endif /* !HAVE_VLAN_RX_REGISTER */
+#ifndef HAVE_NETDEV_VLAN_FEATURES
+
+	/* Copy feature flags from netdev to the vlan netdev for this vid.
+	 * This allows things like TSO to bubble down to our vlan device.
+	 * Some vlans, such as VLAN 0 for DCB will not have a v_netdev so
+	 * we will not have a netdev that needs updating.
+	 */
+	if (vsi->vlgrp) {
+		struct vlan_group *vlgrp = vsi->vlgrp;
+		struct net_device *v_netdev = vlan_group_get_device(vlgrp, vid);
+		if (v_netdev) {
+			v_netdev->features |= netdev->features;
+#ifdef HAVE_ENCAP_CSUM_OFFLOAD
+			v_netdev->enc_features |= netdev->enc_features;
+#endif
+			vlan_group_set_device(vlgrp, vid, v_netdev);
+		}
+	}
+#endif /* HAVE_NETDEV_VLAN_FEATURES */
 
+#ifdef HAVE_INT_NDO_VLAN_RX_ADD_VID
 	return ret;
+#endif
 }
 
 /**
@@ -2901,15 +3459,43 @@
  * @proto: unused protocol value
  * @vid: vlan id to be added
  **/
+#ifdef NETIF_F_HW_VLAN_CTAG_RX
 static void i40e_vlan_rx_add_vid_up(struct net_device *netdev,
 				    __always_unused __be16 proto, u16 vid)
+#else
+static void i40e_vlan_rx_add_vid_up(struct net_device *netdev, u16 vid)
+#endif
 {
+#if (!defined(HAVE_NETDEV_VLAN_FEATURES) || !defined(HAVE_VLAN_RX_REGISTER))
 	struct i40e_netdev_priv *np = netdev_priv(netdev);
 	struct i40e_vsi *vsi = np->vsi;
+#endif
 
 	if (vid >= VLAN_N_VID)
 		return;
+#ifndef HAVE_VLAN_RX_REGISTER
 	set_bit(vid, vsi->active_vlans);
+#endif /* !HAVE_VLAN_RX_REGISTER */
+#ifndef HAVE_NETDEV_VLAN_FEATURES
+
+	/* Copy feature flags from netdev to the vlan netdev for this vid.
+	 * This allows things like TSO to bubble down to our vlan device.
+	 * Some vlans, such as VLAN 0 for DCB will not have a v_netdev so
+	 * we will not have a netdev that needs updating.
+	 */
+	if (vsi->vlgrp) {
+		struct vlan_group *vlgrp = vsi->vlgrp;
+		struct net_device *v_netdev = vlan_group_get_device(vlgrp, vid);
+
+		if (v_netdev) {
+			v_netdev->features |= netdev->features;
+#ifdef HAVE_ENCAP_CSUM_OFFLOAD
+			v_netdev->enc_features |= netdev->enc_features;
+#endif
+			vlan_group_set_device(vlgrp, vid, v_netdev);
+		}
+	}
+#endif /* HAVE_NETDEV_VLAN_FEATURES */
 }
 
 /**
@@ -2920,8 +3506,16 @@
  *
  * net_device_ops implementation for removing vlan ids
  **/
+#ifdef HAVE_INT_NDO_VLAN_RX_ADD_VID
+#ifdef NETIF_F_HW_VLAN_CTAG_RX
 static int i40e_vlan_rx_kill_vid(struct net_device *netdev,
 				 __always_unused __be16 proto, u16 vid)
+#else
+static int i40e_vlan_rx_kill_vid(struct net_device *netdev, u16 vid)
+#endif
+#else
+static void i40e_vlan_rx_kill_vid(struct net_device *netdev, u16 vid)
+#endif
 {
 	struct i40e_netdev_priv *np = netdev_priv(netdev);
 	struct i40e_vsi *vsi = np->vsi;
@@ -2931,10 +3525,14 @@
 	 * already printed from the other function
 	 */
 	i40e_vsi_kill_vlan(vsi, vid);
+#ifndef HAVE_VLAN_RX_REGISTER
 
 	clear_bit(vid, vsi->active_vlans);
+#endif /* HAVE_VLAN_RX_REGISTER */
+#ifdef HAVE_INT_NDO_VLAN_RX_ADD_VID
 
 	return 0;
+#endif
 }
 
 /**
@@ -2948,16 +3546,69 @@
 	if (!vsi->netdev)
 		return;
 
+#ifdef HAVE_VLAN_RX_REGISTER
+	i40e_vlan_rx_register(vsi->netdev, vsi->vlgrp);
+
+	if (vsi->vlgrp) {
+		for (vid = 0; vid < VLAN_N_VID; vid++) {
+			if (!vlan_group_get_device(vsi->vlgrp, vid))
+				continue;
+#ifdef NETIF_F_HW_VLAN_CTAG_RX
+			i40e_vlan_rx_add_vid_up(vsi->netdev, htons(ETH_P_8021Q),
+						vid);
+#else
+			i40e_vlan_rx_add_vid_up(vsi->netdev, vid);
+#endif
+		}
+	}
+#else /* HAVE_VLAN_RX_REGISTER */
+#ifdef NETIF_F_HW_VLAN_CTAG_RX
 	if (vsi->netdev->features & NETIF_F_HW_VLAN_CTAG_RX)
 		i40e_vlan_stripping_enable(vsi);
+#else
+	if (vsi->netdev->features & NETIF_F_HW_VLAN_RX)
+		i40e_vlan_stripping_enable(vsi);
+#endif
 	else
 		i40e_vlan_stripping_disable(vsi);
 
 	for_each_set_bit(vid, vsi->active_vlans, VLAN_N_VID)
+#ifdef NETIF_F_HW_VLAN_CTAG_RX
 		i40e_vlan_rx_add_vid_up(vsi->netdev, htons(ETH_P_8021Q),
 					vid);
+#else
+		i40e_vlan_rx_add_vid_up(vsi->netdev, vid);
+#endif
+#endif
 }
+/**
+ * i40e_vsi_add_ovid - Add outer VLAN for the VSI
+ * @vsi: the vsi being adjusted
+ * @vid: the vlan id to set as a outer VLAN
+ **/
+static int i40e_vsi_add_ovid(struct i40e_vsi *vsi, u16 vid)
+{
+	struct i40e_vsi_context ctxt;
+	i40e_status ret;
+
+	vsi->info.valid_sections = cpu_to_le16(I40E_AQ_VSI_PROP_VLAN_VALID);
+	vsi->info.outer_vlan = cpu_to_le16(vid);
+	vsi->info.outer_vlan_flags = I40E_AQ_VSI_OVLAN_MODE_UNTAGGED |
+		I40E_AQ_VSI_OVLAN_INSERT_PVID |
+		I40E_OVLAN_EMOD_SHIFT(I40E_AQ_VSI_OVLAN_EMOD_HIDE_ALL) |
+		I40E_OVLAN_EMOD_SHIFT(I40E_AQ_VSI_OVLAN_CTRL_ENA);
+	ctxt.seid = vsi->seid;
+	ctxt.info = vsi->info;
+	ret = i40e_aq_update_vsi_params(&vsi->back->hw, &ctxt, NULL);
+	if (ret) {
+		dev_info(&vsi->back->pdev->dev,
+			 "add pvid failed, err %s\n",
+			 i40e_stat_str(&vsi->back->hw, ret));
+		return -ENOENT;
+	}
 
+	return 0;
+}
 /**
  * i40e_vsi_add_pvid - Add pvid for the VSI
  * @vsi: the vsi being adjusted
@@ -2968,11 +3619,13 @@
 	struct i40e_vsi_context ctxt;
 	i40e_status ret;
 
+	if (i40e_is_double_vlan(&vsi->back->hw))
+		return i40e_vsi_add_ovid(vsi, vid);
 	vsi->info.valid_sections = cpu_to_le16(I40E_AQ_VSI_PROP_VLAN_VALID);
 	vsi->info.pvid = cpu_to_le16(vid);
 	vsi->info.port_vlan_flags = I40E_AQ_VSI_PVLAN_MODE_TAGGED |
-				    I40E_AQ_VSI_PVLAN_INSERT_PVID |
-				    I40E_AQ_VSI_PVLAN_EMOD_STR;
+				I40E_AQ_VSI_PVLAN_INSERT_PVID |
+				I40E_AQ_VSI_PVLAN_EMOD_STR;
 
 	ctxt.seid = vsi->seid;
 	ctxt.info = vsi->info;
@@ -2982,7 +3635,7 @@
 			 "add pvid failed, err %s aq_err %s\n",
 			 i40e_stat_str(&vsi->back->hw, ret),
 			 i40e_aq_str(&vsi->back->hw,
-				     vsi->back->hw.aq.asq_last_status));
+				      vsi->back->hw.aq.asq_last_status));
 		return -ENOENT;
 	}
 
@@ -2990,6 +3643,19 @@
 }
 
 /**
+ * i40e_get_current_vid - Return pointer to valid port VLAN id field
+ * @vsi: the vsi being probed
+ *
+ * Get current VLAN id field, which is being used in port VLAN functionality
+ **/
+__le16 *i40e_get_current_vid(struct i40e_vsi *vsi)
+{
+	if (i40e_is_double_vlan(&vsi->back->hw))
+		return &vsi->info.outer_vlan;
+	return &vsi->info.pvid;
+}
+
+/**
  * i40e_vsi_remove_pvid - Remove the pvid from the VSI
  * @vsi: the vsi being adjusted
  *
@@ -2997,12 +3663,82 @@
  **/
 void i40e_vsi_remove_pvid(struct i40e_vsi *vsi)
 {
-	vsi->info.pvid = 0;
+	__le16 *vid;
+
+	vid = i40e_get_current_vid(vsi);
+	*vid = 0;
 
 	i40e_vlan_stripping_disable(vsi);
 }
 
 /**
+ * i40e_get_custom_cloud_filter_type - Get the custom cloud filter type
+ * @flags: set of enabled fields
+ * @type: location to return type
+ *
+ * This function gets the custom cloud filter types
+ *
+ * Returns 0 on success, negative on failure
+ **/
+int i40e_get_custom_cloud_filter_type(u8 flags, u16 *type)
+{
+	static const u16 table[128] = {
+		[I40E_CLOUD_FILTER_FLAGS_OIP1] =
+			I40E_AQC_ADD_CLOUD_FILTER_OIP1,
+		[I40E_CLOUD_FILTER_FLAGS_OIP2] =
+			I40E_AQC_ADD_CLOUD_FILTER_OIP2,
+	};
+
+	if (flags >= ARRAY_SIZE(table) || table[flags] == 0)
+		return I40E_ERR_CONFIG;
+
+	/* Return type if we're given space to do so */
+	if (type)
+		*type = table[flags];
+
+	return 0;
+}
+
+/**
+ * i40e_get_cloud_filter_type - Get cloud filter type
+ * @flags: set of enabled fields
+ * @type: location to return type
+ *
+ * Given the set of flags indicating which fields are active, look up the type
+ * number for programming the cloud filter in firmware. If the flags are
+ * invalid, return I40E_ERR_CONFIG. type may be NULL, in which case the
+ * function may be used to verify that the flags would produce a valid type.
+ **/
+int i40e_get_cloud_filter_type(u8 flags, u16 *type)
+{
+	static const u16 table[128] = {
+		[I40E_CLOUD_FILTER_FLAGS_OMAC]  =
+			I40E_AQC_ADD_CLOUD_FILTER_OMAC,
+		[I40E_CLOUD_FILTER_FLAGS_IMAC]  =
+			I40E_AQC_ADD_CLOUD_FILTER_IMAC,
+		[I40E_CLOUD_FILTER_FLAGS_IMAC_IVLAN]  =
+			I40E_AQC_ADD_CLOUD_FILTER_IMAC_IVLAN,
+		[I40E_CLOUD_FILTER_FLAGS_IMAC_TEN_ID] =
+			I40E_AQC_ADD_CLOUD_FILTER_IMAC_TEN_ID,
+		[I40E_CLOUD_FILTER_FLAGS_OMAC_TEN_ID_IMAC] =
+			I40E_AQC_ADD_CLOUD_FILTER_OMAC_TEN_ID_IMAC,
+		[I40E_CLOUD_FILTER_FLAGS_IMAC_IVLAN_TEN_ID] =
+			I40E_AQC_ADD_CLOUD_FILTER_IMAC_IVLAN_TEN_ID,
+		[I40E_CLOUD_FILTER_FLAGS_IIP] =
+			I40E_AQC_ADD_CLOUD_FILTER_IIP,
+	};
+
+	if (flags >= ARRAY_SIZE(table) || table[flags] == 0)
+		return I40E_ERR_CONFIG;
+
+	/* Return type if we're given space to do so */
+	if (type)
+		*type = table[flags];
+
+	return 0;
+}
+
+/**
  * i40e_vsi_setup_tx_resources - Allocate VSI Tx queue resources
  * @vsi: ptr to the VSI
  *
@@ -3012,7 +3748,7 @@
  *
  * Return 0 on success, negative on failure
  **/
-static int i40e_vsi_setup_tx_resources(struct i40e_vsi *vsi)
+int i40e_vsi_setup_tx_resources(struct i40e_vsi *vsi)
 {
 	int i, err = 0;
 
@@ -3024,7 +3760,6 @@
 
 	for (i = 0; i < vsi->num_queue_pairs && !err; i++)
 		err = i40e_setup_tx_descriptors(vsi->xdp_rings[i]);
-
 	return err;
 }
 
@@ -3061,7 +3796,7 @@
  *
  * Return 0 on success, negative on failure
  **/
-static int i40e_vsi_setup_rx_resources(struct i40e_vsi *vsi)
+int i40e_vsi_setup_rx_resources(struct i40e_vsi *vsi)
 {
 	int i, err = 0;
 
@@ -3097,38 +3832,53 @@
  **/
 static void i40e_config_xps_tx_ring(struct i40e_ring *ring)
 {
+#ifndef HAVE_XPS_QOS_SUPPORT
+	struct i40e_vsi *vsi = ring->vsi;
+#endif
 	int cpu;
 
 	if (!ring->q_vector || !ring->netdev || ring->ch)
 		return;
 
+#ifndef HAVE_XPS_QOS_SUPPORT
+	/* Some older kernels do not support XPS with QoS */
+	if (vsi->tc_config.numtc > 1) {
+#ifndef HAVE_NETDEV_TC_RESETS_XPS
+		/* Additionally, some kernels do not properly clear the XPS
+		 * mapping when the number of traffic classes is changed. In
+		 * order to support these kernels we work around this by
+		 * setting the XPS mapping to the empty cpu set.
+		 */
+		cpumask_var_t mask;
+
+		/* Only clear the settings if we initialized XPS */
+		if (!test_and_clear_bit(__I40E_TX_XPS_INIT_DONE, ring->state))
+			return;
+
+		if (!zalloc_cpumask_var(&mask, GFP_KERNEL))
+			return;
+
+		netif_set_xps_queue(ring->netdev, mask, ring->queue_index);
+		free_cpumask_var(mask);
+#endif /* !HAVE_NETDEV_TC_RESETS_XPS */
+		return;
+	}
+
+#endif /* !HAVE_XPS_QOS_SUPPORT */
 	/* We only initialize XPS once, so as not to overwrite user settings */
 	if (test_and_set_bit(__I40E_TX_XPS_INIT_DONE, ring->state))
 		return;
 
 	cpu = cpumask_local_spread(ring->q_vector->v_idx, -1);
-	netif_set_xps_queue(ring->netdev, get_cpu_mask(cpu),
+#ifndef HAVE_NETIF_SET_XPS_QUEUE_CONST_MASK
+	/* In kernels before 3.12 the second parameter has no const qualifier.
+	 * It is generating warning in older kernels.
+	 */
+	netif_set_xps_queue(ring->netdev, (struct cpumask *)get_cpu_mask(cpu),
 			    ring->queue_index);
-}
-
-/**
- * i40e_xsk_umem - Retrieve the AF_XDP ZC if XDP and ZC is enabled
- * @ring: The Tx or Rx ring
- *
- * Returns the UMEM or NULL.
- **/
-static struct xdp_umem *i40e_xsk_umem(struct i40e_ring *ring)
-{
-	bool xdp_on = i40e_enabled_xdp_vsi(ring->vsi);
-	int qid = ring->queue_index;
-
-	if (ring_is_xdp(ring))
-		qid -= ring->vsi->alloc_queue_pairs;
-
-	if (!xdp_on || !test_bit(qid, ring->vsi->af_xdp_zc_qps))
-		return NULL;
-
-	return xdp_get_umem_from_qid(ring->vsi->netdev, qid);
+#else /* !HAVE_NETIF_SET_XPS_QUEUE_CONST_MASK */
+	netif_set_xps_queue(ring->netdev, get_cpu_mask(cpu), ring->queue_index);
+#endif /* !HAVE_NETIF_SET_XPS_QUEUE_CONST_MASK */
 }
 
 /**
@@ -3143,12 +3893,9 @@
 	u16 pf_q = vsi->base_queue + ring->queue_index;
 	struct i40e_hw *hw = &vsi->back->hw;
 	struct i40e_hmc_obj_txq tx_ctx;
-	i40e_status err = 0;
+	i40e_status err = I40E_SUCCESS;
 	u32 qtx_ctl = 0;
 
-	if (ring_is_xdp(ring))
-		ring->xsk_umem = i40e_xsk_umem(ring);
-
 	/* some ATR related tx ring init */
 	if (vsi->back->flags & I40E_FLAG_FD_ATR_ENABLED) {
 		ring->atr_sample_rate = vsi->back->atr_sample_rate;
@@ -3156,7 +3903,6 @@
 	} else {
 		ring->atr_sample_rate = 0;
 	}
-
 	/* configure XPS */
 	i40e_config_xps_tx_ring(ring);
 
@@ -3168,7 +3914,9 @@
 	tx_ctx.qlen = ring->count;
 	tx_ctx.fd_ena = !!(vsi->back->flags & (I40E_FLAG_FD_SB_ENABLED |
 					       I40E_FLAG_FD_ATR_ENABLED));
+#ifdef HAVE_PTP_1588_CLOCK
 	tx_ctx.timesync_ena = !!(vsi->back->flags & I40E_FLAG_PTP);
+#endif /* HAVE_PTP_1588_CLOCK */
 	/* FDIR VSI tx ring can still use RS bit and writebacks */
 	if (vsi->type != I40E_VSI_FDIR)
 		tx_ctx.head_wb_ena = 1;
@@ -3257,47 +4005,14 @@
 	u16 pf_q = vsi->base_queue + ring->queue_index;
 	struct i40e_hw *hw = &vsi->back->hw;
 	struct i40e_hmc_obj_rxq rx_ctx;
-	i40e_status err = 0;
-	bool ok;
-	int ret;
+	i40e_status err = I40E_SUCCESS;
 
 	bitmap_zero(ring->state, __I40E_RING_STATE_NBITS);
 
 	/* clear the context structure first */
 	memset(&rx_ctx, 0, sizeof(rx_ctx));
 
-	if (ring->vsi->type == I40E_VSI_MAIN)
-		xdp_rxq_info_unreg_mem_model(&ring->xdp_rxq);
-
-	ring->xsk_umem = i40e_xsk_umem(ring);
-	if (ring->xsk_umem) {
-		ring->rx_buf_len = ring->xsk_umem->chunk_size_nohr -
-				   XDP_PACKET_HEADROOM;
-		/* For AF_XDP ZC, we disallow packets to span on
-		 * multiple buffers, thus letting us skip that
-		 * handling in the fast-path.
-		 */
-		chain_len = 1;
-		ring->zca.free = i40e_zca_free;
-		ret = xdp_rxq_info_reg_mem_model(&ring->xdp_rxq,
-						 MEM_TYPE_ZERO_COPY,
-						 &ring->zca);
-		if (ret)
-			return ret;
-		dev_info(&vsi->back->pdev->dev,
-			 "Registered XDP mem model MEM_TYPE_ZERO_COPY on Rx ring %d\n",
-			 ring->queue_index);
-
-	} else {
-		ring->rx_buf_len = vsi->rx_buf_len;
-		if (ring->vsi->type == I40E_VSI_MAIN) {
-			ret = xdp_rxq_info_reg_mem_model(&ring->xdp_rxq,
-							 MEM_TYPE_PAGE_SHARED,
-							 NULL);
-			if (ret)
-				return ret;
-		}
-	}
+	ring->rx_buf_len = vsi->rx_buf_len;
 
 	rx_ctx.dbuff = DIV_ROUND_UP(ring->rx_buf_len,
 				    BIT_ULL(I40E_RXQ_CTX_DBUFF_SHIFT));
@@ -3306,7 +4021,12 @@
 	rx_ctx.qlen = ring->count;
 
 	/* use 32 byte descriptors */
+#ifdef I40E_32BYTE_RX
 	rx_ctx.dsize = 1;
+#else
+	/* use 16 byte descriptors */
+	rx_ctx.dsize = 0;
+#endif
 
 	/* descriptor type is always zero
 	 * rx_ctx.dtype = 0;
@@ -3314,10 +4034,7 @@
 	rx_ctx.hsplit_0 = 0;
 
 	rx_ctx.rxmax = min_t(u16, vsi->max_frame, chain_len * ring->rx_buf_len);
-	if (hw->revision_id == 0)
-		rx_ctx.lrxqthresh = 0;
-	else
-		rx_ctx.lrxqthresh = 1;
+	rx_ctx.lrxqthresh = 1;
 	rx_ctx.crcstrip = 1;
 	rx_ctx.l2tsel = 1;
 	/* this controls whether VLAN is stripped from inner headers */
@@ -3353,18 +4070,7 @@
 	ring->tail = hw->hw_addr + I40E_QRX_TAIL(pf_q);
 	writel(0, ring->tail);
 
-	ok = ring->xsk_umem ?
-	     i40e_alloc_rx_buffers_zc(ring, I40E_DESC_UNUSED(ring)) :
-	     !i40e_alloc_rx_buffers(ring, I40E_DESC_UNUSED(ring));
-	if (!ok) {
-		/* Log this in case the user has forgotten to give the kernel
-		 * any buffers, even later in the application.
-		 */
-		dev_info(&vsi->back->pdev->dev,
-			 "Failed to allocate some buffers on %sRx ring %d (pf_q %d)\n",
-			 ring->xsk_umem ? "UMEM enabled " : "",
-			 ring->queue_index, pf_q);
-	}
+	i40e_alloc_rx_buffers(ring, I40E_DESC_UNUSED(ring));
 
 	return 0;
 }
@@ -3383,7 +4089,7 @@
 	for (i = 0; (i < vsi->num_queue_pairs) && !err; i++)
 		err = i40e_configure_tx_ring(vsi->tx_rings[i]);
 
-	if (err || !i40e_enabled_xdp_vsi(vsi))
+	if (!i40e_enabled_xdp_vsi(vsi))
 		return err;
 
 	for (i = 0; (i < vsi->num_queue_pairs) && !err; i++)
@@ -3402,6 +4108,17 @@
 {
 	int err = 0;
 	u16 i;
+#ifdef CONFIG_I40E_DISABLE_PACKET_SPLIT
+	u16 max_frame = I40E_RXBUFFER_1536 - NET_IP_ALIGN;
+
+	if (!vsi->netdev)
+		max_frame = I40E_RXBUFFER_2048;
+	else if (vsi->netdev->mtu + I40E_PACKET_HDR_PAD > max_frame)
+		max_frame = vsi->netdev->mtu + I40E_PACKET_HDR_PAD;
+
+	vsi->max_frame = max_frame;
+	vsi->rx_buf_len = max_frame;
+#else /* CONFIG_I40E_DISABLE_PACKET_SPLIT */
 
 	if (!vsi->netdev || (vsi->back->flags & I40E_FLAG_LEGACY_RX)) {
 		vsi->max_frame = I40E_MAX_RXBUFFER;
@@ -3417,6 +4134,7 @@
 		vsi->rx_buf_len = (PAGE_SIZE < 8192) ? I40E_RXBUFFER_3072 :
 						       I40E_RXBUFFER_2048;
 	}
+#endif /* CONFIG_I40E_DISABLE_PACKET_SPLIT */
 
 	/* set up individual rings */
 	for (i = 0; i < vsi->num_queue_pairs && !err; i++)
@@ -3469,6 +4187,25 @@
 {
 	if (vsi->netdev)
 		i40e_set_rx_mode(vsi->netdev);
+
+}
+
+/**
+ * i40e_reset_fdir_filter_cnt - Reset flow director filter counters
+ * @pf: Pointer to the targeted PF
+ *
+ * Set all flow director counters to 0.
+ */
+static void i40e_reset_fdir_filter_cnt(struct i40e_pf *pf)
+{
+	pf->fd_tcp4_filter_cnt = 0;
+	pf->fd_udp4_filter_cnt = 0;
+	pf->fd_sctp4_filter_cnt = 0;
+	pf->fd_ip4_filter_cnt = 0;
+	pf->fd_tcp6_filter_cnt = 0;
+	pf->fd_udp6_filter_cnt = 0;
+	pf->fd_sctp6_filter_cnt = 0;
+	pf->fd_ip6_filter_cnt = 0;
 }
 
 /**
@@ -3487,11 +4224,8 @@
 	if (!(pf->flags & I40E_FLAG_FD_SB_ENABLED))
 		return;
 
-	/* Reset FDir counters as we're replaying all existing filters */
-	pf->fd_tcp4_filter_cnt = 0;
-	pf->fd_udp4_filter_cnt = 0;
-	pf->fd_sctp4_filter_cnt = 0;
-	pf->fd_ip4_filter_cnt = 0;
+	/* reset FDIR counters as we're replaying all existing filters */
+	i40e_reset_fdir_filter_cnt(pf);
 
 	hlist_for_each_entry_safe(filter, node,
 				  &pf->fdir_filter_list, fdir_node) {
@@ -3500,6 +4234,226 @@
 }
 
 /**
+ * i40e_cloud_filter_restore - Restore the switch's cloud filters
+ * @pf: Pointer to the targeted VSI
+ *
+ * This function replays the cloud filter hlist into the hw switch
+ **/
+static void i40e_cloud_filter_restore(struct i40e_pf *pf)
+{
+	struct i40e_vsi *vsi = pf->vsi[pf->lan_vsi];
+	struct i40e_cloud_filter *filter;
+	struct hlist_node *node;
+
+	/* Clear the counters for the outer IP tables, they will be incremented
+	 * as the filters are added back to the device.
+	 */
+	pf->outerip_filters[0] = 0;
+	pf->outerip_filters[1] = 0;
+
+	hlist_for_each_entry_safe(filter, node,
+				  &pf->cloud_filter_list, cloud_node) {
+		if (filter->flags & I40E_CLOUD_FIELD_OIP1 ||
+		    filter->flags & I40E_CLOUD_FIELD_OIP2)
+			i40e_add_del_custom_cloud_filter(vsi, filter, true);
+		else
+			i40e_add_del_cloud_filter_ex(pf, filter, true);
+	}
+}
+/**
+ * i40e_set_cld_element - sets cloud filter element data
+ * @filter: cloud filter rule
+ * @cld: ptr to cloud filter element data
+ *
+ * This is helper function to copy data into cloud filter element
+ **/
+static inline void
+i40e_set_cld_element(struct i40e_cloud_filter *filter,
+		     struct i40e_aqc_cloud_filters_element_data *cld)
+{
+	u32 ipa;
+	int i;
+
+	memset(cld, 0, sizeof(*cld));
+	ether_addr_copy(cld->outer_mac, filter->dst_mac);
+	ether_addr_copy(cld->inner_mac, filter->src_mac);
+
+	if (filter->n_proto != ETH_P_IP && filter->n_proto != ETH_P_IPV6) {
+		/* copy parameters from filter to cloud filters element
+		 * which are not specific to IP protos
+		 */
+		ether_addr_copy(cld->outer_mac, filter->outer_mac);
+		ether_addr_copy(cld->inner_mac, filter->inner_mac);
+		cld->inner_vlan = cpu_to_le16(ntohs(filter->inner_vlan));
+		cld->tenant_id = cpu_to_le32(filter->tenant_id);
+		return;
+	}
+
+	if (filter->n_proto == ETH_P_IPV6) {
+#define IPV6_MAX_INDEX	(ARRAY_SIZE(filter->dst_ipv6) - 1)
+		for (i = 0; i < ARRAY_SIZE(filter->dst_ipv6); i++) {
+			ipa = be32_to_cpu(filter->dst_ipv6[IPV6_MAX_INDEX - i]);
+			memcpy(&cld->ipaddr.raw_v6.data[i * 2], &ipa, sizeof(ipa));
+		}
+	} else {
+		ipa = be32_to_cpu(filter->dst_ipv4);
+		memcpy(&cld->ipaddr.v4.data, &ipa, sizeof(ipa));
+	}
+
+	cld->inner_vlan = cpu_to_le16(ntohs(filter->vlan_id));
+
+	/* tenant_id is not supported by FW now, once the support is enabled
+	 * fill the cld->tenant_id with cpu_to_le32(filter->tenant_id)
+	 */
+	if (filter->tenant_id)
+		return;
+}
+
+/* i40e_add_del_cloud_filter_ex - Add/del cloud filter using big_buf
+ * @pf: working PF
+ * @filter: cloud filter rule
+ * @add: if true, add, if false, delete
+ *
+ * Add or delete a cloud filter for a specific flow spec using big buffer.
+ * Returns 0 if the filter were successfully added.
+ **/
+int i40e_add_del_cloud_filter_ex(struct i40e_pf *pf,
+				 struct i40e_cloud_filter *filter,
+				 bool add)
+{
+	struct i40e_aqc_cloud_filters_element_bb cld_filter;
+	int ret;
+	if (filter->queue_id == I40E_CLOUD_FILTER_ANY_QUEUE ||
+	    !i40e_is_l4mode_enabled()) {
+		struct i40e_vsi *vsi = pf->vsi[pf->lan_vsi];
+
+		return i40e_add_del_cloud_filter(vsi, filter, add);
+	}
+
+	/* Make sure port is specified, otherwise bail out, for channel
+	 * specific cloud filter needs 'L4 port' to be non-zero
+	 */
+	if (!filter->dst_port)
+		return -EINVAL;
+
+	memset(&cld_filter, 0, sizeof(cld_filter));
+
+	/* Switch is in Mode 1, so this is an L4 port filter */
+	cld_filter.element.flags =
+	cpu_to_le16(I40E_AQC_ADD_CLOUD_FILTER_IP_PORT);
+
+	/* Now copy L4 port in Byte 6..7 in general fields */
+	cld_filter.general_fields[I40E_AQC_ADD_CLOUD_FV_FLU_0X16_WORD0] =
+	be16_to_cpu(filter->dst_port);
+
+	if (add)
+		ret = i40e_aq_add_cloud_filters_bb(&pf->hw,
+						   filter->seid,
+						   &cld_filter, 1);
+	else
+		ret = i40e_aq_rem_cloud_filters_bb(&pf->hw,
+						   filter->seid,
+						   &cld_filter, 1);
+
+	if (ret)
+		dev_err(&pf->pdev->dev,
+			"fail to %s cloud filter err %d aq_err %d\n",
+			add ? "add" : "delete",
+			ret, pf->hw.aq.asq_last_status);
+
+	dev_info(&pf->pdev->dev,
+		 "%s cloud filter for VSI: %d, L4 port: %d\n",
+		 add ? "add" : "delete",
+		 filter->seid, be16_to_cpu(filter->dst_port));
+
+	return ret;
+}
+
+/**
+ * i40e_add_del_cloud_filter - Add/del cloud filter
+ * @vsi: pointer to VSI
+ * @filter: cloud filter rule
+ * @add: if true, add, if false, delete
+ *
+ * Add or delete a cloud filter for a specific flow spec.
+ * Returns 0 if the filter were successfully added.
+ **/
+int i40e_add_del_cloud_filter(struct i40e_vsi *vsi,
+			      struct i40e_cloud_filter *filter, bool add)
+{
+	struct i40e_aqc_cloud_filters_element_data cld_filter;
+	struct i40e_pf *pf = vsi->back;
+	int ret;
+	static const u16 flag_table[128] = {
+		[I40E_CLOUD_FILTER_FLAGS_OMAC]  =
+			I40E_AQC_ADD_CLOUD_FILTER_OMAC,
+		[I40E_CLOUD_FILTER_FLAGS_IMAC]  =
+			I40E_AQC_ADD_CLOUD_FILTER_IMAC,
+		[I40E_CLOUD_FILTER_FLAGS_IMAC_IVLAN]  =
+			I40E_AQC_ADD_CLOUD_FILTER_IMAC_IVLAN,
+		[I40E_CLOUD_FILTER_FLAGS_IMAC_TEN_ID] =
+			I40E_AQC_ADD_CLOUD_FILTER_IMAC_TEN_ID,
+		[I40E_CLOUD_FILTER_FLAGS_OMAC_TEN_ID_IMAC] =
+			I40E_AQC_ADD_CLOUD_FILTER_OMAC_TEN_ID_IMAC,
+		[I40E_CLOUD_FILTER_FLAGS_IMAC_IVLAN_TEN_ID] =
+			I40E_AQC_ADD_CLOUD_FILTER_IMAC_IVLAN_TEN_ID,
+		[I40E_CLOUD_FILTER_FLAGS_IIP] =
+			I40E_AQC_ADD_CLOUD_FILTER_IIP,
+	};
+
+	if (filter->flags >= ARRAY_SIZE(flag_table))
+		return I40E_ERR_CONFIG;
+
+	memset(&cld_filter, 0, sizeof(cld_filter));
+
+	/* copy element needed to add cloud filter from filter */
+	i40e_set_cld_element(filter, &cld_filter);
+
+	if (filter->tunnel_type != I40E_CLOUD_TNL_TYPE_NONE)
+		cld_filter.flags = cpu_to_le16(filter->tunnel_type <<
+					     I40E_AQC_ADD_CLOUD_TNL_TYPE_SHIFT);
+
+	/* copy queue number from filter to pass to cloud filter engine
+	 * with flags for targeting traffic to specific queue
+	 */
+	if (filter->flags != I40E_CLOUD_FILTER_FLAGS_OMAC) {
+		if (filter->queue_id == I40E_CLOUD_FILTER_ANY_QUEUE) {
+			cld_filter.flags &=
+				~cpu_to_le16(I40E_AQC_ADD_CLOUD_FLAGS_TO_QUEUE);
+			cld_filter.queue_number = 0;
+		} else {
+			cld_filter.flags |=
+				cpu_to_le16(I40E_AQC_ADD_CLOUD_FLAGS_TO_QUEUE);
+			cld_filter.queue_number = cpu_to_le16(filter->queue_id);
+		}
+	}
+
+	if (filter->n_proto == ETH_P_IPV6)
+		cld_filter.flags |= cpu_to_le16(flag_table[filter->flags] |
+						I40E_AQC_ADD_CLOUD_FLAGS_IPV6);
+	else
+		cld_filter.flags |= cpu_to_le16(flag_table[filter->flags] |
+						I40E_AQC_ADD_CLOUD_FLAGS_IPV4);
+
+	if (add)
+		ret = i40e_aq_add_cloud_filters(&pf->hw, filter->seid,
+						&cld_filter, 1);
+	else
+		ret = i40e_aq_rem_cloud_filters(&pf->hw, filter->seid,
+						&cld_filter, 1);
+	if (ret)
+		dev_dbg(&pf->pdev->dev,
+			"Failed to %s cloud filter using l4 port %u, err %d aq_err %d\n",
+			add ? "add" : "delete", filter->dst_port, ret,
+			pf->hw.aq.asq_last_status);
+	else
+		dev_info(&pf->pdev->dev,
+			 "%s cloud filter for VSI: %d\n",
+			 add ? "Added" : "Deleted", filter->seid);
+	return ret;
+}
+
+/**
  * i40e_vsi_configure - Set up the VSI for action
  * @vsi: the VSI being configured
  **/
@@ -3556,43 +4510,29 @@
 		wr32(hw, I40E_PFINT_RATEN(vector - 1),
 		     i40e_intrl_usec_to_reg(vsi->int_rate_limit));
 
-		/* Linked list for the queuepairs assigned to this vector */
+		/* begin of linked list for RX queue assigned to this vector */
 		wr32(hw, I40E_PFINT_LNKLSTN(vector - 1), qp);
 		for (q = 0; q < q_vector->num_ringpairs; q++) {
 			u32 nextqp = has_xdp ? qp + vsi->alloc_queue_pairs : qp;
 			u32 val;
 
-			val = I40E_QINT_RQCTL_CAUSE_ENA_MASK |
-			      (I40E_RX_ITR << I40E_QINT_RQCTL_ITR_INDX_SHIFT) |
-			      (vector << I40E_QINT_RQCTL_MSIX_INDX_SHIFT) |
-			      (nextqp << I40E_QINT_RQCTL_NEXTQ_INDX_SHIFT) |
-			      (I40E_QUEUE_TYPE_TX <<
-			       I40E_QINT_RQCTL_NEXTQ_TYPE_SHIFT);
-
+			/* RX queue in linked list with next queue set to TX */
+			val = I40E_QINT_RQCTL_VAL(nextqp, vector, TX);
 			wr32(hw, I40E_QINT_RQCTL(qp), val);
 
 			if (has_xdp) {
-				val = I40E_QINT_TQCTL_CAUSE_ENA_MASK |
-				      (I40E_TX_ITR << I40E_QINT_TQCTL_ITR_INDX_SHIFT) |
-				      (vector << I40E_QINT_TQCTL_MSIX_INDX_SHIFT) |
-				      (qp << I40E_QINT_TQCTL_NEXTQ_INDX_SHIFT) |
-				      (I40E_QUEUE_TYPE_TX <<
-				       I40E_QINT_TQCTL_NEXTQ_TYPE_SHIFT);
-
+				/* TX queue with next queue set to TX */
+				val = I40E_QINT_TQCTL_VAL(qp, vector, TX);
 				wr32(hw, I40E_QINT_TQCTL(nextqp), val);
 			}
 
-			val = I40E_QINT_TQCTL_CAUSE_ENA_MASK |
-			      (I40E_TX_ITR << I40E_QINT_TQCTL_ITR_INDX_SHIFT) |
-			      (vector << I40E_QINT_TQCTL_MSIX_INDX_SHIFT) |
-			      ((qp + 1) << I40E_QINT_TQCTL_NEXTQ_INDX_SHIFT) |
-			      (I40E_QUEUE_TYPE_RX <<
-			       I40E_QINT_TQCTL_NEXTQ_TYPE_SHIFT);
+			/* TX queue with next RX or end of linked list */
+			val = I40E_QINT_TQCTL_VAL((qp + 1), vector, RX);
 
 			/* Terminate the linked list */
 			if (q == (q_vector->num_ringpairs - 1))
-				val |= (I40E_QUEUE_END_OF_LIST <<
-					I40E_QINT_TQCTL_NEXTQ_INDX_SHIFT);
+				val |= (I40E_QUEUE_END_OF_LIST
+					   << I40E_QINT_TQCTL_NEXTQ_INDX_SHIFT);
 
 			wr32(hw, I40E_QINT_TQCTL(qp), val);
 			qp++;
@@ -3623,12 +4563,13 @@
 	      I40E_PFINT_ICR0_ENA_HMC_ERR_MASK       |
 	      I40E_PFINT_ICR0_ENA_VFLR_MASK          |
 	      I40E_PFINT_ICR0_ENA_ADMINQ_MASK;
-
 	if (pf->flags & I40E_FLAG_IWARP_ENABLED)
 		val |= I40E_PFINT_ICR0_ENA_PE_CRITERR_MASK;
+#ifdef HAVE_PTP_1588_CLOCK
 
 	if (pf->flags & I40E_FLAG_PTP)
 		val |= I40E_PFINT_ICR0_ENA_TIMESYNC_MASK;
+#endif /* HAVE_PTP_1588_CLOCK */
 
 	wr32(hw, I40E_PFINT_ICR0_ENA, val);
 
@@ -3650,7 +4591,6 @@
 	struct i40e_q_vector *q_vector = vsi->q_vectors[0];
 	struct i40e_pf *pf = vsi->back;
 	struct i40e_hw *hw = &pf->hw;
-	u32 val;
 
 	/* set the ITR configuration */
 	q_vector->rx.next_update = jiffies + 1;
@@ -3667,28 +4607,20 @@
 	/* FIRSTQ_INDX = 0, FIRSTQ_TYPE = 0 (rx) */
 	wr32(hw, I40E_PFINT_LNKLST0, 0);
 
-	/* Associate the queue pair to the vector and enable the queue int */
-	val = I40E_QINT_RQCTL_CAUSE_ENA_MASK		       |
-	      (I40E_RX_ITR << I40E_QINT_RQCTL_ITR_INDX_SHIFT)  |
-	      (nextqp	   << I40E_QINT_RQCTL_NEXTQ_INDX_SHIFT)|
-	      (I40E_QUEUE_TYPE_TX << I40E_QINT_TQCTL_NEXTQ_TYPE_SHIFT);
-
-	wr32(hw, I40E_QINT_RQCTL(0), val);
-
-	if (i40e_enabled_xdp_vsi(vsi)) {
-		val = I40E_QINT_TQCTL_CAUSE_ENA_MASK		     |
-		      (I40E_TX_ITR << I40E_QINT_TQCTL_ITR_INDX_SHIFT)|
-		      (I40E_QUEUE_TYPE_TX
-		       << I40E_QINT_TQCTL_NEXTQ_TYPE_SHIFT);
+	/* Associate the queue pair to the vector and enable the queue
+	 * interrupt RX queue in linked list with next queue set to TX
+	 */
+	wr32(hw, I40E_QINT_RQCTL(0), I40E_QINT_RQCTL_VAL(nextqp, 0, TX));
 
-		wr32(hw, I40E_QINT_TQCTL(nextqp), val);
+	if (nextqp) {
+		/* TX queue in linked list with next queue set to TX */
+		wr32(hw, I40E_QINT_TQCTL(nextqp),
+		     I40E_QINT_TQCTL_VAL(nextqp, 0, TX));
 	}
 
-	val = I40E_QINT_TQCTL_CAUSE_ENA_MASK		      |
-	      (I40E_TX_ITR << I40E_QINT_TQCTL_ITR_INDX_SHIFT) |
-	      (I40E_QUEUE_END_OF_LIST << I40E_QINT_TQCTL_NEXTQ_INDX_SHIFT);
-
-	wr32(hw, I40E_QINT_TQCTL(0), val);
+	/* last TX queue so the next RX queue doesn't matter */
+	wr32(hw, I40E_QINT_TQCTL(0),
+	     I40E_QINT_TQCTL_VAL(I40E_QUEUE_END_OF_LIST, 0, RX));
 	i40e_flush(hw);
 }
 
@@ -3739,6 +4671,7 @@
 	return IRQ_HANDLED;
 }
 
+#ifdef HAVE_IRQ_AFFINITY_NOTIFY
 /**
  * i40e_irq_affinity_notify - Callback for affinity changes
  * @notify: context as to what irq was changed
@@ -3765,6 +4698,7 @@
  * receive notifications.
  **/
 static void i40e_irq_affinity_release(struct kref *ref) {}
+#endif /* HAVE_IRQ_AFFINITY_NOTIFY */
 
 /**
  * i40e_vsi_request_irq_msix - Initialize MSI-X interrupts
@@ -3773,7 +4707,7 @@
  *
  * Allocates MSI-X vectors and requests interrupts from the kernel.
  **/
-static int i40e_vsi_request_irq_msix(struct i40e_vsi *vsi, char *basename)
+int i40e_vsi_request_irq_msix(struct i40e_vsi *vsi, char *basename)
 {
 	int q_vectors = vsi->num_q_vectors;
 	struct i40e_pf *pf = vsi->back;
@@ -3782,7 +4716,9 @@
 	int tx_int_idx = 0;
 	int vector, err;
 	int irq_num;
+#ifdef HAVE_IRQ_AFFINITY_HINT
 	int cpu;
+#endif
 
 	for (vector = 0; vector < q_vectors; vector++) {
 		struct i40e_q_vector *q_vector = vsi->q_vectors[vector];
@@ -3814,10 +4750,13 @@
 			goto free_queue_irqs;
 		}
 
+#ifdef HAVE_IRQ_AFFINITY_NOTIFY
 		/* register for affinity change notifications */
 		q_vector->affinity_notify.notify = i40e_irq_affinity_notify;
 		q_vector->affinity_notify.release = i40e_irq_affinity_release;
 		irq_set_affinity_notifier(irq_num, &q_vector->affinity_notify);
+#endif
+#ifdef HAVE_IRQ_AFFINITY_HINT
 		/* Spread affinity hints out across online CPUs.
 		 *
 		 * get_cpu_mask returns a static constant mask with
@@ -3826,6 +4765,7 @@
 		 */
 		cpu = cpumask_local_spread(q_vector->v_idx, -1);
 		irq_set_affinity_hint(irq_num, get_cpu_mask(cpu));
+#endif /* HAVE_IRQ_AFFINITY_HINT */
 	}
 
 	vsi->irqs_ready = true;
@@ -3835,8 +4775,12 @@
 	while (vector) {
 		vector--;
 		irq_num = pf->msix_entries[base + vector].vector;
+#ifdef HAVE_IRQ_AFFINITY_NOTIFY
 		irq_set_affinity_notifier(irq_num, NULL);
+#endif
+#ifdef HAVE_IRQ_AFFINITY_HINT
 		irq_set_affinity_hint(irq_num, NULL);
+#endif
 		free_irq(irq_num, &vsi->q_vectors[vector]);
 	}
 	return err;
@@ -3953,7 +4897,6 @@
 	if (((icr0 & ~I40E_PFINT_ICR0_INTEVENT_MASK) == 0) ||
 	    (icr0 & I40E_PFINT_ICR0_SWINT_MASK))
 		pf->sw_int_count++;
-
 	if ((pf->flags & I40E_FLAG_IWARP_ENABLED) &&
 	    (icr0 & I40E_PFINT_ICR0_ENA_PE_CRITERR_MASK)) {
 		ena_mask &= ~I40E_PFINT_ICR0_ENA_PE_CRITERR_MASK;
@@ -4025,15 +4968,20 @@
 			 rd32(hw, I40E_PFHMC_ERRORDATA));
 	}
 
+#ifdef HAVE_PTP_1588_CLOCK
 	if (icr0 & I40E_PFINT_ICR0_TIMESYNC_MASK) {
 		u32 prttsyn_stat = rd32(hw, I40E_PRTTSYN_STAT_0);
 
-		if (prttsyn_stat & I40E_PRTTSYN_STAT_0_TXTIME_MASK) {
-			icr0 &= ~I40E_PFINT_ICR0_ENA_TIMESYNC_MASK;
+		if (prttsyn_stat & I40E_PRTTSYN_STAT_0_EVENT0_MASK)
+			schedule_work(&pf->ptp_extts0_work);
+
+		if (prttsyn_stat & I40E_PRTTSYN_STAT_0_TXTIME_MASK)
 			i40e_ptp_tx_hwtstamp(pf);
-		}
+
+		icr0 &= ~I40E_PFINT_ICR0_ENA_TIMESYNC_MASK;
 	}
 
+#endif /* HAVE_PTP_1588_CLOCK */
 	/* If a critical error is pending we have no choice but to reset the
 	 * device.
 	 * Report and mask out any remaining unexpected interrupts.
@@ -4426,6 +5374,7 @@
 		ret = i40e_control_wait_tx_q(vsi->seid, pf,
 					     pf_q + vsi->alloc_queue_pairs,
 					     true /*is xdp*/, enable);
+
 		if (ret)
 			break;
 	}
@@ -4543,9 +5492,7 @@
 		}
 	}
 
-	/* Due to HW errata, on Rx disable only, the register can indicate done
-	 * before it really is. Needs 50ms to be sure
-	 */
+	/* HW needs up to 50ms to finish RX queue disable*/
 	if (!enable)
 		mdelay(50);
 
@@ -4641,10 +5588,14 @@
 			    !vsi->q_vectors[i]->num_ringpairs)
 				continue;
 
+#ifdef HAVE_IRQ_AFFINITY_NOTIFY
 			/* clear the affinity notifier in the IRQ descriptor */
 			irq_set_affinity_notifier(irq_num, NULL);
+#endif
+#ifdef HAVE_IRQ_AFFINITY_HINT
 			/* remove our suggested affinity mask for this IRQ */
 			irq_set_affinity_hint(irq_num, NULL);
+#endif
 			synchronize_irq(irq_num);
 			free_irq(irq_num, vsi->q_vectors[i]);
 
@@ -4833,7 +5784,7 @@
 	for (q_idx = 0; q_idx < vsi->num_q_vectors; q_idx++) {
 		struct i40e_q_vector *q_vector = vsi->q_vectors[q_idx];
 
-		if (q_vector->rx.ring || q_vector->tx.ring)
+		if (q_vector->tx.ring || q_vector->rx.ring)
 			napi_enable(&q_vector->napi);
 	}
 }
@@ -4852,7 +5803,7 @@
 	for (q_idx = 0; q_idx < vsi->num_q_vectors; q_idx++) {
 		struct i40e_q_vector *q_vector = vsi->q_vectors[q_idx];
 
-		if (q_vector->rx.ring || q_vector->tx.ring)
+		if (q_vector->tx.ring || q_vector->rx.ring)
 			napi_disable(&q_vector->napi);
 	}
 }
@@ -4879,14 +5830,18 @@
  * i40e_quiesce_vsi - Pause a given VSI
  * @vsi: the VSI being paused
  **/
-static void i40e_quiesce_vsi(struct i40e_vsi *vsi)
+void i40e_quiesce_vsi(struct i40e_vsi *vsi)
 {
 	if (test_bit(__I40E_VSI_DOWN, vsi->state))
 		return;
 
 	set_bit(__I40E_VSI_NEEDS_RESTART, vsi->state);
 	if (vsi->netdev && netif_running(vsi->netdev))
+#ifdef HAVE_NET_DEVICE_OPS
 		vsi->netdev->netdev_ops->ndo_stop(vsi->netdev);
+#else /* HAVE_NET_DEVICE_OPS */
+		vsi->netdev->stop(vsi->netdev);
+#endif /* HAVE_NET_DEVICE_OPS */
 	else
 		i40e_vsi_close(vsi);
 }
@@ -4895,13 +5850,17 @@
  * i40e_unquiesce_vsi - Resume a given VSI
  * @vsi: the VSI being resumed
  **/
-static void i40e_unquiesce_vsi(struct i40e_vsi *vsi)
+void i40e_unquiesce_vsi(struct i40e_vsi *vsi)
 {
 	if (!test_and_clear_bit(__I40E_VSI_NEEDS_RESTART, vsi->state))
 		return;
 
 	if (vsi->netdev && netif_running(vsi->netdev))
+#ifdef HAVE_NET_DEVICE_OPS
 		vsi->netdev->netdev_ops->ndo_open(vsi->netdev);
+#else /* HAVE_NET_DEVICE_OPS */
+		vsi->netdev->open(vsi->netdev);
+#endif /* HAVE_NET_DEVICE_OPS */
 	else
 		i40e_vsi_open(vsi);   /* this clears the DOWN bit */
 }
@@ -4910,7 +5869,7 @@
  * i40e_pf_quiesce_all_vsi - Pause all VSIs on a PF
  * @pf: the PF
  **/
-static void i40e_pf_quiesce_all_vsi(struct i40e_pf *pf)
+void i40e_pf_quiesce_all_vsi(struct i40e_pf *pf)
 {
 	int v;
 
@@ -4924,7 +5883,7 @@
  * i40e_pf_unquiesce_all_vsi - Resume all VSIs on a PF
  * @pf: the PF
  **/
-static void i40e_pf_unquiesce_all_vsi(struct i40e_pf *pf)
+void i40e_pf_unquiesce_all_vsi(struct i40e_pf *pf)
 {
 	int v;
 
@@ -4939,7 +5898,7 @@
  * @vsi: the VSI being configured
  *
  * Wait until all queues on a given VSI have been disabled.
- **/
+**/
 int i40e_vsi_wait_queues_disabled(struct i40e_vsi *vsi)
 {
 	struct i40e_pf *pf = vsi->back;
@@ -4962,6 +5921,7 @@
 		/* Check and wait for the XDP Tx queue */
 		ret = i40e_pf_txq_wait(pf, pf_q + vsi->alloc_queue_pairs,
 				       false);
+
 		if (ret) {
 			dev_info(&pf->pdev->dev,
 				 "VSI seid %d XDP Tx ring %d disable timeout\n",
@@ -4982,7 +5942,7 @@
 	return 0;
 }
 
-#ifdef CONFIG_I40E_DCB
+#ifdef CONFIG_DCB
 /**
  * i40e_pf_wait_queues_disabled - Wait for all queues of PF VSIs to be disabled
  * @pf: the PF
@@ -5004,8 +5964,7 @@
 
 	return ret;
 }
-
-#endif
+#endif /* I40E_DCB */
 
 /**
  * i40e_get_iscsi_tc_map - Return TC map for iSCSI APP
@@ -5097,6 +6056,7 @@
 	return enabled_tc;
 }
 
+#ifdef __TC_MQPRIO_MODE_MAX
 /**
  * i40e_mqprio_get_enabled_tc - Get enabled traffic classes
  * @pf: PF being queried
@@ -5114,6 +6074,7 @@
 		enabled_tc |= BIT(i);
 	return enabled_tc;
 }
+#endif
 
 /**
  * i40e_pf_get_num_tc - Get enabled traffic classes for PF
@@ -5121,15 +6082,17 @@
  *
  * Return number of traffic classes enabled for the given PF
  **/
-static u8 i40e_pf_get_num_tc(struct i40e_pf *pf)
+u8 i40e_pf_get_num_tc(struct i40e_pf *pf)
 {
 	struct i40e_hw *hw = &pf->hw;
-	u8 i, enabled_tc = 1;
+	u8 i, enabled_tc;
 	u8 num_tc = 0;
 	struct i40e_dcbx_config *dcbcfg = &hw->local_dcbx_config;
 
+#ifdef __TC_MQPRIO_MODE_MAX
 	if (pf->flags & I40E_FLAG_TC_MQPRIO)
 		return pf->vsi[pf->lan_vsi]->mqprio_qopt.qopt.num_tc;
+#endif
 
 	/* If neither MQPRIO nor DCB is enabled, then always use single TC */
 	if (!(pf->flags & I40E_FLAG_DCB_ENABLED))
@@ -5143,7 +6106,7 @@
 	if (pf->hw.func_caps.iscsi)
 		enabled_tc =  i40e_get_iscsi_tc_map(pf);
 	else
-		return 1; /* Only TC0 */
+		return 1;/* Only TC0 */
 
 	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
 		if (enabled_tc & BIT(i))
@@ -5153,15 +6116,17 @@
 }
 
 /**
- * i40e_pf_get_pf_tc_map - Get bitmap for enabled traffic classes
+ * i40e_pf_get_tc_map - Get bitmap for enabled traffic classes
  * @pf: PF being queried
  *
  * Return a bitmap for enabled traffic classes for this PF.
  **/
 static u8 i40e_pf_get_tc_map(struct i40e_pf *pf)
 {
+#ifdef __TC_MQPRIO_MODE_MAX
 	if (pf->flags & I40E_FLAG_TC_MQPRIO)
 		return i40e_mqprio_get_enabled_tc(pf);
+#endif
 
 	/* If neither MQPRIO nor DCB is enabled for this PF then just return
 	 * default TC
@@ -5170,7 +6135,8 @@
 		return I40E_DEFAULT_TRAFFIC_CLASS;
 
 	/* SFP mode we want PF to be enabled for all TCs */
-	if (!(pf->flags & I40E_FLAG_MFP_ENABLED))
+	if (!(pf->flags & I40E_FLAG_MFP_ENABLED) ||
+	    (pf->flags & I40E_FLAG_MULTIPLE_TRAFFIC_CLASSES))
 		return i40e_dcb_get_enabled_tc(&pf->hw.local_dcbx_config);
 
 	/* MFP enabled and iSCSI PF type */
@@ -5186,7 +6152,7 @@
  *
  * Returns 0 on success, negative value on failure
  **/
-static int i40e_vsi_get_bw_info(struct i40e_vsi *vsi)
+int i40e_vsi_get_bw_info(struct i40e_vsi *vsi)
 {
 	struct i40e_aqc_query_vsi_ets_sla_config_resp bw_ets_config = {0};
 	struct i40e_aqc_query_vsi_bw_config_resp bw_config = {0};
@@ -5225,14 +6191,14 @@
 		/* Still continuing */
 	}
 
-	vsi->bw_limit = le16_to_cpu(bw_config.port_bw_limit);
+	vsi->bw_limit = LE16_TO_CPU(bw_config.port_bw_limit);
 	vsi->bw_max_quanta = bw_config.max_bw;
-	tc_bw_max = le16_to_cpu(bw_ets_config.tc_bw_max[0]) |
-		    (le16_to_cpu(bw_ets_config.tc_bw_max[1]) << 16);
+	tc_bw_max = LE16_TO_CPU(bw_ets_config.tc_bw_max[0]) |
+		    (LE16_TO_CPU(bw_ets_config.tc_bw_max[1]) << 16);
 	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
 		vsi->bw_ets_share_credits[i] = bw_ets_config.share_credits[i];
 		vsi->bw_ets_limit_credits[i] =
-					le16_to_cpu(bw_ets_config.credits[i]);
+					LE16_TO_CPU(bw_ets_config.credits[i]);
 		/* 3 bits out of 4 for each TC */
 		vsi->bw_ets_max_quanta[i] = (u8)((tc_bw_max >> (i*4)) & 0x7);
 	}
@@ -5256,10 +6222,12 @@
 	i40e_status ret;
 	int i;
 
+#ifdef __TC_MQPRIO_MODE_MAX
 	/* There is no need to reset BW when mqprio mode is on.  */
 	if (pf->flags & I40E_FLAG_TC_MQPRIO)
 		return 0;
-	if (!vsi->mqprio_qopt.qopt.hw && !(pf->flags & I40E_FLAG_DCB_ENABLED)) {
+	if (!vsi->mqprio_qopt.qopt.hw &&
+	    !(pf->flags & I40E_FLAG_DCB_ENABLED)) {
 		ret = i40e_set_bw_limit(vsi, vsi->seid, 0);
 		if (ret)
 			dev_info(&pf->pdev->dev,
@@ -5267,6 +6235,8 @@
 				 vsi->seid);
 		return ret;
 	}
+#endif /* __TC_MQPRIO_MODE_MAX */
+	memset(&bw_data, 0, sizeof(bw_data));
 	bw_data.tc_valid_bits = enabled_tc;
 	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++)
 		bw_data.tc_bw_credits[i] = bw_share[i];
@@ -5285,6 +6255,97 @@
 	return 0;
 }
 
+#define I40E_TC_BW_MAX_TC_BITS(val, tc)\
+	(CPU_TO_LE16(((val) & 0x7) << ((tc) % 4) * 4))
+
+/**
+ * i40e_vsi_configure_tc_max_bw - Configure VSI max BW per TC
+ * @vsi: the VSI being configured
+ *
+ * Configure max BW per TC based on tc info structure.
+ * Returns 0 on success, negative value on failure
+ **/
+int i40e_vsi_configure_tc_max_bw(struct i40e_vsi *vsi)
+{
+	struct i40e_aqc_configure_vsi_ets_sla_bw_data ets_data = {0};
+	struct i40e_pf *pf = vsi->back;
+	struct i40e_hw *hw = &pf->hw;
+	int ret, i;
+
+	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
+		if (vsi->tc_config.enabled_tc & BIT(i)) {
+			/* One credit is worth 50 Mbps*/
+			ets_data.tc_bw_credits[i] = CPU_TO_LE16
+				(vsi->tc_config.tc_info[i].tc_bw_credits);
+			ets_data.tc_valid_bits |= BIT(i);
+			ets_data.tc_bw_max[i / 4] |=
+				I40E_TC_BW_MAX_TC_BITS(0xff, i);
+		}
+	}
+
+	ret = i40e_aq_config_vsi_ets_sla_bw_limit(hw, vsi->seid, &ets_data,
+						  NULL);
+	if (ret) {
+		dev_info(&pf->pdev->dev,
+			 "Error %s, failed setting maximal bandwidth on traffic classes\n",
+			 i40e_stat_str(&pf->hw, ret));
+		ret = -EINVAL;
+		goto err;
+	}
+
+	/* FW writes back QS handles here */
+	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++)
+		vsi->info.qs_handle[i] = ets_data.tc_bw_credits[i];
+
+err:
+	return ret;
+}
+
+/**
+ * i40e_veb_configure_tc_max_bw - Configure VEB max BW per TC
+ * @veb: the VEB being configured
+ * @enabled_tc: data to AdminQ command
+ *
+ * Configure max BW per TC based on tc info structure.
+ * Returns 0 on success, negative value on failure
+ **/
+int i40e_veb_configure_tc_max_bw(struct i40e_veb *veb, u8 enabled_tc)
+{
+	struct i40e_aqc_configure_switching_comp_ets_bw_limit_data bw_data = {
+		0};
+	struct i40e_pf *pf = veb->pf;
+	int ret, i;
+
+	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
+		if (enabled_tc & BIT(i)) {
+			bw_data.tc_valid_bits |= BIT(i);
+			bw_data.tc_bw_max[i / 4] |=
+				I40E_TC_BW_MAX_TC_BITS(0xff, i);
+			bw_data.tc_bw_credit[i] =
+				CPU_TO_LE16(pf->dcb_veb_bw_map[i]);
+		}
+	}
+
+	ret = i40e_aq_config_switch_comp_ets_bw_limit(&pf->hw, veb->seid,
+						      &bw_data, NULL);
+	if (ret) {
+		dev_info(&pf->pdev->dev,
+			 "VEB reconfig failed, err %s\n",
+			 i40e_stat_str(&pf->hw, ret));
+		goto err;
+	}
+
+	/* Update the BW information */
+	ret = i40e_veb_get_bw_info(veb);
+	if (ret) {
+		dev_info(&pf->pdev->dev,
+			 "Failed getting veb bw config, err %s\n",
+			 i40e_stat_str(&pf->hw, ret));
+	}
+err:
+	return ret;
+}
+
 /**
  * i40e_vsi_config_netdev_tc - Setup the netdev TC configuration
  * @vsi: the VSI being configured
@@ -5361,6 +6422,58 @@
 }
 
 /**
+ * i40e_update_adq_vsi_queues - update queue mapping for ADq VSI
+ * @vsi: the VSI being reconfigured
+ * @vsi_offset: offset from main VF VSI
+ *
+ */
+int i40e_update_adq_vsi_queues(struct i40e_vsi *vsi, int vsi_offset)
+{
+	struct i40e_pf *pf;
+	struct i40e_hw *hw;
+	struct i40e_vsi_context ctxt;
+	int ret;
+
+	if (!vsi)
+		return I40E_ERR_PARAM;
+	pf = vsi->back;
+	hw = &pf->hw;
+
+	ctxt.seid = vsi->seid;
+	ctxt.pf_num = hw->pf_id;
+	ctxt.vf_num = vsi->vf_id + hw->func_caps.vf_base_id + vsi_offset;
+	ctxt.uplink_seid = vsi->uplink_seid;
+	ctxt.connection_type = I40E_AQ_VSI_CONN_TYPE_NORMAL;
+	ctxt.flags = I40E_AQ_VSI_TYPE_VF;
+	ctxt.info = vsi->info;
+
+	i40e_vsi_setup_queue_map(vsi, &ctxt, vsi->tc_config.enabled_tc,
+				 false);
+	if (vsi->reconfig_rss) {
+		vsi->rss_size = min_t(int, pf->alloc_rss_size,
+				      vsi->num_queue_pairs);
+		ret = i40e_vsi_config_rss(vsi);
+		if (ret) {
+			dev_info(&pf->pdev->dev, "Failed to reconfig rss for num_queues\n");
+			return ret;
+		}
+		vsi->reconfig_rss = false;
+	}
+
+	ret = i40e_aq_update_vsi_params(hw, &ctxt, NULL);
+	if (ret) {
+		dev_info(&pf->pdev->dev, "Update vsi config failed, err %s aq_err %s\n",
+			 i40e_stat_str(hw, ret),
+			 i40e_aq_str(hw, hw->aq.asq_last_status));
+	}
+	/* update the local VSI info with updated queue map */
+	i40e_vsi_update_queue_map(vsi, &ctxt);
+	vsi->info.valid_sections = 0;
+
+	return ret;
+}
+
+/**
  * i40e_vsi_config_tc - Configure VSI Tx Scheduler for given TC map
  * @vsi: VSI to be configured
  * @enabled_tc: TC bitmap
@@ -5373,7 +6486,7 @@
  * It is expected that the VSI queues have been quisced before calling
  * this function.
  **/
-static int i40e_vsi_config_tc(struct i40e_vsi *vsi, u8 enabled_tc)
+int i40e_vsi_config_tc(struct i40e_vsi *vsi, u8 enabled_tc)
 {
 	u8 bw_share[I40E_MAX_TRAFFIC_CLASS] = {0};
 	struct i40e_pf *pf = vsi->back;
@@ -5381,11 +6494,15 @@
 	struct i40e_vsi_context ctxt;
 	int ret = 0;
 	int i;
-
+#ifdef __TC_MQPRIO_MODE_MAX
 	/* Check if enabled_tc is same as existing or new TCs */
 	if (vsi->tc_config.enabled_tc == enabled_tc &&
 	    vsi->mqprio_qopt.mode != TC_MQPRIO_MODE_CHANNEL)
 		return ret;
+#else
+	if (vsi->tc_config.enabled_tc == enabled_tc)
+		return ret;
+#endif
 
 	/* Enable ETS TCs with equal BW Share for now across all VSIs */
 	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
@@ -5437,6 +6554,8 @@
 	ctxt.vf_num = 0;
 	ctxt.uplink_seid = vsi->uplink_seid;
 	ctxt.info = vsi->info;
+
+#ifdef __TC_MQPRIO_MODE_MAX
 	if (vsi->back->flags & I40E_FLAG_TC_MQPRIO) {
 		ret = i40e_vsi_setup_queue_map_mqprio(vsi, &ctxt, enabled_tc);
 		if (ret)
@@ -5444,7 +6563,6 @@
 	} else {
 		i40e_vsi_setup_queue_map(vsi, &ctxt, enabled_tc, false);
 	}
-
 	/* On destroying the qdisc, reset vsi->rss_size, as number of enabled
 	 * queues changed.
 	 */
@@ -5459,6 +6577,10 @@
 		}
 		vsi->reconfig_rss = false;
 	}
+#else
+	i40e_vsi_setup_queue_map(vsi, &ctxt, enabled_tc, false);
+#endif
+
 	if (vsi->back->flags & I40E_FLAG_IWARP_ENABLED) {
 		ctxt.info.valid_sections |=
 				cpu_to_le16(I40E_AQ_VSI_PROP_QUEUE_OPT_VALID);
@@ -5492,16 +6614,1043 @@
 
 	/* Update the netdev TC setup */
 	i40e_vsi_config_netdev_tc(vsi, enabled_tc);
+
+out:
+	return ret;
+}
+
+/**
+ * i40e_veb_config_tc - Configure TCs for given VEB
+ * @veb: given VEB
+ * @enabled_tc: TC bitmap
+ *
+ * Configures given TC bitmap for VEB (switching) element
+ **/
+int i40e_veb_config_tc(struct i40e_veb *veb, u8 enabled_tc)
+{
+	struct i40e_aqc_configure_switching_comp_bw_config_data bw_data = {0};
+	struct i40e_pf *pf = veb->pf;
+	int ret = 0;
+	int i;
+
+	/* No TCs or already enabled TCs just return */
+	if (!enabled_tc || veb->enabled_tc == enabled_tc)
+		return ret;
+
+	bw_data.tc_valid_bits = enabled_tc;
+	/* bw_data.absolute_credits is not set (relative) */
+
+	/* Enable ETS TCs with equal BW Share for now */
+	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
+		if (enabled_tc & BIT(i))
+			bw_data.tc_bw_share_credits[i] = 1;
+	}
+
+	ret = i40e_aq_config_switch_comp_bw_config(&pf->hw, veb->seid,
+						   &bw_data, NULL);
+	if (ret) {
+		dev_info(&pf->pdev->dev,
+			 "VEB bw config failed, err %s aq_err %s\n",
+			 i40e_stat_str(&pf->hw, ret),
+			 i40e_aq_str(&pf->hw, pf->hw.aq.asq_last_status));
+		goto out;
+	}
+
+	/* Update the BW information */
+	ret = i40e_veb_get_bw_info(veb);
+	if (ret) {
+		dev_info(&pf->pdev->dev,
+			 "Failed getting veb bw config, err %s aq_err %s\n",
+			 i40e_stat_str(&pf->hw, ret),
+			 i40e_aq_str(&pf->hw, pf->hw.aq.asq_last_status));
+	}
+
+out:
+	return ret;
+}
+
+#ifdef CONFIG_DCB
+/**
+ * i40e_dcb_reconfigure - Reconfigure all VEBs and VSIs
+ * @pf: PF struct
+ *
+ * Reconfigure VEB/VSIs on a given PF; it is assumed that
+ * the caller would've quiesce all the VSIs before calling
+ * this function
+ **/
+static void i40e_dcb_reconfigure(struct i40e_pf *pf)
+{
+	u8 tc_map = 0;
+	int ret;
+	u8 v;
+
+	/* Enable the TCs available on PF to all VEBs */
+	tc_map = i40e_pf_get_tc_map(pf);
+	if (tc_map == I40E_DEFAULT_TRAFFIC_CLASS)
+		return;
+
+	for (v = 0; v < I40E_MAX_VEB; v++) {
+		if (!pf->veb[v])
+			continue;
+		if (pf->flags & I40E_FLAG_MULTIPLE_TRAFFIC_CLASSES)
+			ret = i40e_veb_configure_tc_max_bw(pf->veb[v], tc_map);
+		else
+			ret = i40e_veb_config_tc(pf->veb[v], tc_map);
+
+		if (ret) {
+			dev_info(&pf->pdev->dev,
+				 "Failed configuring TC for VEB seid=%d\n",
+				 pf->veb[v]->seid);
+			/* Will try to configure as many components */
+		}
+	}
+
+	/* Update each VSI */
+	for (v = 0; v < pf->num_alloc_vsi; v++) {
+		if (!pf->vsi[v])
+			continue;
+
+		/* - Enable all TCs for the LAN VSI
+		 * - For all others keep them at TC0 for now
+		 */
+		if (v == pf->lan_vsi)
+			tc_map = i40e_pf_get_tc_map(pf);
+		else
+			tc_map = I40E_DEFAULT_TRAFFIC_CLASS;
+
+		ret = i40e_vsi_config_tc(pf->vsi[v], tc_map);
+		if (ret) {
+			dev_info(&pf->pdev->dev,
+				 "Failed configuring TC for VSI seid=%d\n",
+				 pf->vsi[v]->seid);
+			/* Will try to configure as many components */
+		} else {
+			/* Re-configure VSI vectors based on updated TC map */
+			i40e_vsi_map_rings_to_vectors(pf->vsi[v]);
+#ifdef HAVE_DCBNL_IEEE
+			if (pf->vsi[v]->netdev)
+				i40e_dcbnl_set_all(pf->vsi[v]);
+#endif /* HAVE_DCBNL_IEEE */
+		}
+	}
+}
+
+/**
+ * i40e_resume_port_tx - Resume port Tx
+ * @pf: PF struct
+ *
+ * Resume a port's Tx and issue a PF reset in case of failure to
+ * resume.
+ **/
+static int i40e_resume_port_tx(struct i40e_pf *pf)
+{
+	struct i40e_hw *hw = &pf->hw;
+	int ret;
+
+	ret = i40e_aq_resume_port_tx(hw, NULL);
+	if (ret) {
+		dev_info(&pf->pdev->dev,
+			 "Resume Port Tx failed, err %s aq_err %s\n",
+			  i40e_stat_str(&pf->hw, ret),
+			  i40e_aq_str(&pf->hw, pf->hw.aq.asq_last_status));
+		/* Schedule PF reset to recover */
+		set_bit(__I40E_PF_RESET_REQUESTED, pf->state);
+		i40e_service_event_schedule(pf);
+	}
+
+	return ret;
+}
+
+/**
+ * i40e_suspend_port_tx - Suspend port Tx
+ * @pf: PF struct
+ *
+ * Suspend a port's Tx and issue a PF reset in case of failure.
+ **/
+static int i40e_suspend_port_tx(struct i40e_pf *pf)
+{
+	struct i40e_hw *hw = &pf->hw;
+	int ret;
+
+	ret = i40e_aq_suspend_port_tx(hw, pf->mac_seid, NULL);
+	if (ret) {
+		dev_info(&pf->pdev->dev,
+			 "Suspend Port Tx failed, err %s aq_err %s\n",
+			  i40e_stat_str(&pf->hw, ret),
+			  i40e_aq_str(&pf->hw, pf->hw.aq.asq_last_status));
+		/* Schedule PF reset to recover */
+		set_bit(__I40E_PF_RESET_REQUESTED, pf->state);
+		i40e_service_event_schedule(pf);
+	}
+
+	return ret;
+}
+
+/**
+ * i40e_hw_set_dcb_config - Program new DCBX settings into HW
+ * @pf: PF being configured
+ * @new_cfg: New DCBX configuration
+ *
+ * Program DCB settings into HW and reconfigure VEB/VSIs on
+ * given PF. Uses "Set LLDP MIB" AQC to program the hardware.
+ **/
+static int i40e_hw_set_dcb_config(struct i40e_pf *pf,
+				  struct i40e_dcbx_config *new_cfg)
+{
+	struct i40e_dcbx_config *old_cfg = &pf->hw.local_dcbx_config;
+	int ret = -EINVAL;
+
+	/* Check if need reconfiguration */
+	if (!memcmp(&new_cfg, &old_cfg, sizeof(new_cfg))) {
+		dev_dbg(&pf->pdev->dev, "No Change in DCB Config required.\n");
+		return 0;
+	}
+
+	/* Config change disable all VSIs */
+	i40e_pf_quiesce_all_vsi(pf);
+
+	/* Copy the new config to the current config */
+	*old_cfg = *new_cfg;
+	old_cfg->etsrec = old_cfg->etscfg;
+	ret = i40e_set_dcb_config(&pf->hw);
+	if (ret) {
+		dev_info(&pf->pdev->dev,
+			 "Set DCB Config failed, err %s aq_err %s\n",
+			 i40e_stat_str(&pf->hw, ret),
+			 i40e_aq_str(&pf->hw, pf->hw.aq.asq_last_status));
+		goto out;
+	}
+	/* Changes in configuration update VEB/VSI */
+	i40e_dcb_reconfigure(pf);
 out:
+	/* In case of reset do not try to resume anything */
+	if (!test_bit(__I40E_RESET_RECOVERY_PENDING, pf->state)) {
+		/* Re-start the VSIs if disabled */
+		ret = i40e_resume_port_tx(pf);
+		/* In case of error no point in resuming VSIs */
+		if (ret)
+			goto err;
+		i40e_pf_unquiesce_all_vsi(pf);
+	}
+err:
 	return ret;
 }
 
 /**
+ * i40e_update_ets_field - Update field
+ * @cfg: field being updated
+ * @rec: field being updated
+ * @val: value if initialized
+ * @false_val: value if not initialized
+ * @lhs: being compared
+ * @rhs: compare against
+ *
+ * Updates field, only if value is initialized.
+ **/
+static void i40e_update_ets_field(u8 *cfg, u8 *rec, u8 val, u8 false_val,
+				  u8 lhs, u8 rhs)
+{
+	if (lhs != rhs) {
+		*cfg = val;
+		*rec = val;
+	} else {
+		*cfg = false_val;
+		*rec = false_val;
+	}
+}
+
+/**
+ * i40e_update_ets - Update port with user provided ETS settings.
+ * @pf: PF being updated
+ *
+ * Returns 0 on success, negative value on failure
+ **/
+
+int i40e_update_ets(struct i40e_pf *pf)
+{
+	struct i40e_hw *hw = &pf->hw;
+	struct i40e_dcbx_config *old_cfg = &hw->local_dcbx_config;
+	int i, ret;
+
+	if (!(pf->dcbx_cap & DCB_CAP_DCBX_VER_IEEE) ||
+	    (pf->dcbx_cap & DCB_CAP_DCBX_LLD_MANAGED)) {
+		return -EINVAL;
+	}
+
+	/* Copy current config into tmp */
+	pf->tmp_cfg = *old_cfg;
+	pf->tmp_cfg.etscfg.willing = 0;
+	pf->tmp_cfg.etscfg.maxtcs = I40E_MAX_TRAFFIC_CLASS;
+	/* Update the ETS configuration for tmp */
+	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
+		i40e_update_ets_field(&pf->tmp_cfg.etscfg.prioritytable[i],
+				      &pf->tmp_cfg.etsrec.prioritytable[i],
+				      pf->dcb_user_up_map[i], 0,
+				      pf->dcb_user_up_map[i],
+				      I40E_MULTIPLE_TRAFFIC_CLASS_NO_ENTRY);
+
+		if (pf->dcb_user_lsp_map[i] !=
+		    I40E_MULTIPLE_TRAFFIC_CLASS_NO_ENTRY) {
+			i40e_update_ets_field(&pf->tmp_cfg.etscfg.tsatable[i],
+					      &pf->tmp_cfg.etsrec.tsatable[i],
+					      IEEE_8021QAZ_TSA_ETS,
+					      IEEE_8021QAZ_TSA_STRICT,
+					      pf->dcb_user_lsp_map[i], 1);
+		} else {
+			pf->tmp_cfg.etscfg.tsatable[i] =
+				IEEE_8021QAZ_TSA_ETS;
+			pf->tmp_cfg.etsrec.tsatable[i] =
+				IEEE_8021QAZ_TSA_ETS;
+		}
+
+		i40e_update_ets_field(&pf->tmp_cfg.etscfg.tcbwtable[i],
+				      &pf->tmp_cfg.etsrec.tcbwtable[i],
+				      pf->dcb_mib_bw_map[i], 0,
+				      pf->dcb_mib_bw_map[i],
+				      I40E_MULTIPLE_TRAFFIC_CLASS_NO_ENTRY);
+	}
+
+	rtnl_lock();
+	/* Commit changes to HW */
+	ret = i40e_hw_dcb_config(pf, &pf->tmp_cfg);
+	if (ret) {
+		dev_info(&pf->pdev->dev,
+			 "Failed setting DCB ETS configuration err %s\n",
+			 i40e_stat_str(&pf->hw, ret));
+	}
+	rtnl_unlock();
+	return ret;
+}
+
+/**
+ * i40e_hw_dcb_config - Program new DCBX settings into HW
+ * @pf: PF being configured
+ * @new_cfg: New DCBX configuration
+ *
+ * Program DCB settings into HW and reconfigure VEB/VSIs on
+ * given PF
+ **/
+int i40e_hw_dcb_config(struct i40e_pf *pf, struct i40e_dcbx_config *new_cfg)
+{
+	struct i40e_aqc_configure_switching_comp_ets_data ets_data;
+	u8 prio_type[I40E_MAX_TRAFFIC_CLASS] = {0};
+	u32 mfs_tc[I40E_MAX_TRAFFIC_CLASS];
+	u8 mode[I40E_MAX_TRAFFIC_CLASS];
+	struct i40e_rx_pb_config pb_cfg;
+	struct i40e_hw *hw = &pf->hw;
+	bool need_reconfig = false;
+	int ret = -EINVAL;
+	u8 num_ports = 1;
+	u8 new_numtc = 0;
+	u8 lltc_map = 0;
+	u8 tc_map = 0;
+	u8 i;
+	struct i40e_dcbx_config *old_cfg = &hw->local_dcbx_config;
+
+	dev_dbg(&pf->pdev->dev, "Configuring DCB registers directly\n");
+	/* Un-pack information to Program ETS HW via shared API
+	 * numtc, tcmap
+	 * LLTC map
+	 * ETS/NON-ETS arbiter mode
+	 * max exponent (credit refills)
+	 * Total number of ports
+	 * PFC priority bit-map
+	 * Priority Table
+	 * BW % per TC
+	 * Arbiter mode between UPs sharing same TC
+	 * TSA table (ETS or non-ETS)
+	 * EEE enabled or not
+	 * MFS TC table
+	 */
+
+	new_numtc = i40e_dcb_get_num_tc(new_cfg);
+
+	memset(&ets_data, 0, sizeof(ets_data));
+	for (i = 0; i < new_numtc; i++) {
+		tc_map |= BIT(i);
+		switch (new_cfg->etscfg.tsatable[i]) {
+		case I40E_IEEE_TSA_ETS:
+			prio_type[i] = I40E_DCB_PRIO_TYPE_ETS;
+			ets_data.tc_bw_share_credits[i] =
+					new_cfg->etscfg.tcbwtable[i];
+			break;
+		case I40E_IEEE_TSA_STRICT:
+			prio_type[i] = I40E_DCB_PRIO_TYPE_STRICT;
+			lltc_map |= BIT(i);
+			ets_data.tc_bw_share_credits[i] =
+					I40E_DCB_STRICT_PRIO_CREDITS;
+			break;
+		default:
+			/* Invalid TSA type */
+			goto out;
+		}
+	}
+
+	/* Check if need reconfiguration */
+	need_reconfig = i40e_dcb_need_reconfig(pf, old_cfg, new_cfg);
+
+	/* If needed, enable/disable frame tagging, disable all VSIs
+	 * and suspend port tx
+	 */
+	if (need_reconfig) {
+		/* Enable DCB tagging only when more than one TC */
+		if (new_numtc > 1)
+			pf->flags |= I40E_FLAG_DCB_ENABLED;
+		else
+			pf->flags &= ~I40E_FLAG_DCB_ENABLED;
+
+		set_bit(__I40E_PORT_SUSPENDED, pf->state);
+		/* Reconfiguration needed quiesce all VSIs */
+		i40e_pf_quiesce_all_vsi(pf);
+		ret = i40e_suspend_port_tx(pf);
+		if (ret)
+			goto err;
+	}
+
+	/* Configure Port ETS Tx Scheduler */
+	ets_data.tc_valid_bits = tc_map;
+	ets_data.tc_strict_priority_flags = lltc_map;
+	ret = i40e_aq_config_switch_comp_ets(hw, pf->mac_seid, &ets_data,
+					i40e_aqc_opc_modify_switching_comp_ets,
+					NULL);
+	if (ret) {
+		dev_info(&pf->pdev->dev,
+			 "Modify Port ETS failed, err %s aq_err %s\n",
+			  i40e_stat_str(&pf->hw, ret),
+			  i40e_aq_str(&pf->hw, pf->hw.aq.asq_last_status));
+		goto out;
+	}
+
+	/* Configure Rx ETS HW */
+	memset(&mode, I40E_DCB_ARB_MODE_ROUND_ROBIN, sizeof(mode));
+	i40e_dcb_hw_set_num_tc(hw, new_numtc);
+	i40e_dcb_hw_rx_fifo_config(hw, I40E_DCB_ARB_MODE_ROUND_ROBIN,
+				   I40E_DCB_ARB_MODE_STRICT_PRIORITY,
+				   0xB,
+				   lltc_map);
+	i40e_dcb_hw_rx_cmd_monitor_config(hw, new_numtc, num_ports);
+	i40e_dcb_hw_rx_ets_bw_config(hw, new_cfg->etscfg.tcbwtable, mode,
+				     prio_type);
+	i40e_dcb_hw_pfc_config(hw, new_cfg->pfc.pfcenable,
+			       new_cfg->etscfg.prioritytable);
+	i40e_dcb_hw_rx_up2tc_config(hw, new_cfg->etscfg.prioritytable);
+
+	/* Configure Rx Packet Buffers in HW */
+	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
+		mfs_tc[i] = pf->vsi[pf->lan_vsi]->netdev->mtu;
+		mfs_tc[i] += I40E_PACKET_HDR_PAD;
+	}
+
+	i40e_dcb_hw_calculate_pool_sizes(hw, num_ports,
+					 false, new_cfg->pfc.pfcenable,
+					 mfs_tc, &pb_cfg);
+	i40e_dcb_hw_rx_pb_config(hw, &pf->pb_cfg, &pb_cfg);
+
+	/* Update the local Rx Packet buffer config */
+	pf->pb_cfg = pb_cfg;
+
+	/* Inform the FW about changes to DCB configuration */
+	ret = i40e_aq_dcb_updated(&pf->hw, NULL);
+	if (ret) {
+		dev_info(&pf->pdev->dev,
+			 "DCB Updated failed, err %s aq_err %s\n",
+			  i40e_stat_str(&pf->hw, ret),
+			  i40e_aq_str(&pf->hw, pf->hw.aq.asq_last_status));
+		goto out;
+	}
+
+	/* Update the port DCBx configuration */
+	*old_cfg = *new_cfg;
+
+	/* Changes in configuration update VEB/VSI */
+	i40e_dcb_reconfigure(pf);
+out:
+	/* Re-start the VSIs if disabled */
+	if (need_reconfig) {
+		ret = i40e_resume_port_tx(pf);
+
+		clear_bit(__I40E_PORT_SUSPENDED, pf->state);
+		/* In case of error no point in resuming VSIs */
+		if (ret)
+			goto err;
+
+		/* Wait for the PF's queues to be disabled */
+		ret = i40e_pf_wait_queues_disabled(pf);
+		if (ret) {
+			/* Schedule PF reset to recover */
+			set_bit(__I40E_PF_RESET_REQUESTED, pf->state);
+			i40e_service_event_schedule(pf);
+			goto err;
+		} else {
+			i40e_pf_unquiesce_all_vsi(pf);
+			set_bit(__I40E_CLIENT_SERVICE_REQUESTED, pf->state);
+			set_bit(__I40E_CLIENT_L2_CHANGE, pf->state);
+		}
+	/* registers are set, lets apply */
+	if (pf->hw_features & I40E_HW_USE_SET_LLDP_MIB)
+		ret = i40e_hw_set_dcb_config(pf, new_cfg);
+	}
+err:
+	return ret;
+}
+
+/**
+ * i40e_dcb_sw_default_config - Set default DCB configuration when DCB in SW
+ * @pf: PF being queried
+ * @ets_willing: willing bits that will be advertised
+ *
+ * Set default DCB configuration in case DCB is to be done in SW.
+ **/
+int i40e_dcb_sw_default_config(struct i40e_pf *pf, u8 ets_willing)
+{
+	struct i40e_hw *hw = &pf->hw;
+	struct i40e_aqc_configure_switching_comp_ets_data ets_data;
+	struct i40e_dcbx_config *dcb_cfg = &hw->local_dcbx_config;
+	int err = 0;
+
+	if (pf->hw_features & I40E_HW_USE_SET_LLDP_MIB) {
+		/* Update the local cached instance with TC0 ETS */
+		memset(&pf->tmp_cfg, 0, sizeof(struct i40e_dcbx_config));
+		pf->tmp_cfg.etscfg.willing = ets_willing;
+		pf->tmp_cfg.etscfg.maxtcs = 0;
+		pf->tmp_cfg.etscfg.tcbwtable[0] = 100;
+		pf->tmp_cfg.etscfg.tsatable[0] = I40E_IEEE_TSA_ETS;
+		pf->tmp_cfg.pfc.willing = 1;
+		pf->tmp_cfg.pfc.pfccap = I40E_MAX_TRAFFIC_CLASS;
+		/* FW needs one App to configure HW */
+		pf->tmp_cfg.numapps = 1;
+		pf->tmp_cfg.app[0].selector = I40E_APP_SEL_ETHTYPE;
+		pf->tmp_cfg.app[0].priority = 3;
+		pf->tmp_cfg.app[0].protocolid = I40E_APP_PROTOID_FCOE;
+
+		return i40e_hw_set_dcb_config(pf, &pf->tmp_cfg);
+	}
+
+	memset(&ets_data, 0, sizeof(ets_data));
+	ets_data.tc_valid_bits = 0x1; /* TC0 only */
+	ets_data.tc_strict_priority_flags = 0; /* ETS */
+	ets_data.tc_bw_share_credits[0] = 100; /* 100% to TC0 */
+
+	/* Enable ETS on the Physical port */
+	err = i40e_aq_config_switch_comp_ets(hw, pf->mac_seid, &ets_data,
+					i40e_aqc_opc_enable_switching_comp_ets,
+					NULL);
+	if (err) {
+		dev_info(&pf->pdev->dev,
+			 "Enable Port ETS failed, err %s aq_err %s\n",
+			  i40e_stat_str(&pf->hw, err),
+			  i40e_aq_str(&pf->hw, pf->hw.aq.asq_last_status));
+		err = -ENOENT;
+		goto out;
+	}
+
+	/* Update the local cached instance with TC0 ETS */
+	dcb_cfg->etscfg.willing = ets_willing;
+	dcb_cfg->etscfg.cbs = 0;
+	dcb_cfg->etscfg.maxtcs = I40E_MAX_TRAFFIC_CLASS;
+	dcb_cfg->etscfg.tcbwtable[0] = 100;
+
+out:
+	return err;
+}
+
+/**
+ * i40e_init_pf_dcb - Initialize DCB configuration
+ * @pf: PF being configured
+ *
+ * Query the current DCB configuration and cache it
+ * in the hardware structure
+ **/
+static int i40e_init_pf_dcb(struct i40e_pf *pf)
+{
+	struct i40e_hw *hw = &pf->hw;
+	int err = 0;
+
+	/* Do not enable DCB for SW1 and SW2 images even if the FW is capable
+	 * Also do not enable DCBx if FW LLDP agent is disabled
+	 */
+	if (pf->hw_features & I40E_HW_NO_DCB_SUPPORT) {
+		dev_info(&pf->pdev->dev, "DCB is not supported.\n");
+		err = I40E_NOT_SUPPORTED;
+		goto out;
+	}
+
+	if (pf->flags & I40E_FLAG_MULTIPLE_TRAFFIC_CLASSES) {
+		dev_info(&pf->pdev->dev, "Attempting to initialize multiple traffic classes\n");
+		err = i40e_dcb_sw_default_config(pf,
+						 I40E_ETS_NON_WILLING_MODE);
+		if (err) {
+			dev_info(&pf->pdev->dev, "Could not initialize multiple traffic classes, rolling back to FW LLDP\n");
+			pf->flags &= ~(I40E_FLAG_DISABLE_FW_LLDP |
+				       I40E_FLAG_MULTIPLE_TRAFFIC_CLASSES);
+			goto out;
+		}
+		pf->dcbx_cap = DCB_CAP_DCBX_HOST |
+			       DCB_CAP_DCBX_VER_IEEE;
+		/* at init capable but disabled */
+		pf->flags |= I40E_FLAG_DCB_CAPABLE;
+		pf->flags &= ~I40E_FLAG_DCB_ENABLED;
+		goto out;
+	}
+	if (pf->flags & I40E_FLAG_DISABLE_FW_LLDP) {
+		dev_info(&pf->pdev->dev, "FW LLDP is disabled, attempting SW DCB\n");
+		err = i40e_dcb_sw_default_config(pf, I40E_ETS_WILLING_MODE);
+		if (err) {
+			dev_info(&pf->pdev->dev, "Could not initialize SW DCB\n");
+			goto out;
+		}
+		dev_info(&pf->pdev->dev, "SW DCB initialization succeeded.\n");
+		pf->dcbx_cap = DCB_CAP_DCBX_HOST |
+			       DCB_CAP_DCBX_VER_IEEE;
+		/* at init capable but disabled */
+		pf->flags |= I40E_FLAG_DCB_CAPABLE;
+		pf->flags &= ~I40E_FLAG_DCB_ENABLED;
+		goto out;
+	}
+	err = i40e_init_dcb(hw, true);
+	if (!err) {
+		/* Device/Function is not DCBX capable */
+		if ((!hw->func_caps.dcb) ||
+		    (hw->dcbx_status == I40E_DCBX_STATUS_DISABLED)) {
+			dev_info(&pf->pdev->dev,
+				 "DCBX offload is not supported or is disabled for this PF.\n");
+		} else {
+			/* When status is not DISABLED then DCBX in FW */
+			pf->dcbx_cap = DCB_CAP_DCBX_LLD_MANAGED |
+				       DCB_CAP_DCBX_VER_IEEE;
+
+			pf->flags |= I40E_FLAG_DCB_CAPABLE;
+			/* Enable DCB tagging only when more than one TC
+			 * or explicity disable if only one TC
+			 */
+			if (i40e_dcb_get_num_tc(&hw->local_dcbx_config) > 1)
+				pf->flags |= I40E_FLAG_DCB_ENABLED;
+			else
+				pf->flags &= ~I40E_FLAG_DCB_ENABLED;
+			dev_dbg(&pf->pdev->dev,
+				"DCBX offload is supported for this PF.\n");
+		}
+	} else if (pf->hw.aq.asq_last_status == I40E_AQ_RC_EPERM) {
+		dev_info(&pf->pdev->dev, "FW LLDP disabled for this PF.\n");
+		pf->flags |= I40E_FLAG_DISABLE_FW_LLDP;
+	} else {
+		dev_info(&pf->pdev->dev,
+			 "Query for DCB configuration failed, err %s aq_err %s\n",
+			 i40e_stat_str(&pf->hw, err),
+			 i40e_aq_str(&pf->hw, pf->hw.aq.asq_last_status));
+	}
+
+out:
+	return err;
+}
+
+#endif /* CONFIG_DCB */
+#define SPEED_SIZE 14
+#define FC_SIZE 8
+/**
+ * i40e_print_link_message - print link up or down
+ * @vsi: the VSI for which link needs a message
+ * @isup: true of link is up, false otherwise
+ */
+void i40e_print_link_message(struct i40e_vsi *vsi, bool isup)
+{
+	enum i40e_aq_link_speed new_speed;
+	struct i40e_pf *pf = vsi->back;
+	char *speed = "Unknown ";
+	char *fc = "Unknown";
+	char *fec = "";
+	char *req_fec = "";
+	char *an = "";
+
+	if (isup)
+		new_speed = pf->hw.phy.link_info.link_speed;
+	else
+		new_speed = I40E_LINK_SPEED_UNKNOWN;
+
+	if ((vsi->current_isup == isup) && (vsi->current_speed == new_speed))
+		return;
+
+	vsi->current_isup = isup;
+	vsi->current_speed = new_speed;
+
+	if (!isup) {
+		netdev_info(vsi->netdev, "NIC Link is Down\n");
+		return;
+	}
+
+	/* Warn user if link speed on NPAR enabled partition is not at
+	 * least 10GB
+	 */
+	if (pf->hw.func_caps.npar_enable &&
+	    (pf->hw.phy.link_info.link_speed == I40E_LINK_SPEED_1GB ||
+	     pf->hw.phy.link_info.link_speed == I40E_LINK_SPEED_100MB))
+		netdev_warn(vsi->netdev,
+			    "The partition detected link speed that is less than 10Gbps\n");
+
+	switch (pf->hw.phy.link_info.link_speed) {
+	case I40E_LINK_SPEED_40GB:
+		speed = "40 G";
+		break;
+	case I40E_LINK_SPEED_20GB:
+		speed = "20 G";
+		break;
+	case I40E_LINK_SPEED_25GB:
+		speed = "25 G";
+		break;
+	case I40E_LINK_SPEED_10GB:
+		speed = "10 G";
+		break;
+	case I40E_LINK_SPEED_5GB:
+		speed = "5 G";
+		break;
+	case I40E_LINK_SPEED_2_5GB:
+		speed = "2.5 G";
+		break;
+	case I40E_LINK_SPEED_1GB:
+		speed = "1000 M";
+		break;
+	case I40E_LINK_SPEED_100MB:
+		speed = "100 M";
+		break;
+	default:
+		break;
+	}
+
+	switch (pf->hw.fc.current_mode) {
+	case I40E_FC_FULL:
+		fc = "RX/TX";
+		break;
+	case I40E_FC_TX_PAUSE:
+		fc = "TX";
+		break;
+	case I40E_FC_RX_PAUSE:
+		fc = "RX";
+		break;
+	default:
+		fc = "None";
+		break;
+	}
+
+	if (pf->hw.phy.link_info.link_speed == I40E_LINK_SPEED_25GB) {
+		req_fec = "None";
+		fec = "None";
+		an = "False";
+
+		if (pf->hw.phy.link_info.an_info & I40E_AQ_AN_COMPLETED)
+			an = "True";
+
+		if (pf->hw.phy.link_info.fec_info &
+		    I40E_AQ_CONFIG_FEC_KR_ENA)
+			fec = "CL74 FC-FEC/BASE-R";
+		else if (pf->hw.phy.link_info.fec_info &
+			 I40E_AQ_CONFIG_FEC_RS_ENA)
+			fec = "CL108 RS-FEC";
+
+		/* 'CL108 RS-FEC' should be displayed when RS is requested, or
+		 * both RS and FC are requested
+		 */
+		if (pf->hw.phy.link_info.req_fec_info &
+		    (I40E_AQ_REQUEST_FEC_KR | I40E_AQ_REQUEST_FEC_RS)) {
+			if (pf->hw.phy.link_info.req_fec_info &
+			    I40E_AQ_REQUEST_FEC_RS)
+				req_fec = "CL108 RS-FEC";
+			else
+				req_fec = "CL74 FC-FEC/BASE-R";
+		}
+		netdev_info(vsi->netdev,
+			    "NIC Link is Up, %sbps Full Duplex, Requested FEC: %s, Negotiated FEC: %s, Autoneg: %s, Flow Control: %s\n",
+			    speed, req_fec, fec, an, fc);
+	} else if (pf->hw.device_id == I40E_DEV_ID_KX_X722) {
+		req_fec = "None";
+		fec = "None";
+		an = "False";
+
+		if (pf->hw.phy.link_info.an_info & I40E_AQ_AN_COMPLETED)
+			an = "True";
+
+		if (pf->hw.phy.link_info.fec_info &
+		    I40E_AQ_CONFIG_FEC_KR_ENA)
+			fec = "CL74 FC-FEC/BASE-R";
+		else if (pf->hw.phy.link_info.fec_info &
+			 I40E_AQ_CONFIG_FEC_RS_ENA)
+			fec = "CL108 RS-FEC";
+
+		/* 'CL108 RS-FEC' should be displayed when RS is requested, or
+		 * both RS and FC are requested
+		 */
+		if (pf->hw.phy.link_info.req_fec_info &
+		    (I40E_AQ_REQUEST_FEC_KR | I40E_AQ_REQUEST_FEC_RS)) {
+			if (pf->hw.phy.link_info.req_fec_info &
+			    I40E_AQ_REQUEST_FEC_RS)
+				req_fec = "CL108 RS-FEC";
+			else
+				req_fec = "CL74 FC-FEC/BASE-R";
+		}
+
+		netdev_info(vsi->netdev,
+			    "NIC Link is Up, %sbps Full Duplex, Requested FEC: %s, Negotiated FEC: %s, Autoneg: %s, Flow Control: %s\n",
+			    speed, req_fec, fec, an, fc);
+	} else {
+		struct ethtool_eee edata;
+
+		edata.supported = 0;
+		edata.eee_enabled = false;
+#ifdef HAVE_RHEL6_ETHTOOL_OPS_EXT_STRUCT
+		if (get_ethtool_ops_ext(vsi->netdev)->get_eee)
+			get_ethtool_ops_ext(vsi->netdev)
+				->get_eee(vsi->netdev, &edata);
+#else
+		if (vsi->netdev->ethtool_ops->get_eee)
+			vsi->netdev->ethtool_ops->get_eee(vsi->netdev, &edata);
+#endif /* HAVE_RHEL6_ETHTOOL_OPS_EXT_STRUCT */
+
+		if (edata.supported)
+			netdev_info(vsi->netdev,
+				    "NIC Link is Up, %sbps Full Duplex, Flow Control: %s, EEE: %s\n",
+				    speed, fc,
+				    edata.eee_enabled ? "Enabled" : "Disabled");
+		else
+			netdev_info(vsi->netdev,
+				    "NIC Link is Up, %sbps Full Duplex, Flow Control: %s\n",
+				    speed, fc);
+	}
+
+}
+
+/**
+ * i40e_up_complete - Finish the last steps of bringing up a connection
+ * @vsi: the VSI being configured
+ **/
+static int i40e_up_complete(struct i40e_vsi *vsi)
+{
+	struct i40e_pf *pf = vsi->back;
+	int err;
+
+	if (pf->flags & I40E_FLAG_MSIX_ENABLED)
+		i40e_vsi_configure_msix(vsi);
+	else
+		i40e_configure_msi_and_legacy(vsi);
+
+	/* start rings */
+	err = i40e_vsi_start_rings(vsi);
+	if (err)
+		return err;
+
+	clear_bit(__I40E_VSI_DOWN, vsi->state);
+	i40e_napi_enable_all(vsi);
+	i40e_vsi_enable_irq(vsi);
+
+	if ((pf->hw.phy.link_info.link_info & I40E_AQ_LINK_UP) &&
+	    (vsi->netdev)) {
+		i40e_print_link_message(vsi, true);
+		netif_tx_start_all_queues(vsi->netdev);
+		netif_carrier_on(vsi->netdev);
+	}
+
+	/* replay flow filters */
+	if (vsi->type == I40E_VSI_FDIR) {
+		/* reset fd counters */
+		pf->fd_add_err = pf->fd_atr_cnt = 0;
+		i40e_fdir_filter_restore(vsi);
+		i40e_cloud_filter_restore(pf);
+	}
+
+	/* On the next run of the service_task, notify any clients of the new
+	 * opened netdev
+	 */
+	set_bit(__I40E_CLIENT_SERVICE_REQUESTED, pf->state);
+	i40e_service_event_schedule(pf);
+
+	return 0;
+}
+
+/**
+ * i40e_vsi_reinit_locked - Reset the VSI
+ * @vsi: the VSI being configured
+ *
+ * Rebuild the ring structs after some configuration
+ * has changed, e.g. MTU size.
+ **/
+static void i40e_vsi_reinit_locked(struct i40e_vsi *vsi)
+{
+	struct i40e_pf *pf = vsi->back;
+
+	WARN_ON(in_interrupt());
+	while (test_and_set_bit(__I40E_CONFIG_BUSY, pf->state))
+		usleep_range(1000, 2000);
+	i40e_down(vsi);
+	i40e_up(vsi);
+	clear_bit(__I40E_CONFIG_BUSY, pf->state);
+}
+
+/**
+ * i40e_up - Bring the connection back up after being down
+ * @vsi: the VSI being configured
+ **/
+int i40e_up(struct i40e_vsi *vsi)
+{
+	int err;
+
+	if ((vsi->type == I40E_VSI_MAIN &&
+	     vsi->back->flags & I40E_FLAG_LINK_DOWN_ON_CLOSE_ENABLED) ||
+	    (vsi->back->flags & I40E_FLAG_TOTAL_PORT_SHUTDOWN))
+		i40e_force_link_state(vsi->back, true);
+
+	err = i40e_vsi_configure(vsi);
+	if (!err)
+		err = i40e_up_complete(vsi);
+
+	return err;
+}
+
+/**
+ * i40e_force_link_state - Force the link status
+ * @pf: board private structure
+ * @is_up: whether the link state should be forced up or down
+ **/
+static i40e_status i40e_force_link_state(struct i40e_pf *pf, bool is_up)
+{
+	struct i40e_aq_get_phy_abilities_resp abilities;
+	struct i40e_aq_set_phy_config config = {0};
+	struct i40e_hw *hw = &pf->hw;
+	bool non_zero_phy_type;
+	i40e_status err;
+	u64 mask;
+	u8 speed;
+
+	non_zero_phy_type = is_up;
+	/* Card might've been put in an unstable state by other drivers
+	 * and applications, which causes incorrect speed values being
+	 * set on startup. In order to clear speed registers, we call
+	 * get_phy_capabilities twice, once to get initial state of
+	 * available speeds, and once to get current phy config.
+	 */
+	err = i40e_aq_get_phy_capabilities(hw, false, true, &abilities,
+					   NULL);
+	if (err) {
+		dev_err(&pf->pdev->dev,
+			"failed to get phy cap., ret =  %s last_status =  %s\n",
+			i40e_stat_str(hw, err),
+			i40e_aq_str(hw, hw->aq.asq_last_status));
+		return err;
+	}
+	speed = abilities.link_speed;
+
+	/* Get the current phy config */
+	err = i40e_aq_get_phy_capabilities(hw, false, false, &abilities,
+					   NULL);
+	if (err) {
+		dev_err(&pf->pdev->dev,
+			"failed to get phy cap., ret =  %s last_status =  %s\n",
+			i40e_stat_str(hw, err),
+			i40e_aq_str(hw, hw->aq.asq_last_status));
+		return err;
+	}
+
+	/* If link needs to go up, but was not forced to go down,
+	 * and its speed values are OK, no need for a flap
+	 * if non_zero_phy_type was set, still need to force up
+	 */
+	if (pf->flags & I40E_FLAG_TOTAL_PORT_SHUTDOWN)
+		non_zero_phy_type = true;
+	else if (is_up && abilities.phy_type != 0 && abilities.link_speed != 0)
+		return I40E_SUCCESS;
+
+	/* To force link we need to set bits for all supported PHY types,
+	 * but there are now more than 32, so we need to split the bitmap
+	 * across two fields.
+	 */
+	mask = I40E_PHY_TYPES_BITMASK;
+	config.phy_type =
+		non_zero_phy_type ? cpu_to_le32((u32)(mask & 0xffffffff)) : 0;
+	config.phy_type_ext =
+		non_zero_phy_type ? (u8)((mask >> 32) & 0xff) : 0;
+	/* Copy the old settings, except of phy_type */
+	config.abilities = abilities.abilities;
+	if (pf->flags & I40E_FLAG_TOTAL_PORT_SHUTDOWN) {
+		if (is_up)
+			config.abilities |= I40E_AQ_PHY_ENABLE_LINK;
+		else
+			config.abilities &= ~(I40E_AQ_PHY_ENABLE_LINK);
+	}
+	if (abilities.link_speed != 0)
+		config.link_speed = abilities.link_speed;
+	else
+		config.link_speed = speed;
+	config.eee_capability = abilities.eee_capability;
+	config.eeer = abilities.eeer_val;
+	config.low_power_ctrl = abilities.d3_lpan;
+	config.fec_config = abilities.fec_cfg_curr_mod_ext_info &
+			    I40E_AQ_PHY_FEC_CONFIG_MASK;
+	err = i40e_aq_set_phy_config(hw, &config, NULL);
+
+	if (err) {
+		dev_err(&pf->pdev->dev,
+			"set phy config ret =  %s last_status =  %s\n",
+			i40e_stat_str(&pf->hw, err),
+			i40e_aq_str(&pf->hw, pf->hw.aq.asq_last_status));
+		return err;
+	}
+
+	/* Update the link info */
+	err = i40e_update_link_info(hw);
+	if (err) {
+		/* Wait a little bit (on 40G cards it sometimes takes a really
+		 * long time for link to come back from the atomic reset)
+		 * and try once more
+		 */
+		msleep(1000);
+		i40e_update_link_info(hw);
+	}
+
+	i40e_aq_set_link_restart_an(hw, is_up, NULL);
+
+	return I40E_SUCCESS;
+}
+
+/**
+ * i40e_down - Shutdown the connection processing
+ * @vsi: the VSI being stopped
+ **/
+void i40e_down(struct i40e_vsi *vsi)
+{
+	int i;
+
+	/* It is assumed that the caller of this function
+	 * sets the vsi->state __I40E_VSI_DOWN bit.
+	 */
+	if (vsi->netdev) {
+		netif_carrier_off(vsi->netdev);
+		netif_tx_disable(vsi->netdev);
+	}
+	i40e_vsi_disable_irq(vsi);
+	i40e_vsi_stop_rings(vsi);
+	if ((vsi->type == I40E_VSI_MAIN &&
+	     vsi->back->flags & I40E_FLAG_LINK_DOWN_ON_CLOSE_ENABLED) ||
+	    (vsi->back->flags & I40E_FLAG_TOTAL_PORT_SHUTDOWN))
+		i40e_force_link_state(vsi->back, false);
+	i40e_napi_disable_all(vsi);
+
+	for (i = 0; i < vsi->num_queue_pairs; i++) {
+		i40e_clean_tx_ring(vsi->tx_rings[i]);
+		if (i40e_enabled_xdp_vsi(vsi)) {
+			/* Make sure that in-progress ndo_xdp_xmit
+			 * calls are completed.
+			 */
+			synchronize_rcu();
+			i40e_clean_tx_ring(vsi->xdp_rings[i]);
+		}
+		i40e_clean_rx_ring(vsi->rx_rings[i]);
+	}
+
+}
+
+/**
  * i40e_get_link_speed - Returns link speed for the interface
  * @vsi: VSI to be configured
  *
  **/
-static int i40e_get_link_speed(struct i40e_vsi *vsi)
+int i40e_get_link_speed(struct i40e_vsi *vsi)
 {
 	struct i40e_pf *pf = vsi->back;
 
@@ -5532,7 +7681,6 @@
 int i40e_set_bw_limit(struct i40e_vsi *vsi, u16 seid, u64 max_tx_rate)
 {
 	struct i40e_pf *pf = vsi->back;
-	u64 credits = 0;
 	int speed = 0;
 	int ret = 0;
 
@@ -5550,9 +7698,8 @@
 	}
 
 	/* Tx rate credits are in values of 50Mbps, 0 is disabled */
-	credits = max_tx_rate;
-	do_div(credits, I40E_BW_CREDIT_DIVISOR);
-	ret = i40e_aq_config_vsi_bw_limit(&pf->hw, seid, credits,
+	ret = i40e_aq_config_vsi_bw_limit(&pf->hw, seid,
+					  max_tx_rate / I40E_BW_CREDIT_DIVISOR,
 					  I40E_MAX_BW_INACTIVE_ACCUM, NULL);
 	if (ret)
 		dev_err(&pf->pdev->dev,
@@ -5562,6 +7709,181 @@
 	return ret;
 }
 
+#ifdef __TC_MQPRIO_MODE_MAX
+/**
+ * i40e_validate_and_set_switch_mode - sets up switch mode correctly
+ * @vsi: ptr to VSI which has PF backing
+ *
+ * Sets up switch mode correctly if it needs to be changed and perform
+ * what are allowed modes.
+ **/
+static int i40e_validate_and_set_switch_mode(struct i40e_vsi *vsi)
+{
+	u8 mode;
+	struct i40e_pf *pf = vsi->back;
+	struct i40e_hw *hw = &pf->hw;
+	int ret;
+
+	ret = i40e_get_capabilities(pf, i40e_aqc_opc_list_dev_capabilities);
+	if (ret)
+		return -EINVAL;
+
+	if (hw->dev_caps.switch_mode) {
+		/* if switch mode is set, support mode2 (non-tunneled for
+		 * cloud filter) for now
+		 */
+		u32 switch_mode = hw->dev_caps.switch_mode &
+				  I40E_SWITCH_MODE_MASK;
+		if (switch_mode >= I40E_CLOUD_FILTER_MODE1) {
+			if (switch_mode == I40E_CLOUD_FILTER_MODE2)
+				return 0;
+			dev_err(&pf->pdev->dev,
+				"Invalid switch_mode (%d), only non-tunneled mode for cloud filter is supported\n",
+				hw->dev_caps.switch_mode);
+			return -EINVAL;
+		}
+	}
+
+	/* Set Bit 7 to be valid */
+	mode = I40E_AQ_SET_SWITCH_BIT7_VALID;
+
+	/* Set L4type for TCP support */
+	mode |= I40E_AQ_SET_SWITCH_L4_TYPE_TCP;
+
+	/* Set cloud filter mode */
+	mode |= I40E_AQ_SET_SWITCH_MODE_NON_TUNNEL;
+
+	/* Prep mode field for set_switch_config */
+	ret = i40e_aq_set_switch_config(hw, pf->last_sw_conf_flags,
+					pf->last_sw_conf_valid_flags,
+					mode, NULL);
+	if (ret && hw->aq.asq_last_status != I40E_AQ_RC_ESRCH)
+		dev_err(&pf->pdev->dev,
+			"couldn't set switch config bits, err %s aq_err %s\n",
+			i40e_stat_str(hw, ret),
+			i40e_aq_str(hw,
+				    hw->aq.asq_last_status));
+
+	return ret;
+}
+
+/**
+ * i40e_add_del_cloud_filter_big_buf - Add/del cloud filter using big_buf
+ * @vsi: pointer to VSI
+ * @filter: cloud filter rule
+ * @add: if true, add, if false, delete
+ *
+ * Add or delete a cloud filter for a specific flow spec using big buffer.
+ * Returns 0 if the filter were successfully added.
+ **/
+int i40e_add_del_cloud_filter_big_buf(struct i40e_vsi *vsi,
+				      struct i40e_cloud_filter *filter,
+				      bool add)
+{
+	struct i40e_aqc_cloud_filters_element_bb cld_filter;
+	struct i40e_pf *pf = vsi->back;
+	int ret;
+
+	if (i40e_is_l4mode_enabled()) {
+		dev_err(&pf->pdev->dev,
+			"Not expected to run in L4 cloud filter mode\n");
+		return -EINVAL;
+	}
+
+	/* Both (src/dst) valid mac_addr are not supported */
+	if ((is_valid_ether_addr(filter->dst_mac) &&
+	     is_valid_ether_addr(filter->src_mac)) ||
+	    (is_multicast_ether_addr(filter->dst_mac) &&
+	     is_multicast_ether_addr(filter->src_mac)))
+		return -EOPNOTSUPP;
+
+	/* Big buffer cloud filter needs 'L4 port' to be non-zero. Also, UDP
+	 * ports are not supported via big buffer now.
+	 */
+	if (!filter->dst_port || filter->ip_proto == IPPROTO_UDP)
+		return -EOPNOTSUPP;
+
+	/* adding filter using src_port/src_ip is not supported at this stage */
+	if (filter->src_port ||
+	    (filter->src_ipv4 && filter->n_proto != ETH_P_IPV6) ||
+	    !ipv6_addr_any(&filter->ip.v6.src_ip6))
+		return -EOPNOTSUPP;
+
+	memset(&cld_filter, 0, sizeof(cld_filter));
+
+	/* copy element needed to add cloud filter from filter */
+	i40e_set_cld_element(filter, &cld_filter.element);
+
+	if (is_valid_ether_addr(filter->dst_mac) ||
+	    is_valid_ether_addr(filter->src_mac) ||
+	    is_multicast_ether_addr(filter->dst_mac) ||
+	    is_multicast_ether_addr(filter->src_mac)) {
+		/* MAC + IP : unsupported mode */
+		if (filter->dst_ipv4)
+			return -EOPNOTSUPP;
+
+		/* since we validated that L4 port must be valid before
+		 * we get here, start with respective "flags" value
+		 * and update if vlan is present or not
+		 */
+		cld_filter.element.flags =
+			cpu_to_le16(I40E_AQC_ADD_CLOUD_FILTER_MAC_PORT);
+
+		if (filter->vlan_id) {
+			cld_filter.element.flags =
+			cpu_to_le16(I40E_AQC_ADD_CLOUD_FILTER_MAC_VLAN_PORT);
+		}
+
+	} else if ((filter->dst_ipv4 && filter->n_proto != ETH_P_IPV6) ||
+		   !ipv6_addr_any(&filter->ip.v6.dst_ip6)) {
+		cld_filter.element.flags =
+				cpu_to_le16(I40E_AQC_ADD_CLOUD_FILTER_IP_PORT);
+		if (filter->n_proto == ETH_P_IPV6)
+			cld_filter.element.flags |=
+				cpu_to_le16(I40E_AQC_ADD_CLOUD_FLAGS_IPV6);
+		else
+			cld_filter.element.flags |=
+				cpu_to_le16(I40E_AQC_ADD_CLOUD_FLAGS_IPV4);
+	} else {
+		dev_err(&pf->pdev->dev,
+			"either mac or ip has to be valid for cloud filter\n");
+		return -EINVAL;
+	}
+
+	/* Now copy L4 port in Byte 6..7 in general fields */
+	cld_filter.general_fields[I40E_AQC_ADD_CLOUD_FV_FLU_0X16_WORD0] =
+						be16_to_cpu(filter->dst_port);
+
+	if (add) {
+		/* Validate current device switch mode, change if necessary */
+		ret = i40e_validate_and_set_switch_mode(vsi);
+		if (ret) {
+			dev_err(&pf->pdev->dev,
+				"failed to set switch mode, ret %d\n",
+				ret);
+			return ret;
+		}
+
+		ret = i40e_aq_add_cloud_filters_bb(&pf->hw, filter->seid,
+						   &cld_filter, 1);
+	} else {
+		ret = i40e_aq_rem_cloud_filters_bb(&pf->hw, filter->seid,
+						   &cld_filter, 1);
+	}
+
+	if (ret)
+		dev_dbg(&pf->pdev->dev,
+			"Failed to %s cloud filter(big buffer) err %d aq_err %d\n",
+			add ? "add" : "delete", ret, pf->hw.aq.asq_last_status);
+	else
+		dev_info(&pf->pdev->dev,
+			 "%s cloud filter for VSI: %d, L4 port: %d\n",
+			 add ? "add" : "delete", filter->seid,
+			 ntohs(filter->dst_port));
+	return ret;
+}
+#endif /* __TC_MQPRIO_MODE_MAX */
+#ifdef __TC_MQPRIO_MODE_MAX
 /**
  * i40e_remove_queue_channels - Remove queue channels for the TCs
  * @vsi: VSI to be configured
@@ -5638,6 +7960,29 @@
 			kfree(cfilter);
 		}
 
+		/* delete cloud filters associated with this channel */
+		hlist_for_each_entry_safe(cfilter, node,
+					  &pf->cloud_filter_list, cloud_node) {
+			if (cfilter->seid != ch->seid)
+				continue;
+
+			hash_del(&cfilter->cloud_node);
+			if (cfilter->dst_port)
+				ret = i40e_add_del_cloud_filter_big_buf(vsi,
+									cfilter,
+									false);
+			else
+				ret = i40e_add_del_cloud_filter(vsi, cfilter,
+								false);
+			last_aq_status = pf->hw.aq.asq_last_status;
+			if (ret)
+				dev_info(&pf->pdev->dev,
+					 "Failed to delete cloud filter, err %s aq_err %s\n",
+					 i40e_stat_str(&pf->hw, ret),
+					 i40e_aq_str(&pf->hw, last_aq_status));
+			kfree(cfilter);
+		}
+
 		/* delete VSI from FW */
 		ret = i40e_aq_delete_element(&vsi->back->hw, ch->seid,
 					     NULL);
@@ -5651,24 +7996,6 @@
 }
 
 /**
- * i40e_is_any_channel - channel exist or not
- * @vsi: ptr to VSI to which channels are associated with
- *
- * Returns true or false if channel(s) exist for associated VSI or not
- **/
-static bool i40e_is_any_channel(struct i40e_vsi *vsi)
-{
-	struct i40e_channel *ch, *ch_tmp;
-
-	list_for_each_entry_safe(ch, ch_tmp, &vsi->ch_list, list) {
-		if (ch->initialized)
-			return true;
-	}
-
-	return false;
-}
-
-/**
  * i40e_get_max_queues_for_channel
  * @vsi: ptr to VSI to which channels are associated with
  *
@@ -5804,29 +8131,31 @@
 /**
  * i40e_channel_setup_queue_map - Setup a channel queue map
  * @pf: ptr to PF device
- * @vsi: the VSI being setup
  * @ctxt: VSI context structure
  * @ch: ptr to channel structure
  *
  * Setup queue map for a specific channel
  **/
-static void i40e_channel_setup_queue_map(struct i40e_pf *pf,
-					 struct i40e_vsi_context *ctxt,
-					 struct i40e_channel *ch)
+static int i40e_channel_setup_queue_map(struct i40e_pf *pf,
+					struct i40e_vsi_context *ctxt,
+					struct i40e_channel *ch)
 {
-	u16 qcount, qmap, sections = 0;
+	u16 qmap, sections = 0;
 	u8 offset = 0;
 	int pow;
 
+	if (ch->num_queue_pairs > pf->num_lan_msix) {
+		dev_err(&pf->pdev->dev,
+			"Requested queues number exceeded max available MSI-X vectors. Refused to set queue map\n");
+		return -EINVAL;
+	}
+
 	sections = I40E_AQ_VSI_PROP_QUEUE_MAP_VALID;
 	sections |= I40E_AQ_VSI_PROP_SCHED_VALID;
 
-	qcount = min_t(int, ch->num_queue_pairs, pf->num_lan_msix);
-	ch->num_queue_pairs = qcount;
-
 	/* find the next higher power-of-2 of num queue pairs */
-	pow = ilog2(qcount);
-	if (!is_power_of_2(qcount))
+	pow = ilog2(ch->num_queue_pairs);
+	if (!is_power_of_2(ch->num_queue_pairs))
 		pow++;
 
 	qmap = (offset << I40E_AQ_VSI_TC_QUE_OFFSET_SHIFT) |
@@ -5839,6 +8168,8 @@
 	ctxt->info.mapping_flags |= cpu_to_le16(I40E_AQ_VSI_QUE_MAP_CONTIG);
 	ctxt->info.queue_mapping[0] = cpu_to_le16(ch->base_queue);
 	ctxt->info.valid_sections |= cpu_to_le16(sections);
+
+	return 0;
 }
 
 /**
@@ -5879,7 +8210,9 @@
 	}
 
 	/* Set queue map for a given VSI context */
-	i40e_channel_setup_queue_map(pf, &ctxt, ch);
+	ret = i40e_channel_setup_queue_map(pf, &ctxt, ch);
+	if (ret)
+		return ret;
 
 	/* Now time to create VSI */
 	ret = i40e_aq_add_vsi(hw, &ctxt, NULL);
@@ -5892,13 +8225,11 @@
 		return -ENOENT;
 	}
 
-	/* Success, update channel, set enabled_tc only if the channel
-	 * is not a macvlan
-	 */
-	ch->enabled_tc = !i40e_is_channel_macvlan(ch) && enabled_tc;
+	/* Success, update channel */
+	ch->enabled_tc = enabled_tc;
 	ch->seid = ctxt.seid;
 	ch->vsi_number = ctxt.vsi_number;
-	ch->stat_counter_idx = cpu_to_le16(ctxt.info.stat_counter_idx);
+	ch->stat_counter_idx = le16_to_cpu(ctxt.info.stat_counter_idx);
 
 	/* copy just the sections touched not the entire info
 	 * since not all sections are valid as returned by
@@ -5920,6 +8251,7 @@
 	i40e_status ret;
 	int i;
 
+	memset(&bw_data, 0, sizeof(bw_data));
 	bw_data.tc_valid_bits = ch->enabled_tc;
 	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++)
 		bw_data.tc_bw_credits[i] = bw_share[i];
@@ -6047,8 +8379,7 @@
 /**
  * i40e_setup_channel - setup new channel using uplink element
  * @pf: ptr to PF device
- * @type: type of channel to be created (VMDq2/VF)
- * @uplink_seid: underlying HW switching element (VEB) ID
+ * @vsi: the VSI being setup
  * @ch: ptr to channel structure
  *
  * Setup new channel (VSI) based on specified type (VMDq2/VF)
@@ -6083,63 +8414,6 @@
 }
 
 /**
- * i40e_validate_and_set_switch_mode - sets up switch mode correctly
- * @vsi: ptr to VSI which has PF backing
- *
- * Sets up switch mode correctly if it needs to be changed and perform
- * what are allowed modes.
- **/
-static int i40e_validate_and_set_switch_mode(struct i40e_vsi *vsi)
-{
-	u8 mode;
-	struct i40e_pf *pf = vsi->back;
-	struct i40e_hw *hw = &pf->hw;
-	int ret;
-
-	ret = i40e_get_capabilities(pf, i40e_aqc_opc_list_dev_capabilities);
-	if (ret)
-		return -EINVAL;
-
-	if (hw->dev_caps.switch_mode) {
-		/* if switch mode is set, support mode2 (non-tunneled for
-		 * cloud filter) for now
-		 */
-		u32 switch_mode = hw->dev_caps.switch_mode &
-				  I40E_SWITCH_MODE_MASK;
-		if (switch_mode >= I40E_CLOUD_FILTER_MODE1) {
-			if (switch_mode == I40E_CLOUD_FILTER_MODE2)
-				return 0;
-			dev_err(&pf->pdev->dev,
-				"Invalid switch_mode (%d), only non-tunneled mode for cloud filter is supported\n",
-				hw->dev_caps.switch_mode);
-			return -EINVAL;
-		}
-	}
-
-	/* Set Bit 7 to be valid */
-	mode = I40E_AQ_SET_SWITCH_BIT7_VALID;
-
-	/* Set L4type for TCP support */
-	mode |= I40E_AQ_SET_SWITCH_L4_TYPE_TCP;
-
-	/* Set cloud filter mode */
-	mode |= I40E_AQ_SET_SWITCH_MODE_NON_TUNNEL;
-
-	/* Prep mode field for set_switch_config */
-	ret = i40e_aq_set_switch_config(hw, pf->last_sw_conf_flags,
-					pf->last_sw_conf_valid_flags,
-					mode, NULL);
-	if (ret && hw->aq.asq_last_status != I40E_AQ_RC_ESRCH)
-		dev_err(&pf->pdev->dev,
-			"couldn't set switch config bits, err %s aq_err %s\n",
-			i40e_stat_str(hw, ret),
-			i40e_aq_str(hw,
-				    hw->aq.asq_last_status));
-
-	return ret;
-}
-
-/**
  * i40e_create_queue_channel - function to create channel
  * @vsi: VSI to be configured
  * @ch: ptr to channel (it contains channel specific params)
@@ -6175,30 +8449,18 @@
 	/* By default we are in VEPA mode, if this is the first VF/VMDq
 	 * VSI to be added switch to VEB mode.
 	 */
-	if ((!(pf->flags & I40E_FLAG_VEB_MODE_ENABLED)) ||
-	    (!i40e_is_any_channel(vsi))) {
-		if (!is_power_of_2(vsi->tc_config.tc_info[0].qcount)) {
-			dev_dbg(&pf->pdev->dev,
-				"Failed to create channel. Override queues (%u) not power of 2\n",
-				vsi->tc_config.tc_info[0].qcount);
-			return -EINVAL;
-		}
-
-		if (!(pf->flags & I40E_FLAG_VEB_MODE_ENABLED)) {
-			pf->flags |= I40E_FLAG_VEB_MODE_ENABLED;
+	if (!(pf->flags & I40E_FLAG_VEB_MODE_ENABLED)) {
+		pf->flags |= I40E_FLAG_VEB_MODE_ENABLED;
 
-			if (vsi->type == I40E_VSI_MAIN) {
-				if (pf->flags & I40E_FLAG_TC_MQPRIO)
-					i40e_do_reset(pf, I40E_PF_RESET_FLAG,
-						      true);
-				else
-					i40e_do_reset_safe(pf,
-							   I40E_PF_RESET_FLAG);
-			}
+		if (vsi->type == I40E_VSI_MAIN) {
+			if (pf->flags & I40E_FLAG_TC_MQPRIO)
+				i40e_do_reset(pf, I40E_PF_RESET_FLAG, true);
+			else
+				i40e_do_reset_safe(pf, I40E_PF_RESET_FLAG);
 		}
-		/* now onwards for main VSI, number of queues will be value
-		 * of TC0's queue count
-		 */
+	/* now onwards for main VSI, number of queues will be value
+	 * of TC0's queue count
+	 */
 	}
 
 	/* By this time, vsi->cnt_q_avail shall be set to non-zero and
@@ -6212,7 +8474,7 @@
 	}
 
 	/* reconfig_rss only if vsi type is MAIN_VSI */
-	if (reconfig_rss && (vsi->type == I40E_VSI_MAIN)) {
+	if (reconfig_rss && vsi->type == I40E_VSI_MAIN) {
 		err = i40e_vsi_reconfig_rss(vsi, ch->num_queue_pairs);
 		if (err) {
 			dev_info(&pf->pdev->dev,
@@ -6233,17 +8495,13 @@
 
 	/* configure VSI for BW limit */
 	if (ch->max_tx_rate) {
-		u64 credits = ch->max_tx_rate;
-
 		if (i40e_set_bw_limit(vsi, ch->seid, ch->max_tx_rate))
 			return -EINVAL;
 
-		do_div(credits, I40E_BW_CREDIT_DIVISOR);
 		dev_dbg(&pf->pdev->dev,
 			"Set tx rate of %llu Mbps (count of 50Mbps %llu) for vsi->seid %u\n",
 			ch->max_tx_rate,
-			credits,
-			ch->seid);
+			ch->max_tx_rate / I40E_BW_CREDIT_DIVISOR, ch->seid);
 	}
 
 	/* in case of VF, this will be main SRIOV VSI */
@@ -6264,7 +8522,6 @@
 static int i40e_configure_queue_channels(struct i40e_vsi *vsi)
 {
 	struct i40e_channel *ch;
-	u64 max_rate = 0;
 	int ret = 0, i;
 
 	/* Create app vsi with the TCs. Main VSI with TC0 is already set up */
@@ -6286,9 +8543,8 @@
 			/* Bandwidth limit through tc interface is in bytes/s,
 			 * change to Mbit/s
 			 */
-			max_rate = vsi->mqprio_qopt.max_rate[i];
-			do_div(max_rate, I40E_BW_MBPS_DIVISOR);
-			ch->max_tx_rate = max_rate;
+			ch->max_tx_rate =
+				vsi->mqprio_qopt.max_rate[i] / (1000000 / 8);
 
 			list_add_tail(&ch->list, &vsi->ch_list);
 
@@ -6310,529 +8566,6 @@
 }
 
 /**
- * i40e_veb_config_tc - Configure TCs for given VEB
- * @veb: given VEB
- * @enabled_tc: TC bitmap
- *
- * Configures given TC bitmap for VEB (switching) element
- **/
-int i40e_veb_config_tc(struct i40e_veb *veb, u8 enabled_tc)
-{
-	struct i40e_aqc_configure_switching_comp_bw_config_data bw_data = {0};
-	struct i40e_pf *pf = veb->pf;
-	int ret = 0;
-	int i;
-
-	/* No TCs or already enabled TCs just return */
-	if (!enabled_tc || veb->enabled_tc == enabled_tc)
-		return ret;
-
-	bw_data.tc_valid_bits = enabled_tc;
-	/* bw_data.absolute_credits is not set (relative) */
-
-	/* Enable ETS TCs with equal BW Share for now */
-	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
-		if (enabled_tc & BIT(i))
-			bw_data.tc_bw_share_credits[i] = 1;
-	}
-
-	ret = i40e_aq_config_switch_comp_bw_config(&pf->hw, veb->seid,
-						   &bw_data, NULL);
-	if (ret) {
-		dev_info(&pf->pdev->dev,
-			 "VEB bw config failed, err %s aq_err %s\n",
-			 i40e_stat_str(&pf->hw, ret),
-			 i40e_aq_str(&pf->hw, pf->hw.aq.asq_last_status));
-		goto out;
-	}
-
-	/* Update the BW information */
-	ret = i40e_veb_get_bw_info(veb);
-	if (ret) {
-		dev_info(&pf->pdev->dev,
-			 "Failed getting veb bw config, err %s aq_err %s\n",
-			 i40e_stat_str(&pf->hw, ret),
-			 i40e_aq_str(&pf->hw, pf->hw.aq.asq_last_status));
-	}
-
-out:
-	return ret;
-}
-
-#ifdef CONFIG_I40E_DCB
-/**
- * i40e_dcb_reconfigure - Reconfigure all VEBs and VSIs
- * @pf: PF struct
- *
- * Reconfigure VEB/VSIs on a given PF; it is assumed that
- * the caller would've quiesce all the VSIs before calling
- * this function
- **/
-static void i40e_dcb_reconfigure(struct i40e_pf *pf)
-{
-	u8 tc_map = 0;
-	int ret;
-	u8 v;
-
-	/* Enable the TCs available on PF to all VEBs */
-	tc_map = i40e_pf_get_tc_map(pf);
-	for (v = 0; v < I40E_MAX_VEB; v++) {
-		if (!pf->veb[v])
-			continue;
-		ret = i40e_veb_config_tc(pf->veb[v], tc_map);
-		if (ret) {
-			dev_info(&pf->pdev->dev,
-				 "Failed configuring TC for VEB seid=%d\n",
-				 pf->veb[v]->seid);
-			/* Will try to configure as many components */
-		}
-	}
-
-	/* Update each VSI */
-	for (v = 0; v < pf->num_alloc_vsi; v++) {
-		if (!pf->vsi[v])
-			continue;
-
-		/* - Enable all TCs for the LAN VSI
-		 * - For all others keep them at TC0 for now
-		 */
-		if (v == pf->lan_vsi)
-			tc_map = i40e_pf_get_tc_map(pf);
-		else
-			tc_map = I40E_DEFAULT_TRAFFIC_CLASS;
-
-		ret = i40e_vsi_config_tc(pf->vsi[v], tc_map);
-		if (ret) {
-			dev_info(&pf->pdev->dev,
-				 "Failed configuring TC for VSI seid=%d\n",
-				 pf->vsi[v]->seid);
-			/* Will try to configure as many components */
-		} else {
-			/* Re-configure VSI vectors based on updated TC map */
-			i40e_vsi_map_rings_to_vectors(pf->vsi[v]);
-			if (pf->vsi[v]->netdev)
-				i40e_dcbnl_set_all(pf->vsi[v]);
-		}
-	}
-}
-
-/**
- * i40e_resume_port_tx - Resume port Tx
- * @pf: PF struct
- *
- * Resume a port's Tx and issue a PF reset in case of failure to
- * resume.
- **/
-static int i40e_resume_port_tx(struct i40e_pf *pf)
-{
-	struct i40e_hw *hw = &pf->hw;
-	int ret;
-
-	ret = i40e_aq_resume_port_tx(hw, NULL);
-	if (ret) {
-		dev_info(&pf->pdev->dev,
-			 "Resume Port Tx failed, err %s aq_err %s\n",
-			  i40e_stat_str(&pf->hw, ret),
-			  i40e_aq_str(&pf->hw, pf->hw.aq.asq_last_status));
-		/* Schedule PF reset to recover */
-		set_bit(__I40E_PF_RESET_REQUESTED, pf->state);
-		i40e_service_event_schedule(pf);
-	}
-
-	return ret;
-}
-
-/**
- * i40e_init_pf_dcb - Initialize DCB configuration
- * @pf: PF being configured
- *
- * Query the current DCB configuration and cache it
- * in the hardware structure
- **/
-static int i40e_init_pf_dcb(struct i40e_pf *pf)
-{
-	struct i40e_hw *hw = &pf->hw;
-	int err = 0;
-
-	/* Do not enable DCB for SW1 and SW2 images even if the FW is capable
-	 * Also do not enable DCBx if FW LLDP agent is disabled
-	 */
-	if ((pf->hw_features & I40E_HW_NO_DCB_SUPPORT) ||
-	    (pf->flags & I40E_FLAG_DISABLE_FW_LLDP)) {
-		dev_info(&pf->pdev->dev, "DCB is not supported or FW LLDP is disabled\n");
-		err = I40E_NOT_SUPPORTED;
-		goto out;
-	}
-
-	err = i40e_init_dcb(hw, true);
-	if (!err) {
-		/* Device/Function is not DCBX capable */
-		if ((!hw->func_caps.dcb) ||
-		    (hw->dcbx_status == I40E_DCBX_STATUS_DISABLED)) {
-			dev_info(&pf->pdev->dev,
-				 "DCBX offload is not supported or is disabled for this PF.\n");
-		} else {
-			/* When status is not DISABLED then DCBX in FW */
-			pf->dcbx_cap = DCB_CAP_DCBX_LLD_MANAGED |
-				       DCB_CAP_DCBX_VER_IEEE;
-
-			pf->flags |= I40E_FLAG_DCB_CAPABLE;
-			/* Enable DCB tagging only when more than one TC
-			 * or explicitly disable if only one TC
-			 */
-			if (i40e_dcb_get_num_tc(&hw->local_dcbx_config) > 1)
-				pf->flags |= I40E_FLAG_DCB_ENABLED;
-			else
-				pf->flags &= ~I40E_FLAG_DCB_ENABLED;
-			dev_dbg(&pf->pdev->dev,
-				"DCBX offload is supported for this PF.\n");
-		}
-	} else if (pf->hw.aq.asq_last_status == I40E_AQ_RC_EPERM) {
-		dev_info(&pf->pdev->dev, "FW LLDP disabled for this PF.\n");
-		pf->flags |= I40E_FLAG_DISABLE_FW_LLDP;
-	} else {
-		dev_info(&pf->pdev->dev,
-			 "Query for DCB configuration failed, err %s aq_err %s\n",
-			 i40e_stat_str(&pf->hw, err),
-			 i40e_aq_str(&pf->hw, pf->hw.aq.asq_last_status));
-	}
-
-out:
-	return err;
-}
-#endif /* CONFIG_I40E_DCB */
-#define SPEED_SIZE 14
-#define FC_SIZE 8
-/**
- * i40e_print_link_message - print link up or down
- * @vsi: the VSI for which link needs a message
- * @isup: true of link is up, false otherwise
- */
-void i40e_print_link_message(struct i40e_vsi *vsi, bool isup)
-{
-	enum i40e_aq_link_speed new_speed;
-	struct i40e_pf *pf = vsi->back;
-	char *speed = "Unknown";
-	char *fc = "Unknown";
-	char *fec = "";
-	char *req_fec = "";
-	char *an = "";
-
-	if (isup)
-		new_speed = pf->hw.phy.link_info.link_speed;
-	else
-		new_speed = I40E_LINK_SPEED_UNKNOWN;
-
-	if ((vsi->current_isup == isup) && (vsi->current_speed == new_speed))
-		return;
-	vsi->current_isup = isup;
-	vsi->current_speed = new_speed;
-	if (!isup) {
-		netdev_info(vsi->netdev, "NIC Link is Down\n");
-		return;
-	}
-
-	/* Warn user if link speed on NPAR enabled partition is not at
-	 * least 10GB
-	 */
-	if (pf->hw.func_caps.npar_enable &&
-	    (pf->hw.phy.link_info.link_speed == I40E_LINK_SPEED_1GB ||
-	     pf->hw.phy.link_info.link_speed == I40E_LINK_SPEED_100MB))
-		netdev_warn(vsi->netdev,
-			    "The partition detected link speed that is less than 10Gbps\n");
-
-	switch (pf->hw.phy.link_info.link_speed) {
-	case I40E_LINK_SPEED_40GB:
-		speed = "40 G";
-		break;
-	case I40E_LINK_SPEED_20GB:
-		speed = "20 G";
-		break;
-	case I40E_LINK_SPEED_25GB:
-		speed = "25 G";
-		break;
-	case I40E_LINK_SPEED_10GB:
-		speed = "10 G";
-		break;
-	case I40E_LINK_SPEED_5GB:
-		speed = "5 G";
-		break;
-	case I40E_LINK_SPEED_2_5GB:
-		speed = "2.5 G";
-		break;
-	case I40E_LINK_SPEED_1GB:
-		speed = "1000 M";
-		break;
-	case I40E_LINK_SPEED_100MB:
-		speed = "100 M";
-		break;
-	default:
-		break;
-	}
-
-	switch (pf->hw.fc.current_mode) {
-	case I40E_FC_FULL:
-		fc = "RX/TX";
-		break;
-	case I40E_FC_TX_PAUSE:
-		fc = "TX";
-		break;
-	case I40E_FC_RX_PAUSE:
-		fc = "RX";
-		break;
-	default:
-		fc = "None";
-		break;
-	}
-
-	if (pf->hw.phy.link_info.link_speed == I40E_LINK_SPEED_25GB) {
-		req_fec = "None";
-		fec = "None";
-		an = "False";
-
-		if (pf->hw.phy.link_info.an_info & I40E_AQ_AN_COMPLETED)
-			an = "True";
-
-		if (pf->hw.phy.link_info.fec_info &
-		    I40E_AQ_CONFIG_FEC_KR_ENA)
-			fec = "CL74 FC-FEC/BASE-R";
-		else if (pf->hw.phy.link_info.fec_info &
-			 I40E_AQ_CONFIG_FEC_RS_ENA)
-			fec = "CL108 RS-FEC";
-
-		/* 'CL108 RS-FEC' should be displayed when RS is requested, or
-		 * both RS and FC are requested
-		 */
-		if (vsi->back->hw.phy.link_info.req_fec_info &
-		    (I40E_AQ_REQUEST_FEC_KR | I40E_AQ_REQUEST_FEC_RS)) {
-			if (vsi->back->hw.phy.link_info.req_fec_info &
-			    I40E_AQ_REQUEST_FEC_RS)
-				req_fec = "CL108 RS-FEC";
-			else
-				req_fec = "CL74 FC-FEC/BASE-R";
-		}
-		netdev_info(vsi->netdev,
-			    "NIC Link is Up, %sbps Full Duplex, Requested FEC: %s, Negotiated FEC: %s, Autoneg: %s, Flow Control: %s\n",
-			    speed, req_fec, fec, an, fc);
-	} else {
-		netdev_info(vsi->netdev,
-			    "NIC Link is Up, %sbps Full Duplex, Flow Control: %s\n",
-			    speed, fc);
-	}
-
-}
-
-/**
- * i40e_up_complete - Finish the last steps of bringing up a connection
- * @vsi: the VSI being configured
- **/
-static int i40e_up_complete(struct i40e_vsi *vsi)
-{
-	struct i40e_pf *pf = vsi->back;
-	int err;
-
-	if (pf->flags & I40E_FLAG_MSIX_ENABLED)
-		i40e_vsi_configure_msix(vsi);
-	else
-		i40e_configure_msi_and_legacy(vsi);
-
-	/* start rings */
-	err = i40e_vsi_start_rings(vsi);
-	if (err)
-		return err;
-
-	clear_bit(__I40E_VSI_DOWN, vsi->state);
-	i40e_napi_enable_all(vsi);
-	i40e_vsi_enable_irq(vsi);
-
-	if ((pf->hw.phy.link_info.link_info & I40E_AQ_LINK_UP) &&
-	    (vsi->netdev)) {
-		i40e_print_link_message(vsi, true);
-		netif_tx_start_all_queues(vsi->netdev);
-		netif_carrier_on(vsi->netdev);
-	}
-
-	/* replay FDIR SB filters */
-	if (vsi->type == I40E_VSI_FDIR) {
-		/* reset fd counters */
-		pf->fd_add_err = 0;
-		pf->fd_atr_cnt = 0;
-		i40e_fdir_filter_restore(vsi);
-	}
-
-	/* On the next run of the service_task, notify any clients of the new
-	 * opened netdev
-	 */
-	set_bit(__I40E_CLIENT_SERVICE_REQUESTED, pf->state);
-	i40e_service_event_schedule(pf);
-
-	return 0;
-}
-
-/**
- * i40e_vsi_reinit_locked - Reset the VSI
- * @vsi: the VSI being configured
- *
- * Rebuild the ring structs after some configuration
- * has changed, e.g. MTU size.
- **/
-static void i40e_vsi_reinit_locked(struct i40e_vsi *vsi)
-{
-	struct i40e_pf *pf = vsi->back;
-
-	WARN_ON(in_interrupt());
-	while (test_and_set_bit(__I40E_CONFIG_BUSY, pf->state))
-		usleep_range(1000, 2000);
-	i40e_down(vsi);
-
-	i40e_up(vsi);
-	clear_bit(__I40E_CONFIG_BUSY, pf->state);
-}
-
-/**
- * i40e_up - Bring the connection back up after being down
- * @vsi: the VSI being configured
- **/
-int i40e_up(struct i40e_vsi *vsi)
-{
-	int err;
-
-	err = i40e_vsi_configure(vsi);
-	if (!err)
-		err = i40e_up_complete(vsi);
-
-	return err;
-}
-
-/**
- * i40e_force_link_state - Force the link status
- * @pf: board private structure
- * @is_up: whether the link state should be forced up or down
- **/
-static i40e_status i40e_force_link_state(struct i40e_pf *pf, bool is_up)
-{
-	struct i40e_aq_get_phy_abilities_resp abilities;
-	struct i40e_aq_set_phy_config config = {0};
-	struct i40e_hw *hw = &pf->hw;
-	i40e_status err;
-	u64 mask;
-	u8 speed;
-
-	/* Card might've been put in an unstable state by other drivers
-	 * and applications, which causes incorrect speed values being
-	 * set on startup. In order to clear speed registers, we call
-	 * get_phy_capabilities twice, once to get initial state of
-	 * available speeds, and once to get current PHY config.
-	 */
-	err = i40e_aq_get_phy_capabilities(hw, false, true, &abilities,
-					   NULL);
-	if (err) {
-		dev_err(&pf->pdev->dev,
-			"failed to get phy cap., ret =  %s last_status =  %s\n",
-			i40e_stat_str(hw, err),
-			i40e_aq_str(hw, hw->aq.asq_last_status));
-		return err;
-	}
-	speed = abilities.link_speed;
-
-	/* Get the current phy config */
-	err = i40e_aq_get_phy_capabilities(hw, false, false, &abilities,
-					   NULL);
-	if (err) {
-		dev_err(&pf->pdev->dev,
-			"failed to get phy cap., ret =  %s last_status =  %s\n",
-			i40e_stat_str(hw, err),
-			i40e_aq_str(hw, hw->aq.asq_last_status));
-		return err;
-	}
-
-	/* If link needs to go up, but was not forced to go down,
-	 * and its speed values are OK, no need for a flap
-	 */
-	if (is_up && abilities.phy_type != 0 && abilities.link_speed != 0)
-		return I40E_SUCCESS;
-
-	/* To force link we need to set bits for all supported PHY types,
-	 * but there are now more than 32, so we need to split the bitmap
-	 * across two fields.
-	 */
-	mask = I40E_PHY_TYPES_BITMASK;
-	config.phy_type = is_up ? cpu_to_le32((u32)(mask & 0xffffffff)) : 0;
-	config.phy_type_ext = is_up ? (u8)((mask >> 32) & 0xff) : 0;
-	/* Copy the old settings, except of phy_type */
-	config.abilities = abilities.abilities;
-	if (abilities.link_speed != 0)
-		config.link_speed = abilities.link_speed;
-	else
-		config.link_speed = speed;
-	config.eee_capability = abilities.eee_capability;
-	config.eeer = abilities.eeer_val;
-	config.low_power_ctrl = abilities.d3_lpan;
-	config.fec_config = abilities.fec_cfg_curr_mod_ext_info &
-			    I40E_AQ_PHY_FEC_CONFIG_MASK;
-	err = i40e_aq_set_phy_config(hw, &config, NULL);
-
-	if (err) {
-		dev_err(&pf->pdev->dev,
-			"set phy config ret =  %s last_status =  %s\n",
-			i40e_stat_str(&pf->hw, err),
-			i40e_aq_str(&pf->hw, pf->hw.aq.asq_last_status));
-		return err;
-	}
-
-	/* Update the link info */
-	err = i40e_update_link_info(hw);
-	if (err) {
-		/* Wait a little bit (on 40G cards it sometimes takes a really
-		 * long time for link to come back from the atomic reset)
-		 * and try once more
-		 */
-		msleep(1000);
-		i40e_update_link_info(hw);
-	}
-
-	i40e_aq_set_link_restart_an(hw, true, NULL);
-
-	return I40E_SUCCESS;
-}
-
-/**
- * i40e_down - Shutdown the connection processing
- * @vsi: the VSI being stopped
- **/
-void i40e_down(struct i40e_vsi *vsi)
-{
-	int i;
-
-	/* It is assumed that the caller of this function
-	 * sets the vsi->state __I40E_VSI_DOWN bit.
-	 */
-	if (vsi->netdev) {
-		netif_carrier_off(vsi->netdev);
-		netif_tx_disable(vsi->netdev);
-	}
-	i40e_vsi_disable_irq(vsi);
-	i40e_vsi_stop_rings(vsi);
-	if (vsi->type == I40E_VSI_MAIN &&
-	    vsi->back->flags & I40E_FLAG_LINK_DOWN_ON_CLOSE_ENABLED)
-		i40e_force_link_state(vsi->back, false);
-	i40e_napi_disable_all(vsi);
-
-	for (i = 0; i < vsi->num_queue_pairs; i++) {
-		i40e_clean_tx_ring(vsi->tx_rings[i]);
-		if (i40e_enabled_xdp_vsi(vsi)) {
-			/* Make sure that in-progress ndo_xdp_xmit and
-			 * ndo_xsk_wakeup calls are completed.
-			 */
-			synchronize_rcu();
-			i40e_clean_tx_ring(vsi->xdp_rings[i]);
-		}
-		i40e_clean_rx_ring(vsi->rx_rings[i]);
-	}
-
-}
-
-/**
  * i40e_validate_mqprio_qopt- validate queue mapping info
  * @vsi: the VSI being configured
  * @mqprio_qopt: queue parametrs
@@ -6841,7 +8574,6 @@
 				     struct tc_mqprio_qopt_offload *mqprio_qopt)
 {
 	u64 sum_max_rate = 0;
-	u64 max_rate = 0;
 	int i;
 
 	if (mqprio_qopt->qopt.offset[0] != 0 ||
@@ -6856,9 +8588,7 @@
 				"Invalid min tx rate (greater than 0) specified\n");
 			return -EINVAL;
 		}
-		max_rate = mqprio_qopt->max_rate[i];
-		do_div(max_rate, I40E_BW_MBPS_DIVISOR);
-		sum_max_rate += max_rate;
+		sum_max_rate += (mqprio_qopt->max_rate[i] / (1000000 / 8));
 
 		if (i >= mqprio_qopt->qopt.num_tc - 1)
 			break;
@@ -6868,6 +8598,8 @@
 	}
 	if (vsi->num_queue_pairs <
 	    (mqprio_qopt->qopt.offset[i] + mqprio_qopt->qopt.count[i])) {
+		dev_err(&vsi->back->pdev->dev,
+			"Failed to create traffic channel, insufficient number of queues.\n");
 		return -EINVAL;
 	}
 	if (sum_max_rate > i40e_get_link_speed(vsi)) {
@@ -6902,512 +8634,146 @@
 		else
 			vsi->tc_config.tc_info[i].qcount = 1;
 		vsi->tc_config.tc_info[i].netdev_tc = 0;
+		vsi->tc_config.tc_info[i].tc_bw_credits = 0;
 	}
 }
 
 /**
- * i40e_del_macvlan_filter
- * @hw: pointer to the HW structure
- * @seid: seid of the channel VSI
- * @macaddr: the mac address to apply as a filter
- * @aq_err: store the admin Q error
- *
- * This function deletes a mac filter on the channel VSI which serves as the
- * macvlan. Returns 0 on success.
- **/
-static i40e_status i40e_del_macvlan_filter(struct i40e_hw *hw, u16 seid,
-					   const u8 *macaddr, int *aq_err)
-{
-	struct i40e_aqc_remove_macvlan_element_data element;
-	i40e_status status;
-
-	memset(&element, 0, sizeof(element));
-	ether_addr_copy(element.mac_addr, macaddr);
-	element.vlan_tag = 0;
-	element.flags = I40E_AQC_MACVLAN_DEL_PERFECT_MATCH;
-	status = i40e_aq_remove_macvlan(hw, seid, &element, 1, NULL);
-	*aq_err = hw->aq.asq_last_status;
-
-	return status;
-}
-
-/**
- * i40e_add_macvlan_filter
- * @hw: pointer to the HW structure
- * @seid: seid of the channel VSI
- * @macaddr: the mac address to apply as a filter
- * @aq_err: store the admin Q error
+ * i40e_rebuild_cloud_filters - Rebuilds cloud filters for VSIs
+ * @vsi: PF main vsi
+ * @seid: seid of main or channel VSIs
  *
- * This function adds a mac filter on the channel VSI which serves as the
- * macvlan. Returns 0 on success.
+ * Rebuilds cloud filters associated with main VSI and channel VSIs if they
+ * existed before reset
  **/
-static i40e_status i40e_add_macvlan_filter(struct i40e_hw *hw, u16 seid,
-					   const u8 *macaddr, int *aq_err)
-{
-	struct i40e_aqc_add_macvlan_element_data element;
-	i40e_status status;
-	u16 cmd_flags = 0;
-
-	ether_addr_copy(element.mac_addr, macaddr);
-	element.vlan_tag = 0;
-	element.queue_number = 0;
-	element.match_method = I40E_AQC_MM_ERR_NO_RES;
-	cmd_flags |= I40E_AQC_MACVLAN_ADD_PERFECT_MATCH;
-	element.flags = cpu_to_le16(cmd_flags);
-	status = i40e_aq_add_macvlan(hw, seid, &element, 1, NULL);
-	*aq_err = hw->aq.asq_last_status;
-
-	return status;
-}
-
-/**
- * i40e_reset_ch_rings - Reset the queue contexts in a channel
- * @vsi: the VSI we want to access
- * @ch: the channel we want to access
- */
-static void i40e_reset_ch_rings(struct i40e_vsi *vsi, struct i40e_channel *ch)
-{
-	struct i40e_ring *tx_ring, *rx_ring;
-	u16 pf_q;
-	int i;
-
-	for (i = 0; i < ch->num_queue_pairs; i++) {
-		pf_q = ch->base_queue + i;
-		tx_ring = vsi->tx_rings[pf_q];
-		tx_ring->ch = NULL;
-		rx_ring = vsi->rx_rings[pf_q];
-		rx_ring->ch = NULL;
-	}
-}
-
-/**
- * i40e_free_macvlan_channels
- * @vsi: the VSI we want to access
- *
- * This function frees the Qs of the channel VSI from
- * the stack and also deletes the channel VSIs which
- * serve as macvlans.
- */
-static void i40e_free_macvlan_channels(struct i40e_vsi *vsi)
-{
-	struct i40e_channel *ch, *ch_tmp;
-	int ret;
-
-	if (list_empty(&vsi->macvlan_list))
-		return;
-
-	list_for_each_entry_safe(ch, ch_tmp, &vsi->macvlan_list, list) {
-		struct i40e_vsi *parent_vsi;
-
-		if (i40e_is_channel_macvlan(ch)) {
-			i40e_reset_ch_rings(vsi, ch);
-			clear_bit(ch->fwd->bit_no, vsi->fwd_bitmask);
-			netdev_unbind_sb_channel(vsi->netdev, ch->fwd->netdev);
-			netdev_set_sb_channel(ch->fwd->netdev, 0);
-			kfree(ch->fwd);
-			ch->fwd = NULL;
-		}
-
-		list_del(&ch->list);
-		parent_vsi = ch->parent_vsi;
-		if (!parent_vsi || !ch->initialized) {
-			kfree(ch);
-			continue;
-		}
-
-		/* remove the VSI */
-		ret = i40e_aq_delete_element(&vsi->back->hw, ch->seid,
-					     NULL);
-		if (ret)
-			dev_err(&vsi->back->pdev->dev,
-				"unable to remove channel (%d) for parent VSI(%d)\n",
-				ch->seid, parent_vsi->seid);
-		kfree(ch);
-	}
-	vsi->macvlan_cnt = 0;
-}
-
-/**
- * i40e_fwd_ring_up - bring the macvlan device up
- * @vsi: the VSI we want to access
- * @vdev: macvlan netdevice
- * @fwd: the private fwd structure
- */
-static int i40e_fwd_ring_up(struct i40e_vsi *vsi, struct net_device *vdev,
-			    struct i40e_fwd_adapter *fwd)
-{
-	int ret = 0, num_tc = 1,  i, aq_err;
-	struct i40e_channel *ch, *ch_tmp;
-	struct i40e_pf *pf = vsi->back;
-	struct i40e_hw *hw = &pf->hw;
-
-	if (list_empty(&vsi->macvlan_list))
-		return -EINVAL;
-
-	/* Go through the list and find an available channel */
-	list_for_each_entry_safe(ch, ch_tmp, &vsi->macvlan_list, list) {
-		if (!i40e_is_channel_macvlan(ch)) {
-			ch->fwd = fwd;
-			/* record configuration for macvlan interface in vdev */
-			for (i = 0; i < num_tc; i++)
-				netdev_bind_sb_channel_queue(vsi->netdev, vdev,
-							     i,
-							     ch->num_queue_pairs,
-							     ch->base_queue);
-			for (i = 0; i < ch->num_queue_pairs; i++) {
-				struct i40e_ring *tx_ring, *rx_ring;
-				u16 pf_q;
-
-				pf_q = ch->base_queue + i;
-
-				/* Get to TX ring ptr */
-				tx_ring = vsi->tx_rings[pf_q];
-				tx_ring->ch = ch;
-
-				/* Get the RX ring ptr */
-				rx_ring = vsi->rx_rings[pf_q];
-				rx_ring->ch = ch;
-			}
-			break;
-		}
-	}
-
-	/* Guarantee all rings are updated before we update the
-	 * MAC address filter.
-	 */
-	wmb();
-
-	/* Add a mac filter */
-	ret = i40e_add_macvlan_filter(hw, ch->seid, vdev->dev_addr, &aq_err);
-	if (ret) {
-		/* if we cannot add the MAC rule then disable the offload */
-		macvlan_release_l2fw_offload(vdev);
-		for (i = 0; i < ch->num_queue_pairs; i++) {
-			struct i40e_ring *rx_ring;
-			u16 pf_q;
-
-			pf_q = ch->base_queue + i;
-			rx_ring = vsi->rx_rings[pf_q];
-			rx_ring->netdev = NULL;
-		}
-		dev_info(&pf->pdev->dev,
-			 "Error adding mac filter on macvlan err %s, aq_err %s\n",
-			  i40e_stat_str(hw, ret),
-			  i40e_aq_str(hw, aq_err));
-		netdev_err(vdev, "L2fwd offload disabled to L2 filter error\n");
-	}
-
-	return ret;
-}
-
-/**
- * i40e_setup_macvlans - create the channels which will be macvlans
- * @vsi: the VSI we want to access
- * @macvlan_cnt: no. of macvlans to be setup
- * @qcnt: no. of Qs per macvlan
- * @vdev: macvlan netdevice
- */
-static int i40e_setup_macvlans(struct i40e_vsi *vsi, u16 macvlan_cnt, u16 qcnt,
-			       struct net_device *vdev)
+static int i40e_rebuild_cloud_filters(struct i40e_vsi *vsi, u16 seid)
 {
+	struct i40e_cloud_filter *cfilter;
 	struct i40e_pf *pf = vsi->back;
-	struct i40e_hw *hw = &pf->hw;
-	struct i40e_vsi_context ctxt;
-	u16 sections, qmap, num_qps;
-	struct i40e_channel *ch;
-	int i, pow, ret = 0;
-	u8 offset = 0;
-
-	if (vsi->type != I40E_VSI_MAIN || !macvlan_cnt)
-		return -EINVAL;
-
-	num_qps = vsi->num_queue_pairs - (macvlan_cnt * qcnt);
-
-	/* find the next higher power-of-2 of num queue pairs */
-	pow = fls(roundup_pow_of_two(num_qps) - 1);
-
-	qmap = (offset << I40E_AQ_VSI_TC_QUE_OFFSET_SHIFT) |
-		(pow << I40E_AQ_VSI_TC_QUE_NUMBER_SHIFT);
-
-	/* Setup context bits for the main VSI */
-	sections = I40E_AQ_VSI_PROP_QUEUE_MAP_VALID;
-	sections |= I40E_AQ_VSI_PROP_SCHED_VALID;
-	memset(&ctxt, 0, sizeof(ctxt));
-	ctxt.seid = vsi->seid;
-	ctxt.pf_num = vsi->back->hw.pf_id;
-	ctxt.vf_num = 0;
-	ctxt.uplink_seid = vsi->uplink_seid;
-	ctxt.info = vsi->info;
-	ctxt.info.tc_mapping[0] = cpu_to_le16(qmap);
-	ctxt.info.mapping_flags |= cpu_to_le16(I40E_AQ_VSI_QUE_MAP_CONTIG);
-	ctxt.info.queue_mapping[0] = cpu_to_le16(vsi->base_queue);
-	ctxt.info.valid_sections |= cpu_to_le16(sections);
+	struct hlist_node *node;
+	i40e_status ret;
 
-	/* Reconfigure RSS for main VSI with new max queue count */
-	vsi->rss_size = max_t(u16, num_qps, qcnt);
-	ret = i40e_vsi_config_rss(vsi);
-	if (ret) {
-		dev_info(&pf->pdev->dev,
-			 "Failed to reconfig RSS for num_queues (%u)\n",
-			 vsi->rss_size);
-		return ret;
-	}
-	vsi->reconfig_rss = true;
-	dev_dbg(&vsi->back->pdev->dev,
-		"Reconfigured RSS with num_queues (%u)\n", vsi->rss_size);
-	vsi->next_base_queue = num_qps;
-	vsi->cnt_q_avail = vsi->num_queue_pairs - num_qps;
+	/* Add cloud filters back if they exist */
+	hlist_for_each_entry_safe(cfilter, node, &pf->cloud_filter_list,
+				  cloud_node) {
+		if (cfilter->seid != seid)
+			continue;
 
-	/* Update the VSI after updating the VSI queue-mapping
-	 * information
-	 */
-	ret = i40e_aq_update_vsi_params(hw, &ctxt, NULL);
-	if (ret) {
-		dev_info(&pf->pdev->dev,
-			 "Update vsi tc config failed, err %s aq_err %s\n",
-			 i40e_stat_str(hw, ret),
-			 i40e_aq_str(hw, hw->aq.asq_last_status));
-		return ret;
-	}
-	/* update the local VSI info with updated queue map */
-	i40e_vsi_update_queue_map(vsi, &ctxt);
-	vsi->info.valid_sections = 0;
+		if (cfilter->dst_port)
+			ret = i40e_add_del_cloud_filter_big_buf(vsi, cfilter,
+								true);
+		else
+			ret = i40e_add_del_cloud_filter(vsi, cfilter, true);
 
-	/* Create channels for macvlans */
-	INIT_LIST_HEAD(&vsi->macvlan_list);
-	for (i = 0; i < macvlan_cnt; i++) {
-		ch = kzalloc(sizeof(*ch), GFP_KERNEL);
-		if (!ch) {
-			ret = -ENOMEM;
-			goto err_free;
-		}
-		INIT_LIST_HEAD(&ch->list);
-		ch->num_queue_pairs = qcnt;
-		if (!i40e_setup_channel(pf, vsi, ch)) {
-			ret = -EINVAL;
-			kfree(ch);
-			goto err_free;
+		if (ret) {
+			dev_dbg(&pf->pdev->dev,
+				"Failed to rebuild cloud filter, err %s aq_err %s\n",
+				i40e_stat_str(&pf->hw, ret),
+				i40e_aq_str(&pf->hw,
+					    pf->hw.aq.asq_last_status));
+			return ret;
 		}
-		ch->parent_vsi = vsi;
-		vsi->cnt_q_avail -= ch->num_queue_pairs;
-		vsi->macvlan_cnt++;
-		list_add_tail(&ch->list, &vsi->macvlan_list);
 	}
-
-	return ret;
-
-err_free:
-	dev_info(&pf->pdev->dev, "Failed to setup macvlans\n");
-	i40e_free_macvlan_channels(vsi);
-
-	return ret;
+	return 0;
 }
 
 /**
- * i40e_fwd_add - configure macvlans
- * @netdev: net device to configure
- * @vdev: macvlan netdevice
+ * i40e_rebuild_channels - Rebuilds channel VSIs if they existed before reset
+ * @vsi: PF main vsi
+ *
+ * Rebuilds channel VSIs if they existed before reset
  **/
-static void *i40e_fwd_add(struct net_device *netdev, struct net_device *vdev)
-{
-	struct i40e_netdev_priv *np = netdev_priv(netdev);
-	u16 q_per_macvlan = 0, macvlan_cnt = 0, vectors;
-	struct i40e_vsi *vsi = np->vsi;
-	struct i40e_pf *pf = vsi->back;
-	struct i40e_fwd_adapter *fwd;
-	int avail_macvlan, ret;
-
-	if ((pf->flags & I40E_FLAG_DCB_ENABLED)) {
-		netdev_info(netdev, "Macvlans are not supported when DCB is enabled\n");
-		return ERR_PTR(-EINVAL);
-	}
-	if ((pf->flags & I40E_FLAG_TC_MQPRIO)) {
-		netdev_info(netdev, "Macvlans are not supported when HW TC offload is on\n");
-		return ERR_PTR(-EINVAL);
-	}
-	if (pf->num_lan_msix < I40E_MIN_MACVLAN_VECTORS) {
-		netdev_info(netdev, "Not enough vectors available to support macvlans\n");
-		return ERR_PTR(-EINVAL);
-	}
-
-	/* The macvlan device has to be a single Q device so that the
-	 * tc_to_txq field can be reused to pick the tx queue.
-	 */
-	if (netif_is_multiqueue(vdev))
-		return ERR_PTR(-ERANGE);
-
-	if (!vsi->macvlan_cnt) {
-		/* reserve bit 0 for the pf device */
-		set_bit(0, vsi->fwd_bitmask);
-
-		/* Try to reserve as many queues as possible for macvlans. First
-		 * reserve 3/4th of max vectors, then half, then quarter and
-		 * calculate Qs per macvlan as you go
-		 */
-		vectors = pf->num_lan_msix;
-		if (vectors <= I40E_MAX_MACVLANS && vectors > 64) {
-			/* allocate 4 Qs per macvlan and 32 Qs to the PF*/
-			q_per_macvlan = 4;
-			macvlan_cnt = (vectors - 32) / 4;
-		} else if (vectors <= 64 && vectors > 32) {
-			/* allocate 2 Qs per macvlan and 16 Qs to the PF*/
-			q_per_macvlan = 2;
-			macvlan_cnt = (vectors - 16) / 2;
-		} else if (vectors <= 32 && vectors > 16) {
-			/* allocate 1 Q per macvlan and 16 Qs to the PF*/
-			q_per_macvlan = 1;
-			macvlan_cnt = vectors - 16;
-		} else if (vectors <= 16 && vectors > 8) {
-			/* allocate 1 Q per macvlan and 8 Qs to the PF */
-			q_per_macvlan = 1;
-			macvlan_cnt = vectors - 8;
-		} else {
-			/* allocate 1 Q per macvlan and 1 Q to the PF */
-			q_per_macvlan = 1;
-			macvlan_cnt = vectors - 1;
-		}
-
-		if (macvlan_cnt == 0)
-			return ERR_PTR(-EBUSY);
-
-		/* Quiesce VSI queues */
-		i40e_quiesce_vsi(vsi);
-
-		/* sets up the macvlans but does not "enable" them */
-		ret = i40e_setup_macvlans(vsi, macvlan_cnt, q_per_macvlan,
-					  vdev);
-		if (ret)
-			return ERR_PTR(ret);
-
-		/* Unquiesce VSI */
-		i40e_unquiesce_vsi(vsi);
-	}
-	avail_macvlan = find_first_zero_bit(vsi->fwd_bitmask,
-					    vsi->macvlan_cnt);
-	if (avail_macvlan >= I40E_MAX_MACVLANS)
-		return ERR_PTR(-EBUSY);
-
-	/* create the fwd struct */
-	fwd = kzalloc(sizeof(*fwd), GFP_KERNEL);
-	if (!fwd)
-		return ERR_PTR(-ENOMEM);
-
-	set_bit(avail_macvlan, vsi->fwd_bitmask);
-	fwd->bit_no = avail_macvlan;
-	netdev_set_sb_channel(vdev, avail_macvlan);
-	fwd->netdev = vdev;
-
-	if (!netif_running(netdev))
-		return fwd;
-
-	/* Set fwd ring up */
-	ret = i40e_fwd_ring_up(vsi, vdev, fwd);
-	if (ret) {
-		/* unbind the queues and drop the subordinate channel config */
-		netdev_unbind_sb_channel(netdev, vdev);
-		netdev_set_sb_channel(vdev, 0);
-
-		kfree(fwd);
-		return ERR_PTR(-EINVAL);
-	}
-
-	return fwd;
-}
-
-/**
- * i40e_del_all_macvlans - Delete all the mac filters on the channels
- * @vsi: the VSI we want to access
- */
-static void i40e_del_all_macvlans(struct i40e_vsi *vsi)
+static int i40e_rebuild_channels(struct i40e_vsi *vsi)
 {
 	struct i40e_channel *ch, *ch_tmp;
-	struct i40e_pf *pf = vsi->back;
-	struct i40e_hw *hw = &pf->hw;
-	int aq_err, ret = 0;
+	i40e_status ret;
 
-	if (list_empty(&vsi->macvlan_list))
-		return;
+	if (list_empty(&vsi->ch_list))
+		return 0;
 
-	list_for_each_entry_safe(ch, ch_tmp, &vsi->macvlan_list, list) {
-		if (i40e_is_channel_macvlan(ch)) {
-			ret = i40e_del_macvlan_filter(hw, ch->seid,
-						      i40e_channel_mac(ch),
-						      &aq_err);
-			if (!ret) {
-				/* Reset queue contexts */
-				i40e_reset_ch_rings(vsi, ch);
-				clear_bit(ch->fwd->bit_no, vsi->fwd_bitmask);
-				netdev_unbind_sb_channel(vsi->netdev,
-							 ch->fwd->netdev);
-				netdev_set_sb_channel(ch->fwd->netdev, 0);
-				kfree(ch->fwd);
-				ch->fwd = NULL;
-			}
+	list_for_each_entry_safe(ch, ch_tmp, &vsi->ch_list, list) {
+		if (!ch->initialized)
+			break;
+		/* Proceed with creation of channel (VMDq2) VSI */
+		ret = i40e_add_channel(vsi->back, vsi->uplink_seid, ch);
+		if (ret) {
+			dev_info(&vsi->back->pdev->dev,
+				 "failed to rebuild channels using uplink_seid %u\n",
+				 vsi->uplink_seid);
+			return ret;
 		}
-	}
-}
-
-/**
- * i40e_fwd_del - delete macvlan interfaces
- * @netdev: net device to configure
- * @vdev: macvlan netdevice
- */
-static void i40e_fwd_del(struct net_device *netdev, void *vdev)
-{
-	struct i40e_netdev_priv *np = netdev_priv(netdev);
-	struct i40e_fwd_adapter *fwd = vdev;
-	struct i40e_channel *ch, *ch_tmp;
-	struct i40e_vsi *vsi = np->vsi;
-	struct i40e_pf *pf = vsi->back;
-	struct i40e_hw *hw = &pf->hw;
-	int aq_err, ret = 0;
+		if (ch->max_tx_rate) {
+			if (i40e_set_bw_limit(vsi, ch->seid,
+					      ch->max_tx_rate))
+				return -EINVAL;
 
-	/* Find the channel associated with the macvlan and del mac filter */
-	list_for_each_entry_safe(ch, ch_tmp, &vsi->macvlan_list, list) {
-		if (i40e_is_channel_macvlan(ch) &&
-		    ether_addr_equal(i40e_channel_mac(ch),
-				     fwd->netdev->dev_addr)) {
-			ret = i40e_del_macvlan_filter(hw, ch->seid,
-						      i40e_channel_mac(ch),
-						      &aq_err);
-			if (!ret) {
-				/* Reset queue contexts */
-				i40e_reset_ch_rings(vsi, ch);
-				clear_bit(ch->fwd->bit_no, vsi->fwd_bitmask);
-				netdev_unbind_sb_channel(netdev, fwd->netdev);
-				netdev_set_sb_channel(fwd->netdev, 0);
-				kfree(ch->fwd);
-				ch->fwd = NULL;
-			} else {
-				dev_info(&pf->pdev->dev,
-					 "Error deleting mac filter on macvlan err %s, aq_err %s\n",
-					  i40e_stat_str(hw, ret),
-					  i40e_aq_str(hw, aq_err));
-			}
-			break;
+			dev_dbg(&vsi->back->pdev->dev,
+				"Set tx rate of %llu Mbps (count of 50Mbps %llu) for vsi->seid %u\n",
+				ch->max_tx_rate,
+				ch->max_tx_rate / I40E_BW_CREDIT_DIVISOR,
+				ch->seid);
+		}
+		ret = i40e_rebuild_cloud_filters(vsi, ch->seid);
+		if (ret) {
+			dev_dbg(&vsi->back->pdev->dev,
+				"Failed to rebuild cloud filters for channel VSI %u\n",
+				ch->seid);
+			return ret;
 		}
 	}
+	return 0;
 }
+#endif /* __TC_MQPRIO_MODE_MAX */
 
+#ifdef HAVE_SETUP_TC
 /**
  * i40e_setup_tc - configure multiple traffic classes
  * @netdev: net device to configure
  * @type_data: tc offload data
  **/
+#ifdef __TC_MQPRIO_MODE_MAX
 static int i40e_setup_tc(struct net_device *netdev, void *type_data)
+#else
+static int i40e_setup_tc(struct net_device *netdev, u8 tc)
+#endif
 {
+#ifdef __TC_MQPRIO_MODE_MAX
 	struct tc_mqprio_qopt_offload *mqprio_qopt = type_data;
+#endif
 	struct i40e_netdev_priv *np = netdev_priv(netdev);
 	struct i40e_vsi *vsi = np->vsi;
 	struct i40e_pf *pf = vsi->back;
-	u8 enabled_tc = 0, num_tc, hw;
+	u8 enabled_tc = 0, num_tc;
+#ifdef __TC_MQPRIO_MODE_MAX
 	bool need_reset = false;
 	int old_queue_pairs;
+#endif
 	int ret = -EINVAL;
+#ifdef __TC_MQPRIO_MODE_MAX
 	u16 mode;
+	u8 hw;
+#endif
 	int i;
 
+#ifdef __TC_MQPRIO_MODE_MAX
 	old_queue_pairs = vsi->num_queue_pairs;
+#endif
+	/* Check if MFP enabled */
+	if (pf->flags & I40E_FLAG_MFP_ENABLED) {
+		netdev_info(netdev,
+			    "Configuring TC not supported in MFP mode\n");
+		goto exit;
+	}
+
+#ifndef HAVE_NDO_SETUP_TC_REMOVE_TC_TO_NETDEV
+	/* Check whether tc count is within enabled limit */
+	if (tc > i40e_pf_get_num_tc(pf)) {
+		netdev_info(netdev, "TC count greater than enabled on link for adapter\n");
+		goto exit;
+	}
+#endif
+
+#ifdef __TC_MQPRIO_MODE_MAX
 	num_tc = mqprio_qopt->qopt.num_tc;
 	hw = mqprio_qopt->qopt.hw;
 	mode = mqprio_qopt->mode;
@@ -7417,12 +8783,6 @@
 		goto config_tc;
 	}
 
-	/* Check if MFP enabled */
-	if (pf->flags & I40E_FLAG_MFP_ENABLED) {
-		netdev_info(netdev,
-			    "Configuring TC not supported in MFP mode\n");
-		return ret;
-	}
 	switch (mode) {
 	case TC_MQPRIO_MODE_DCB:
 		pf->flags &= ~I40E_FLAG_TC_MQPRIO;
@@ -7462,48 +8822,74 @@
 	}
 
 config_tc:
+#else
+	/* Check if DCB enabled to continue */
+	if (!(pf->flags & I40E_FLAG_DCB_ENABLED)) {
+		netdev_info(netdev, "DCB is not enabled for adapter\n");
+		goto exit;
+	}
+	num_tc = tc;
+#endif
 	/* Generate TC map for number of tc requested */
 	for (i = 0; i < num_tc; i++)
 		enabled_tc |= BIT(i);
 
 	/* Requesting same TC configuration as already enabled */
+#ifdef __TC_MQPRIO_MODE_MAX
 	if (enabled_tc == vsi->tc_config.enabled_tc &&
 	    mode != TC_MQPRIO_MODE_CHANNEL)
 		return 0;
+#else
+	if (enabled_tc == vsi->tc_config.enabled_tc)
+		return 0;
+#endif
 
 	/* Quiesce VSI queues */
 	i40e_quiesce_vsi(vsi);
 
+#ifdef __TC_MQPRIO_MODE_MAX
 	if (!hw && !(pf->flags & I40E_FLAG_TC_MQPRIO))
 		i40e_remove_queue_channels(vsi);
+#endif
 
 	/* Configure VSI for enabled TCs */
 	ret = i40e_vsi_config_tc(vsi, enabled_tc);
 	if (ret) {
 		netdev_info(netdev, "Failed configuring TC for VSI seid=%d\n",
 			    vsi->seid);
+#ifdef __TC_MQPRIO_MODE_MAX
 		need_reset = true;
+#endif
 		goto exit;
-	} else {
-		dev_info(&vsi->back->pdev->dev,
-			 "Setup channel (id:%u) utilizing num_queues %d\n",
-			 vsi->seid, vsi->tc_config.tc_info[0].qcount);
+#ifdef __TC_MQPRIO_MODE_MAX
+	} else if (enabled_tc &&
+		   (!is_power_of_2(vsi->tc_config.tc_info[0].qcount))) {
+		netdev_info(netdev,
+			    "Failed to create channel. Override queues (%u) not power of 2\n",
+			    vsi->tc_config.tc_info[0].qcount);
+		pf->flags &= ~I40E_FLAG_TC_MQPRIO;
+		vsi->num_queue_pairs = old_queue_pairs;
+		ret = -EINVAL;
+		need_reset = true;
+		goto exit;
+#endif
 	}
 
+	dev_info(&vsi->back->pdev->dev,
+		 "Setup channel (id:%u) utilizing num_queues %d\n",
+		 vsi->seid, vsi->tc_config.tc_info[0].qcount);
+
+#ifdef __TC_MQPRIO_MODE_MAX
 	if (pf->flags & I40E_FLAG_TC_MQPRIO) {
 		if (vsi->mqprio_qopt.max_rate[0]) {
-			u64 max_tx_rate = vsi->mqprio_qopt.max_rate[0];
-
-			do_div(max_tx_rate, I40E_BW_MBPS_DIVISOR);
+			u64 max_tx_rate = vsi->mqprio_qopt.max_rate[0] /
+								(1000000 / 8);
 			ret = i40e_set_bw_limit(vsi, vsi->seid, max_tx_rate);
 			if (!ret) {
-				u64 credits = max_tx_rate;
-
-				do_div(credits, I40E_BW_CREDIT_DIVISOR);
 				dev_dbg(&vsi->back->pdev->dev,
 					"Set tx rate of %llu Mbps (count of 50Mbps %llu) for vsi->seid %u\n",
 					max_tx_rate,
-					credits,
+					max_tx_rate / I40E_BW_CREDIT_DIVISOR,
 					vsi->seid);
 			} else {
 				need_reset = true;
@@ -7520,239 +8906,27 @@
 		}
 	}
 
+#endif
+
 exit:
+#ifdef __TC_MQPRIO_MODE_MAX
 	/* Reset the configuration data to defaults, only TC0 is enabled */
 	if (need_reset) {
 		i40e_vsi_set_default_tc_config(vsi);
 		need_reset = false;
 	}
-
+#endif
 	/* Unquiesce VSI */
 	i40e_unquiesce_vsi(vsi);
 	return ret;
 }
 
-/**
- * i40e_set_cld_element - sets cloud filter element data
- * @filter: cloud filter rule
- * @cld: ptr to cloud filter element data
- *
- * This is helper function to copy data into cloud filter element
- **/
-static inline void
-i40e_set_cld_element(struct i40e_cloud_filter *filter,
-		     struct i40e_aqc_cloud_filters_element_data *cld)
-{
-	int i, j;
-	u32 ipa;
-
-	memset(cld, 0, sizeof(*cld));
-	ether_addr_copy(cld->outer_mac, filter->dst_mac);
-	ether_addr_copy(cld->inner_mac, filter->src_mac);
-
-	if (filter->n_proto != ETH_P_IP && filter->n_proto != ETH_P_IPV6)
-		return;
-
-	if (filter->n_proto == ETH_P_IPV6) {
-#define IPV6_MAX_INDEX	(ARRAY_SIZE(filter->dst_ipv6) - 1)
-		for (i = 0, j = 0; i < ARRAY_SIZE(filter->dst_ipv6);
-		     i++, j += 2) {
-			ipa = be32_to_cpu(filter->dst_ipv6[IPV6_MAX_INDEX - i]);
-			ipa = cpu_to_le32(ipa);
-			memcpy(&cld->ipaddr.raw_v6.data[j], &ipa, sizeof(ipa));
-		}
-	} else {
-		ipa = be32_to_cpu(filter->dst_ipv4);
-		memcpy(&cld->ipaddr.v4.data, &ipa, sizeof(ipa));
-	}
-
-	cld->inner_vlan = cpu_to_le16(ntohs(filter->vlan_id));
-
-	/* tenant_id is not supported by FW now, once the support is enabled
-	 * fill the cld->tenant_id with cpu_to_le32(filter->tenant_id)
-	 */
-	if (filter->tenant_id)
-		return;
-}
-
-/**
- * i40e_add_del_cloud_filter - Add/del cloud filter
- * @vsi: pointer to VSI
- * @filter: cloud filter rule
- * @add: if true, add, if false, delete
- *
- * Add or delete a cloud filter for a specific flow spec.
- * Returns 0 if the filter were successfully added.
- **/
-int i40e_add_del_cloud_filter(struct i40e_vsi *vsi,
-			      struct i40e_cloud_filter *filter, bool add)
-{
-	struct i40e_aqc_cloud_filters_element_data cld_filter;
-	struct i40e_pf *pf = vsi->back;
-	int ret;
-	static const u16 flag_table[128] = {
-		[I40E_CLOUD_FILTER_FLAGS_OMAC]  =
-			I40E_AQC_ADD_CLOUD_FILTER_OMAC,
-		[I40E_CLOUD_FILTER_FLAGS_IMAC]  =
-			I40E_AQC_ADD_CLOUD_FILTER_IMAC,
-		[I40E_CLOUD_FILTER_FLAGS_IMAC_IVLAN]  =
-			I40E_AQC_ADD_CLOUD_FILTER_IMAC_IVLAN,
-		[I40E_CLOUD_FILTER_FLAGS_IMAC_TEN_ID] =
-			I40E_AQC_ADD_CLOUD_FILTER_IMAC_TEN_ID,
-		[I40E_CLOUD_FILTER_FLAGS_OMAC_TEN_ID_IMAC] =
-			I40E_AQC_ADD_CLOUD_FILTER_OMAC_TEN_ID_IMAC,
-		[I40E_CLOUD_FILTER_FLAGS_IMAC_IVLAN_TEN_ID] =
-			I40E_AQC_ADD_CLOUD_FILTER_IMAC_IVLAN_TEN_ID,
-		[I40E_CLOUD_FILTER_FLAGS_IIP] =
-			I40E_AQC_ADD_CLOUD_FILTER_IIP,
-	};
-
-	if (filter->flags >= ARRAY_SIZE(flag_table))
-		return I40E_ERR_CONFIG;
-
-	/* copy element needed to add cloud filter from filter */
-	i40e_set_cld_element(filter, &cld_filter);
-
-	if (filter->tunnel_type != I40E_CLOUD_TNL_TYPE_NONE)
-		cld_filter.flags = cpu_to_le16(filter->tunnel_type <<
-					     I40E_AQC_ADD_CLOUD_TNL_TYPE_SHIFT);
-
-	if (filter->n_proto == ETH_P_IPV6)
-		cld_filter.flags |= cpu_to_le16(flag_table[filter->flags] |
-						I40E_AQC_ADD_CLOUD_FLAGS_IPV6);
-	else
-		cld_filter.flags |= cpu_to_le16(flag_table[filter->flags] |
-						I40E_AQC_ADD_CLOUD_FLAGS_IPV4);
-
-	if (add)
-		ret = i40e_aq_add_cloud_filters(&pf->hw, filter->seid,
-						&cld_filter, 1);
-	else
-		ret = i40e_aq_rem_cloud_filters(&pf->hw, filter->seid,
-						&cld_filter, 1);
-	if (ret)
-		dev_dbg(&pf->pdev->dev,
-			"Failed to %s cloud filter using l4 port %u, err %d aq_err %d\n",
-			add ? "add" : "delete", filter->dst_port, ret,
-			pf->hw.aq.asq_last_status);
-	else
-		dev_info(&pf->pdev->dev,
-			 "%s cloud filter for VSI: %d\n",
-			 add ? "Added" : "Deleted", filter->seid);
-	return ret;
-}
-
-/**
- * i40e_add_del_cloud_filter_big_buf - Add/del cloud filter using big_buf
- * @vsi: pointer to VSI
- * @filter: cloud filter rule
- * @add: if true, add, if false, delete
- *
- * Add or delete a cloud filter for a specific flow spec using big buffer.
- * Returns 0 if the filter were successfully added.
- **/
-int i40e_add_del_cloud_filter_big_buf(struct i40e_vsi *vsi,
-				      struct i40e_cloud_filter *filter,
-				      bool add)
-{
-	struct i40e_aqc_cloud_filters_element_bb cld_filter;
-	struct i40e_pf *pf = vsi->back;
-	int ret;
-
-	/* Both (src/dst) valid mac_addr are not supported */
-	if ((is_valid_ether_addr(filter->dst_mac) &&
-	     is_valid_ether_addr(filter->src_mac)) ||
-	    (is_multicast_ether_addr(filter->dst_mac) &&
-	     is_multicast_ether_addr(filter->src_mac)))
-		return -EOPNOTSUPP;
-
-	/* Big buffer cloud filter needs 'L4 port' to be non-zero. Also, UDP
-	 * ports are not supported via big buffer now.
-	 */
-	if (!filter->dst_port || filter->ip_proto == IPPROTO_UDP)
-		return -EOPNOTSUPP;
-
-	/* adding filter using src_port/src_ip is not supported at this stage */
-	if (filter->src_port || filter->src_ipv4 ||
-	    !ipv6_addr_any(&filter->ip.v6.src_ip6))
-		return -EOPNOTSUPP;
-
-	/* copy element needed to add cloud filter from filter */
-	i40e_set_cld_element(filter, &cld_filter.element);
-
-	if (is_valid_ether_addr(filter->dst_mac) ||
-	    is_valid_ether_addr(filter->src_mac) ||
-	    is_multicast_ether_addr(filter->dst_mac) ||
-	    is_multicast_ether_addr(filter->src_mac)) {
-		/* MAC + IP : unsupported mode */
-		if (filter->dst_ipv4)
-			return -EOPNOTSUPP;
-
-		/* since we validated that L4 port must be valid before
-		 * we get here, start with respective "flags" value
-		 * and update if vlan is present or not
-		 */
-		cld_filter.element.flags =
-			cpu_to_le16(I40E_AQC_ADD_CLOUD_FILTER_MAC_PORT);
-
-		if (filter->vlan_id) {
-			cld_filter.element.flags =
-			cpu_to_le16(I40E_AQC_ADD_CLOUD_FILTER_MAC_VLAN_PORT);
-		}
-
-	} else if (filter->dst_ipv4 ||
-		   !ipv6_addr_any(&filter->ip.v6.dst_ip6)) {
-		cld_filter.element.flags =
-				cpu_to_le16(I40E_AQC_ADD_CLOUD_FILTER_IP_PORT);
-		if (filter->n_proto == ETH_P_IPV6)
-			cld_filter.element.flags |=
-				cpu_to_le16(I40E_AQC_ADD_CLOUD_FLAGS_IPV6);
-		else
-			cld_filter.element.flags |=
-				cpu_to_le16(I40E_AQC_ADD_CLOUD_FLAGS_IPV4);
-	} else {
-		dev_err(&pf->pdev->dev,
-			"either mac or ip has to be valid for cloud filter\n");
-		return -EINVAL;
-	}
-
-	/* Now copy L4 port in Byte 6..7 in general fields */
-	cld_filter.general_fields[I40E_AQC_ADD_CLOUD_FV_FLU_0X16_WORD0] =
-						be16_to_cpu(filter->dst_port);
-
-	if (add) {
-		/* Validate current device switch mode, change if necessary */
-		ret = i40e_validate_and_set_switch_mode(vsi);
-		if (ret) {
-			dev_err(&pf->pdev->dev,
-				"failed to set switch mode, ret %d\n",
-				ret);
-			return ret;
-		}
-
-		ret = i40e_aq_add_cloud_filters_bb(&pf->hw, filter->seid,
-						   &cld_filter, 1);
-	} else {
-		ret = i40e_aq_rem_cloud_filters_bb(&pf->hw, filter->seid,
-						   &cld_filter, 1);
-	}
-
-	if (ret)
-		dev_dbg(&pf->pdev->dev,
-			"Failed to %s cloud filter(big buffer) err %d aq_err %d\n",
-			add ? "add" : "delete", ret, pf->hw.aq.asq_last_status);
-	else
-		dev_info(&pf->pdev->dev,
-			 "%s cloud filter for VSI: %d, L4 port: %d\n",
-			 add ? "add" : "delete", filter->seid,
-			 ntohs(filter->dst_port));
-	return ret;
-}
+#ifdef __TC_MQPRIO_MODE_MAX
 
 /**
  * i40e_parse_cls_flower - Parse tc flower filters provided by kernel
  * @vsi: Pointer to VSI
- * @cls_flower: Pointer to struct flow_cls_offload
+ * @f: Pointer to struct flow_cls_offload
  * @filter: Pointer to cloud filter structure
  *
  **/
@@ -7770,16 +8944,21 @@
 	    ~(BIT(FLOW_DISSECTOR_KEY_CONTROL) |
 	      BIT(FLOW_DISSECTOR_KEY_BASIC) |
 	      BIT(FLOW_DISSECTOR_KEY_ETH_ADDRS) |
+#ifndef HAVE_TC_FLOWER_VLAN_IN_TAGS
 	      BIT(FLOW_DISSECTOR_KEY_VLAN) |
+#endif
 	      BIT(FLOW_DISSECTOR_KEY_IPV4_ADDRS) |
 	      BIT(FLOW_DISSECTOR_KEY_IPV6_ADDRS) |
-	      BIT(FLOW_DISSECTOR_KEY_PORTS) |
-	      BIT(FLOW_DISSECTOR_KEY_ENC_KEYID))) {
+#ifdef HAVE_TC_FLOWER_ENC
+	      BIT(FLOW_DISSECTOR_KEY_ENC_KEYID) |
+#endif
+	      BIT(FLOW_DISSECTOR_KEY_PORTS))) {
 		dev_err(&pf->pdev->dev, "Unsupported key used: 0x%x\n",
 			dissector->used_keys);
 		return -EOPNOTSUPP;
 	}
 
+#ifdef HAVE_TC_FLOWER_ENC
 	if (flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_ENC_KEYID)) {
 		struct flow_match_enc_keyid match;
 
@@ -7789,6 +8968,7 @@
 
 		filter->tenant_id = be32_to_cpu(match.key->keyid);
 	}
+#endif /* HAVE_TC_FLOWER_ENC */
 
 	if (flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_BASIC)) {
 		struct flow_match_basic match;
@@ -7834,6 +9014,7 @@
 		ether_addr_copy(filter->src_mac, match.key->src);
 	}
 
+#ifndef HAVE_TC_FLOWER_VLAN_IN_TAGS
 	if (flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_VLAN)) {
 		struct flow_match_vlan match;
 
@@ -7851,6 +9032,7 @@
 
 		filter->vlan_id = cpu_to_be16(match.key->vlan_id);
 	}
+#endif /* !HAVE_TC_FLOWER_VLAN_IN_TAGS */
 
 	if (flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_CONTROL)) {
 		struct flow_match_control match;
@@ -8007,7 +9189,7 @@
 
 	if (tc < 0) {
 		dev_err(&vsi->back->pdev->dev, "Invalid traffic class\n");
-		return -EOPNOTSUPP;
+		return -EINVAL;
 	}
 
 	if (test_bit(__I40E_RESET_RECOVERY_PENDING, pf->state) ||
@@ -8018,8 +9200,16 @@
 	    (!hlist_empty(&pf->fdir_filter_list))) {
 		dev_err(&vsi->back->pdev->dev,
 			"Flow Director Sideband filters exists, turn ntuple off to configure cloud filters\n");
-		return -EINVAL;
+		return -EOPNOTSUPP;
 	}
+#ifdef NETIF_F_HW_TC
+		if (!(vsi->netdev->features & NETIF_F_HW_TC) &&
+		    !(pf->flags & I40E_FLAG_CLS_FLOWER)) {
+			dev_err(&vsi->back->pdev->dev,
+				"Can't apply TC flower filters, turn ON hw-tc-offload and try again\n");
+			return -EOPNOTSUPP;
+		}
+#endif /* NETIF_F_HW_TC */
 
 	if (vsi->back->flags & I40E_FLAG_FD_SB_ENABLED) {
 		dev_err(&vsi->back->pdev->dev,
@@ -8050,8 +9240,7 @@
 
 	if (err) {
 		dev_err(&pf->pdev->dev,
-			"Failed to add cloud filter, err %s\n",
-			i40e_stat_str(&pf->hw, err));
+			"Failed to add cloud filter, err %d\n", err);
 		goto err;
 	}
 
@@ -8133,14 +9322,17 @@
 
 /**
  * i40e_setup_tc_cls_flower - flower classifier offloads
- * @netdev: net device to configure
- * @type_data: offload data
+ * @np: net device to configure
+ * @cls_flower: pointer to cls flower offload structure
  **/
 static int i40e_setup_tc_cls_flower(struct i40e_netdev_priv *np,
 				    struct flow_cls_offload *cls_flower)
 {
 	struct i40e_vsi *vsi = np->vsi;
 
+	if (cls_flower->common.chain_index)
+		return -EOPNOTSUPP;
+
 	switch (cls_flower->command) {
 	case FLOW_CLS_REPLACE:
 		return i40e_configure_clsflower(vsi, cls_flower);
@@ -8158,9 +9350,6 @@
 {
 	struct i40e_netdev_priv *np = cb_priv;
 
-	if (!tc_cls_can_offload_and_chain0(np->vsi->netdev, type_data))
-		return -EOPNOTSUPP;
-
 	switch (type) {
 	case TC_SETUP_CLSFLOWER:
 		return i40e_setup_tc_cls_flower(np, type_data);
@@ -8171,12 +9360,35 @@
 }
 
 static LIST_HEAD(i40e_block_cb_list);
+#endif /* __TC_MQPRIO_MODE_MAX */
 
+#ifdef NETIF_F_HW_TC
+#ifdef HAVE_NDO_SETUP_TC_REMOVE_TC_TO_NETDEV
 static int __i40e_setup_tc(struct net_device *netdev, enum tc_setup_type type,
 			   void *type_data)
+#elif defined(HAVE_NDO_SETUP_TC_CHAIN_INDEX)
+static int __i40e_setup_tc(struct net_device *netdev, u32 handle,
+			   u32 chain_index, __be16 proto,
+			   struct tc_to_netdev *tc)
+#else
+static int __i40e_setup_tc(struct net_device *netdev, u32 handle, __be16 proto,
+			   struct tc_to_netdev *tc)
+#endif
 {
+#ifdef __TC_MQPRIO_MODE_MAX
 	struct i40e_netdev_priv *np = netdev_priv(netdev);
-
+#endif
+#ifdef HAVE_NDO_SETUP_TC_REMOVE_TC_TO_NETDEV
+#ifndef __TC_MQPRIO_MODE_MAX
+	struct tc_mqprio_qopt *mqprio = type_data;
+#endif
+#else
+#ifdef TC_MQPRIO_HW_OFFLOAD_MAX
+	struct tc_mqprio_qopt *mqprio = tc->mqprio;
+#endif /* TC_MQPRIO_HW_OFFLOAD_MAX*/
+	unsigned int type = tc->type;
+#endif /* HAVE_NDO_SETUP_TC_REMOVE_TC_TO_NETDEV */
+#ifdef __TC_MQPRIO_MODE_MAX
 	switch (type) {
 	case TC_SETUP_QDISC_MQPRIO:
 		return i40e_setup_tc(netdev, type_data);
@@ -8188,7 +9400,21 @@
 	default:
 		return -EOPNOTSUPP;
 	}
+#else
+	if (type != TC_SETUP_QDISC_MQPRIO)
+		return -EINVAL;
+
+#ifdef TC_MQPRIO_HW_OFFLOAD_MAX
+	mqprio->hw = TC_MQPRIO_HW_OFFLOAD_TCS;
+
+	return i40e_setup_tc(netdev, mqprio->num_tc);
+#else
+	return i40e_setup_tc(netdev, tc->tc);
+#endif /* TC_MQPRIO_HW_OFFLOAD_MAX */
+#endif /* __TC_MQPRIO_MODE_MAX */
 }
+#endif /* NETIF_F_HW_TC */
+#endif /* HAVE_SETUP_TC */
 
 /**
  * i40e_open - Called when a network interface is made active
@@ -8222,7 +9448,6 @@
 	err = i40e_vsi_open(vsi);
 	if (err)
 		return err;
-
 	/* configure global TSO hardware offload settings */
 	wr32(&pf->hw, I40E_GLLAN_TSOMSK_F, be32_to_cpu(TCP_FLAG_PSH |
 						       TCP_FLAG_FIN) >> 16);
@@ -8231,7 +9456,20 @@
 						       TCP_FLAG_CWR) >> 16);
 	wr32(&pf->hw, I40E_GLLAN_TSOMSK_L, be32_to_cpu(TCP_FLAG_CWR) >> 16);
 
+#ifdef HAVE_VXLAN_RX_OFFLOAD
+#if IS_ENABLED(CONFIG_VXLAN)
+	vxlan_get_rx_port(netdev);
+#endif
+#endif /* HAVE_VXLAN_RX_OFFLOAD */
+#ifdef HAVE_GENEVE_RX_OFFLOAD
+#if IS_ENABLED(CONFIG_GENEVE)
+	if (pf->hw_features & I40E_HW_GENEVE_OFFLOAD_CAPABLE)
+		geneve_get_rx_port(netdev);
+#endif
+#endif /* HAVE_GENEVE_RX_OFFLOAD */
+#ifdef HAVE_UDP_ENC_RX_OFFLOAD
 	udp_tunnel_get_rx_info(netdev);
+#endif /* HAVE_UDP_ENC_RX_OFFLOAD */
 
 	return 0;
 }
@@ -8277,6 +9515,15 @@
 		if (err)
 			goto err_set_queues;
 
+		/* When reducing the number of Tx queues, any pre-existing
+		 * skbuffs might target a now removed queue. Older versions of
+		 * the Linux kernel do not check for this, and it can result
+		 * in a kernel panic. Avoid this by flushing all skbs now, so
+		 * that we avoid attempting to transmit one that has an
+		 * invalid queue mapping.
+		 */
+		qdisc_reset_all_tx_gt(vsi->netdev, 0);
+
 		err = netif_set_real_num_rx_queues(vsi->netdev,
 						   vsi->num_queue_pairs);
 		if (err)
@@ -8345,40 +9592,58 @@
 	INIT_LIST_HEAD(&pf->l4_flex_pit_list);
 
 	pf->fdir_pf_active_filters = 0;
-	pf->fd_tcp4_filter_cnt = 0;
-	pf->fd_udp4_filter_cnt = 0;
-	pf->fd_sctp4_filter_cnt = 0;
-	pf->fd_ip4_filter_cnt = 0;
+	i40e_reset_fdir_filter_cnt(pf);
 
 	/* Reprogram the default input set for TCP/IPv4 */
 	i40e_write_fd_input_set(pf, I40E_FILTER_PCTYPE_NONF_IPV4_TCP,
 				I40E_L3_SRC_MASK | I40E_L3_DST_MASK |
 				I40E_L4_SRC_MASK | I40E_L4_DST_MASK);
 
+	/* Reprogram the default input set for TCP/IPv6 */
+	i40e_write_fd_input_set(pf, I40E_FILTER_PCTYPE_NONF_IPV6_TCP,
+				I40E_L3_V6_SRC_MASK | I40E_L3_V6_DST_MASK |
+				I40E_L4_SRC_MASK | I40E_L4_DST_MASK);
+
 	/* Reprogram the default input set for UDP/IPv4 */
 	i40e_write_fd_input_set(pf, I40E_FILTER_PCTYPE_NONF_IPV4_UDP,
 				I40E_L3_SRC_MASK | I40E_L3_DST_MASK |
 				I40E_L4_SRC_MASK | I40E_L4_DST_MASK);
 
+	/* Reprogram the default input set for UDP/IPv6 */
+	i40e_write_fd_input_set(pf, I40E_FILTER_PCTYPE_NONF_IPV6_UDP,
+				I40E_L3_V6_SRC_MASK | I40E_L3_V6_DST_MASK |
+				I40E_L4_SRC_MASK | I40E_L4_DST_MASK);
+
 	/* Reprogram the default input set for SCTP/IPv4 */
 	i40e_write_fd_input_set(pf, I40E_FILTER_PCTYPE_NONF_IPV4_SCTP,
 				I40E_L3_SRC_MASK | I40E_L3_DST_MASK |
 				I40E_L4_SRC_MASK | I40E_L4_DST_MASK);
 
+	/* Reprogram the default input set for SCTP/IPv6 */
+	i40e_write_fd_input_set(pf, I40E_FILTER_PCTYPE_NONF_IPV6_SCTP,
+				I40E_L3_V6_SRC_MASK | I40E_L3_V6_DST_MASK |
+				I40E_L4_SRC_MASK | I40E_L4_DST_MASK);
+
 	/* Reprogram the default input set for Other/IPv4 */
 	i40e_write_fd_input_set(pf, I40E_FILTER_PCTYPE_NONF_IPV4_OTHER,
 				I40E_L3_SRC_MASK | I40E_L3_DST_MASK);
 
 	i40e_write_fd_input_set(pf, I40E_FILTER_PCTYPE_FRAG_IPV4,
 				I40E_L3_SRC_MASK | I40E_L3_DST_MASK);
+
+	/* Reprogram the default input set for Other/IPv6 */
+	i40e_write_fd_input_set(pf, I40E_FILTER_PCTYPE_NONF_IPV6_OTHER,
+				I40E_L3_SRC_MASK | I40E_L3_DST_MASK);
+
+	i40e_write_fd_input_set(pf, I40E_FILTER_PCTYPE_FRAG_IPV6,
+				I40E_L3_SRC_MASK | I40E_L3_DST_MASK);
 }
 
 /**
- * i40e_cloud_filter_exit - Cleans up the cloud filters
+ * i40e_cloud_filter_exit - Cleans up Cloud Filters
  * @pf: Pointer to PF
  *
- * This function destroys the hlist where all the cloud filters
- * were saved.
+ * This function destroys the hlist which keeps all the Cloud Filters.
  **/
 static void i40e_cloud_filter_exit(struct i40e_pf *pf)
 {
@@ -8478,12 +9743,17 @@
 		 */
 		dev_dbg(&pf->pdev->dev, "PFR requested\n");
 		i40e_handle_reset_warning(pf, lock_acquired);
-
+	} else if (reset_flags & I40E_PF_RESET_AND_REBUILD_FLAG) {
+		/* Request a PF Reset
+		 *
+		 * Resets PF and reinitializes PFs VSI.
+		 */
+		i40e_prep_for_reset(pf);
+		i40e_reset_and_rebuild(pf, true, lock_acquired);
 		dev_info(&pf->pdev->dev,
 			 pf->flags & I40E_FLAG_DISABLE_FW_LLDP ?
 			 "FW LLDP is disabled\n" :
 			 "FW LLDP is enabled\n");
-
 	} else if (reset_flags & BIT_ULL(__I40E_REINIT_REQUESTED)) {
 		int v;
 
@@ -8519,7 +9789,20 @@
 	}
 }
 
-#ifdef CONFIG_I40E_DCB
+/**
+ * i40e_do_reset_safe - Protected reset path for userland calls.
+ * @pf: board private structure
+ * @reset_flags: which reset is requested
+ *
+ **/
+void i40e_do_reset_safe(struct i40e_pf *pf, u32 reset_flags)
+{
+	rtnl_lock();
+	i40e_do_reset(pf, reset_flags, true);
+	rtnl_unlock();
+}
+
+#ifdef CONFIG_DCB
 /**
  * i40e_dcb_need_reconfig - Check if DCB needs reconfig
  * @pf: board private structure
@@ -8591,6 +9874,14 @@
 	int ret = 0;
 	u8 type;
 
+	/* X710-T*L 2.5G and 5G speeds don't support DCB */
+	if (I40E_IS_X710TL_DEVICE(hw->device_id) &&
+	    (hw->phy.link_info.link_speed &
+	     ~(I40E_LINK_SPEED_2_5GB | I40E_LINK_SPEED_5GB)) &&
+	     !(pf->flags & I40E_FLAG_DCB_CAPABLE))
+		/* let firmware decide if the DCB should be disabled */
+		pf->flags |= I40E_FLAG_DCB_CAPABLE;
+
 	/* Not DCB capable or capability disabled */
 	if (!(pf->flags & I40E_FLAG_DCB_CAPABLE))
 		return ret;
@@ -8622,10 +9913,20 @@
 	/* Get updated DCBX data from firmware */
 	ret = i40e_get_dcb_config(&pf->hw);
 	if (ret) {
-		dev_info(&pf->pdev->dev,
-			 "Failed querying DCB configuration data from firmware, err %s aq_err %s\n",
-			 i40e_stat_str(&pf->hw, ret),
-			 i40e_aq_str(&pf->hw, pf->hw.aq.asq_last_status));
+		/* X710-T*L 2.5G and 5G speeds don't support DCB */
+		if (I40E_IS_X710TL_DEVICE(hw->device_id) &&
+		    (hw->phy.link_info.link_speed &
+		     (I40E_LINK_SPEED_2_5GB | I40E_LINK_SPEED_5GB))) {
+			dev_warn(&pf->pdev->dev,
+				 "DCB is not supported for X710-T*L 2.5/5G speeds\n");
+			pf->flags &= ~I40E_FLAG_DCB_CAPABLE;
+		} else {
+			dev_info(&pf->pdev->dev,
+				 "Failed querying DCB configuration data from firmware, err %s aq_err %s\n",
+				 i40e_stat_str(&pf->hw, ret),
+				 i40e_aq_str(&pf->hw,
+					     pf->hw.aq.asq_last_status));
+		}
 		goto exit;
 	}
 
@@ -8639,7 +9940,9 @@
 	need_reconfig = i40e_dcb_need_reconfig(pf, &tmp_dcbx_cfg,
 					       &hw->local_dcbx_config);
 
+#ifdef HAVE_DCBNL_IEEE
 	i40e_dcbnl_flush_apps(pf, &tmp_dcbx_cfg, &hw->local_dcbx_config);
+#endif /* HAVE_DCBNL_IEEE */
 
 	if (!need_reconfig)
 		goto exit;
@@ -8672,28 +9975,15 @@
 		i40e_service_event_schedule(pf);
 	} else {
 		i40e_pf_unquiesce_all_vsi(pf);
-		set_bit(__I40E_CLIENT_SERVICE_REQUESTED, pf->state);
-		set_bit(__I40E_CLIENT_L2_CHANGE, pf->state);
+	set_bit(__I40E_CLIENT_SERVICE_REQUESTED, pf->state);
+	set_bit(__I40E_CLIENT_L2_CHANGE, pf->state);
 	}
 
 exit:
 	return ret;
 }
-#endif /* CONFIG_I40E_DCB */
-
-/**
- * i40e_do_reset_safe - Protected reset path for userland calls.
- * @pf: board private structure
- * @reset_flags: which reset is requested
- *
- **/
-void i40e_do_reset_safe(struct i40e_pf *pf, u32 reset_flags)
-{
-	rtnl_lock();
-	i40e_do_reset(pf, reset_flags, true);
-	rtnl_unlock();
-}
 
+#endif /* CONFIG_DCB */
 /**
  * i40e_handle_lan_overflow_event - Handler for LAN queue overflow event
  * @pf: board private structure
@@ -8829,8 +10119,17 @@
 	case SCTP_V4_FLOW:
 		pf->fd_sctp4_filter_cnt--;
 		break;
+	case TCP_V6_FLOW:
+		pf->fd_tcp6_filter_cnt--;
+		break;
+	case UDP_V6_FLOW:
+		pf->fd_udp6_filter_cnt--;
+		break;
+	case SCTP_V6_FLOW:
+		pf->fd_udp6_filter_cnt--;
+		break;
 	case IP_USER_FLOW:
-		switch (filter->ip4_proto) {
+		switch (filter->ipl4_proto) {
 		case IPPROTO_TCP:
 			pf->fd_tcp4_filter_cnt--;
 			break;
@@ -8845,6 +10144,24 @@
 			break;
 		}
 		break;
+#ifdef HAVE_ETHTOOL_FLOW_UNION_IP6_SPEC
+	case IPV6_USER_FLOW:
+		switch (filter->ipl4_proto) {
+		case IPPROTO_TCP:
+			pf->fd_tcp6_filter_cnt--;
+			break;
+		case IPPROTO_UDP:
+			pf->fd_udp6_filter_cnt--;
+			break;
+		case IPPROTO_SCTP:
+			pf->fd_sctp6_filter_cnt--;
+			break;
+		case IPPROTO_IP:
+			pf->fd_ip6_filter_cnt--;
+			break;
+		}
+		break;
+#endif /* HAVE_ETHTOOL_FLOW_UNION_IP6_SPEC */
 	}
 
 	/* Remove the filter from the list and free memory */
@@ -8878,7 +10195,7 @@
 	 * rules active.
 	 */
 	if ((fcnt_prog < (fcnt_avail - I40E_FDIR_BUFFER_HEAD_ROOM_FOR_ATR)) &&
-	    (pf->fd_tcp4_filter_cnt == 0))
+	    (pf->fd_tcp4_filter_cnt == 0) && (pf->fd_tcp6_filter_cnt == 0))
 		i40e_reenable_fdir_atr(pf);
 
 	/* if hw had a problem adding a filter, delete it */
@@ -8937,6 +10254,7 @@
 		if (!(reg & I40E_PFQF_CTL_1_CLEARFDTABLE_MASK))
 			break;
 	} while (flush_wait_retry--);
+
 	if (reg & I40E_PFQF_CTL_1_CLEARFDTABLE_MASK) {
 		dev_warn(&pf->pdev->dev, "FD table did not flush, needs more time\n");
 	} else {
@@ -8951,7 +10269,7 @@
 }
 
 /**
- * i40e_get_current_atr_count - Get the count of total FD ATR filters programmed
+ * i40e_get_current_atr_cnt - Get the count of total FD ATR filters programmed
  * @pf: board private structure
  **/
 u32 i40e_get_current_atr_cnt(struct i40e_pf *pf)
@@ -8965,7 +10283,6 @@
  * reacting will make sure we don't cause flush too often.
  */
 #define I40E_MAX_FD_PROGRAM_ERROR 256
-
 /**
  * i40e_fdir_reinit_subtask - Worker thread to reinit FDIR filter table
  * @pf: board private structure
@@ -8991,7 +10308,7 @@
  **/
 static void i40e_vsi_link_event(struct i40e_vsi *vsi, bool link_up)
 {
-	if (!vsi || test_bit(__I40E_VSI_DOWN, vsi->state))
+	if (!vsi || (test_bit(__I40E_VSI_DOWN, vsi->state)))
 		return;
 
 	switch (vsi->type) {
@@ -9094,9 +10411,42 @@
 
 	if (pf->vf)
 		i40e_vc_notify_link_state(pf);
+#ifdef HAVE_PTP_1588_CLOCK
 
 	if (pf->flags & I40E_FLAG_PTP)
 		i40e_ptp_set_increment(pf);
+#endif /* HAVE_PTP_1588_CLOCK */
+#ifdef CONFIG_DCB
+	if (new_link == old_link)
+		return;
+	/* Not SW DCB so firmware will take care of default settings */
+	if (pf->dcbx_cap & DCB_CAP_DCBX_LLD_MANAGED)
+		return;
+
+	/* We cover here only link down, as after link up in case of SW DCB
+	 * SW LLDP agent will take care of setting it up
+	 */
+	if (!new_link) {
+		int err;
+		dev_dbg(&pf->pdev->dev, "Reconfig DCB to single TC as result of Link Down\n");
+		memset(&pf->tmp_cfg, 0, sizeof(pf->tmp_cfg));
+		if (pf->flags & I40E_FLAG_MULTIPLE_TRAFFIC_CLASSES)
+			err = i40e_dcb_sw_default_config
+				(pf, I40E_ETS_NON_WILLING_MODE);
+		else
+			err = i40e_dcb_sw_default_config
+				(pf, I40E_ETS_WILLING_MODE);
+		if (err) {
+			pf->flags &= ~(I40E_FLAG_DCB_CAPABLE |
+				       I40E_FLAG_DCB_ENABLED);
+		} else {
+			pf->dcbx_cap = DCB_CAP_DCBX_HOST |
+				       DCB_CAP_DCBX_VER_IEEE;
+			pf->flags |= I40E_FLAG_DCB_CAPABLE;
+			pf->flags &= ~I40E_FLAG_DCB_ENABLED;
+		}
+	}
+#endif /* CONFIG_DCB */
 }
 
 /**
@@ -9135,9 +10485,11 @@
 			if (pf->veb[i])
 				i40e_update_veb_stats(pf->veb[i]);
 	}
+#ifdef HAVE_PTP_1588_CLOCK
 
 	i40e_ptp_rx_hang(pf);
 	i40e_ptp_tx_hang(pf);
+#endif /* HAVE_PTP_1588_CLOCK */
 }
 
 /**
@@ -9148,32 +10500,22 @@
 {
 	u32 reset_flags = 0;
 
-	if (test_bit(__I40E_REINIT_REQUESTED, pf->state)) {
+	if (test_and_clear_bit(__I40E_REINIT_REQUESTED, pf->state))
 		reset_flags |= BIT(__I40E_REINIT_REQUESTED);
-		clear_bit(__I40E_REINIT_REQUESTED, pf->state);
-	}
-	if (test_bit(__I40E_PF_RESET_REQUESTED, pf->state)) {
+	if (test_and_clear_bit(__I40E_PF_RESET_REQUESTED, pf->state))
 		reset_flags |= BIT(__I40E_PF_RESET_REQUESTED);
-		clear_bit(__I40E_PF_RESET_REQUESTED, pf->state);
-	}
-	if (test_bit(__I40E_CORE_RESET_REQUESTED, pf->state)) {
+	if (test_and_clear_bit(__I40E_CORE_RESET_REQUESTED, pf->state))
 		reset_flags |= BIT(__I40E_CORE_RESET_REQUESTED);
-		clear_bit(__I40E_CORE_RESET_REQUESTED, pf->state);
-	}
-	if (test_bit(__I40E_GLOBAL_RESET_REQUESTED, pf->state)) {
+	if (test_and_clear_bit(__I40E_GLOBAL_RESET_REQUESTED, pf->state))
 		reset_flags |= BIT(__I40E_GLOBAL_RESET_REQUESTED);
-		clear_bit(__I40E_GLOBAL_RESET_REQUESTED, pf->state);
-	}
-	if (test_bit(__I40E_DOWN_REQUESTED, pf->state)) {
+	if (test_and_clear_bit(__I40E_DOWN_REQUESTED, pf->state))
 		reset_flags |= BIT(__I40E_DOWN_REQUESTED);
-		clear_bit(__I40E_DOWN_REQUESTED, pf->state);
-	}
 
 	/* If there's a recovery already waiting, it takes
 	 * precedence before starting a new reset sequence.
 	 */
 	if (test_bit(__I40E_RESET_INTR_RECEIVED, pf->state)) {
-		i40e_prep_for_reset(pf, false);
+		i40e_prep_for_reset(pf);
 		i40e_reset(pf);
 		i40e_rebuild(pf, false, false);
 	}
@@ -9220,7 +10562,7 @@
 		    (!(status->link_info & I40E_AQ_LINK_UP)) &&
 		    (!(pf->flags & I40E_FLAG_LINK_DOWN_ON_CLOSE_ENABLED))) {
 			dev_err(&pf->pdev->dev,
-				"Rx/Tx is disabled on this device because an unsupported SFP module type was detected.\n");
+				"Rx/Tx is disabled on this device because an unsupported SFP+ module type was detected.\n");
 			dev_err(&pf->pdev->dev,
 				"Refer to the Intel(R) Ethernet Adapters and Devices User Guide for a list of supported modules.\n");
 		}
@@ -9301,11 +10643,13 @@
 			break;
 		}
 
-		opcode = le16_to_cpu(event.desc.opcode);
+		opcode = LE16_TO_CPU(event.desc.opcode);
 		switch (opcode) {
 
 		case i40e_aqc_opc_get_link_status:
+			rtnl_lock();
 			i40e_handle_link_event(pf, &event);
+			rtnl_unlock();
 			break;
 		case i40e_aqc_opc_send_msg_to_pf:
 			ret = i40e_vc_process_vf_msg(pf,
@@ -9317,11 +10661,11 @@
 			break;
 		case i40e_aqc_opc_lldp_update_mib:
 			dev_dbg(&pf->pdev->dev, "ARQ: Update LLDP MIB event received\n");
-#ifdef CONFIG_I40E_DCB
+#ifdef CONFIG_DCB
 			rtnl_lock();
 			ret = i40e_handle_lldp_event(pf, &event);
 			rtnl_unlock();
-#endif /* CONFIG_I40E_DCB */
+#endif /* CONFIG_DCB */
 			break;
 		case i40e_aqc_opc_event_lan_overflow:
 			dev_dbg(&pf->pdev->dev, "ARQ LAN queue overflow event received\n");
@@ -9466,6 +10810,7 @@
 {
 	struct i40e_pf *pf = veb->pf;
 
+#ifdef HAVE_BRIDGE_ATTRIBS
 	if (pf->hw.debug_mask & I40E_DEBUG_LAN)
 		dev_info(&pf->pdev->dev, "enabling bridge mode: %s\n",
 			 veb->bridge_mode == BRIDGE_MODE_VEPA ? "VEPA" : "VEB");
@@ -9473,6 +10818,12 @@
 		i40e_disable_pf_switch_lb(pf);
 	else
 		i40e_enable_pf_switch_lb(pf);
+#else
+	if (pf->flags & I40E_FLAG_VEB_MODE_ENABLED)
+		i40e_enable_pf_switch_lb(pf);
+	else
+		i40e_disable_pf_switch_lb(pf);
+#endif
 }
 
 /**
@@ -9522,10 +10873,12 @@
 	if (ret)
 		goto end_reconstitute;
 
+#ifdef HAVE_BRIDGE_ATTRIBS
 	if (pf->flags & I40E_FLAG_VEB_MODE_ENABLED)
 		veb->bridge_mode = BRIDGE_MODE_VEB;
 	else
 		veb->bridge_mode = BRIDGE_MODE_VEPA;
+#endif
 	i40e_config_bridge_mode(veb);
 
 	/* create the remaining VSIs attached to this VEB */
@@ -9561,10 +10914,10 @@
 end_reconstitute:
 	return ret;
 }
-
 /**
  * i40e_get_capabilities - get info about the HW
  * @pf: the PF struct
+ * @list_type: admin queue opcode list type
  **/
 static int i40e_get_capabilities(struct i40e_pf *pf,
 				 enum i40e_admin_queue_opc list_type)
@@ -9627,17 +10980,7 @@
 				 pf->hw.dev_caps.num_tx_qp);
 		}
 	}
-	if (list_type == i40e_aqc_opc_list_func_capabilities) {
-#define DEF_NUM_VSI (1 + (pf->hw.func_caps.fcoe ? 1 : 0) \
-		       + pf->hw.func_caps.num_vfs)
-		if (pf->hw.revision_id == 0 &&
-		    pf->hw.func_caps.num_vsis < DEF_NUM_VSI) {
-			dev_info(&pf->pdev->dev,
-				 "got num_vsis %d, setting num_vsis to %d\n",
-				 pf->hw.func_caps.num_vsis, DEF_NUM_VSI);
-			pf->hw.func_caps.num_vsis = DEF_NUM_VSI;
-		}
-	}
+
 	return 0;
 }
 
@@ -9651,21 +10994,6 @@
 {
 	struct i40e_vsi *vsi;
 
-	/* quick workaround for an NVM issue that leaves a critical register
-	 * uninitialized
-	 */
-	if (!rd32(&pf->hw, I40E_GLQF_HKEY(0))) {
-		static const u32 hkey[] = {
-			0xe640d33f, 0xcdfe98ab, 0x73fa7161, 0x0d7a7d36,
-			0xeacb7d61, 0xaa4f05b6, 0x9c5c89ed, 0xfc425ddb,
-			0xa4654832, 0xfc7461d4, 0x8f827619, 0xf5c63c21,
-			0x95b3a76d};
-		int i;
-
-		for (i = 0; i <= I40E_GLQF_HKEY_MAX_INDEX; i++)
-			wr32(&pf->hw, I40E_GLQF_HKEY(i), hkey[i]);
-	}
-
 	if (!(pf->flags & I40E_FLAG_FD_SB_ENABLED))
 		return;
 
@@ -9696,123 +11024,22 @@
 	struct i40e_vsi *vsi;
 
 	i40e_fdir_filter_exit(pf);
+	i40e_cloud_filter_exit(pf);
 	vsi = i40e_find_vsi_by_type(pf, I40E_VSI_FDIR);
 	if (vsi)
 		i40e_vsi_release(vsi);
 }
 
 /**
- * i40e_rebuild_cloud_filters - Rebuilds cloud filters for VSIs
- * @vsi: PF main vsi
- * @seid: seid of main or channel VSIs
- *
- * Rebuilds cloud filters associated with main VSI and channel VSIs if they
- * existed before reset
- **/
-static int i40e_rebuild_cloud_filters(struct i40e_vsi *vsi, u16 seid)
-{
-	struct i40e_cloud_filter *cfilter;
-	struct i40e_pf *pf = vsi->back;
-	struct hlist_node *node;
-	i40e_status ret;
-
-	/* Add cloud filters back if they exist */
-	hlist_for_each_entry_safe(cfilter, node, &pf->cloud_filter_list,
-				  cloud_node) {
-		if (cfilter->seid != seid)
-			continue;
-
-		if (cfilter->dst_port)
-			ret = i40e_add_del_cloud_filter_big_buf(vsi, cfilter,
-								true);
-		else
-			ret = i40e_add_del_cloud_filter(vsi, cfilter, true);
-
-		if (ret) {
-			dev_dbg(&pf->pdev->dev,
-				"Failed to rebuild cloud filter, err %s aq_err %s\n",
-				i40e_stat_str(&pf->hw, ret),
-				i40e_aq_str(&pf->hw,
-					    pf->hw.aq.asq_last_status));
-			return ret;
-		}
-	}
-	return 0;
-}
-
-/**
- * i40e_rebuild_channels - Rebuilds channel VSIs if they existed before reset
- * @vsi: PF main vsi
- *
- * Rebuilds channel VSIs if they existed before reset
- **/
-static int i40e_rebuild_channels(struct i40e_vsi *vsi)
-{
-	struct i40e_channel *ch, *ch_tmp;
-	i40e_status ret;
-
-	if (list_empty(&vsi->ch_list))
-		return 0;
-
-	list_for_each_entry_safe(ch, ch_tmp, &vsi->ch_list, list) {
-		if (!ch->initialized)
-			break;
-		/* Proceed with creation of channel (VMDq2) VSI */
-		ret = i40e_add_channel(vsi->back, vsi->uplink_seid, ch);
-		if (ret) {
-			dev_info(&vsi->back->pdev->dev,
-				 "failed to rebuild channels using uplink_seid %u\n",
-				 vsi->uplink_seid);
-			return ret;
-		}
-		/* Reconfigure TX queues using QTX_CTL register */
-		ret = i40e_channel_config_tx_ring(vsi->back, vsi, ch);
-		if (ret) {
-			dev_info(&vsi->back->pdev->dev,
-				 "failed to configure TX rings for channel %u\n",
-				 ch->seid);
-			return ret;
-		}
-		/* update 'next_base_queue' */
-		vsi->next_base_queue = vsi->next_base_queue +
-							ch->num_queue_pairs;
-		if (ch->max_tx_rate) {
-			u64 credits = ch->max_tx_rate;
-
-			if (i40e_set_bw_limit(vsi, ch->seid,
-					      ch->max_tx_rate))
-				return -EINVAL;
-
-			do_div(credits, I40E_BW_CREDIT_DIVISOR);
-			dev_dbg(&vsi->back->pdev->dev,
-				"Set tx rate of %llu Mbps (count of 50Mbps %llu) for vsi->seid %u\n",
-				ch->max_tx_rate,
-				credits,
-				ch->seid);
-		}
-		ret = i40e_rebuild_cloud_filters(vsi, ch->seid);
-		if (ret) {
-			dev_dbg(&vsi->back->pdev->dev,
-				"Failed to rebuild cloud filters for channel VSI %u\n",
-				ch->seid);
-			return ret;
-		}
-	}
-	return 0;
-}
-
-/**
  * i40e_prep_for_reset - prep for the core to reset
  * @pf: board private structure
- * @lock_acquired: indicates whether or not the lock has been acquired
- * before this function was called.
  *
  * Close up the VFs and other things in prep for PF Reset.
   **/
-static void i40e_prep_for_reset(struct i40e_pf *pf, bool lock_acquired)
+static void i40e_prep_for_reset(struct i40e_pf *pf)
 {
+	i40e_status ret = I40E_SUCCESS;
 	struct i40e_hw *hw = &pf->hw;
-	i40e_status ret = 0;
 	u32 v;
 
 	clear_bit(__I40E_RESET_INTR_RECEIVED, pf->state);
@@ -9823,13 +11050,9 @@
 
 	dev_dbg(&pf->pdev->dev, "Tearing down internal switch for reset\n");
 
+	/* TODO: warn any registered clients */
 	/* quiesce the VSIs and their queues that are not already DOWN */
-	/* pf_quiesce_all_vsi modifies netdev structures -rtnl_lock needed */
-	if (!lock_acquired)
-		rtnl_lock();
 	i40e_pf_quiesce_all_vsi(pf);
-	if (!lock_acquired)
-		rtnl_unlock();
 
 	for (v = 0; v < pf->num_alloc_vsi; v++) {
 		if (pf->vsi[v])
@@ -9846,10 +11069,12 @@
 				 "shutdown_lan_hmc failed: %d\n", ret);
 	}
 
+#ifdef HAVE_PTP_1588_CLOCK
 	/* Save the current PTP time so that we can restore the time after the
 	 * reset completes.
 	 */
 	i40e_ptp_save_hw_time(pf);
+#endif /* HAVE_PTP_1588_CLOCK */
 }
 
 /**
@@ -9863,7 +11088,7 @@
 	dv.major_version = DRV_VERSION_MAJOR;
 	dv.minor_version = DRV_VERSION_MINOR;
 	dv.build_version = DRV_VERSION_BUILD;
-	dv.subbuild_version = 0;
+	dv.subbuild_version = DRV_VERSION_SUBBUILD;
 	strlcpy(dv.driver_string, DRV_VERSION, sizeof(dv.driver_string));
 	i40e_aq_send_driver_version(&pf->hw, &dv, NULL);
 }
@@ -9890,7 +11115,7 @@
 
 	/* Check if pointer to OEM version block is valid. */
 	i40e_read_nvm_word(hw, I40E_SR_NVM_OEM_VERSION_PTR, &block_offset);
-	if (block_offset == 0xffff)
+	if ((block_offset & 0x7fff) == 0x7fff)
 		return;
 
 	/* Check if OEM version block has correct length. */
@@ -9942,22 +11167,21 @@
  **/
 static void i40e_rebuild(struct i40e_pf *pf, bool reinit, bool lock_acquired)
 {
-	int old_recovery_mode_bit = test_bit(__I40E_RECOVERY_MODE, pf->state);
+	const bool is_recovery_mode_reported = i40e_check_recovery_mode(pf);
 	struct i40e_vsi *vsi = pf->vsi[pf->lan_vsi];
 	struct i40e_hw *hw = &pf->hw;
-	u8 set_fc_aq_fail = 0;
 	i40e_status ret;
 	u32 val;
 	int v;
 
+#ifdef SIOCETHTOOL
 	if (test_bit(__I40E_EMP_RESET_INTR_RECEIVED, pf->state) &&
-	    i40e_check_recovery_mode(pf)) {
+	    is_recovery_mode_reported)
 		i40e_set_ethtool_ops(pf->vsi[pf->lan_vsi]->netdev);
-	}
+#endif
 
 	if (test_bit(__I40E_DOWN, pf->state) &&
-	    !test_bit(__I40E_RECOVERY_MODE, pf->state) &&
-	    !old_recovery_mode_bit)
+	    !test_bit(__I40E_RECOVERY_MODE, pf->state))
 		goto clear_recovery;
 	dev_dbg(&pf->pdev->dev, "Rebuilding internal switch\n");
 
@@ -9971,32 +11195,24 @@
 	}
 	i40e_get_oem_version(&pf->hw);
 
-	if (test_bit(__I40E_EMP_RESET_INTR_RECEIVED, pf->state) &&
-	    ((hw->aq.fw_maj_ver == 4 && hw->aq.fw_min_ver <= 33) ||
-	     hw->aq.fw_maj_ver < 4) && hw->mac.type == I40E_MAC_XL710) {
-		/* The following delay is necessary for 4.33 firmware and older
-		 * to recover after EMP reset. 200 ms should suffice but we
-		 * put here 300 ms to be sure that FW is ready to operate
-		 * after reset.
-		 */
-		mdelay(300);
-	}
-
 	/* re-verify the eeprom if we just had an EMP reset */
-	if (test_and_clear_bit(__I40E_EMP_RESET_INTR_RECEIVED, pf->state))
+	if (test_and_clear_bit(__I40E_EMP_RESET_INTR_RECEIVED, pf->state)) {
+		/* The following delay is necessary for firmware update. */
+		mdelay(1000);
+		dev_info(&pf->pdev->dev, "Reset Requested! (EMPR)\n");
 		i40e_verify_eeprom(pf);
+	}
 
 	/* if we are going out of or into recovery mode we have to act
 	 * accordingly with regard to resources initialization
 	 * and deinitialization
 	 */
-	if (test_bit(__I40E_RECOVERY_MODE, pf->state) ||
-	    old_recovery_mode_bit) {
+	if (test_bit(__I40E_RECOVERY_MODE, pf->state)) {
 		if (i40e_get_capabilities(pf,
 					  i40e_aqc_opc_list_func_capabilities))
 			goto end_unlock;
 
-		if (test_bit(__I40E_RECOVERY_MODE, pf->state)) {
+		if (is_recovery_mode_reported) {
 			/* we're staying in recovery mode so we'll reinitialize
 			 * misc vector here
 			 */
@@ -10041,17 +11257,24 @@
 		goto end_core_reset;
 	}
 
-	/* Enable FW to write a default DCB config on link-up */
-	i40e_aq_set_dcb_parameters(hw, true, NULL);
-
-#ifdef CONFIG_I40E_DCB
-	ret = i40e_init_pf_dcb(pf);
-	if (ret) {
-		dev_info(&pf->pdev->dev, "DCB init failed %d, disabled\n", ret);
-		pf->flags &= ~I40E_FLAG_DCB_CAPABLE;
-		/* Continue without DCB enabled */
+#ifdef CONFIG_DCB
+	/* Enable FW to write a default DCB config on link-up
+	 * unless I40E_FLAG_TC_MQPRIO is enabled
+	 */
+	if (pf->flags & I40E_FLAG_TC_MQPRIO) {
+		i40e_aq_set_dcb_parameters(hw, false, NULL);
+	} else {
+		i40e_aq_set_dcb_parameters(hw, true, NULL);
+		ret = i40e_init_pf_dcb(pf);
+		if (ret) {
+			dev_info(&pf->pdev->dev, "DCB init failed %d, disabled\n",
+				 ret);
+			pf->flags &= ~I40E_FLAG_DCB_CAPABLE;
+			/* Continue without DCB enabled */
+		}
 	}
-#endif /* CONFIG_I40E_DCB */
+
+#endif /* CONFIG_DCB */
 	/* do basic switch setup */
 	if (!lock_acquired)
 		rtnl_lock();
@@ -10070,14 +11293,6 @@
 		dev_info(&pf->pdev->dev, "set phy mask fail, err %s aq_err %s\n",
 			 i40e_stat_str(&pf->hw, ret),
 			 i40e_aq_str(&pf->hw, pf->hw.aq.asq_last_status));
-
-	/* make sure our flow control settings are restored */
-	ret = i40e_set_fc(&pf->hw, &set_fc_aq_fail, true);
-	if (ret)
-		dev_dbg(&pf->pdev->dev, "setting flow control: ret = %s last_status = %s\n",
-			i40e_stat_str(&pf->hw, ret),
-			i40e_aq_str(&pf->hw, pf->hw.aq.asq_last_status));
-
 	/* Rebuild the VSIs and VEBs that existed before reset.
 	 * They are still in our local switch element arrays, so only
 	 * need to rebuild the switch model in the HW.
@@ -10131,34 +11346,33 @@
 		}
 	}
 
+#ifdef __TC_MQPRIO_MODE_MAX
 	if (vsi->mqprio_qopt.max_rate[0]) {
-		u64 max_tx_rate = vsi->mqprio_qopt.max_rate[0];
-		u64 credits = 0;
+		u64 max_tx_rate = vsi->mqprio_qopt.max_rate[0] / (1000000 / 8);
 
-		do_div(max_tx_rate, I40E_BW_MBPS_DIVISOR);
 		ret = i40e_set_bw_limit(vsi, vsi->seid, max_tx_rate);
-		if (ret)
+		if (!ret)
+			dev_dbg(&vsi->back->pdev->dev,
+				"Set tx rate of %llu Mbps (count of 50Mbps %llu) for vsi->seid %u\n",
+				max_tx_rate,
+				max_tx_rate / I40E_BW_CREDIT_DIVISOR,
+				vsi->seid);
+		else
 			goto end_unlock;
-
-		credits = max_tx_rate;
-		do_div(credits, I40E_BW_CREDIT_DIVISOR);
-		dev_dbg(&vsi->back->pdev->dev,
-			"Set tx rate of %llu Mbps (count of 50Mbps %llu) for vsi->seid %u\n",
-			max_tx_rate,
-			credits,
-			vsi->seid);
 	}
+#endif
 
-	ret = i40e_rebuild_cloud_filters(vsi, vsi->seid);
-	if (ret)
-		goto end_unlock;
-
-	/* PF Main VSI is rebuild by now, go ahead and rebuild channel VSIs
-	 * for this main VSI if they exist
-	 */
-	ret = i40e_rebuild_channels(vsi);
-	if (ret)
-		goto end_unlock;
+#ifdef __TC_MQPRIO_MODE_MAX
+	/* Not going to support channel VSI in L4 cloud filter mode */
+	if (!i40e_is_l4mode_enabled()) {
+		/* PF Main VSI is rebuild by now, go ahead and
+		 * rebuild channel VSIs for this main VSI if they exist
+		 */
+		ret = i40e_rebuild_channels(vsi);
+		if (ret)
+			goto end_unlock;
+	}
+#endif
 
 	/* Reconfigure hardware for allowing smaller MSS in the case
 	 * of TSO, so that we avoid the MDD being fired and causing
@@ -10211,9 +11425,51 @@
 			 pf->cur_promisc ? "on" : "off",
 			 i40e_stat_str(&pf->hw, ret),
 			 i40e_aq_str(&pf->hw, pf->hw.aq.asq_last_status));
+#ifdef HAVE_NDO_SET_VF_LINK_STATE
+	/* Restore all VF-d config */
+
+	/* If uncommitted changes were made to the BW share settings, then warn
+	 * the user that the configuration may not be restored correctly.
+	 */
+	if (!pf->vf_bw_applied)
+		dev_info(&pf->pdev->dev, "VF BW shares not restored\n");
+
+	if (I40E_IS_MIRROR_VLAN_ID_VALID(pf->ingress_vlan)) {
+		u16 rule_type;
+
+		/* The Admin Queue mirroring rules refer to the traffic
+		 * directions from the perspective of the switch, not the VSI
+		 * we apply the mirroring rule on - so the behaviour of a VSI
+		 * ingress mirror is classified as an egress rule
+		 */
+		rule_type = I40E_AQC_MIRROR_RULE_TYPE_VPORT_EGRESS;
+		ret = i40e_restore_ingress_egress_mirror(vsi, pf->ingress_vlan,
+							 rule_type,
+							 &pf->ingress_rule_id);
+		if (ret)
+			pf->ingress_vlan = I40E_NO_VF_MIRROR;
+	}
+
+	if (I40E_IS_MIRROR_VLAN_ID_VALID(pf->egress_vlan)) {
+		u16 rule_type;
+
+		/* The Admin Queue mirroring rules refer to the traffic
+		 * directions from the perspective of the switch, not the VSI
+		 * we apply the mirroring rule on - so the behaviour of a VSI
+		 * egress mirror is classified as an ingress rule
+		 */
+		rule_type = I40E_AQC_MIRROR_RULE_TYPE_VPORT_INGRESS;
+		ret = i40e_restore_ingress_egress_mirror(vsi, pf->egress_vlan,
+							 rule_type,
+							 &pf->egress_rule_id);
+		if (ret)
+			pf->egress_vlan = I40E_NO_VF_MIRROR;
+	}
+#endif /* HAVE_NDO_SET_VF_LINK_STATE */
 
 	i40e_reset_all_vfs(pf, true);
 
+	/* TODO: restart clients */
 	/* tell the firmware that we're starting */
 	i40e_send_version(pf);
 
@@ -10261,7 +11517,7 @@
  **/
 static void i40e_handle_reset_warning(struct i40e_pf *pf, bool lock_acquired)
 {
-	i40e_prep_for_reset(pf, lock_acquired);
+	i40e_prep_for_reset(pf);
 	i40e_reset_and_rebuild(pf, false, lock_acquired);
 }
 
@@ -10363,6 +11619,8 @@
 	i40e_flush(hw);
 }
 
+#if defined(HAVE_VXLAN_RX_OFFLOAD) || defined(HAVE_UDP_ENC_RX_OFFLOAD)
+#if defined(HAVE_UDP_ENC_TUNNEL) || defined(HAVE_UDP_ENC_RX_OFFLOAD)
 static const char *i40e_tunnel_name(u8 type)
 {
 	switch (type) {
@@ -10412,7 +11670,7 @@
 	for (i = 0; i < I40E_MAX_PF_UDP_OFFLOAD_PORTS; i++) {
 		if (pf->pending_udp_bitmap & BIT_ULL(i)) {
 			struct i40e_udp_port_config *udp_port;
-			i40e_status ret = 0;
+			i40e_status ret = I40E_SUCCESS;
 
 			udp_port = &pf->udp_ports[i];
 			pf->pending_udp_bitmap &= ~BIT_ULL(i);
@@ -10445,7 +11703,7 @@
 					 filter_index,
 					 i40e_stat_str(&pf->hw, ret),
 					 i40e_aq_str(&pf->hw,
-						     pf->hw.aq.asq_last_status));
+						    pf->hw.aq.asq_last_status));
 				if (port) {
 					/* failed to add, just reset port,
 					 * drop pending bit for any deletion
@@ -10463,6 +11721,9 @@
 	rtnl_unlock();
 }
 
+#endif /* HAVE_UDP_ENC_TUNNEL || HAVE_UDP_ENC_RX_OFFLOAD */
+#endif /* HAVE_VXLAN_RX_OFFLOAD || HAVE_UDP_ENC_RX_OFFLOAD */
+
 /**
  * i40e_service_task - Run the driver's async subtasks
  * @work: pointer to work_struct containing our data
@@ -10476,8 +11737,9 @@
 
 	/* don't bother with service tasks if a reset is in progress */
 	if (test_bit(__I40E_RESET_RECOVERY_PENDING, pf->state) ||
-	    test_bit(__I40E_SUSPENDED, pf->state))
+	    test_bit(__I40E_SUSPENDED, pf->state)) {
 		return;
+	}
 
 	if (test_and_set_bit(__I40E_SERVICE_SCHED, pf->state))
 		return;
@@ -10492,17 +11754,21 @@
 		i40e_fdir_reinit_subtask(pf);
 		if (test_and_clear_bit(__I40E_CLIENT_RESET, pf->state)) {
 			/* Client subtask will reopen next time through. */
-			i40e_notify_client_of_netdev_close(pf->vsi[pf->lan_vsi],
-							   true);
+			i40e_notify_client_of_netdev_close(
+						pf->vsi[pf->lan_vsi], true);
 		} else {
 			i40e_client_subtask(pf);
 			if (test_and_clear_bit(__I40E_CLIENT_L2_CHANGE,
 					       pf->state))
 				i40e_notify_client_of_l2_param_changes(
-								pf->vsi[pf->lan_vsi]);
+							pf->vsi[pf->lan_vsi]);
 		}
-		i40e_sync_filters_subtask(pf);
+#if defined(HAVE_VXLAN_RX_OFFLOAD) || defined(HAVE_UDP_ENC_RX_OFFLOAD)
+#if defined(HAVE_UDP_ENC_TUNNEL) || defined(HAVE_UDP_ENC_RX_OFFLOAD)
 		i40e_sync_udp_filters_subtask(pf);
+
+#endif
+#endif /* HAVE_VXLAN_RX_OFFLOAD || HAVE_UDP_ENC_RX_OFFLOAD */
 	} else {
 		i40e_reset_subtask(pf);
 	}
@@ -10526,7 +11792,7 @@
 
 /**
  * i40e_service_timer - timer callback
- * @data: pointer to PF struct
+ * @t: pointer to timer_list struct
  **/
 static void i40e_service_timer(struct timer_list *t)
 {
@@ -10554,8 +11820,9 @@
 		if (!vsi->num_rx_desc)
 			vsi->num_rx_desc = ALIGN(I40E_DEFAULT_NUM_DESCRIPTORS,
 						 I40E_REQ_DESCRIPTOR_MULTIPLE);
-		if (pf->flags & I40E_FLAG_MSIX_ENABLED)
+		if (pf->flags & I40E_FLAG_MSIX_ENABLED) {
 			vsi->num_q_vectors = pf->num_lan_msix;
+		}
 		else
 			vsi->num_q_vectors = 1;
 
@@ -10596,6 +11863,11 @@
 		return -ENODATA;
 	}
 
+	if (is_kdump_kernel()) {
+		vsi->num_tx_desc = I40E_MIN_NUM_DESCRIPTORS;
+		vsi->num_rx_desc = I40E_MIN_NUM_DESCRIPTORS;
+	}
+
 	return 0;
 }
 
@@ -10616,6 +11888,7 @@
 	/* allocate memory for both Tx, XDP Tx and Rx ring pointers */
 	size = sizeof(struct i40e_ring *) * vsi->alloc_queue_pairs *
 	       (i40e_enabled_xdp_vsi(vsi) ? 3 : 2);
+
 	vsi->tx_rings = kzalloc(size, GFP_KERNEL);
 	if (!vsi->tx_rings)
 		return -ENOMEM;
@@ -10650,7 +11923,7 @@
  * On error: returns error code (negative)
  * On success: returns vsi index in PF (positive)
  **/
-static int i40e_vsi_mem_alloc(struct i40e_pf *pf, enum i40e_vsi_type type)
+int i40e_vsi_mem_alloc(struct i40e_pf *pf, enum i40e_vsi_type type)
 {
 	int ret = -ENODEV;
 	struct i40e_vsi *vsi;
@@ -10701,16 +11974,12 @@
 	hash_init(vsi->mac_filter_hash);
 	vsi->irqs_ready = false;
 
-	if (type == I40E_VSI_MAIN) {
-		vsi->af_xdp_zc_qps = bitmap_zalloc(pf->num_lan_qps, GFP_KERNEL);
-		if (!vsi->af_xdp_zc_qps)
-			goto err_rings;
-	}
-
 	ret = i40e_set_num_rings_in_vsi(vsi);
 	if (ret)
 		goto err_rings;
 
+	vsi->block_tx_timeout = false;
+
 	ret = i40e_vsi_alloc_arrays(vsi, true);
 	if (ret)
 		goto err_rings;
@@ -10722,10 +11991,10 @@
 	spin_lock_init(&vsi->mac_filter_hash_lock);
 	pf->vsi[vsi_idx] = vsi;
 	ret = vsi_idx;
+
 	goto unlock_pf;
 
 err_rings:
-	bitmap_free(vsi->af_xdp_zc_qps);
 	pf->next_vsi = i - 1;
 	kfree(vsi);
 unlock_pf:
@@ -10755,23 +12024,6 @@
 }
 
 /**
- * i40e_clear_rss_config_user - clear the user configured RSS hash keys
- * and lookup table
- * @vsi: Pointer to VSI structure
- */
-static void i40e_clear_rss_config_user(struct i40e_vsi *vsi)
-{
-	if (!vsi)
-		return;
-
-	kfree(vsi->rss_hkey_user);
-	vsi->rss_hkey_user = NULL;
-
-	kfree(vsi->rss_lut_user);
-	vsi->rss_lut_user = NULL;
-}
-
-/**
  * i40e_vsi_clear - Deallocate the VSI provided
  * @vsi: the VSI being un-configured
  **/
@@ -10806,7 +12058,6 @@
 	i40e_put_lump(pf->qp_pile, vsi->base_queue, vsi->idx);
 	i40e_put_lump(pf->irq_pile, vsi->base_vector, vsi->idx);
 
-	bitmap_free(vsi->af_xdp_zc_qps);
 	i40e_vsi_free_arrays(vsi, true);
 	i40e_clear_rss_config_user(vsi);
 
@@ -10833,10 +12084,10 @@
 	if (vsi->tx_rings && vsi->tx_rings[0]) {
 		for (i = 0; i < vsi->alloc_queue_pairs; i++) {
 			kfree_rcu(vsi->tx_rings[i], rcu);
-			WRITE_ONCE(vsi->tx_rings[i], NULL);
-			WRITE_ONCE(vsi->rx_rings[i], NULL);
+			vsi->tx_rings[i] = NULL;
+			vsi->rx_rings[i] = NULL;
 			if (vsi->xdp_rings)
-				WRITE_ONCE(vsi->xdp_rings[i], NULL);
+				vsi->xdp_rings[i] = NULL;
 		}
 	}
 }
@@ -10867,10 +12118,11 @@
 		ring->count = vsi->num_tx_desc;
 		ring->size = 0;
 		ring->dcb_tc = 0;
+
 		if (vsi->back->hw_features & I40E_HW_WB_ON_ITR_CAPABLE)
 			ring->flags = I40E_TXR_FLAGS_WB_ON_ITR;
 		ring->itr_setting = pf->tx_itr_default;
-		WRITE_ONCE(vsi->tx_rings[i], ring++);
+		vsi->tx_rings[i] = ring++;
 
 		if (!i40e_enabled_xdp_vsi(vsi))
 			goto setup_rx;
@@ -10888,7 +12140,7 @@
 			ring->flags = I40E_TXR_FLAGS_WB_ON_ITR;
 		set_ring_xdp(ring);
 		ring->itr_setting = pf->tx_itr_default;
-		WRITE_ONCE(vsi->xdp_rings[i], ring++);
+		vsi->xdp_rings[i] = ring++;
 
 setup_rx:
 		ring->queue_index = i;
@@ -10901,7 +12153,7 @@
 		ring->size = 0;
 		ring->dcb_tc = 0;
 		ring->itr_setting = pf->rx_itr_default;
-		WRITE_ONCE(vsi->rx_rings[i], ring);
+		vsi->rx_rings[i] = ring;
 	}
 
 	return 0;
@@ -10910,25 +12162,28 @@
 	i40e_vsi_clear_rings(vsi);
 	return -ENOMEM;
 }
+#if !defined(I40E_LEGACY_INTERRUPT) && !defined(I40E_MSI_INTERRUPT)
 
 /**
  * i40e_reserve_msix_vectors - Reserve MSI-X vectors in the kernel
  * @pf: board private structure
- * @vectors: the number of MSI-X vectors to request
+ * @v_budget: the number of MSI-X vectors to request
  *
  * Returns the number of vectors reserved, or error
  **/
-static int i40e_reserve_msix_vectors(struct i40e_pf *pf, int vectors)
+static int i40e_reserve_msix_vectors(struct i40e_pf *pf, int v_budget)
 {
-	vectors = pci_enable_msix_range(pf->pdev, pf->msix_entries,
-					I40E_MIN_MSIX, vectors);
-	if (vectors < 0) {
+	int v_actual = 0;
+
+	v_actual = pci_enable_msix_range(pf->pdev,
+					 pf->msix_entries,
+					 I40E_MIN_MSIX,
+					 v_budget);
+	if (v_actual < 0)
 		dev_info(&pf->pdev->dev,
-			 "MSI-X vector reservation failed: %d\n", vectors);
-		vectors = 0;
-	}
+			 "MSI-X vector reservation failed: %d\n", v_actual);
 
-	return vectors;
+	return v_actual;
 }
 
 /**
@@ -10954,6 +12209,7 @@
 	/* The number of vectors we'll request will be comprised of:
 	 *   - Add 1 for "other" cause for Admin Queue events, etc.
 	 *   - The number of LAN queue pairs
+	 *	- This takes into account queues for each TC in DCB mode.
 	 *	- Queues being used for RSS.
 	 *		We don't need as many as max_rss_size vectors.
 	 *		use rss_size instead in the calculation since that
@@ -11020,11 +12276,11 @@
 			int vmdq_vecs =
 				min_t(int, vectors_left, vmdq_vecs_wanted);
 
-			/* if we're short on vectors for what's desired, we limit
-			 * the queues per vmdq.  If this is still more than are
-			 * available, the user will need to change the number of
-			 * queues/vectors used by the PF later with the ethtool
-			 * channels command
+			/* if we're short on vectors for what's desired, we
+			 * limit the queues per vmdq.  If this is still more
+			 * than are available, the user will need to change
+			 * the number of queues/vectors used by the PF later
+			 * with the ethtool channels command
 			 */
 			if (vectors_left < vmdq_vecs_wanted) {
 				pf->num_vmdq_qps = 1;
@@ -11065,22 +12321,22 @@
 	for (i = 0; i < v_budget; i++)
 		pf->msix_entries[i].entry = i;
 	v_actual = i40e_reserve_msix_vectors(pf, v_budget);
-
 	if (v_actual < I40E_MIN_MSIX) {
 		pf->flags &= ~I40E_FLAG_MSIX_ENABLED;
 		kfree(pf->msix_entries);
 		pf->msix_entries = NULL;
 		pci_disable_msix(pf->pdev);
 		return -ENODEV;
+	}
 
-	} else if (v_actual == I40E_MIN_MSIX) {
+	if (v_actual == I40E_MIN_MSIX) {
 		/* Adjust for minimal MSIX use */
 		pf->num_vmdq_vsis = 0;
 		pf->num_vmdq_qps = 0;
 		pf->num_lan_qps = 1;
 		pf->num_lan_msix = 1;
 
-	} else if (v_actual != v_budget) {
+	} else if (!vectors_left) {
 		/* If we have limited resources, we will start with no vectors
 		 * for the special features and then allocate vectors to some
 		 * of these features based on the policy and at the end disable
@@ -11089,8 +12345,7 @@
 		int vec;
 
 		dev_info(&pf->pdev->dev,
-			 "MSI-X vector limit reached with %d, wanted %d, attempting to redistribute vectors\n",
-			 v_actual, v_budget);
+			 "MSI-X vector limit reached, attempting to redistribute vectors\n");
 		/* reserve the misc vector */
 		vec = v_actual - 1;
 
@@ -11108,8 +12363,6 @@
 			if (pf->flags & I40E_FLAG_IWARP_ENABLED) {
 				pf->num_lan_msix = 1;
 				pf->num_iwarp_msix = 1;
-			} else {
-				pf->num_lan_msix = 2;
 			}
 			break;
 		default:
@@ -11129,7 +12382,6 @@
 			pf->num_lan_msix = min_t(int,
 			       (vec - (pf->num_iwarp_msix + pf->num_vmdq_vsis)),
 							      pf->num_lan_msix);
-			pf->num_lan_qps = pf->num_lan_msix;
 			break;
 		}
 	}
@@ -11151,25 +12403,18 @@
 		dev_info(&pf->pdev->dev, "IWARP disabled, not enough MSI-X vectors\n");
 		pf->flags &= ~I40E_FLAG_IWARP_ENABLED;
 	}
-	i40e_debug(&pf->hw, I40E_DEBUG_INIT,
-		   "MSI-X vector distribution: PF %d, VMDq %d, FDSB %d, iWARP %d\n",
-		   pf->num_lan_msix,
-		   pf->num_vmdq_msix * pf->num_vmdq_vsis,
-		   pf->num_fdsb_msix,
-		   pf->num_iwarp_msix);
-
 	return v_actual;
 }
+#endif
 
 /**
  * i40e_vsi_alloc_q_vector - Allocate memory for a single interrupt vector
  * @vsi: the VSI being configured
  * @v_idx: index of the vector in the vsi struct
- * @cpu: cpu to be used on affinity_mask
  *
  * We allocate one q_vector.  If allocation fails we return -ENOMEM.
  **/
-static int i40e_vsi_alloc_q_vector(struct i40e_vsi *vsi, int v_idx, int cpu)
+static int i40e_vsi_alloc_q_vector(struct i40e_vsi *vsi, int v_idx)
 {
 	struct i40e_q_vector *q_vector;
 
@@ -11180,8 +12425,9 @@
 
 	q_vector->vsi = vsi;
 	q_vector->v_idx = v_idx;
+#ifdef HAVE_IRQ_AFFINITY_NOTIFY
 	cpumask_copy(&q_vector->affinity_mask, cpu_possible_mask);
-
+#endif
 	if (vsi->netdev)
 		netif_napi_add(vsi->netdev, &q_vector->napi,
 			       i40e_napi_poll, NAPI_POLL_WEIGHT);
@@ -11202,7 +12448,8 @@
 static int i40e_vsi_alloc_q_vectors(struct i40e_vsi *vsi)
 {
 	struct i40e_pf *pf = vsi->back;
-	int err, v_idx, num_q_vectors, current_cpu;
+	int v_idx, num_q_vectors;
+	int err;
 
 	/* if not MSIX, give the one vector only to the LAN VSI */
 	if (pf->flags & I40E_FLAG_MSIX_ENABLED)
@@ -11212,15 +12459,10 @@
 	else
 		return -EINVAL;
 
-	current_cpu = cpumask_first(cpu_online_mask);
-
 	for (v_idx = 0; v_idx < num_q_vectors; v_idx++) {
-		err = i40e_vsi_alloc_q_vector(vsi, v_idx, current_cpu);
+		err = i40e_vsi_alloc_q_vector(vsi, v_idx);
 		if (err)
 			goto err_out;
-		current_cpu = cpumask_next(current_cpu, cpu_online_mask);
-		if (unlikely(current_cpu >= nr_cpu_ids))
-			current_cpu = cpumask_first(cpu_online_mask);
 	}
 
 	return 0;
@@ -11242,7 +12484,11 @@
 	ssize_t size;
 
 	if (pf->flags & I40E_FLAG_MSIX_ENABLED) {
+#if !defined(I40E_LEGACY_INTERRUPT) && !defined(I40E_MSI_INTERRUPT)
 		vectors = i40e_init_msix(pf);
+#else
+		vectors = -1;
+#endif
 		if (vectors < 0) {
 			pf->flags &= ~(I40E_FLAG_MSIX_ENABLED	|
 				       I40E_FLAG_IWARP_ENABLED	|
@@ -11263,7 +12509,11 @@
 	if (!(pf->flags & I40E_FLAG_MSIX_ENABLED) &&
 	    (pf->flags & I40E_FLAG_MSI_ENABLED)) {
 		dev_info(&pf->pdev->dev, "MSI-X not available, trying MSI\n");
+#ifndef I40E_LEGACY_INTERRUPT
 		vectors = pci_enable_msi(pf->pdev);
+#else
+		vectors = -1;
+#endif
 		if (vectors < 0) {
 			dev_info(&pf->pdev->dev, "MSI init failed - %d\n",
 				 vectors);
@@ -11278,71 +12528,19 @@
 	/* set up vector assignment tracking */
 	size = sizeof(struct i40e_lump_tracking) + (sizeof(u16) * vectors);
 	pf->irq_pile = kzalloc(size, GFP_KERNEL);
-	if (!pf->irq_pile)
+	if (!pf->irq_pile) {
+		dev_err(&pf->pdev->dev, "error allocating irq_pile memory\n");
 		return -ENOMEM;
-
+	}
 	pf->irq_pile->num_entries = vectors;
-	pf->irq_pile->search_hint = 0;
 
 	/* track first vector for misc interrupts, ignore return */
-	(void)i40e_get_lump(pf, pf->irq_pile, 1, I40E_PILE_VALID_BIT - 1);
+	(void)i40e_get_lump(pf, pf->irq_pile, 1, I40E_PILE_VALID_BIT-1);
 
 	return 0;
 }
 
 /**
- * i40e_restore_interrupt_scheme - Restore the interrupt scheme
- * @pf: private board data structure
- *
- * Restore the interrupt scheme that was cleared when we suspended the
- * device. This should be called during resume to re-allocate the q_vectors
- * and reacquire IRQs.
- */
-static int i40e_restore_interrupt_scheme(struct i40e_pf *pf)
-{
-	int err, i;
-
-	/* We cleared the MSI and MSI-X flags when disabling the old interrupt
-	 * scheme. We need to re-enabled them here in order to attempt to
-	 * re-acquire the MSI or MSI-X vectors
-	 */
-	pf->flags |= (I40E_FLAG_MSIX_ENABLED | I40E_FLAG_MSI_ENABLED);
-
-	err = i40e_init_interrupt_scheme(pf);
-	if (err)
-		return err;
-
-	/* Now that we've re-acquired IRQs, we need to remap the vectors and
-	 * rings together again.
-	 */
-	for (i = 0; i < pf->num_alloc_vsi; i++) {
-		if (pf->vsi[i]) {
-			err = i40e_vsi_alloc_q_vectors(pf->vsi[i]);
-			if (err)
-				goto err_unwind;
-			i40e_vsi_map_rings_to_vectors(pf->vsi[i]);
-		}
-	}
-
-	err = i40e_setup_misc_vector(pf);
-	if (err)
-		goto err_unwind;
-
-	if (pf->flags & I40E_FLAG_IWARP_ENABLED)
-		i40e_client_update_msix_info(pf);
-
-	return 0;
-
-err_unwind:
-	while (i--) {
-		if (pf->vsi[i])
-			i40e_vsi_free_q_vectors(pf->vsi[i]);
-	}
-
-	return err;
-}
-
-/**
  * i40e_setup_misc_vector_for_recovery_mode - Setup the misc vector to handle
  * non queue events in recovery mode
  * @pf: board private structure
@@ -11424,6 +12622,58 @@
 }
 
 /**
+ * i40e_restore_interrupt_scheme - Restore the interrupt scheme
+ * @pf: private board data structure
+ *
+ * Restore the interrupt scheme that was cleared when we suspended the
+ * device. This should be called during resume to re-allocate the q_vectors
+ * and reacquire IRQs.
+ */
+static int i40e_restore_interrupt_scheme(struct i40e_pf *pf)
+{
+	int err, i;
+
+	/* We cleared the MSI and MSI-X flags when disabling the old interrupt
+	 * scheme. We need to re-enabled them here in order to attempt to
+	 * re-acquire the MSI or MSI-X vectors
+	 */
+	pf->flags |= (I40E_FLAG_MSIX_ENABLED | I40E_FLAG_MSI_ENABLED);
+
+	err = i40e_init_interrupt_scheme(pf);
+	if (err)
+		return err;
+
+	/* Now that we've re-acquired IRQs, we need to remap the vectors and
+	 * rings together again.
+	 */
+	for (i = 0; i < pf->num_alloc_vsi; i++) {
+		if (pf->vsi[i]) {
+			err = i40e_vsi_alloc_q_vectors(pf->vsi[i]);
+			if (err)
+				goto err_unwind;
+			i40e_vsi_map_rings_to_vectors(pf->vsi[i]);
+		}
+	}
+
+	err = i40e_setup_misc_vector(pf);
+	if (err)
+		goto err_unwind;
+
+	if (pf->flags & I40E_FLAG_IWARP_ENABLED)
+		i40e_client_update_msix_info(pf);
+
+	return 0;
+
+err_unwind:
+	while (i--) {
+		if (pf->vsi[i])
+			i40e_vsi_free_q_vectors(pf->vsi[i]);
+	}
+
+	return err;
+}
+
+/**
  * i40e_get_rss_aq - Get RSS keys and lut by using AQ commands
  * @vsi: Pointer to vsi structure
  * @seed: Buffter to store the hash keys
@@ -11465,7 +12715,6 @@
 			return ret;
 		}
 	}
-
 	return ret;
 }
 
@@ -11489,7 +12738,6 @@
 	/* Fill out hash function seed */
 	if (seed) {
 		u32 *seed_dw = (u32 *)seed;
-
 		if (vsi->type == I40E_VSI_MAIN) {
 			for (i = 0; i <= I40E_PFQF_HKEY_MAX_INDEX; i++)
 				wr32(hw, I40E_PFQF_HKEY(i), seed_dw[i]);
@@ -11607,8 +12855,9 @@
 {
 	u16 i;
 
-	for (i = 0; i < rss_table_size; i++)
+	for (i = 0; i < rss_table_size; i++) {
 		lut[i] = i % rss_size;
+	}
 }
 
 /**
@@ -11619,10 +12868,10 @@
 {
 	struct i40e_vsi *vsi = pf->vsi[pf->lan_vsi];
 	u8 seed[I40E_HKEY_ARRAY_SIZE];
-	u8 *lut;
 	struct i40e_hw *hw = &pf->hw;
 	u32 reg_val;
 	u64 hena;
+	u8 *lut;
 	int ret;
 
 	/* By default we enable TCP/UDP with IPv4/IPv6 ptypes */
@@ -11679,6 +12928,31 @@
 }
 
 /**
+ * i40e_clear_rss_config_user - clear the user configured lookup table.
+ * The RSS hash key can be clear only in case that it is still set to default
+ * value. In other case it should remain unchanged.
+ * @vsi: Pointer to VSI structure
+ */
+static void i40e_clear_rss_config_user(struct i40e_vsi *vsi)
+{
+	u8 seed[I40E_HKEY_ARRAY_SIZE];
+
+	if (!vsi)
+		return;
+
+	if (vsi->rss_hkey_user) {
+		netdev_rss_key_fill((void *)seed, I40E_HKEY_ARRAY_SIZE);
+		if (!memcmp(seed, vsi->rss_hkey_user, I40E_HKEY_ARRAY_SIZE)) {
+			kfree(vsi->rss_hkey_user);
+			vsi->rss_hkey_user = NULL;
+		}
+	}
+
+	kfree(vsi->rss_lut_user);
+	vsi->rss_lut_user = NULL;
+}
+
+/**
  * i40e_reconfig_rss_queues - change number of queues for rss and rebuild
  * @pf: board private structure
  * @queue_count: the requested queue count for rss.
@@ -11695,14 +12969,13 @@
 	if (!(pf->flags & I40E_FLAG_RSS_ENABLED))
 		return 0;
 
-	queue_count = min_t(int, queue_count, num_online_cpus());
 	new_rss_size = min_t(int, queue_count, pf->rss_size_max);
 
 	if (queue_count != vsi->num_queue_pairs) {
 		u16 qcount;
 
 		vsi->req_queue_pairs = queue_count;
-		i40e_prep_for_reset(pf, true);
+		i40e_prep_for_reset(pf);
 
 		pf->alloc_rss_size = new_rss_size;
 
@@ -11739,7 +13012,7 @@
 	u32 max_bw, min_bw;
 
 	status = i40e_read_bw_from_alt_ram(&pf->hw, &max_bw, &min_bw,
-					   &min_valid, &max_valid);
+			&min_valid, &max_valid);
 
 	if (!status) {
 		if (min_valid)
@@ -11760,6 +13033,7 @@
 	struct i40e_aqc_configure_partition_bw_data bw_data;
 	i40e_status status;
 
+	memset(&bw_data, 0, sizeof(bw_data));
 	/* Set the valid bit for this PF */
 	bw_data.pf_valid_bits = cpu_to_le16(BIT(pf->hw.pf_id));
 	bw_data.max_bw[pf->hw.pf_id] = pf->max_bw & I40E_ALT_BW_VALUE_MASK;
@@ -11855,6 +13129,60 @@
 }
 
 /**
+ * i40e_is_total_port_shutdown_enabled - read nvm and return value
+ * if total port shutdown feature is enabled for this pf
+ * @pf: board private structure
+ **/
+static bool i40e_is_total_port_shutdown_enabled(struct i40e_pf *pf)
+{
+#define I40E_TOTAL_PORT_SHUTDOWN_ENABLED	BIT(4)
+#define I40E_FEATURES_ENABLE_PTR		0x2A
+#define I40E_CURRENT_SETTING_PTR		0x2B
+#define I40E_LINK_BEHAVIOR_WORD_OFFSET		0x2D
+#define I40E_LINK_BEHAVIOR_WORD_LENGTH		0x1
+#define I40E_LINK_BEHAVIOR_OS_FORCED_ENABLED	BIT(0)
+#define I40E_LINK_BEHAVIOR_PORT_BIT_LENGTH	4
+	i40e_status read_status = I40E_SUCCESS;
+	u16 sr_emp_sr_settings_ptr = 0;
+	u16 features_enable = 0;
+	u16 link_behavior = 0;
+	bool ret = false;
+
+	read_status = i40e_read_nvm_word(&pf->hw,
+					 I40E_SR_EMP_SR_SETTINGS_PTR,
+					 &sr_emp_sr_settings_ptr);
+	if (read_status)
+		goto err_nvm;
+	read_status = i40e_read_nvm_word(&pf->hw,
+					 sr_emp_sr_settings_ptr +
+					 I40E_FEATURES_ENABLE_PTR,
+					 &features_enable);
+	if (read_status)
+		goto err_nvm;
+	if (I40E_TOTAL_PORT_SHUTDOWN_ENABLED & features_enable) {
+		read_status =
+		i40e_read_nvm_module_data(&pf->hw,
+					  I40E_SR_EMP_SR_SETTINGS_PTR,
+					  I40E_CURRENT_SETTING_PTR,
+					  I40E_LINK_BEHAVIOR_WORD_OFFSET,
+					  I40E_LINK_BEHAVIOR_WORD_LENGTH,
+					  &link_behavior);
+		if (read_status)
+			goto err_nvm;
+		link_behavior >>=
+		(pf->hw.port * I40E_LINK_BEHAVIOR_PORT_BIT_LENGTH);
+		ret = I40E_LINK_BEHAVIOR_OS_FORCED_ENABLED & link_behavior;
+	}
+	return ret;
+
+err_nvm:
+	dev_warn(&pf->pdev->dev,
+		 "Total Port Shutdown feature is off due to read nvm error:%d\n",
+		 read_status);
+	return ret;
+}
+
+/**
  * i40e_sw_init - Initialize general software structures (struct i40e_pf)
  * @pf: board private structure to initialize
  *
@@ -11866,6 +13194,17 @@
 {
 	int err = 0;
 	int size;
+	u16 pow;
+	int i;
+
+	pf->msg_enable = netif_msg_init(I40E_DEFAULT_MSG_ENABLE,
+				(NETIF_MSG_DRV|NETIF_MSG_PROBE|NETIF_MSG_LINK));
+	if (debug != -1 && debug != I40E_DEFAULT_MSG_ENABLE) {
+		if (I40E_DEBUG_USER & debug)
+			pf->hw.debug_mask = debug;
+		pf->msg_enable = netif_msg_init((debug & ~I40E_DEBUG_USER),
+						I40E_DEFAULT_MSG_ENABLE);
+	}
 
 	/* Set default capability flags */
 	pf->flags = I40E_FLAG_RX_CSUM_ENABLED |
@@ -11875,7 +13214,6 @@
 	/* Set default ITR */
 	pf->rx_itr_default = I40E_ITR_RX_DEF;
 	pf->tx_itr_default = I40E_ITR_TX_DEF;
-
 	/* Depending on PF configurations, it is possible that the RSS
 	 * maximum might end up larger than the available queues
 	 */
@@ -11884,15 +13222,20 @@
 	pf->rss_table_size = pf->hw.func_caps.rss_table_size;
 	pf->rss_size_max = min_t(int, pf->rss_size_max,
 				 pf->hw.func_caps.num_tx_qp);
+
+	/* find the next higher power-of-2 of num cpus */
+	pow = roundup_pow_of_two(num_online_cpus());
+	pf->rss_size_max = min_t(int, pf->rss_size_max, pow);
+
 	if (pf->hw.func_caps.rss) {
 		pf->flags |= I40E_FLAG_RSS_ENABLED;
 		pf->alloc_rss_size = min_t(int, pf->rss_size_max,
 					   num_online_cpus());
 	}
-
 	/* MFP mode enabled */
 	if (pf->hw.func_caps.npar_enable || pf->hw.func_caps.flex10_enable) {
 		pf->flags |= I40E_FLAG_MFP_ENABLED;
+
 		dev_info(&pf->pdev->dev, "MFP mode Enabled\n");
 		if (i40e_get_partition_bw_setting(pf)) {
 			dev_warn(&pf->pdev->dev,
@@ -11938,9 +13281,8 @@
 
 #define I40E_FDEVICT_PCTYPE_DEFAULT 0xc03
 		if (rd32(&pf->hw, I40E_GLQF_FDEVICTENA(1)) !=
-		    I40E_FDEVICT_PCTYPE_DEFAULT) {
-			dev_warn(&pf->pdev->dev,
-				 "FD EVICT PCTYPES are not right, disable FD HW EVICT\n");
+						I40E_FDEVICT_PCTYPE_DEFAULT) {
+			dev_warn(&pf->pdev->dev, "FD EVICT PCTYPES are not right, disable FD HW EVICT\n");
 			pf->hw_features &= ~I40E_HW_ATR_EVICT_CAPABLE;
 		}
 	} else if ((pf->hw.aq.api_maj_ver > 1) ||
@@ -11948,6 +13290,9 @@
 		    (pf->hw.aq.api_min_ver > 4))) {
 		/* Supported in FW API version higher than 1.4 */
 		pf->hw_features |= I40E_HW_GENEVE_OFFLOAD_CAPABLE;
+
+		/* supports mpls header skip and csum for following headers */
+		pf->hw_features |= I40E_HW_MPLS_HDR_OFFLOAD_CAPABLE;
 	}
 
 	/* Enable HW ATR eviction if possible */
@@ -11975,10 +13320,15 @@
 		pf->hw_features |= I40E_HW_USE_SET_LLDP_MIB;
 
 	/* Enable PTP L4 if FW > v6.0 */
-	if (pf->hw.mac.type == I40E_MAC_XL710 &&
-	    pf->hw.aq.fw_maj_ver >= 6)
+	if ((pf->hw.mac.type == I40E_MAC_XL710) &&
+	    (pf->hw.aq.fw_maj_ver >= 6))
 		pf->hw_features |= I40E_HW_PTP_L4_CAPABLE;
 
+	/* Enable outer VLAN processing if FW > v8.3 */
+	if (pf->hw.aq.fw_maj_ver > 8 ||
+	    (pf->hw.aq.fw_maj_ver == 8 && pf->hw.aq.fw_min_ver > 3))
+		pf->hw_features |= I40E_HW_OUTER_VLAN_CAPABLE;
+
 	if (pf->hw.func_caps.vmdq && num_online_cpus() != 1) {
 		pf->num_vmdq_vsis = I40E_DEFAULT_NUM_VMDQ_VSI;
 		pf->flags |= I40E_FLAG_VMDQ_ENABLED;
@@ -12000,13 +13350,37 @@
 	    (pf->hw.flags & I40E_HW_FLAG_FW_LLDP_STOPPABLE))
 		pf->hw.flags &= ~I40E_HW_FLAG_FW_LLDP_STOPPABLE;
 
+#ifndef HAVE_SWIOTLB_SKIP_CPU_SYNC
+	/* force legacy Rx if SKIP_CPU_SYNC is not supported */
+	pf->flags |= I40E_FLAG_LEGACY_RX;
+#endif
 #ifdef CONFIG_PCI_IOV
 	if (pf->hw.func_caps.num_vfs && pf->hw.partition_id == 1) {
-		pf->num_vf_qps = I40E_DEFAULT_QUEUES_PER_VF;
+#if !defined(HAVE_SRIOV_CONFIGURE) && !defined(HAVE_RHEL6_SRIOV_CONFIGURE)
+		pf->num_req_vfs = 0;
+		if (max_vfs[pf->instance] > 0 &&
+		    max_vfs[pf->instance] <= pf->hw.func_caps.num_vfs) {
+			pf->flags |= I40E_FLAG_SRIOV_ENABLED;
+			/* assign number of SR-IOV VFs */
+			pf->num_vf_qps = I40E_DEFAULT_QUEUES_PER_VF;
+			pf->num_req_vfs = max_vfs[pf->instance];
+		} else if (max_vfs[pf->instance] == 0)  {
+			dev_info(&pf->pdev->dev,
+				 " SR-IOV is disabled, Module Parameter max_vfs value %d = disabled\n",
+				 max_vfs[pf->instance]);
+		} else if (max_vfs[pf->instance] != -1) {
+			dev_err(&pf->pdev->dev,
+				"Module Parameter max_vfs value %d is out of range. Maximum value for the device: %d - resetting to zero\n",
+				max_vfs[pf->instance],
+				pf->hw.func_caps.num_vfs);
+		}
+#else
 		pf->flags |= I40E_FLAG_SRIOV_ENABLED;
+		pf->num_vf_qps = I40E_DEFAULT_QUEUES_PER_VF;
 		pf->num_req_vfs = min_t(int,
 					pf->hw.func_caps.num_vfs,
 					I40E_MAX_VF_COUNT);
+#endif /* HAVE_SRIOV_CONFIGURE */
 	}
 #endif /* CONFIG_PCI_IOV */
 	pf->eeprom_version = 0xDEAD;
@@ -12025,16 +13399,38 @@
 		goto sw_init_done;
 	}
 	pf->qp_pile->num_entries = pf->hw.func_caps.num_tx_qp;
-	pf->qp_pile->search_hint = 0;
 
 	pf->tx_timeout_recovery_level = 1;
 
+	if (pf->hw.mac.type != I40E_MAC_X722 &&
+	    i40e_is_total_port_shutdown_enabled(pf)) {
+		/* Link down on close must be on when total port shutdown
+		 * is enabled for a given port
+		 */
+		pf->flags |= (I40E_FLAG_TOTAL_PORT_SHUTDOWN
+			  | I40E_FLAG_LINK_DOWN_ON_CLOSE_ENABLED);
+		dev_info(&pf->pdev->dev,
+			 "Total Port Shutdown is enabled, link-down-on-close forced on\n");
+	}
+
+	/* Add default values for ingress and egress vlan */
+	pf->ingress_vlan = I40E_NO_VF_MIRROR;
+	pf->egress_vlan = I40E_NO_VF_MIRROR;
+	for (i = 0; i < I40E_MAX_USER_PRIORITY; i++) {
+		pf->dcb_user_up_map[i] = I40E_MULTIPLE_TRAFFIC_CLASS_NO_ENTRY;
+		pf->dcb_user_lsp_map[i] = I40E_MULTIPLE_TRAFFIC_CLASS_NO_ENTRY;
+		pf->dcb_veb_bw_map[i] = I40E_MULTIPLE_TRAFFIC_CLASS_NO_ENTRY;
+		pf->dcb_mib_bw_map[i] = I40E_MULTIPLE_TRAFFIC_CLASS_NO_ENTRY;
+	}
+
 	mutex_init(&pf->switch_mutex);
 
 sw_init_done:
 	return err;
 }
 
+#ifdef HAVE_NDO_SET_FEATURES
+#endif
 /**
  * i40e_set_ntuple - set the ntuple feature flag and take action
  * @pf: board private structure to initialize
@@ -12065,14 +13461,13 @@
 		if (pf->flags & I40E_FLAG_FD_SB_ENABLED) {
 			need_reset = true;
 			i40e_fdir_filter_exit(pf);
+			i40e_cloud_filter_exit(pf);
 		}
 		pf->flags &= ~I40E_FLAG_FD_SB_ENABLED;
 		clear_bit(__I40E_FD_SB_AUTO_DISABLED, pf->state);
 		pf->flags |= I40E_FLAG_FD_SB_INACTIVE;
-
 		/* reset fd counters */
-		pf->fd_add_err = 0;
-		pf->fd_atr_cnt = 0;
+		pf->fd_add_err = pf->fd_atr_cnt = 0;
 		/* if ATR was auto disabled it can be re-enabled. */
 		if (test_and_clear_bit(__I40E_FD_ATR_AUTO_DISABLED, pf->state))
 			if ((pf->flags & I40E_FLAG_FD_ATR_ENABLED) &&
@@ -12082,6 +13477,7 @@
 	return need_reset;
 }
 
+#ifdef HAVE_NDO_SET_FEATURES
 /**
  * i40e_clear_rss_lut - clear the rx hash lookup table
  * @vsi: the VSI being configured
@@ -12110,8 +13506,12 @@
  * @features: the feature set that the stack is suggesting
  * Note: expects to be called while under rtnl_lock()
  **/
+#ifdef HAVE_RHEL6_NET_DEVICE_OPS_EXT
+static int i40e_set_features(struct net_device *netdev, u32 features)
+#else
 static int i40e_set_features(struct net_device *netdev,
 			     netdev_features_t features)
+#endif
 {
 	struct i40e_netdev_priv *np = netdev_priv(netdev);
 	struct i40e_vsi *vsi = np->vsi;
@@ -12124,28 +13524,39 @@
 		 netdev->features & NETIF_F_RXHASH)
 		i40e_clear_rss_lut(vsi);
 
+#ifdef NETIF_F_HW_VLAN_CTAG_RX
 	if (features & NETIF_F_HW_VLAN_CTAG_RX)
+#else
+	if (features & NETIF_F_HW_VLAN_RX)
+#endif
 		i40e_vlan_stripping_enable(vsi);
 	else
 		i40e_vlan_stripping_disable(vsi);
 
+#ifdef NETIF_F_HW_TC
 	if (!(features & NETIF_F_HW_TC) && pf->num_cloud_filters) {
 		dev_err(&pf->pdev->dev,
 			"Offloaded tc filters active, can't turn hw_tc_offload off");
 		return -EINVAL;
 	}
 
-	if (!(features & NETIF_F_HW_L2FW_DOFFLOAD) && vsi->macvlan_cnt)
-		i40e_del_all_macvlans(vsi);
-
+	if ((features & NETIF_F_HW_TC) &&
+	    !(netdev->features & NETIF_F_HW_TC))
+		pf->flags |= I40E_FLAG_CLS_FLOWER;
+	else
+		pf->flags &= ~I40E_FLAG_CLS_FLOWER;
+#endif
 	need_reset = i40e_set_ntuple(pf, features);
 
-	if (need_reset)
-		i40e_do_reset(pf, I40E_PF_RESET_FLAG, true);
+	if (need_reset) {
+		i40e_prep_for_reset(pf);
+		i40e_reset_and_rebuild(pf, true, true);
+	}
 
 	return 0;
 }
 
+#endif /* HAVE_NDO_SET_FEATURES */
 /**
  * i40e_get_udp_port_idx - Lookup a possibly offloaded for Rx UDP port
  * @pf: board private structure
@@ -12175,8 +13586,8 @@
  * @netdev: This physical port's netdev
  * @ti: Tunnel endpoint information
  **/
-static void i40e_udp_tunnel_add(struct net_device *netdev,
-				struct udp_tunnel_info *ti)
+static void __maybe_unused i40e_udp_tunnel_add(struct net_device *netdev,
+					       struct udp_tunnel_info *ti)
 {
 	struct i40e_netdev_priv *np = netdev_priv(netdev);
 	struct i40e_vsi *vsi = np->vsi;
@@ -12227,8 +13638,8 @@
  * @netdev: This physical port's netdev
  * @ti: Tunnel endpoint information
  **/
-static void i40e_udp_tunnel_del(struct net_device *netdev,
-				struct udp_tunnel_info *ti)
+static void __maybe_unused i40e_udp_tunnel_del(struct net_device *netdev,
+					       struct udp_tunnel_info *ti)
 {
 	struct i40e_netdev_priv *np = netdev_priv(netdev);
 	struct i40e_vsi *vsi = np->vsi;
@@ -12248,6 +13659,8 @@
 			goto not_found;
 		break;
 	case UDP_TUNNEL_TYPE_GENEVE:
+		if (!(pf->hw_features & I40E_HW_GENEVE_OFFLOAD_CAPABLE))
+			return;
 		if (pf->udp_ports[idx].type != I40E_AQC_TUNNEL_TYPE_NGE)
 			goto not_found;
 		break;
@@ -12273,6 +13686,86 @@
 		    port);
 }
 
+#if defined(HAVE_VXLAN_RX_OFFLOAD) && !defined(HAVE_UDP_ENC_RX_OFFLOAD)
+#if IS_ENABLED(CONFIG_VXLAN)
+/**
+ * i40e_add_vxlan_port - Get notifications about vxlan ports that come up
+ * @netdev: This physical port's netdev
+ * @sa_family: Socket Family that vxlan is notifying us about
+ * @port: New UDP port number that vxlan started listening to
+ **/
+static void i40e_add_vxlan_port(struct net_device *netdev,
+				sa_family_t sa_family, __be16 port)
+{
+	struct udp_tunnel_info ti = {
+		.type = UDP_TUNNEL_TYPE_VXLAN,
+		.sa_family = sa_family,
+		.port = port,
+	};
+
+	i40e_udp_tunnel_add(netdev, &ti);
+}
+
+/**
+ * i40e_del_vxlan_port - Get notifications about vxlan ports that go away
+ * @netdev: This physical port's netdev
+ * @sa_family: Socket Family that vxlan is notifying us about
+ * @port: UDP port number that vxlan stopped listening to
+ **/
+static void i40e_del_vxlan_port(struct net_device *netdev,
+				sa_family_t sa_family, __be16 port)
+{
+	struct udp_tunnel_info ti = {
+		.type = UDP_TUNNEL_TYPE_VXLAN,
+		.sa_family = sa_family,
+		.port = port,
+	};
+
+	i40e_udp_tunnel_del(netdev, &ti);
+}
+#endif /* CONFIG_VXLAN */
+#endif /* HAVE_VXLAN_RX_OFFLOAD && !HAVE_UDP_ENC_RX_OFFLOAD */
+#if defined(HAVE_GENEVE_RX_OFFLOAD) && !defined(HAVE_UDP_ENC_RX_OFFLOAD)
+#if IS_ENABLED(CONFIG_GENEVE)
+/**
+ * i40e_add_geneve_port - Get notifications about GENEVE ports that come up
+ * @netdev: This physical port's netdev
+ * @sa_family: Socket Family that GENEVE is notifying us about
+ * @port: New UDP port number that GENEVE started listening to
+ **/
+static void i40e_add_geneve_port(struct net_device *netdev,
+				 sa_family_t sa_family, __be16 port)
+{
+	struct udp_tunnel_info ti = {
+		.type = UDP_TUNNEL_TYPE_GENEVE,
+		.sa_family = sa_family,
+		.port = port,
+	};
+
+	i40e_udp_tunnel_add(netdev, &ti);
+}
+
+/*
+ * i40e_del_geneve_port - Get notifications about GENEVE ports that go away
+ * @netdev: This physical port's netdev
+ * @sa_family: Socket Family that GENEVE is notifying us about
+ * @port: UDP port number that GENEVE stopped listening to
+ **/
+static void i40e_del_geneve_port(struct net_device *netdev,
+				 sa_family_t sa_family, __be16 port)
+{
+	struct udp_tunnel_info ti = {
+		.type = UDP_TUNNEL_TYPE_GENEVE,
+		.sa_family = sa_family,
+		.port = port,
+	};
+
+	i40e_udp_tunnel_del(netdev, &ti);
+}
+
+#endif /* CONFIG_GENEVE */
+#endif /* HAVE_GENEVE_RX_OFFLOAD  && !HAVE_UDP_ENC_RX_OFFLOAD */
+#ifdef HAVE_NDO_GET_PHYS_PORT_ID
 static int i40e_get_phys_port_id(struct net_device *netdev,
 				 struct netdev_phys_item_id *ppid)
 {
@@ -12283,12 +13776,14 @@
 	if (!(pf->hw_features & I40E_HW_PORT_ID_VALID))
 		return -EOPNOTSUPP;
 
-	ppid->id_len = min_t(int, sizeof(hw->mac.port_addr), sizeof(ppid->id));
+	ppid->id_len = min_t(int, sizeof(hw->mac.port_addr),
+			     sizeof(ppid->id));
 	memcpy(ppid->id, hw->mac.port_addr, ppid->id_len);
 
 	return 0;
 }
 
+#endif /* HAVE_NDO_GET_PHYS_PORT_ID */
 /**
  * i40e_ndo_fdb_add - add an entry to the hardware database
  * @ndm: the input from the stack
@@ -12297,12 +13792,28 @@
  * @addr: the MAC address entry being added
  * @vid: VLAN ID
  * @flags: instructions from stack about fdb operation
+ * @extack: netdev extended ack structure
  */
+#ifdef HAVE_FDB_OPS
+#if defined(HAVE_NDO_FDB_ADD_EXTACK)
+static int i40e_ndo_fdb_add(struct ndmsg *ndm, struct nlattr *tb[],
+			    struct net_device *dev, const unsigned char *addr,
+			    u16 vid, u16 flags, struct netlink_ext_ack *extack)
+#elif defined(HAVE_NDO_FDB_ADD_VID)
 static int i40e_ndo_fdb_add(struct ndmsg *ndm, struct nlattr *tb[],
-			    struct net_device *dev,
-			    const unsigned char *addr, u16 vid,
-			    u16 flags,
-			    struct netlink_ext_ack *extack)
+			    struct net_device *dev, const unsigned char *addr,
+			    u16 vid, u16 flags)
+#elif defined(HAVE_NDO_FDB_ADD_NLATTR)
+static int i40e_ndo_fdb_add(struct ndmsg *ndm, struct nlattr *tb[],
+			    struct net_device *dev, const unsigned char *addr,
+			    u16 flags)
+#elif defined(USE_CONST_DEV_UC_CHAR)
+static int i40e_ndo_fdb_add(struct ndmsg *ndm, struct net_device *dev,
+			    const unsigned char *addr, u16 flags)
+#else
+static int i40e_ndo_fdb_add(struct ndmsg *ndm, struct net_device *dev,
+			    unsigned char *addr, u16 flags)
+#endif
 {
 	struct i40e_netdev_priv *np = netdev_priv(dev);
 	struct i40e_pf *pf = np->vsi->back;
@@ -12311,11 +13822,6 @@
 	if (!(pf->flags & I40E_FLAG_SRIOV_ENABLED))
 		return -EOPNOTSUPP;
 
-	if (vid) {
-		pr_info("%s: vlans aren't supported yet for dev_uc|mc_add()\n", dev->name);
-		return -EINVAL;
-	}
-
 	/* Hardware does not support aging addresses so if a
 	 * ndm_state is given only allow permanent addresses
 	 */
@@ -12338,125 +13844,7 @@
 	return err;
 }
 
-/**
- * i40e_ndo_bridge_setlink - Set the hardware bridge mode
- * @dev: the netdev being configured
- * @nlh: RTNL message
- * @flags: bridge flags
- * @extack: netlink extended ack
- *
- * Inserts a new hardware bridge if not already created and
- * enables the bridging mode requested (VEB or VEPA). If the
- * hardware bridge has already been inserted and the request
- * is to change the mode then that requires a PF reset to
- * allow rebuild of the components with required hardware
- * bridge mode enabled.
- *
- * Note: expects to be called while under rtnl_lock()
- **/
-static int i40e_ndo_bridge_setlink(struct net_device *dev,
-				   struct nlmsghdr *nlh,
-				   u16 flags,
-				   struct netlink_ext_ack *extack)
-{
-	struct i40e_netdev_priv *np = netdev_priv(dev);
-	struct i40e_vsi *vsi = np->vsi;
-	struct i40e_pf *pf = vsi->back;
-	struct i40e_veb *veb = NULL;
-	struct nlattr *attr, *br_spec;
-	int i, rem;
-
-	/* Only for PF VSI for now */
-	if (vsi->seid != pf->vsi[pf->lan_vsi]->seid)
-		return -EOPNOTSUPP;
-
-	/* Find the HW bridge for PF VSI */
-	for (i = 0; i < I40E_MAX_VEB && !veb; i++) {
-		if (pf->veb[i] && pf->veb[i]->seid == vsi->uplink_seid)
-			veb = pf->veb[i];
-	}
-
-	br_spec = nlmsg_find_attr(nlh, sizeof(struct ifinfomsg), IFLA_AF_SPEC);
-
-	nla_for_each_nested(attr, br_spec, rem) {
-		__u16 mode;
-
-		if (nla_type(attr) != IFLA_BRIDGE_MODE)
-			continue;
-
-		mode = nla_get_u16(attr);
-		if ((mode != BRIDGE_MODE_VEPA) &&
-		    (mode != BRIDGE_MODE_VEB))
-			return -EINVAL;
-
-		/* Insert a new HW bridge */
-		if (!veb) {
-			veb = i40e_veb_setup(pf, 0, vsi->uplink_seid, vsi->seid,
-					     vsi->tc_config.enabled_tc);
-			if (veb) {
-				veb->bridge_mode = mode;
-				i40e_config_bridge_mode(veb);
-			} else {
-				/* No Bridge HW offload available */
-				return -ENOENT;
-			}
-			break;
-		} else if (mode != veb->bridge_mode) {
-			/* Existing HW bridge but different mode needs reset */
-			veb->bridge_mode = mode;
-			/* TODO: If no VFs or VMDq VSIs, disallow VEB mode */
-			if (mode == BRIDGE_MODE_VEB)
-				pf->flags |= I40E_FLAG_VEB_MODE_ENABLED;
-			else
-				pf->flags &= ~I40E_FLAG_VEB_MODE_ENABLED;
-			i40e_do_reset(pf, I40E_PF_RESET_FLAG, true);
-			break;
-		}
-	}
-
-	return 0;
-}
-
-/**
- * i40e_ndo_bridge_getlink - Get the hardware bridge mode
- * @skb: skb buff
- * @pid: process id
- * @seq: RTNL message seq #
- * @dev: the netdev being configured
- * @filter_mask: unused
- * @nlflags: netlink flags passed in
- *
- * Return the mode in which the hardware bridge is operating in
- * i.e VEB or VEPA.
- **/
-static int i40e_ndo_bridge_getlink(struct sk_buff *skb, u32 pid, u32 seq,
-				   struct net_device *dev,
-				   u32 __always_unused filter_mask,
-				   int nlflags)
-{
-	struct i40e_netdev_priv *np = netdev_priv(dev);
-	struct i40e_vsi *vsi = np->vsi;
-	struct i40e_pf *pf = vsi->back;
-	struct i40e_veb *veb = NULL;
-	int i;
-
-	/* Only for PF VSI for now */
-	if (vsi->seid != pf->vsi[pf->lan_vsi]->seid)
-		return -EOPNOTSUPP;
-
-	/* Find the HW bridge for the PF VSI */
-	for (i = 0; i < I40E_MAX_VEB && !veb; i++) {
-		if (pf->veb[i] && pf->veb[i]->seid == vsi->uplink_seid)
-			veb = pf->veb[i];
-	}
-
-	if (!veb)
-		return 0;
-
-	return ndo_dflt_bridge_getlink(skb, pid, seq, dev, veb->bridge_mode,
-				       0, 0, nlflags, filter_mask, NULL);
-}
-
+#ifdef HAVE_NDO_FEATURES_CHECK
 /**
  * i40e_features_check - Validate encapsulated packet conforms to limits
  * @skb: skb buff
@@ -12515,60 +13903,7 @@
 	return features & ~(NETIF_F_CSUM_MASK | NETIF_F_GSO_MASK);
 }
 
-/**
- * i40e_xdp_setup - add/remove an XDP program
- * @vsi: VSI to changed
- * @prog: XDP program
- **/
-static int i40e_xdp_setup(struct i40e_vsi *vsi,
-			  struct bpf_prog *prog)
-{
-	int frame_size = vsi->netdev->mtu + ETH_HLEN + ETH_FCS_LEN + VLAN_HLEN;
-	struct i40e_pf *pf = vsi->back;
-	struct bpf_prog *old_prog;
-	bool need_reset;
-	int i;
-
-	/* Don't allow frames that span over multiple buffers */
-	if (frame_size > vsi->rx_buf_len)
-		return -EINVAL;
-
-	if (!i40e_enabled_xdp_vsi(vsi) && !prog)
-		return 0;
-
-	/* When turning XDP on->off/off->on we reset and rebuild the rings. */
-	need_reset = (i40e_enabled_xdp_vsi(vsi) != !!prog);
-
-	if (need_reset)
-		i40e_prep_for_reset(pf, true);
-
-	old_prog = xchg(&vsi->xdp_prog, prog);
-
-	if (need_reset) {
-		if (!prog)
-			/* Wait until ndo_xsk_wakeup completes. */
-			synchronize_rcu();
-		i40e_reset_and_rebuild(pf, true, true);
-	}
-
-	for (i = 0; i < vsi->num_queue_pairs; i++)
-		WRITE_ONCE(vsi->rx_rings[i]->xdp_prog, vsi->xdp_prog);
-
-	if (old_prog)
-		bpf_prog_put(old_prog);
-
-	/* Kick start the NAPI context if there is an AF_XDP socket open
-	 * on that queue id. This so that receiving will start.
-	 */
-	if (need_reset && prog)
-		for (i = 0; i < vsi->num_queue_pairs; i++)
-			if (vsi->xdp_rings[i]->xsk_umem)
-				(void)i40e_xsk_wakeup(vsi->netdev, i,
-						      XDP_WAKEUP_RX);
-
-	return 0;
-}
-
+#ifdef HAVE_XDP_SUPPORT
 /**
  * i40e_enter_busy_conf - Enters busy config state
  * @vsi: vsi
@@ -12627,8 +13962,8 @@
 {
 	i40e_clean_tx_ring(vsi->tx_rings[queue_pair]);
 	if (i40e_enabled_xdp_vsi(vsi)) {
-		/* Make sure that in-progress ndo_xdp_xmit calls are
-		 * completed.
+		/* Make sure that in-progress ndo_xdp_xmit
+		 * calls are completed.
 		 */
 		synchronize_rcu();
 		i40e_clean_tx_ring(vsi->xdp_rings[queue_pair]);
@@ -12825,12 +14160,65 @@
 }
 
 /**
- * i40e_xdp - implements ndo_bpf for i40e
+ * i40e_xdp_setup - add/remove an XDP program
+ * @vsi: VSI to changed
+ * @prog: XDP program
+ * @extack: netlink extended ack
+ **/
+static int i40e_xdp_setup(struct i40e_vsi *vsi, struct bpf_prog *prog,
+			  struct netlink_ext_ack *extack)
+{
+	int frame_size = vsi->netdev->mtu + ETH_HLEN + ETH_FCS_LEN + VLAN_HLEN;
+	struct i40e_pf *pf = vsi->back;
+	struct bpf_prog *old_prog;
+	bool need_reset;
+	int i;
+
+	/* Don't allow frames that span over multiple buffers */
+	if (frame_size > vsi->rx_buf_len) {
+		NL_SET_ERR_MSG_MOD(extack, "MTU too large to enable XDP");
+		return -EINVAL;
+	}
+
+	if (!i40e_enabled_xdp_vsi(vsi) && !prog)
+		return 0;
+
+	/* When turning XDP on->off/off->on we reset and rebuild the rings. */
+	need_reset = (i40e_enabled_xdp_vsi(vsi) != !!prog);
+
+	if (need_reset) {
+		if (!!prog)
+			dev_info(&pf->pdev->dev,
+				 "Loading XDP program, please note: XDP_REDIRECT action requires the same number of queues on both interfaces\n");
+		i40e_prep_for_reset(pf);
+	}
+
+	old_prog = xchg(&vsi->xdp_prog, prog);
+
+	if (need_reset)
+		i40e_reset_and_rebuild(pf, true, true);
+
+	for (i = 0; i < vsi->num_queue_pairs; i++)
+		WRITE_ONCE(vsi->rx_rings[i]->xdp_prog, vsi->xdp_prog);
+
+	if (old_prog)
+		bpf_prog_put(old_prog);
+
+	return 0;
+}
+
+/**
+ * i40e_xdp - implements ndo_xdp for i40e
  * @dev: netdevice
  * @xdp: XDP command
  **/
+#ifdef HAVE_NDO_BPF
 static int i40e_xdp(struct net_device *dev,
 		    struct netdev_bpf *xdp)
+#else
+static int i40e_xdp(struct net_device *dev,
+		    struct netdev_xdp *xdp)
+#endif
 {
 	struct i40e_netdev_priv *np = netdev_priv(dev);
 	struct i40e_vsi *vsi = np->vsi;
@@ -12840,56 +14228,408 @@
 
 	switch (xdp->command) {
 	case XDP_SETUP_PROG:
-		return i40e_xdp_setup(vsi, xdp->prog);
+		return i40e_xdp_setup(vsi, xdp->prog, xdp->extack);
+#ifdef HAVE_XDP_QUERY_PROG
 	case XDP_QUERY_PROG:
+#ifndef NO_NETDEV_BPF_PROG_ATTACHED
+		xdp->prog_attached = i40e_enabled_xdp_vsi(vsi);
+#endif /* !NO_NETDEV_BPF_PROG_ATTACHED */
 		xdp->prog_id = vsi->xdp_prog ? vsi->xdp_prog->aux->id : 0;
 		return 0;
-	case XDP_SETUP_XSK_UMEM:
-		return i40e_xsk_umem_setup(vsi, xdp->xsk.umem,
-					   xdp->xsk.queue_id);
+#endif /* HAVE_XDP_QUERY_PROG */
 	default:
 		return -EINVAL;
 	}
 }
+#endif /* HAVE_XDP_SUPPORT */
+#endif /* HAVE_NDO_FEATURES_CHECK */
+#ifndef USE_DEFAULT_FDB_DEL_DUMP
+#if defined(HAVE_NDO_FDB_ADD_VID)
+static int i40e_ndo_fdb_del(struct ndmsg *ndm, struct nlattr *tb[],
+			    struct net_device *dev, const unsigned char *addr,
+			    u16 vid)
+#elif defined(HAVE_FDB_DEL_NLATTR)
+static int i40e_ndo_fdb_del(struct ndmsg *ndm, struct nlattr *tb[],
+			    struct net_device *dev, const unsigned char *addr)
+#elif defined(USE_CONST_DEV_UC_CHAR)
+static int i40e_ndo_fdb_del(struct ndmsg *ndm, struct net_device *dev,
+			    const unsigned char *addr)
+#else
+static int i40e_ndo_fdb_del(struct ndmsg *ndm, struct net_device *dev,
+			    unsigned char *addr)
+#endif /* HAVE_NDO_FDB_ADD_VID */
+{
+	struct i40e_netdev_priv *np = netdev_priv(dev);
+	struct i40e_pf *pf = np->vsi->back;
+	int err = -EOPNOTSUPP;
 
+	if (ndm->ndm_state & NUD_PERMANENT) {
+		netdev_info(dev, "FDB only supports static addresses\n");
+		return -EINVAL;
+	}
+
+	if (pf->flags & I40E_FLAG_SRIOV_ENABLED) {
+		if (is_unicast_ether_addr(addr))
+			err = dev_uc_del(dev, addr);
+		else if (is_multicast_ether_addr(addr))
+			err = dev_mc_del(dev, addr);
+		else
+			err = -EINVAL;
+	}
+
+	return err;
+}
+
+static int i40e_ndo_fdb_dump(struct sk_buff *skb,
+			      struct netlink_callback *cb,
+			      struct net_device *dev,
+			      int idx)
+{
+	struct i40e_netdev_priv *np = netdev_priv(dev);
+	struct i40e_pf *pf = np->vsi->back;
+
+	if (pf->flags & I40E_FLAG_SRIOV_ENABLED)
+		idx = ndo_dflt_fdb_dump(skb, cb, dev, idx);
+
+	return idx;
+}
+
+#endif /* USE_DEFAULT_FDB_DEL_DUMP */
+#ifdef HAVE_BRIDGE_ATTRIBS
+/**
+ * i40e_ndo_bridge_setlink - Set the hardware bridge mode
+ * @dev: the netdev being configured
+ * @nlh: RTNL message
+ * @flags: bridge flags
+ * @extack: netdev extended ack structure
+ *
+ * Inserts a new hardware bridge if not already created and
+ * enables the bridging mode requested (VEB or VEPA). If the
+ * hardware bridge has already been inserted and the request
+ * is to change the mode then that requires a PF reset to
+ * allow rebuild of the components with required hardware
+ * bridge mode enabled.
+ *
+ * Note: expects to be called while under rtnl_lock()
+ **/
+#if defined(HAVE_NDO_BRIDGE_SETLINK_EXTACK)
+static int i40e_ndo_bridge_setlink(struct net_device *dev,
+				   struct nlmsghdr *nlh,
+				   u16 flags,
+				   struct netlink_ext_ack *extack)
+#elif defined(HAVE_NDO_BRIDGE_SET_DEL_LINK_FLAGS)
+static int i40e_ndo_bridge_setlink(struct net_device *dev,
+				   struct nlmsghdr *nlh,
+				   u16 flags)
+#else
+static int i40e_ndo_bridge_setlink(struct net_device *dev,
+				   struct nlmsghdr *nlh)
+#endif
+{
+	struct i40e_netdev_priv *np = netdev_priv(dev);
+	struct i40e_vsi *vsi = np->vsi;
+	struct i40e_pf *pf = vsi->back;
+	struct i40e_veb *veb = NULL;
+	struct nlattr *attr, *br_spec;
+	int i, rem;
+
+	/* Only for PF VSI for now */
+	if (vsi->seid != pf->vsi[pf->lan_vsi]->seid)
+		return -EOPNOTSUPP;
+
+	/* Find the HW bridge for PF VSI */
+	for (i = 0; i < I40E_MAX_VEB && !veb; i++) {
+		if (pf->veb[i] && pf->veb[i]->seid == vsi->uplink_seid)
+			veb = pf->veb[i];
+	}
+
+	br_spec = nlmsg_find_attr(nlh, sizeof(struct ifinfomsg), IFLA_AF_SPEC);
+
+	nla_for_each_nested(attr, br_spec, rem) {
+		__u16 mode;
+
+		if (nla_type(attr) != IFLA_BRIDGE_MODE)
+			continue;
+
+		mode = nla_get_u16(attr);
+		if ((mode != BRIDGE_MODE_VEPA) &&
+		    (mode != BRIDGE_MODE_VEB))
+			return -EINVAL;
+
+		/* Insert a new HW bridge */
+		if (!veb) {
+			veb = i40e_veb_setup(pf, 0, vsi->uplink_seid, vsi->seid,
+					     vsi->tc_config.enabled_tc);
+			if (veb) {
+				veb->bridge_mode = mode;
+				i40e_config_bridge_mode(veb);
+			} else {
+				/* No Bridge HW offload available */
+				return -ENOENT;
+			}
+			break;
+		} else if (mode != veb->bridge_mode) {
+			/* Existing HW bridge but different mode needs reset */
+			veb->bridge_mode = mode;
+			if (mode == BRIDGE_MODE_VEB)
+				pf->flags |= I40E_FLAG_VEB_MODE_ENABLED;
+			else
+				pf->flags &= ~I40E_FLAG_VEB_MODE_ENABLED;
+			i40e_do_reset(pf, I40E_PF_RESET_FLAG, true);
+			break;
+		}
+	}
+
+	return 0;
+}
+
+/**
+ * i40e_ndo_bridge_getlink - Get the hardware bridge mode
+ * @skb: skb buff
+ * @pid: process id
+ * @seq: RTNL message seq #
+ * @dev: the netdev being configured
+ * @filter_mask: unused
+ * @nlflags: netlink flags passed in
+ *
+ * Return the mode in which the hardware bridge is operating in
+ * i.e VEB or VEPA.
+ **/
+#ifdef HAVE_NDO_BRIDGE_GETLINK_NLFLAGS
+static int i40e_ndo_bridge_getlink(struct sk_buff *skb, u32 pid, u32 seq,
+				   struct net_device *dev,
+				   u32 __always_unused filter_mask,
+				   int nlflags)
+#elif defined(HAVE_BRIDGE_FILTER)
+static int i40e_ndo_bridge_getlink(struct sk_buff *skb, u32 pid, u32 seq,
+				   struct net_device *dev,
+				   u32 __always_unused filter_mask)
+#else
+static int i40e_ndo_bridge_getlink(struct sk_buff *skb, u32 pid, u32 seq,
+				   struct net_device *dev)
+#endif /* NDO_BRIDGE_STUFF */
+{
+	struct i40e_netdev_priv *np = netdev_priv(dev);
+	struct i40e_vsi *vsi = np->vsi;
+	struct i40e_pf *pf = vsi->back;
+	struct i40e_veb *veb = NULL;
+	int i;
+
+	/* Only for PF VSI for now */
+	if (vsi->seid != pf->vsi[pf->lan_vsi]->seid)
+		return -EOPNOTSUPP;
+
+	/* Find the HW bridge for the PF VSI */
+	for (i = 0; i < I40E_MAX_VEB && !veb; i++) {
+		if (pf->veb[i] && pf->veb[i]->seid == vsi->uplink_seid)
+			veb = pf->veb[i];
+	}
+
+	if (!veb)
+		return 0;
+
+#ifdef HAVE_NDO_DFLT_BRIDGE_GETLINK_VLAN_SUPPORT
+	return ndo_dflt_bridge_getlink(skb, pid, seq, dev, veb->bridge_mode,
+				       0, 0, nlflags, filter_mask, NULL);
+#elif defined(HAVE_NDO_BRIDGE_GETLINK_NLFLAGS)
+	return ndo_dflt_bridge_getlink(skb, pid, seq, dev, veb->bridge_mode,
+				       0, 0, nlflags);
+#elif defined(HAVE_NDO_FDB_ADD_VID) || \
+	defined NDO_DFLT_BRIDGE_GETLINK_HAS_BRFLAGS
+	return ndo_dflt_bridge_getlink(skb, pid, seq, dev, veb->bridge_mode,
+				       0, 0);
+#else
+	return ndo_dflt_bridge_getlink(skb, pid, seq, dev, veb->bridge_mode);
+#endif /* HAVE_NDO_BRIDGE_XX */
+}
+#endif /* HAVE_BRIDGE_ATTRIBS */
+#endif /* HAVE_FDB_OPS */
+
+#ifdef HAVE_NET_DEVICE_OPS
 static const struct net_device_ops i40e_netdev_ops = {
 	.ndo_open		= i40e_open,
 	.ndo_stop		= i40e_close,
 	.ndo_start_xmit		= i40e_lan_xmit_frame,
+#if defined(HAVE_NDO_GET_STATS64) || defined(HAVE_VOID_NDO_GET_STATS64)
 	.ndo_get_stats64	= i40e_get_netdev_stats_struct,
+#else
+	.ndo_get_stats		= i40e_get_netdev_stats_struct,
+#endif
 	.ndo_set_rx_mode	= i40e_set_rx_mode,
 	.ndo_validate_addr	= eth_validate_addr,
 	.ndo_set_mac_address	= i40e_set_mac,
+#ifdef HAVE_RHEL7_EXTENDED_MIN_MAX_MTU
+	.extended.ndo_change_mtu = i40e_change_mtu,
+#else
 	.ndo_change_mtu		= i40e_change_mtu,
+#endif /* HAVE_RHEL7_EXTENDED_MIN_MAX_MTU */
+#if defined(HAVE_PTP_1588_CLOCK) || defined(HAVE_I40E_INTELCIM_IOCTL)
 	.ndo_do_ioctl		= i40e_ioctl,
+#endif
 	.ndo_tx_timeout		= i40e_tx_timeout,
+#ifdef HAVE_VLAN_RX_REGISTER
+	.ndo_vlan_rx_register	= i40e_vlan_rx_register,
+#endif
 	.ndo_vlan_rx_add_vid	= i40e_vlan_rx_add_vid,
 	.ndo_vlan_rx_kill_vid	= i40e_vlan_rx_kill_vid,
 #ifdef CONFIG_NET_POLL_CONTROLLER
 	.ndo_poll_controller	= i40e_netpoll,
 #endif
+#ifdef HAVE_SETUP_TC
+#ifdef HAVE_RHEL7_NETDEV_OPS_EXT_NDO_SETUP_TC
+	.extended.ndo_setup_tc_rh = __i40e_setup_tc,
+#else
+#ifdef NETIF_F_HW_TC
 	.ndo_setup_tc		= __i40e_setup_tc,
-	.ndo_set_features	= i40e_set_features,
+#else
+	.ndo_setup_tc		= i40e_setup_tc,
+#endif /* NETIF_F_HW_TC */
+#endif /* HAVE_RHEL7_NETDEV_OPS_EXT_NDO_SETUP_TC */
+#endif /* HAVE_SETUP_TC */
+#ifdef HAVE_NETDEV_SELECT_QUEUE
+	.ndo_select_queue	= i40e_lan_select_queue,
+#endif
+#ifdef HAVE_RHEL7_NET_DEVICE_OPS_EXT
+/* RHEL7 requires this to be defined to enable extended ops.  RHEL7 uses the
+ * function get_ndo_ext to retrieve offsets for extended fields from with the
+ * net_device_ops struct and ndo_size is checked to determine whether or not
+ * the offset is valid.
+ */
+	.ndo_size		= sizeof(const struct net_device_ops),
+#endif
+#ifdef IFLA_VF_MAX
 	.ndo_set_vf_mac		= i40e_ndo_set_vf_mac,
+#ifdef HAVE_RHEL7_NETDEV_OPS_EXT_NDO_SET_VF_VLAN
+	.extended.ndo_set_vf_vlan = i40e_ndo_set_vf_port_vlan,
+#else
 	.ndo_set_vf_vlan	= i40e_ndo_set_vf_port_vlan,
+#endif
+#ifdef HAVE_VF_STATS
+	.ndo_get_vf_stats	= i40e_get_vf_stats,
+#endif
+#ifdef HAVE_NDO_SET_VF_MIN_MAX_TX_RATE
 	.ndo_set_vf_rate	= i40e_ndo_set_vf_bw,
+#else
+	.ndo_set_vf_tx_rate	= i40e_ndo_set_vf_bw,
+#endif
 	.ndo_get_vf_config	= i40e_ndo_get_vf_config,
-	.ndo_set_vf_link_state	= i40e_ndo_set_vf_link_state,
+#ifdef HAVE_VF_SPOOFCHK_CONFIGURE
 	.ndo_set_vf_spoofchk	= i40e_ndo_set_vf_spoofchk,
+#endif
+#ifdef HAVE_NDO_SET_VF_TRUST
+#ifdef HAVE_RHEL7_NET_DEVICE_OPS_EXT
+	.extended.ndo_set_vf_trust = i40e_ndo_set_vf_trust,
+#else
 	.ndo_set_vf_trust	= i40e_ndo_set_vf_trust,
+#endif /* HAVE_RHEL7_NET_DEVICE_OPS_EXT */
+#endif /* HAVE_NDO_SET_VF_TRUST */
+#endif /* IFLA_VF_MAX */
+#ifdef HAVE_UDP_ENC_RX_OFFLOAD
+#ifdef HAVE_RHEL7_NETDEV_OPS_EXT_NDO_UDP_TUNNEL
+	.extended.ndo_udp_tunnel_add = i40e_udp_tunnel_add,
+	.extended.ndo_udp_tunnel_del = i40e_udp_tunnel_del,
+#else /* !HAVE_RHEL7_NETDEV_OPS_EXT_NDO_UDP_TUNNEL */
+#ifndef HAVE_UDP_TUNNEL_NIC_INFO
 	.ndo_udp_tunnel_add	= i40e_udp_tunnel_add,
 	.ndo_udp_tunnel_del	= i40e_udp_tunnel_del,
-	.ndo_get_phys_port_id	= i40e_get_phys_port_id,
+#endif /* !HAVE_UDP_TUNNEL_NIC_INFO */
+#endif /* HAVE_RHEL7_NETDEV_OPS_EXT_NDO_UDP_TUNNEL */
+#else /* !HAVE_UDP_ENC_RX_OFFLOAD */
+#ifdef HAVE_VXLAN_RX_OFFLOAD
+#if IS_ENABLED(CONFIG_VXLAN)
+	.ndo_add_vxlan_port	= i40e_add_vxlan_port,
+	.ndo_del_vxlan_port	= i40e_del_vxlan_port,
+#endif
+#endif /* HAVE_VXLAN_RX_OFFLOAD */
+#ifdef HAVE_GENEVE_RX_OFFLOAD
+#if IS_ENABLED(CONFIG_GENEVE)
+	.ndo_add_geneve_port	= i40e_add_geneve_port,
+	.ndo_del_geneve_port	= i40e_del_geneve_port,
+#endif
+#endif /* HAVE_GENEVE_RX_OFFLOAD */
+#endif /* HAVE_UDP_ENC_RX_OFFLOAD */
+#ifdef HAVE_NDO_GET_PHYS_PORT_ID
+	.ndo_get_phys_port_id   = i40e_get_phys_port_id,
+#endif /* HAVE_NDO_GET_PHYS_PORT_ID */
+#ifdef HAVE_FDB_OPS
 	.ndo_fdb_add		= i40e_ndo_fdb_add,
+#ifndef USE_DEFAULT_FDB_DEL_DUMP
+	.ndo_fdb_del		= i40e_ndo_fdb_del,
+	.ndo_fdb_dump		= i40e_ndo_fdb_dump,
+#endif
+#ifdef HAVE_NDO_FEATURES_CHECK
 	.ndo_features_check	= i40e_features_check,
+#endif /* HAVE_NDO_FEATURES_CHECK */
+#ifdef HAVE_BRIDGE_ATTRIBS
 	.ndo_bridge_getlink	= i40e_ndo_bridge_getlink,
 	.ndo_bridge_setlink	= i40e_ndo_bridge_setlink,
+#endif /* HAVE_BRIDGE_ATTRIBS */
+#endif /* HAVE_FDB_OPS */
+#ifdef HAVE_XDP_SUPPORT
+#ifdef HAVE_NDO_BPF
 	.ndo_bpf		= i40e_xdp,
 	.ndo_xdp_xmit		= i40e_xdp_xmit,
-	.ndo_xsk_wakeup	        = i40e_xsk_wakeup,
-	.ndo_dfwd_add_station	= i40e_fwd_add,
-	.ndo_dfwd_del_station	= i40e_fwd_del,
+#else
+	.ndo_xdp                = i40e_xdp,
+	.ndo_xdp_xmit		= i40e_xdp_xmit,
+	.ndo_xdp_flush		= i40e_xdp_flush,
+#endif /* HAVE_NDO_BPF */
+#endif /* HAVE_XDP_SUPPORT */
+#ifdef HAVE_RHEL6_NET_DEVICE_OPS_EXT
+};
+
+/* RHEL6 keeps these operations in a separate structure */
+static const struct net_device_ops_ext i40e_netdev_ops_ext = {
+	.size			= sizeof(struct net_device_ops_ext),
+#endif /* HAVE_RHEL6_NET_DEVICE_OPS_EXT */
+#ifdef HAVE_NDO_SET_FEATURES
+	.ndo_set_features	= i40e_set_features,
+#endif /* HAVE_NDO_SET_FEATURES */
+#ifdef HAVE_NDO_SET_VF_LINK_STATE
+	.ndo_set_vf_link_state	= i40e_ndo_set_vf_link_state,
+#endif
 };
+#else /* HAVE_NET_DEVICE_OPS */
+/**
+ * i40e_assign_netdev_ops - Initialize netdev operations function pointers
+ * @dev: ptr to the netdev struct
+ **/
+#ifdef HAVE_CONFIG_HOTPLUG
+static void __devinit i40e_assign_netdev_ops(struct net_device *dev)
+#else
+static void i40e_assign_netdev_ops(struct net_device *dev)
+#endif
+{
+	dev->open = i40e_open;
+	dev->stop = i40e_close;
+	dev->hard_start_xmit = i40e_lan_xmit_frame;
+	dev->get_stats = i40e_get_netdev_stats_struct;
+
+#ifdef HAVE_SET_RX_MODE
+	dev->set_rx_mode = i40e_set_rx_mode;
+#endif
+	dev->set_multicast_list = i40e_set_rx_mode;
+	dev->set_mac_address = i40e_set_mac;
+	dev->change_mtu = i40e_change_mtu;
+#if defined(HAVE_PTP_1588_CLOCK) || defined(HAVE_I40E_INTELCIM_IOCTL)
+	dev->do_ioctl = i40e_ioctl;
+#endif
+	dev->tx_timeout = i40e_tx_timeout;
+#ifdef NETIF_F_HW_VLAN_TX
+#ifdef HAVE_VLAN_RX_REGISTER
+	dev->vlan_rx_register = i40e_vlan_rx_register;
+#endif
+	dev->vlan_rx_add_vid = i40e_vlan_rx_add_vid;
+	dev->vlan_rx_kill_vid = i40e_vlan_rx_kill_vid;
+#endif
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	dev->poll_controller = i40e_netpoll;
+#endif
+#ifdef HAVE_NETDEV_SELECT_QUEUE
+	dev->select_queue = i40e_lan_select_queue;
+#endif /* HAVE_NETDEV_SELECT_QUEUE */
+}
+#endif /* HAVE_NET_DEVICE_OPS */
 
 /**
  * i40e_config_netdev - Setup the netdev flags
@@ -12920,48 +14660,114 @@
 
 	hw_enc_features = NETIF_F_SG			|
 			  NETIF_F_IP_CSUM		|
+#ifdef NETIF_F_IPV6_CSUM
 			  NETIF_F_IPV6_CSUM		|
+#endif
 			  NETIF_F_HIGHDMA		|
+#ifdef NETIF_F_SOFT_FEATURES
 			  NETIF_F_SOFT_FEATURES		|
+#endif
 			  NETIF_F_TSO			|
+#ifdef HAVE_ENCAP_TSO_OFFLOAD
 			  NETIF_F_TSO_ECN		|
 			  NETIF_F_TSO6			|
+#ifdef HAVE_GRE_ENCAP_OFFLOAD
 			  NETIF_F_GSO_GRE		|
+#ifdef NETIF_F_GSO_PARTIAL
 			  NETIF_F_GSO_GRE_CSUM		|
 			  NETIF_F_GSO_PARTIAL		|
+#endif
+#ifdef NETIF_F_GSO_IPXIP4
 			  NETIF_F_GSO_IPXIP4		|
+#ifdef NETIF_F_GSO_IPXIP6
 			  NETIF_F_GSO_IPXIP6		|
+#endif
+#else
+#ifdef NETIF_F_GSO_IPIP
+			  NETIF_F_GSO_IPIP		|
+#endif
+#ifdef NETIF_F_GSO_SIT
+			  NETIF_F_GSO_SIT		|
+#endif
+#endif
+#endif
 			  NETIF_F_GSO_UDP_TUNNEL	|
 			  NETIF_F_GSO_UDP_TUNNEL_CSUM	|
+#endif /* HAVE_ENCAP_TSO_OFFLOAD */
 			  NETIF_F_SCTP_CRC		|
+#ifdef NETIF_F_RXHASH
 			  NETIF_F_RXHASH		|
+#endif
+#ifdef HAVE_NDO_SET_FEATURES
 			  NETIF_F_RXCSUM		|
+#endif
 			  0;
 
 	if (!(pf->hw_features & I40E_HW_OUTER_UDP_CSUM_CAPABLE))
+#ifndef NETIF_F_GSO_PARTIAL
+		hw_enc_features ^= NETIF_F_GSO_UDP_TUNNEL_CSUM;
+#else
 		netdev->gso_partial_features |= NETIF_F_GSO_UDP_TUNNEL_CSUM;
 
 	netdev->gso_partial_features |= NETIF_F_GSO_GRE_CSUM;
+#endif
 
+#ifdef HAVE_ENCAP_CSUM_OFFLOAD
 	netdev->hw_enc_features |= hw_enc_features;
+#endif
 
+#ifdef HAVE_NETDEV_VLAN_FEATURES
 	/* record features VLANs can make use of */
+#ifdef NETIF_F_GSO_PARTIAL
 	netdev->vlan_features |= hw_enc_features | NETIF_F_TSO_MANGLEID;
-
-	/* enable macvlan offloads */
-	netdev->hw_features |= NETIF_F_HW_L2FW_DOFFLOAD;
+#else
+	netdev->vlan_features |= hw_enc_features;
+#endif
+#endif
 
 	hw_features = hw_enc_features		|
+#ifdef NETIF_F_HW_VLAN_CTAG_RX
 		      NETIF_F_HW_VLAN_CTAG_TX	|
 		      NETIF_F_HW_VLAN_CTAG_RX;
+#else /* NETIF_F_HW_VLAN_CTAG_RX */
+		      NETIF_F_HW_VLAN_TX	|
+		      NETIF_F_HW_VLAN_RX;
+#endif /* !NETIF_F_HW_VLAN_CTAG_RX */
 
+#if defined(HAVE_NDO_SET_FEATURES) || defined(ETHTOOL_GRXRINGS)
 	if (!(pf->flags & I40E_FLAG_MFP_ENABLED))
+#ifdef NETIF_F_HW_TC
 		hw_features |= NETIF_F_NTUPLE | NETIF_F_HW_TC;
+#else
+		hw_features |= NETIF_F_NTUPLE;
+#endif
+#endif
 
+#ifdef HAVE_NDO_SET_FEATURES
+#ifdef HAVE_RHEL6_NET_DEVICE_OPS_EXT
+	hw_features |= get_netdev_hw_features(netdev);
+	set_netdev_hw_features(netdev, hw_features);
+#else
 	netdev->hw_features |= hw_features;
+#endif
+#endif /* HAVE_NDO_SET_FEATURES */
 
+#ifdef NETIF_F_HW_VLAN_CTAG_RX
 	netdev->features |= hw_features | NETIF_F_HW_VLAN_CTAG_FILTER;
+#else
+	netdev->features |= hw_features | NETIF_F_HW_VLAN_FILTER;
+#endif
+	netdev->features |= hw_features | NETIF_F_HW_VLAN_STAG_FILTER;
+
+#ifdef NETIF_F_GSO_PARTIAL
 	netdev->hw_enc_features |= NETIF_F_TSO_MANGLEID;
+#endif
+
+#ifndef HAVE_NDO_SET_FEATURES
+#ifdef NETIF_F_GRO
+	netdev->features |= NETIF_F_GRO;
+#endif
+#endif
 
 	if (vsi->type == I40E_VSI_MAIN) {
 		SET_NETDEV_DEV(netdev, &pf->pdev->dev);
@@ -13015,23 +14821,47 @@
 	spin_unlock_bh(&vsi->mac_filter_hash_lock);
 
 	ether_addr_copy(netdev->dev_addr, mac_addr);
+#ifdef ETHTOOL_GPERMADDR
 	ether_addr_copy(netdev->perm_addr, mac_addr);
+#endif
 
-	/* i40iw_net_event() reads 16 bytes from neigh->primary_key */
-	netdev->neigh_priv_len = sizeof(u32) * 4;
+#ifdef HAVE_MPLS_FEATURES
+	if (pf->hw_features & I40E_HW_MPLS_HDR_OFFLOAD_CAPABLE)
+		netdev->mpls_features =  NETIF_F_HW_CSUM;
+#endif
 
+#ifdef IFF_UNICAST_FLT
 	netdev->priv_flags |= IFF_UNICAST_FLT;
+#endif
+#ifdef IFF_SUPP_NOFCS
 	netdev->priv_flags |= IFF_SUPP_NOFCS;
+#endif
 	/* Setup netdev TC information */
 	i40e_vsi_config_netdev_tc(vsi, vsi->tc_config.enabled_tc);
 
+#ifdef HAVE_NET_DEVICE_OPS
 	netdev->netdev_ops = &i40e_netdev_ops;
+#ifdef HAVE_RHEL6_NET_DEVICE_OPS_EXT
+	set_netdev_ops_ext(netdev, &i40e_netdev_ops_ext);
+#endif /* HAVE_RHEL6_NET_DEVICE_OPS_EXT */
+#else /* HAVE_NET_DEVICE_OPS */
+	i40e_assign_netdev_ops(netdev);
+#endif /* HAVE_NET_DEVICE_OPS */
 	netdev->watchdog_timeo = 5 * HZ;
+#ifdef SIOCETHTOOL
 	i40e_set_ethtool_ops(netdev);
+#endif
 
+#ifdef HAVE_NETDEVICE_MIN_MAX_MTU
 	/* MTU range: 68 - 9706 */
+#ifdef HAVE_RHEL7_EXTENDED_MIN_MAX_MTU
+	netdev->extended->min_mtu = ETH_MIN_MTU;
+	netdev->extended->max_mtu = I40E_MAX_RXBUFFER - I40E_PACKET_HDR_PAD;
+#else
 	netdev->min_mtu = ETH_MIN_MTU;
 	netdev->max_mtu = I40E_MAX_RXBUFFER - I40E_PACKET_HDR_PAD;
+#endif /* HAVE_RHEL7_EXTENDED_MIN_MAX_MTU */
+#endif /* HAVE_NETDEVICE_MIN_MAX_NTU */
 
 	return 0;
 }
@@ -13073,6 +14903,7 @@
 		return -ENOENT;
 	}
 
+#ifdef HAVE_BRIDGE_ATTRIBS
 	/* Uplink is a bridge in VEPA mode */
 	if (veb->bridge_mode & BRIDGE_MODE_VEPA) {
 		return 0;
@@ -13080,6 +14911,10 @@
 		/* Uplink is a bridge in VEB mode */
 		return 1;
 	}
+#else
+	if (pf->flags & I40E_FLAG_VEB_MODE_ENABLED)
+		return 1;
+#endif
 
 	/* VEPA is now default bridge, so return 0 */
 	return 0;
@@ -13104,6 +14939,7 @@
 
 	u8 enabled_tc = 0x1; /* TC0 enabled */
 	int f_count = 0;
+	u32 val;
 
 	memset(&ctxt, 0, sizeof(ctxt));
 	switch (vsi->type) {
@@ -13144,7 +14980,7 @@
 			ctxt.pf_num = pf->hw.pf_id;
 			ctxt.vf_num = 0;
 			ctxt.info.valid_sections |=
-				     cpu_to_le16(I40E_AQ_VSI_PROP_SWITCH_VALID);
+			     cpu_to_le16(I40E_AQ_VSI_PROP_SWITCH_VALID);
 			ctxt.info.switch_id =
 				   cpu_to_le16(I40E_AQ_VSI_SW_ID_FLAG_LOCAL_LB);
 			ret = i40e_aq_update_vsi_params(hw, &ctxt, NULL);
@@ -13153,7 +14989,7 @@
 					 "update vsi failed, err %s aq_err %s\n",
 					 i40e_stat_str(&pf->hw, ret),
 					 i40e_aq_str(&pf->hw,
-						     pf->hw.aq.asq_last_status));
+						    pf->hw.aq.asq_last_status));
 				ret = -ENOENT;
 				goto err;
 			}
@@ -13265,13 +15101,25 @@
 		}
 
 		ctxt.info.valid_sections |= cpu_to_le16(I40E_AQ_VSI_PROP_VLAN_VALID);
-		ctxt.info.port_vlan_flags |= I40E_AQ_VSI_PVLAN_MODE_ALL;
-		if (pf->vf[vsi->vf_id].spoofchk) {
+		if (i40e_is_double_vlan(hw)) {
+			ctxt.info.port_vlan_flags |=
+				I40E_AQ_VSI_PVLAN_MODE_ALL |
+				I40E_AQ_VSI_PVLAN_EMOD_NOTHING;
+			ctxt.info.outer_vlan_flags |=
+				I40E_AQ_VSI_OVLAN_MODE_ALL |
+				I40E_OVLAN_EMOD_SHIFT
+					(I40E_AQ_VSI_OVLAN_EMOD_NOTHING) |
+				I40E_OVLAN_EMOD_SHIFT
+					(I40E_AQ_VSI_OVLAN_CTRL_ENA);
+		} else {
+			ctxt.info.port_vlan_flags |= I40E_AQ_VSI_PVLAN_MODE_ALL;
+		}
+
+		if (pf->vf[vsi->vf_id].mac_anti_spoof) {
 			ctxt.info.valid_sections |=
 				cpu_to_le16(I40E_AQ_VSI_PROP_SECURITY_VALID);
 			ctxt.info.sec_flags |=
-				(I40E_AQ_VSI_SEC_FLAG_ENABLE_VLAN_CHK |
-				 I40E_AQ_VSI_SEC_FLAG_ENABLE_MAC_CHK);
+				I40E_AQ_VSI_SEC_FLAG_ENABLE_MAC_CHK;
 		}
 		/* Setup the VSI tx/rx queue map for TC0 only for now */
 		i40e_vsi_setup_queue_map(vsi, &ctxt, enabled_tc, true);
@@ -13300,6 +15148,24 @@
 		vsi->info.valid_sections = 0;
 		vsi->seid = ctxt.seid;
 		vsi->id = ctxt.vsi_number;
+		val = rd32(&pf->hw, 0x208800 + (4*(vsi->id)));
+		if (!(val & 0x1)) /* MACVSIPRUNEENABLE = 1*/
+			dev_warn(&vsi->back->pdev->dev,
+				 "Note: VSI source pruning is not being set correctly by FW\n");
+		if ((pf->flags & I40E_FLAG_MULTIPLE_TRAFFIC_CLASSES) &&
+		    vsi->type == I40E_VSI_SRIOV) {
+			/* Reconfigure VF VSI, so it has same traffic classes
+			 * as Main VSI
+			 */
+			enabled_tc = i40e_pf_get_tc_map(pf);
+			ret = i40e_vsi_config_tc(vsi, enabled_tc);
+			if (ret) {
+				dev_warn(&pf->pdev->dev,
+					 "Failed to configure TCs for main VSI tc_map 0x%08x, err %s\n",
+					 enabled_tc,
+					 i40e_stat_str(&pf->hw, ret));
+			}
+		}
 	}
 
 	vsi->active_filters = 0;
@@ -13361,6 +15227,7 @@
 		return -ENODEV;
 	}
 
+	set_bit(__I40E_VSI_RELEASING, vsi->state);
 	uplink_seid = vsi->uplink_seid;
 	if (vsi->type != I40E_VSI_SRIOV) {
 		if (vsi->netdev_registered) {
@@ -13465,6 +15332,7 @@
 		goto vector_setup_out;
 	}
 
+#if !defined(I40E_LEGACY_INTERRUPT) && !defined(I40E_MSI_INTERRUPT)
 	/* In Legacy mode, we do not have to get any other vector since we
 	 * piggyback on the misc/ICR0 for queue interrupts.
 	*/
@@ -13482,6 +15350,7 @@
 		goto vector_setup_out;
 	}
 
+#endif
 vector_setup_out:
 	return ret;
 }
@@ -13579,7 +15448,6 @@
 {
 	struct i40e_vsi *vsi = NULL;
 	struct i40e_veb *veb = NULL;
-	u16 alloc_queue_pairs;
 	int ret, i;
 	int v_idx;
 
@@ -13629,6 +15497,7 @@
 					 "New VSI creation error, uplink seid of LAN VSI expected.\n");
 				return NULL;
 			}
+#ifdef HAVE_BRIDGE_ATTRIBS
 			/* We come up by default in VEPA mode if SRIOV is not
 			 * already enabled, in which case we can't force VEPA
 			 * mode.
@@ -13637,6 +15506,7 @@
 				veb->bridge_mode = BRIDGE_MODE_VEPA;
 				pf->flags &= ~I40E_FLAG_VEB_MODE_ENABLED;
 			}
+#endif
 			i40e_config_bridge_mode(veb);
 		}
 		for (i = 0; i < I40E_MAX_VEB && !veb; i++) {
@@ -13667,14 +15537,12 @@
 	else if (type == I40E_VSI_SRIOV)
 		vsi->vf_id = param1;
 	/* assign it some queues */
-	alloc_queue_pairs = vsi->alloc_queue_pairs *
-			    (i40e_enabled_xdp_vsi(vsi) ? 2 : 1);
-
-	ret = i40e_get_lump(pf, pf->qp_pile, alloc_queue_pairs, vsi->idx);
+	ret = i40e_get_lump(pf, pf->qp_pile, vsi->alloc_queue_pairs,
+				vsi->idx);
 	if (ret < 0) {
 		dev_info(&pf->pdev->dev,
 			 "failed to get tracking for %d queues for VSI %d err=%d\n",
-			 alloc_queue_pairs, vsi->seid, ret);
+			 vsi->alloc_queue_pairs, vsi->seid, ret);
 		goto err_vsi;
 	}
 	vsi->base_queue = ret;
@@ -13697,10 +15565,14 @@
 			goto err_netdev;
 		vsi->netdev_registered = true;
 		netif_carrier_off(vsi->netdev);
-#ifdef CONFIG_I40E_DCB
+		/* make sure transmit queues start off as stopped */
+		netif_tx_stop_all_queues(vsi->netdev);
+#ifdef CONFIG_DCB
+#ifdef HAVE_DCBNL_IEEE
 		/* Setup DCB netlink interface */
 		i40e_dcbnl_setup(vsi);
-#endif /* CONFIG_I40E_DCB */
+#endif /* HAVE_DCBNL_IEEE */
+#endif /* CONFIG_DCB */
 		/* fall through */
 
 	case I40E_VSI_FDIR:
@@ -13783,16 +15655,16 @@
 		goto out;
 	}
 
-	veb->bw_limit = le16_to_cpu(ets_data.port_bw_limit);
+	veb->bw_limit = LE16_TO_CPU(ets_data.port_bw_limit);
 	veb->bw_max_quanta = ets_data.tc_bw_max;
 	veb->is_abs_credits = bw_data.absolute_credits_enable;
 	veb->enabled_tc = ets_data.tc_valid_bits;
-	tc_bw_max = le16_to_cpu(bw_data.tc_bw_max[0]) |
-		    (le16_to_cpu(bw_data.tc_bw_max[1]) << 16);
+	tc_bw_max = LE16_TO_CPU(bw_data.tc_bw_max[0]) |
+		    (LE16_TO_CPU(bw_data.tc_bw_max[1]) << 16);
 	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
 		veb->bw_tc_share_credits[i] = bw_data.tc_bw_share_credits[i];
 		veb->bw_tc_limit_credits[i] =
-					le16_to_cpu(bw_data.tc_bw_limits[i]);
+					LE16_TO_CPU(bw_data.tc_bw_limits[i]);
 		veb->bw_tc_max_quanta[i] = ((tc_bw_max >> (i*4)) & 0x7);
 	}
 
@@ -14340,11 +16212,17 @@
 	pf->fc_autoneg_status = ((pf->hw.phy.link_info.an_info &
 				  I40E_AQ_AN_COMPLETED) ? true : false);
 
+#ifdef HAVE_PTP_1588_CLOCK
 	i40e_ptp_init(pf);
 
+#endif /* HAVE_PTP_1588_CLOCK */
+#if defined(HAVE_VXLAN_RX_OFFLOAD) || defined(HAVE_UDP_ENC_RX_OFFLOAD)
+#if defined(HAVE_UDP_ENC_TUNNEL) || defined(HAVE_UDP_ENC_RX_OFFLOAD)
 	/* repopulate tunnel port filters */
 	i40e_sync_udp_filters(pf);
 
+#endif /* HAVE_UDP_ENC_TUNNEL || HAVE_UDP_ENC_RX_OFFLOAD */
+#endif /* HAVE_VXLAN_RX_OFFLOAD || HAVE_UDP_ENC_RX_OFFLOAD */
 	return ret;
 }
 
@@ -14355,7 +16233,6 @@
 static void i40e_determine_queue_usage(struct i40e_pf *pf)
 {
 	int queues_left;
-	int q_max;
 
 	pf->num_lan_qps = 0;
 
@@ -14398,18 +16275,31 @@
 		pf->flags |= I40E_FLAG_FD_SB_INACTIVE;
 	} else {
 		/* Not enough queues for all TCs */
+		bool is_max_n_of_queues_required;
+
 		if ((pf->flags & I40E_FLAG_DCB_CAPABLE) &&
 		    (queues_left < I40E_MAX_TRAFFIC_CLASS)) {
 			pf->flags &= ~(I40E_FLAG_DCB_CAPABLE |
 					I40E_FLAG_DCB_ENABLED);
 			dev_info(&pf->pdev->dev, "not enough queues for DCB. DCB is disabled.\n");
 		}
-
-		/* limit lan qps to the smaller of qps, cpus or msix */
-		q_max = max_t(int, pf->rss_size_max, num_online_cpus());
-		q_max = min_t(int, q_max, pf->hw.func_caps.num_tx_qp);
-		q_max = min_t(int, q_max, pf->hw.func_caps.num_msix_vectors);
-		pf->num_lan_qps = q_max;
+		pf->num_lan_qps = max_t(int, pf->rss_size_max,
+					num_online_cpus());
+		is_max_n_of_queues_required = pf->hw.func_caps.num_tx_qp <
+			pf->num_req_vfs * pf->num_vf_qps + pf->num_vf_qps;
+		if (is_max_n_of_queues_required)
+			dev_warn(&pf->pdev->dev, "not enough %u queues for PF and %u VFs. Using maximum available queues for PF.\n",
+				 pf->hw.func_caps.num_tx_qp, pf->num_req_vfs);
+		if (pf->hw.func_caps.npar_enable ||
+		    is_max_n_of_queues_required)
+			pf->num_lan_qps = min_t
+				(int, pf->num_lan_qps,
+				 pf->hw.func_caps.num_tx_qp);
+		else
+			pf->num_lan_qps = min_t
+				(int, pf->num_lan_qps,
+				 pf->hw.func_caps.num_tx_qp -
+				 pf->num_req_vfs * pf->num_vf_qps);
 
 		queues_left -= pf->num_lan_qps;
 	}
@@ -14506,12 +16396,22 @@
 		i += snprintf(&buf[i], REMAIN(i), " FD_SB");
 		i += snprintf(&buf[i], REMAIN(i), " NTUPLE");
 	}
+	i += snprintf(&buf[i], REMAIN(i), " CloudF");
 	if (pf->flags & I40E_FLAG_DCB_CAPABLE)
 		i += snprintf(&buf[i], REMAIN(i), " DCB");
+#if IS_ENABLED(CONFIG_VXLAN)
 	i += snprintf(&buf[i], REMAIN(i), " VxLAN");
+#endif
+#if IS_ENABLED(CONFIG_GENEVE)
 	i += snprintf(&buf[i], REMAIN(i), " Geneve");
+#endif
+#ifdef HAVE_GRE_ENCAP_OFFLOAD
+	i += snprintf(&buf[i], REMAIN(i), " NVGRE");
+#endif
+#ifdef HAVE_PTP_1588_CLOCK
 	if (pf->flags & I40E_FLAG_PTP)
 		i += snprintf(&buf[i], REMAIN(i), " PTP");
+#endif
 	if (pf->flags & I40E_FLAG_VEB_MODE_ENABLED)
 		i += snprintf(&buf[i], REMAIN(i), " VEB");
 	else
@@ -14539,29 +16439,6 @@
 }
 
 /**
- * i40e_set_fec_in_flags - helper function for setting FEC options in flags
- * @fec_cfg: FEC option to set in flags
- * @flags: ptr to flags in which we set FEC option
- **/
-void i40e_set_fec_in_flags(u8 fec_cfg, u32 *flags)
-{
-	if (fec_cfg & I40E_AQ_SET_FEC_AUTO)
-		*flags |= I40E_FLAG_RS_FEC | I40E_FLAG_BASE_R_FEC;
-	if ((fec_cfg & I40E_AQ_SET_FEC_REQUEST_RS) ||
-	    (fec_cfg & I40E_AQ_SET_FEC_ABILITY_RS)) {
-		*flags |= I40E_FLAG_RS_FEC;
-		*flags &= ~I40E_FLAG_BASE_R_FEC;
-	}
-	if ((fec_cfg & I40E_AQ_SET_FEC_REQUEST_KR) ||
-	    (fec_cfg & I40E_AQ_SET_FEC_ABILITY_KR)) {
-		*flags |= I40E_FLAG_BASE_R_FEC;
-		*flags &= ~I40E_FLAG_RS_FEC;
-	}
-	if (fec_cfg == 0)
-		*flags &= ~(I40E_FLAG_RS_FEC | I40E_FLAG_BASE_R_FEC);
-}
-
-/**
  * i40e_check_recovery_mode - check if we are running transition firmware
  * @pf: board private structure
  *
@@ -14572,28 +16449,17 @@
  **/
 static bool i40e_check_recovery_mode(struct i40e_pf *pf)
 {
-	u32 val = rd32(&pf->hw, I40E_GL_FWSTS) & I40E_GL_FWSTS_FWS1B_MASK;
-	bool is_recovery_mode = false;
+	u32 val = rd32(&pf->hw, I40E_GL_FWSTS);
 
-	if (pf->hw.mac.type == I40E_MAC_XL710)
-		is_recovery_mode =
-		val == I40E_XL710_GL_FWSTS_FWS1B_REC_MOD_CORER_MASK ||
-		val == I40E_XL710_GL_FWSTS_FWS1B_REC_MOD_GLOBR_MASK ||
-		val == I40E_XL710_GL_FWSTS_FWS1B_REC_MOD_TRANSITION_MASK ||
-		val == I40E_XL710_GL_FWSTS_FWS1B_REC_MOD_NVM_MASK;
-	if (pf->hw.mac.type == I40E_MAC_X722)
-		is_recovery_mode =
-		val == I40E_X722_GL_FWSTS_FWS1B_REC_MOD_CORER_MASK ||
-		val == I40E_X722_GL_FWSTS_FWS1B_REC_MOD_GLOBR_MASK;
-	if (is_recovery_mode) {
-		dev_notice(&pf->pdev->dev, "Firmware recovery mode detected. Limiting functionality.\n");
-		dev_notice(&pf->pdev->dev, "Refer to the Intel(R) Ethernet Adapters and Devices User Guide for details on firmware recovery mode.\n");
+	if (val & I40E_GL_FWSTS_FWS1B_MASK) {
+		dev_crit(&pf->pdev->dev, "Firmware recovery mode detected. Limiting functionality.\n");
+		dev_crit(&pf->pdev->dev, "Refer to the Intel(R) Ethernet Adapters and Devices User Guide for details on firmware recovery mode.\n");
 		set_bit(__I40E_RECOVERY_MODE, pf->state);
 
 		return true;
 	}
-	if (test_and_clear_bit(__I40E_RECOVERY_MODE, pf->state))
-		dev_info(&pf->pdev->dev, "Reinitializing in normal mode with full functionality.\n");
+	if (test_bit(__I40E_RECOVERY_MODE, pf->state))
+		dev_info(&pf->pdev->dev, "Please do Power-On Reset to initialize adapter in normal mode with full functionality.\n");
 
 	return false;
 }
@@ -14619,31 +16485,72 @@
  *
  * Return 0 on success, negative on failure.
  **/
-static i40e_status i40e_pf_loop_reset(struct i40e_pf *pf)
+static i40e_status i40e_pf_loop_reset(struct i40e_pf * const pf)
 {
-	const unsigned short MAX_CNT = 1000;
-	const unsigned short MSECS = 10;
+	/* wait max 10 seconds for PF reset to succeed */
+	const unsigned long time_end = jiffies + 10 * HZ;
+
 	struct i40e_hw *hw = &pf->hw;
 	i40e_status ret;
-	int cnt;
 
-	for (cnt = 0; cnt < MAX_CNT; ++cnt) {
+	ret = i40e_pf_reset(hw);
+	while (ret != I40E_SUCCESS && time_before(jiffies, time_end)) {
+		usleep_range(10000, 20000);
 		ret = i40e_pf_reset(hw);
-		if (!ret)
-			break;
-		msleep(MSECS);
 	}
 
-	if (cnt == MAX_CNT) {
+	if (ret == I40E_SUCCESS)
+		pf->pfr_count++;
+	else
 		dev_info(&pf->pdev->dev, "PF reset failed: %d\n", ret);
-		return ret;
-	}
 
-	pf->pfr_count++;
 	return ret;
 }
 
 /**
+ * i40e_check_fw_empr - check if FW issued unexpected EMP Reset
+ * @pf: board private structure
+ *
+ * Check FW registers to determine if FW issued unexpected EMP Reset.
+ * Every time when unexpected EMP Reset occurs the FW increments
+ * a counter of unexpected EMP Resets. When the counter reaches 10
+ * the FW should enter the Recovery mode
+ *
+ * Returns true if FW issued unexpected EMP Reset
+ **/
+static inline bool i40e_check_fw_empr(struct i40e_pf * const pf)
+{
+	const u32 fw_sts = rd32(&pf->hw, I40E_GL_FWSTS) &
+			   I40E_GL_FWSTS_FWS1B_MASK;
+	const bool is_empr = (fw_sts > I40E_GL_FWSTS_FWS1B_EMPR_0) &&
+			     (fw_sts <= I40E_GL_FWSTS_FWS1B_EMPR_10);
+
+	return is_empr;
+}
+
+/**
+ * i40e_handle_resets - handle EMP resets and PF resets
+ * @pf: board private structure
+ *
+ * Handle both EMP resets and PF resets and conclude whether there are
+ * any issues regarding these resets. If there are any issues then
+ * generate log entry.
+ *
+ * Return 0 if NIC is healthy or negative value when there are issues
+ * with resets
+ **/
+static inline i40e_status i40e_handle_resets(struct i40e_pf * const pf)
+{
+	const i40e_status pfr = i40e_pf_loop_reset(pf);
+	const bool is_empr = i40e_check_fw_empr(pf);
+
+	if (is_empr || pfr != I40E_SUCCESS)
+		dev_crit(&pf->pdev->dev, "Entering recovery mode due to repeated FW resets. This may take several minutes. Refer to the Intel(R) Ethernet Adapters and Devices User Guide.\n");
+
+	return is_empr ? I40E_ERR_RESET_FAILED : pfr;
+}
+
+/**
  * i40e_init_recovery_mode - initialize subsystems needed in recovery mode
  * @pf: board private structure
  * @hw: ptr to the hardware info
@@ -14659,7 +16566,9 @@
 	int err;
 	int v_idx;
 
+#ifdef HAVE_PCI_ERS
 	pci_save_state(pf->pdev);
+#endif
 
 	/* set up periodic task facility */
 	timer_setup(&pf->service_timer, i40e_service_timer, 0);
@@ -14726,14 +16635,48 @@
 err_switch_setup:
 	i40e_reset_interrupt_capability(pf);
 	del_timer_sync(&pf->service_timer);
-	i40e_shutdown_adminq(hw);
-	iounmap(hw->hw_addr);
-	pci_disable_pcie_error_reporting(pf->pdev);
-	pci_release_mem_regions(pf->pdev);
-	pci_disable_device(pf->pdev);
-	kfree(pf);
+	dev_warn(&pf->pdev->dev, "previous errors forcing module to load in debug mode\n");
+	i40e_dbg_pf_init(pf);
+	set_bit(__I40E_DEBUG_MODE, pf->state);
+	return 0;
+}
 
-	return err;
+/**
+ * i40e_set_fec_in_flags - helper function for setting FEC options in flags
+ * @fec_cfg: FEC option to set in flags
+ * @flags: ptr to flags in which we set FEC option
+ **/
+void i40e_set_fec_in_flags(u8 fec_cfg, u64 *flags)
+{
+	if (fec_cfg & I40E_AQ_SET_FEC_AUTO)
+		*flags |= I40E_FLAG_RS_FEC | I40E_FLAG_BASE_R_FEC;
+	else if ((fec_cfg & I40E_AQ_SET_FEC_REQUEST_RS) ||
+		 (fec_cfg & I40E_AQ_SET_FEC_ABILITY_RS)) {
+		*flags |= I40E_FLAG_RS_FEC;
+		*flags &= ~I40E_FLAG_BASE_R_FEC;
+	} else if ((fec_cfg & I40E_AQ_SET_FEC_REQUEST_KR) ||
+		   (fec_cfg & I40E_AQ_SET_FEC_ABILITY_KR)) {
+		*flags |= I40E_FLAG_BASE_R_FEC;
+		*flags &= ~I40E_FLAG_RS_FEC;
+	}
+	if (fec_cfg == 0)
+		*flags &= ~(I40E_FLAG_RS_FEC | I40E_FLAG_BASE_R_FEC);
+}
+
+/**
+ * i40e_set_subsystem_device_id - set subsystem device id
+ * @hw: pointer to the hardware info
+ *
+ * Set PCI subsystem device id either from a pci_dev structure or
+ * a specific FW register.
+ **/
+static inline void i40e_set_subsystem_device_id(struct i40e_hw *hw)
+{
+	struct pci_dev *pdev = ((struct i40e_pf *)hw->back)->pdev;
+
+	hw->subsystem_device_id = pdev->subsystem_device ?
+		pdev->subsystem_device :
+		(ushort)(rd32(hw, I40E_PFPCI_SUBSYSID) & USHRT_MAX);
 }
 
 /**
@@ -14747,19 +16690,27 @@
  *
  * Returns 0 on success, negative on failure
  **/
+#ifdef HAVE_CONFIG_HOTPLUG
+static int __devinit i40e_probe(struct pci_dev *pdev,
+				const struct pci_device_id *ent)
+#else
 static int i40e_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
+#endif
 {
 	struct i40e_aq_get_phy_abilities_resp abilities;
+#ifdef CONFIG_DCB
+	enum i40e_get_fw_lldp_status_resp lldp_status;
+	i40e_status status;
+#endif /* CONFIG_DCB */
+	u16 sw_flags = 0, valid_flags = 0;
 	struct i40e_pf *pf;
 	struct i40e_hw *hw;
 	static u16 pfs_found;
 	u16 wol_nvm_bits;
 	u16 link_status;
-	int err;
+	int err = 0;
 	u32 val;
 	u32 i;
-	u8 set_fc_aq_fail;
-
 	err = pci_enable_device_mem(pdev);
 	if (err)
 		return err;
@@ -14779,7 +16730,7 @@
 	err = pci_request_mem_regions(pdev, i40e_driver_name);
 	if (err) {
 		dev_info(&pdev->dev,
-			 "pci_request_selected_regions failed %d\n", err);
+			 "pci_request_mem_regions failed %d\n", err);
 		goto err_pci_reg;
 	}
 
@@ -14798,6 +16749,7 @@
 	}
 	pf->next_vsi = 0;
 	pf->pdev = pdev;
+	pci_set_drvdata(pdev, pf);
 	set_bit(__I40E_DOWN, pf->state);
 
 	hw = &pf->hw;
@@ -14828,7 +16780,7 @@
 	hw->device_id = pdev->device;
 	pci_read_config_byte(pdev, PCI_REVISION_ID, &hw->revision_id);
 	hw->subsystem_vendor_id = pdev->subsystem_vendor;
-	hw->subsystem_device_id = pdev->subsystem_device;
+	i40e_set_subsystem_device_id(hw);
 	hw->bus.device = PCI_SLOT(pdev->devfn);
 	hw->bus.func = PCI_FUNC(pdev->devfn);
 	hw->bus.bus_id = pdev->bus->number;
@@ -14845,31 +16797,17 @@
 	INIT_LIST_HEAD(&pf->l4_flex_pit_list);
 	INIT_LIST_HEAD(&pf->ddp_old_prof);
 
-	/* set up the locks for the AQ, do this only once in probe
+	/* set up the spinlocks for the AQ, do this only once in probe
 	 * and destroy them only once in remove
 	 */
-	mutex_init(&hw->aq.asq_mutex);
-	mutex_init(&hw->aq.arq_mutex);
-
-	pf->msg_enable = netif_msg_init(debug,
-					NETIF_MSG_DRV |
-					NETIF_MSG_PROBE |
-					NETIF_MSG_LINK);
-	if (debug < -1)
-		pf->hw.debug_mask = debug;
-
-	/* do a special CORER for clearing PXE mode once at init */
-	if (hw->revision_id == 0 &&
-	    (rd32(hw, I40E_GLLAN_RCTL_0) & I40E_GLLAN_RCTL_0_PXE_MODE_MASK)) {
-		wr32(hw, I40E_GLGEN_RTRIG, I40E_GLGEN_RTRIG_CORER_MASK);
-		i40e_flush(hw);
-		msleep(200);
-		pf->corer_count++;
+	i40e_init_spinlock_d(&hw->aq.asq_spinlock);
+	i40e_init_spinlock_d(&hw->aq.arq_spinlock);
 
-		i40e_clear_pxe_mode(hw);
-	}
+	if (debug != -1)
+		pf->msg_enable = pf->hw.debug_mask = debug;
 
 	/* Reset here to make sure all is clean and to define PF 'n' */
+	/* have to do the PF reset first to "graceful abort" all queues */
 	i40e_clear_hw(hw);
 
 	err = i40e_set_mac_type(hw);
@@ -14879,16 +16817,19 @@
 		goto err_pf_reset;
 	}
 
-	err = i40e_pf_loop_reset(pf);
-	if (err) {
-		dev_info(&pdev->dev, "Initial pf_reset failed: %d\n", err);
+	err = i40e_handle_resets(pf);
+	if (err)
 		goto err_pf_reset;
-	}
 
 	i40e_check_recovery_mode(pf);
 
-	hw->aq.num_arq_entries = I40E_AQ_LEN;
-	hw->aq.num_asq_entries = I40E_AQ_LEN;
+	if (is_kdump_kernel()) {
+		hw->aq.num_arq_entries = I40E_MIN_ARQ_LEN;
+		hw->aq.num_asq_entries = I40E_MIN_ASQ_LEN;
+	} else {
+		hw->aq.num_arq_entries = I40E_AQ_LEN;
+		hw->aq.num_asq_entries = I40E_AQ_LEN;
+	}
 	hw->aq.arq_buf_size = I40E_MAX_AQ_BUF_SIZE;
 	hw->aq.asq_buf_size = I40E_MAX_AQ_BUF_SIZE;
 	pf->adminq_work_limit = I40E_AQ_WORK_LIMIT;
@@ -14924,12 +16865,11 @@
 	}
 	i40e_get_oem_version(hw);
 
-	/* provide nvm, fw, api versions, vendor:device id, subsys vendor:device id */
-	dev_info(&pdev->dev, "fw %d.%d.%05d api %d.%d nvm %s [%04x:%04x] [%04x:%04x]\n",
+	/* provide nvm, fw, api versions */
+	dev_info(&pdev->dev, "fw %d.%d.%05d api %d.%d nvm %s\n",
 		 hw->aq.fw_maj_ver, hw->aq.fw_min_ver, hw->aq.fw_build,
 		 hw->aq.api_maj_ver, hw->aq.api_min_ver,
-		 i40e_nvm_version_str(hw), hw->vendor_id, hw->device_id,
-		 hw->subsystem_vendor_id, hw->subsystem_device_id);
+		 i40e_nvm_version_str(hw));
 
 	if (hw->aq.api_maj_ver == I40E_FW_API_VERSION_MAJOR &&
 	    hw->aq.api_min_ver > I40E_FW_MINOR_VERSION(hw))
@@ -15004,26 +16944,39 @@
 	i40e_get_port_mac_addr(hw, hw->mac.port_addr);
 	if (is_valid_ether_addr(hw->mac.port_addr))
 		pf->hw_features |= I40E_HW_PORT_ID_VALID;
+#ifdef HAVE_PTP_1588_CLOCK
+	i40e_ptp_alloc_pins(pf);
+#endif /* HAVE_PTP_1588_CLOCK */
 
-	pci_set_drvdata(pdev, pf);
+#ifdef HAVE_PCI_ERS
 	pci_save_state(pdev);
-
+#endif
+#ifdef CONFIG_DCB
+	status = i40e_get_fw_lldp_status(&pf->hw, &lldp_status);
+	(status == I40E_SUCCESS &&
+	 lldp_status == I40E_GET_FW_LLDP_STATUS_ENABLED) ?
+		(pf->flags &= ~I40E_FLAG_DISABLE_FW_LLDP) :
+		(pf->flags |= I40E_FLAG_DISABLE_FW_LLDP);
 	dev_info(&pdev->dev,
-		 (pf->flags & I40E_FLAG_DISABLE_FW_LLDP) ?
-			"FW LLDP is disabled\n" :
-			"FW LLDP is enabled\n");
+		 (pf->flags & (I40E_FLAG_DISABLE_FW_LLDP |
+			       I40E_FLAG_MULTIPLE_TRAFFIC_CLASSES)) ?
+		 "FW LLDP is disabled\n" :
+		 "FW LLDP is enabled\n");
 
 	/* Enable FW to write default DCB config on link-up */
 	i40e_aq_set_dcb_parameters(hw, true, NULL);
 
-#ifdef CONFIG_I40E_DCB
 	err = i40e_init_pf_dcb(pf);
 	if (err) {
 		dev_info(&pdev->dev, "DCB init failed %d, disabled\n", err);
 		pf->flags &= ~(I40E_FLAG_DCB_CAPABLE | I40E_FLAG_DCB_ENABLED);
 		/* Continue without DCB enabled */
 	}
-#endif /* CONFIG_I40E_DCB */
+#endif /* CONFIG_DCB */
+
+#ifdef NETIF_F_HW_TC
+	pf->flags |= I40E_FLAG_CLS_FLOWER;
+#endif /* NET_F_HW_TC */
 
 	/* set up periodic task facility */
 	timer_setup(&pf->service_timer, i40e_service_timer, 0);
@@ -15034,7 +16987,7 @@
 
 	/* NVM bit on means WoL disabled for the port */
 	i40e_read_nvm_word(hw, I40E_SR_NVM_WAKE_ON_LAN, &wol_nvm_bits);
-	if (BIT (hw->port) & wol_nvm_bits || hw->partition_id != 1)
+	if (BIT(hw->port) & wol_nvm_bits || hw->partition_id != 1)
 		pf->wol_en = false;
 	else
 		pf->wol_en = true;
@@ -15046,6 +16999,14 @@
 	if (err)
 		goto err_switch_setup;
 
+	/* Reduce tx and rx pairs for kdump
+	 * When MSI-X is enabled, it's not allowed to use more TC queue
+	 * pairs than MSI-X vectors (pf->num_lan_msix) exist. Thus
+	 * vsi->num_queue_pairs will be equal to pf->num_lan_msix, i.e., 1.
+	 */
+	if (is_kdump_kernel())
+		pf->num_lan_msix = 1;
+
 	/* The number of VSIs reported by the FW is the minimum guaranteed
 	 * to us; HW supports far more and we share the remaining pool with
 	 * the other PFs. We allocate space for more than the guarantee with
@@ -15071,32 +17032,77 @@
 	    !test_bit(__I40E_BAD_EEPROM, pf->state)) {
 		if (pci_num_vf(pdev))
 			pf->flags |= I40E_FLAG_VEB_MODE_ENABLED;
+#if !defined(HAVE_SRIOV_CONFIGURE) && !defined(HAVE_RHEL6_SRIOV_CONFIGURE)
+		else if (pf->num_req_vfs)
+			pf->flags |= I40E_FLAG_VEB_MODE_ENABLED;
+#endif
 	}
 #endif
+
+	if (pf->hw_features & I40E_HW_OUTER_VLAN_CAPABLE) {
+		/* Configure double VLAN */
+		if (pf->hw.func_caps.switch_mode != 1) {
+			err = i40e_aq_set_port_parameters(hw, 0, false, true,
+							  true, NULL);
+			if (err)
+				dev_info(&pf->pdev->dev, "could not set double VLAN\n");
+			else
+				hw->is_double_vlan = true;
+		} else {
+			dev_info(&pdev->dev, "switch mode is set to VLAN, can't set double VLAN\n");
+		}
+
+		/* Outer VLAN processing works only in double VLAN mode */
+		if (hw->is_double_vlan) {
+			sw_flags = I40E_AQ_SET_SWITCH_CFG_OUTER_VLAN;
+			valid_flags = I40E_AQ_SET_SWITCH_CFG_OUTER_VLAN;
+			pf->hw.first_tag = ETH_P_8021Q;
+
+			err = i40e_aq_set_switch_config(hw, sw_flags,
+							valid_flags, 0, NULL);
+			if (err) {
+				/* not a fatal problem, just keep going */
+				dev_info(&pf->pdev->dev, "set outer vlan processing fail, disabling double VLAN\n");
+				pf->hw.first_tag = ETH_P_8021AD;
+				i40e_aq_set_switch_config(hw, 0,
+							  valid_flags, 0,
+							  NULL);
+				/* Disable double VLAN */
+				err = i40e_aq_set_port_parameters(hw, 0, false,
+								  true, false,
+								  NULL);
+				if (err)
+					dev_info(&pf->pdev->dev, "could not disable double VLAN\n");
+				else
+					hw->is_double_vlan = false;
+			} else {
+				hw->is_outer_vlan_processing = true;
+			}
+		}
+	} else {
+		hw->is_double_vlan = false;
+		hw->is_outer_vlan_processing = false;
+	}
+
 	err = i40e_setup_pf_switch(pf, false);
 	if (err) {
 		dev_info(&pdev->dev, "setup_pf_switch failed: %d\n", err);
 		goto err_vsis;
 	}
-	INIT_LIST_HEAD(&pf->vsi[pf->lan_vsi]->ch_list);
+	if (i40e_is_l4mode_enabled() && hw->pf_id == 0) {
+		u8 l4type = I40E_AQ_SET_SWITCH_L4_TYPE_BOTH;
 
-	/* Make sure flow control is set according to current settings */
-	err = i40e_set_fc(hw, &set_fc_aq_fail, true);
-	if (set_fc_aq_fail & I40E_SET_FC_AQ_FAIL_GET)
-		dev_dbg(&pf->pdev->dev,
-			"Set fc with err %s aq_err %s on get_phy_cap\n",
-			i40e_stat_str(hw, err),
-			i40e_aq_str(hw, hw->aq.asq_last_status));
-	if (set_fc_aq_fail & I40E_SET_FC_AQ_FAIL_SET)
-		dev_dbg(&pf->pdev->dev,
-			"Set fc with err %s aq_err %s on set_phy_config\n",
-			i40e_stat_str(hw, err),
-			i40e_aq_str(hw, hw->aq.asq_last_status));
-	if (set_fc_aq_fail & I40E_SET_FC_AQ_FAIL_UPDATE)
-		dev_dbg(&pf->pdev->dev,
-			"Set fc with err %s aq_err %s on get_link_info\n",
-			i40e_stat_str(hw, err),
-			i40e_aq_str(hw, hw->aq.asq_last_status));
+		switch (l4mode) {
+		case L4_MODE_UDP:
+			l4type = I40E_AQ_SET_SWITCH_L4_TYPE_UDP;
+			break;
+		case L4_MODE_TCP:
+			l4type = I40E_AQ_SET_SWITCH_L4_TYPE_TCP;
+			break;
+		}
+		i40e_set_switch_mode(pf, l4type);
+	}
+	INIT_LIST_HEAD(&pf->vsi[pf->lan_vsi]->ch_list);
 
 	/* if FDIR VSI was set up, start it now */
 	for (i = 0; i < pf->num_alloc_vsi; i++) {
@@ -15168,6 +17174,19 @@
 		val &= ~I40E_PFGEN_PORTMDIO_NUM_VFLINK_STAT_ENA_MASK;
 		wr32(hw, I40E_PFGEN_PORTMDIO_NUM, val);
 		i40e_flush(hw);
+		/* allocate memory for VFs mac list backup */
+		val = max_t(int, pci_num_vf(pdev), pf->num_req_vfs);
+		if (val) {
+			u32 index;
+
+			pf->mac_list = kcalloc(val,
+					       sizeof(struct list_head),
+					       GFP_KERNEL);
+			if (!pf->mac_list)
+				goto err_vsis;
+			for (index = 0; index < val; index++)
+				INIT_LIST_HEAD(&pf->mac_list[index]);
+		}
 
 		if (pci_num_vf(pdev)) {
 			dev_info(&pdev->dev,
@@ -15177,6 +17196,15 @@
 				dev_info(&pdev->dev,
 					 "Error %d allocating resources for existing VFs\n",
 					 err);
+#if !defined(HAVE_SRIOV_CONFIGURE) && !defined(HAVE_RHEL6_SRIOV_CONFIGURE)
+		} else if (pf->num_req_vfs) {
+			err = i40e_alloc_vfs(pf, pf->num_req_vfs);
+			if (err) {
+				pf->flags &= ~I40E_FLAG_SRIOV_ENABLED;
+				dev_info(&pdev->dev,
+					 "failed to alloc vfs: %d\n", err);
+			}
+#endif /* HAVE_SRIOV_CONFIGURE */
 		}
 	}
 #endif /* CONFIG_PCI_IOV */
@@ -15193,6 +17221,7 @@
 		}
 	}
 
+	pfs_found++;
 	i40e_dbg_pf_init(pf);
 
 	/* tell the firmware that we're starting */
@@ -15209,7 +17238,6 @@
 			dev_info(&pdev->dev, "Failed to add PF to client API service list: %d\n",
 				 err);
 	}
-
 #define PCI_SPEED_SIZE 8
 #define PCI_WIDTH_SIZE 8
 	/* Devices on the IOSF bus do not have this information
@@ -15279,6 +17307,14 @@
 			i40e_stat_str(&pf->hw, err),
 			i40e_aq_str(&pf->hw, pf->hw.aq.asq_last_status));
 
+	/* make sure the MFS hasn't been set lower than the default */
+#define MAX_FRAME_SIZE_DEFAULT 0x2600
+	val = ((rd32(&pf->hw, I40E_PRTGL_SAH) &
+		I40E_PRTGL_SAH_MFS_MASK) >> I40E_PRTGL_SAH_MFS_SHIFT);
+	if (val < MAX_FRAME_SIZE_DEFAULT)
+		dev_warn(&pdev->dev, "MFS has been set below the default: %x\n",
+			 val);
+
 	/* Add a filter to drop all Flow control frames from any VSI from being
 	 * transmitted. By doing so we stop a malicious VF from sending out
 	 * PAUSE or PFC frames and potentially controlling traffic for other
@@ -15289,7 +17325,7 @@
 						       pf->main_vsi_seid);
 
 	if ((pf->hw.device_id == I40E_DEV_ID_10G_BASE_T) ||
-		(pf->hw.device_id == I40E_DEV_ID_10G_BASE_T4))
+	    (pf->hw.device_id == I40E_DEV_ID_10G_BASE_T4))
 		pf->hw_features |= I40E_HW_PHY_CONTROLS_LEDS;
 	if (pf->hw.device_id == I40E_DEV_ID_SFP_I_X722)
 		pf->hw_features |= I40E_HW_HAVE_CRT_RETIMER;
@@ -15335,7 +17371,11 @@
  * Hot-Plug event, or because the driver is going to be removed from
  * memory.
  **/
+#ifdef HAVE_CONFIG_HOTPLUG
+static void __devexit i40e_remove(struct pci_dev *pdev)
+#else
 static void i40e_remove(struct pci_dev *pdev)
+#endif
 {
 	struct i40e_pf *pf = pci_get_drvdata(pdev);
 	struct i40e_hw *hw = &pf->hw;
@@ -15343,12 +17383,13 @@
 	int i;
 
 	i40e_dbg_pf_exit(pf);
-
+#ifdef HAVE_PTP_1588_CLOCK
 	i40e_ptp_stop(pf);
 
 	/* Disable RSS in hw */
 	i40e_write_rx_ctl(hw, I40E_PFQF_HENA(0), 0);
 	i40e_write_rx_ctl(hw, I40E_PFQF_HENA(1), 0);
+#endif /* HAVE_PTP_1588_CLOCK */
 
 	while (test_bit(__I40E_RESET_RECOVERY_PENDING, pf->state))
 		usleep_range(1000, 2000);
@@ -15358,6 +17399,10 @@
 		i40e_free_vfs(pf);
 		pf->flags &= ~I40E_FLAG_SRIOV_ENABLED;
 	}
+	/* Turn off double VLAN */
+	if (i40e_is_double_vlan(&pf->hw))
+		i40e_aq_set_port_parameters(&pf->hw, 0, false, true, false,
+					    NULL);
 	/* no more scheduling of any task */
 	set_bit(__I40E_SUSPENDED, pf->state);
 	set_bit(__I40E_DOWN, pf->state);
@@ -15365,7 +17410,12 @@
 		del_timer_sync(&pf->service_timer);
 	if (pf->service_task.func)
 		cancel_work_sync(&pf->service_task);
-
+	/* Client close must be called explicitly here because the timer
+	 * has been stopped.
+	 */
+	i40e_notify_client_of_netdev_close(pf->vsi[pf->lan_vsi], false);
+	if (test_bit(__I40E_DEBUG_MODE, pf->state))
+		goto debug_mode_clear;
 	if (test_bit(__I40E_RECOVERY_MODE, pf->state)) {
 		struct i40e_vsi *vsi = pf->vsi[0];
 
@@ -15375,14 +17425,12 @@
 		 */
 		unregister_netdev(vsi->netdev);
 		free_netdev(vsi->netdev);
+		vsi->netdev = NULL;
 
 		goto unmap;
 	}
+	kfree(pf->mac_list);
 
-	/* Client close must be called explicitly here because the timer
-	 * has been stopped.
-	 */
-	i40e_notify_client_of_netdev_close(pf->vsi[pf->lan_vsi], false);
 
 	i40e_fdir_teardown(pf);
 
@@ -15424,18 +17472,16 @@
 	}
 
 unmap:
-	/* Free MSI/legacy interrupt 0 when in recovery mode. */
+	/* Free MSI/legacy interrupt 0 when in recovery mode.
+	 * This is normally done in i40e_vsi_free_irq on
+	 * VSI close but since recovery mode doesn't allow to up
+	 * an interface and we do not allocate all Rx/Tx resources
+	 * for it we'll just do it here
+	 */
 	if (test_bit(__I40E_RECOVERY_MODE, pf->state) &&
 	    !(pf->flags & I40E_FLAG_MSIX_ENABLED))
 		free_irq(pf->pdev->irq, pf);
 
-	/* shutdown the adminq */
-	i40e_shutdown_adminq(hw);
-
-	/* destroy the locks only once, here */
-	mutex_destroy(&hw->aq.arq_mutex);
-	mutex_destroy(&hw->aq.asq_mutex);
-
 	/* Clear all dynamic memory lists of rings, q_vectors, and VSIs */
 	rtnl_lock();
 	i40e_clear_interrupt_scheme(pf);
@@ -15449,6 +17495,14 @@
 	}
 	rtnl_unlock();
 
+debug_mode_clear:
+	/* shutdown the adminq */
+	i40e_shutdown_adminq(hw);
+
+	/* destroy the locks only once, here */
+	i40e_destroy_spinlock_d(&hw->aq.arq_spinlock);
+	i40e_destroy_spinlock_d(&hw->aq.asq_spinlock);
+
 	for (i = 0; i < I40E_MAX_VEB; i++) {
 		kfree(pf->veb[i]);
 		pf->veb[i] = NULL;
@@ -15465,6 +17519,7 @@
 	pci_disable_device(pdev);
 }
 
+#ifdef HAVE_PCI_ERS
 /**
  * i40e_pci_error_detected - warning that something funky happened in PCI land
  * @pdev: PCI device information struct
@@ -15475,7 +17530,7 @@
  * remediation.
  **/
 static pci_ers_result_t i40e_pci_error_detected(struct pci_dev *pdev,
-						enum pci_channel_state error)
+						pci_channel_state_t error)
 {
 	struct i40e_pf *pf = pci_get_drvdata(pdev);
 
@@ -15483,13 +17538,13 @@
 
 	if (!pf) {
 		dev_info(&pdev->dev,
-			 "Cannot recover - error happened during device probe\n");
+			 "Cannot recover -error happened during device probe\n");
 		return PCI_ERS_RESULT_DISCONNECT;
 	}
 
 	/* shutdown all operations */
 	if (!test_bit(__I40E_SUSPENDED, pf->state))
-		i40e_prep_for_reset(pf, false);
+		i40e_prep_for_reset(pf);
 
 	/* Request a slot reset */
 	return PCI_ERS_RESULT_NEED_RESET;
@@ -15508,6 +17563,7 @@
 {
 	struct i40e_pf *pf = pci_get_drvdata(pdev);
 	pci_ers_result_t result;
+	int err;
 	u32 reg;
 
 	dev_dbg(&pdev->dev, "%s\n", __func__);
@@ -15528,9 +17584,18 @@
 			result = PCI_ERS_RESULT_DISCONNECT;
 	}
 
+	err = pci_aer_clear_nonfatal_status(pdev);
+	if (err) {
+		dev_info(&pdev->dev,
+			 "pci_aer_clear_nonfatal_status failed 0x%0x\n",
+			 err);
+		/* non-fatal, continue */
+	}
+
 	return result;
 }
 
+#if defined(HAVE_PCI_ERROR_HANDLER_RESET_PREPARE) || defined(HAVE_PCI_ERROR_HANDLER_RESET_NOTIFY) || defined(HAVE_RHEL7_PCI_RESET_NOTIFY)
 /**
  * i40e_pci_error_reset_prepare - prepare device driver for pci reset
  * @pdev: PCI device information struct
@@ -15539,7 +17604,7 @@
 {
 	struct i40e_pf *pf = pci_get_drvdata(pdev);
 
-	i40e_prep_for_reset(pf, false);
+	i40e_prep_for_reset(pf);
 }
 
 /**
@@ -15551,8 +17616,32 @@
 	struct i40e_pf *pf = pci_get_drvdata(pdev);
 
 	i40e_reset_and_rebuild(pf, false, false);
+
+#ifdef HAVE_SRIOV_CONFIGURE
+	i40e_restore_all_vfs_msi_state(pdev);
+#endif
+}
+
+#endif
+#if defined(HAVE_PCI_ERROR_HANDLER_RESET_NOTIFY) || defined(HAVE_RHEL7_PCI_RESET_NOTIFY)
+/**
+ * i40e_pci_error_reset_notify - notify device driver of pci reset
+ * @pdev: PCI device information struct
+ * @prepare: true if device is about to be reset; false if reset attempt
+ * completed
+ *
+ * Called to perform pf reset when a pci function level reset is triggered
+ **/
+static void i40e_pci_error_reset_notify(struct pci_dev *pdev, bool prepare)
+{
+	dev_dbg(&pdev->dev, "%s\n", __func__);
+	if (prepare)
+		i40e_pci_error_reset_prepare(pdev);
+	else
+		i40e_pci_error_reset_done(pdev);
 }
 
+#endif
 /**
  * i40e_pci_error_resume - restart operations after PCI error recovery
  * @pdev: PCI device information struct
@@ -15618,6 +17707,11 @@
 			"Failed to enable Multicast Magic Packet wake up\n");
 }
 
+/* FW state indicating on X722 that we need to disable WoL to
+ * allow adapter to shutdown completely
+ */
+#define I40E_GL_FWSTS_FWS0B_STAGE_FW_UPDATE_POR_REQUIRED 0x0F
+
 /**
  * i40e_shutdown - PCI callback for shutting down
  * @pdev: PCI device information struct
@@ -15626,10 +17720,14 @@
 {
 	struct i40e_pf *pf = pci_get_drvdata(pdev);
 	struct i40e_hw *hw = &pf->hw;
+	u32 val = 0;
 
 	set_bit(__I40E_SUSPENDED, pf->state);
 	set_bit(__I40E_DOWN, pf->state);
 
+	if (test_bit(__I40E_DEBUG_MODE, pf->state))
+		goto debug_mode;
+
 	del_timer_sync(&pf->service_timer);
 	cancel_work_sync(&pf->service_task);
 	i40e_cloud_filter_exit(pf);
@@ -15640,17 +17738,42 @@
 	 */
 	i40e_notify_client_of_netdev_close(pf->vsi[pf->lan_vsi], false);
 
-	if (pf->wol_en && (pf->hw_features & I40E_HW_WOL_MC_MAGIC_PKT_WAKE))
-		i40e_enable_mc_magic_wake(pf);
+	val = rd32(hw, I40E_GL_FWSTS) & I40E_GL_FWSTS_FWS0B_MASK;
+
+	if (pf->hw.mac.type == I40E_MAC_X722) {
+		/* We check here if we need to disable the WoL to allow adapter
+		 * to shutdown completely after a FW update
+		 */
+		if (val != I40E_GL_FWSTS_FWS0B_STAGE_FW_UPDATE_POR_REQUIRED &&
+		    pf->wol_en) {
+			if (pf->hw_features & I40E_HW_WOL_MC_MAGIC_PKT_WAKE)
+				i40e_enable_mc_magic_wake(pf);
+
+			i40e_prep_for_reset(pf);
+
+			wr32(hw, I40E_PFPM_APM, I40E_PFPM_APM_APME_MASK);
+			wr32(hw, I40E_PFPM_WUFC, I40E_PFPM_WUFC_MAG_MASK);
+		} else {
+			i40e_prep_for_reset(pf);
 
-	i40e_prep_for_reset(pf, false);
+			wr32(hw, I40E_PFPM_APM, 0);
+			wr32(hw, I40E_PFPM_WUFC, 0);
+		}
+	} else {
+		i40e_prep_for_reset(pf);
 
-	wr32(hw, I40E_PFPM_APM,
-	     (pf->wol_en ? I40E_PFPM_APM_APME_MASK : 0));
-	wr32(hw, I40E_PFPM_WUFC,
-	     (pf->wol_en ? I40E_PFPM_WUFC_MAG_MASK : 0));
+		wr32(hw, I40E_PFPM_APM,
+		     (pf->wol_en ? I40E_PFPM_APM_APME_MASK : 0));
+		wr32(hw, I40E_PFPM_WUFC,
+		     (pf->wol_en ? I40E_PFPM_WUFC_MAG_MASK : 0));
+	}
 
-	/* Free MSI/legacy interrupt 0 when in recovery mode. */
+	/* Free MSI/legacy interrupt 0 when in recovery mode.
+	 * This is normally done in i40e_vsi_free_irq on
+	 * VSI close but since recovery mode doesn't allow to up
+	 * an interface and we do not allocate all Rx/Tx resources
+	 * for it we'll just do it here
+	 */
 	if (test_bit(__I40E_RECOVERY_MODE, pf->state) &&
 	    !(pf->flags & I40E_FLAG_MSIX_ENABLED))
 		free_irq(pf->pdev->irq, pf);
@@ -15663,19 +17786,27 @@
 	i40e_clear_interrupt_scheme(pf);
 	rtnl_unlock();
 
-	if (system_state == SYSTEM_POWER_OFF) {
+debug_mode:
+
+	if (pf->hw.mac.type == I40E_MAC_X722 &&
+	    val == I40E_GL_FWSTS_FWS0B_STAGE_FW_UPDATE_POR_REQUIRED) {
+		pci_wake_from_d3(pdev, false);
+		device_set_wakeup_enable(&pdev->dev, false);
+	} else if (system_state == SYSTEM_POWER_OFF) {
 		pci_wake_from_d3(pdev, pf->wol_en);
-		pci_set_power_state(pdev, PCI_D3hot);
 	}
+	pci_set_power_state(pdev, PCI_D3hot);
 }
 
+#ifdef CONFIG_PM
 /**
  * i40e_suspend - PM callback for moving to D3
  * @dev: generic device information structure
  **/
-static int __maybe_unused i40e_suspend(struct device *dev)
+static int i40e_suspend(struct device *dev)
 {
-	struct i40e_pf *pf = dev_get_drvdata(dev);
+	struct pci_dev *pdev = to_pci_dev(dev);
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
 	struct i40e_hw *hw = &pf->hw;
 
 	/* If we're already suspended, then there is nothing to do */
@@ -15693,6 +17824,9 @@
 	 */
 	i40e_notify_client_of_netdev_close(pf->vsi[pf->lan_vsi], false);
 
+	if (test_bit(__I40E_DEBUG_MODE, pf->state))
+		return 0;
+
 	if (pf->wol_en && (pf->hw_features & I40E_HW_WOL_MC_MAGIC_PKT_WAKE))
 		i40e_enable_mc_magic_wake(pf);
 
@@ -15702,7 +17836,7 @@
 	 */
 	rtnl_lock();
 
-	i40e_prep_for_reset(pf, true);
+	i40e_prep_for_reset(pf);
 
 	wr32(hw, I40E_PFPM_APM, (pf->wol_en ? I40E_PFPM_APM_APME_MASK : 0));
 	wr32(hw, I40E_PFPM_WUFC, (pf->wol_en ? I40E_PFPM_WUFC_MAG_MASK : 0));
@@ -15723,9 +17857,10 @@
  * i40e_resume - PM callback for waking up from D3
  * @dev: generic device information structure
  **/
-static int __maybe_unused i40e_resume(struct device *dev)
+static int i40e_resume(struct device *dev)
 {
-	struct i40e_pf *pf = dev_get_drvdata(dev);
+	struct pci_dev *pdev = to_pci_dev(dev);
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
 	int err;
 
 	/* If we're not suspended, then there is nothing to do */
@@ -15742,7 +17877,7 @@
 	 */
 	err = i40e_restore_interrupt_scheme(pf);
 	if (err) {
-		dev_err(dev, "Cannot restore interrupt scheme: %d\n",
+		dev_err(&pdev->dev, "Cannot restore interrupt scheme: %d\n",
 			err);
 	}
 
@@ -15761,27 +17896,131 @@
 	return 0;
 }
 
+#ifdef USE_LEGACY_PM_SUPPORT
+/**
+ * i40e_legacy_suspend - PCI callback for moving to D3
+ * @pdev: PCI device information struct
+ * @state: PCI power state
+ *
+ * Legacy suspend handler for older kernels which do not support the newer
+ * generic callbacks
+ **/
+static int i40e_legacy_suspend(struct pci_dev *pdev, pm_message_t state)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	int retval = i40e_suspend(&pdev->dev);
+
+	/* Some older kernels may not handle state correctly for legacy power
+	 * management, so we'll handle it here ourselves
+	 */
+	retval = pci_save_state(pdev);
+	if (retval)
+		return retval;
+
+	pci_disable_device(pdev);
+	pci_wake_from_d3(pdev, pf->wol_en);
+	pci_set_power_state(pdev, PCI_D3hot);
+
+	return retval;
+}
+
+/**
+ * i40e_legacy_resume - PCI callback for waking up from D3
+ * @pdev: PCI device information struct
+ *
+ * Legacy resume handler for kernels which do not support the newer generic
+ * callbacks.
+ **/
+static int i40e_legacy_resume(struct pci_dev *pdev)
+{
+	u32 err;
+
+	pci_set_power_state(pdev, PCI_D0);
+	pci_restore_state(pdev);
+	/* pci_restore_state() clears dev->state_saves, so
+	 * call pci_save_state() again to restore it.
+	 */
+	pci_save_state(pdev);
+
+	err = pci_enable_device_mem(pdev);
+	if (err) {
+		dev_err(pci_dev_to_dev(pdev), "Cannot enable PCI device from suspend\n");
+		return err;
+	}
+	pci_set_master(pdev);
+
+	/* no wakeup events while running */
+	pci_wake_from_d3(pdev, false);
+
+	return i40e_resume(&pdev->dev);
+}
+#endif /* USE_LEGACY_PM_SUPPORT */
+#endif /* CONFIG_PM */
+
+#ifdef HAVE_CONST_STRUCT_PCI_ERROR_HANDLERS
 static const struct pci_error_handlers i40e_err_handler = {
+#else
+static struct pci_error_handlers i40e_err_handler = {
+#endif
 	.error_detected = i40e_pci_error_detected,
 	.slot_reset = i40e_pci_error_slot_reset,
+#ifdef HAVE_PCI_ERROR_HANDLER_RESET_NOTIFY
+	.reset_notify = i40e_pci_error_reset_notify,
+#endif
+#ifdef HAVE_PCI_ERROR_HANDLER_RESET_PREPARE
 	.reset_prepare = i40e_pci_error_reset_prepare,
 	.reset_done = i40e_pci_error_reset_done,
+#endif
 	.resume = i40e_pci_error_resume,
 };
 
+#if defined(HAVE_RHEL6_SRIOV_CONFIGURE) || defined(HAVE_RHEL7_PCI_DRIVER_RH)
+static struct pci_driver_rh i40e_driver_rh = {
+#ifdef HAVE_RHEL6_SRIOV_CONFIGURE
+	.sriov_configure = i40e_pci_sriov_configure,
+#elif defined(HAVE_RHEL7_PCI_RESET_NOTIFY)
+	.reset_notify = i40e_pci_error_reset_notify,
+#endif
+};
+
+#endif
+#endif /* HAVE_PCI_ERS */
+#if defined(CONFIG_PM) && !defined(USE_LEGACY_PM_SUPPORT)
 static SIMPLE_DEV_PM_OPS(i40e_pm_ops, i40e_suspend, i40e_resume);
+#endif /* CONFIG_PM && !USE_LEGACY_PM_SUPPORT */
 
 static struct pci_driver i40e_driver = {
 	.name     = i40e_driver_name,
 	.id_table = i40e_pci_tbl,
 	.probe    = i40e_probe,
+#ifdef HAVE_CONFIG_HOTPLUG
+	.remove   = __devexit_p(i40e_remove),
+#else
 	.remove   = i40e_remove,
+#endif
+#ifdef CONFIG_PM
+#ifdef USE_LEGACY_PM_SUPPORT
+	.suspend  = i40e_legacy_suspend,
+	.resume   = i40e_legacy_resume,
+#else /* USE_LEGACY_PM_SUPPORT */
 	.driver   = {
 		.pm = &i40e_pm_ops,
 	},
+#endif /* !USE_LEGACY_PM_SUPPORT */
+#endif /* CONFIG_PM */
 	.shutdown = i40e_shutdown,
+#ifdef HAVE_PCI_ERS
 	.err_handler = &i40e_err_handler,
+#endif
+#ifdef HAVE_SRIOV_CONFIGURE
 	.sriov_configure = i40e_pci_sriov_configure,
+#endif
+#ifdef HAVE_RHEL6_SRIOV_CONFIGURE
+	.rh_reserved = &i40e_driver_rh,
+#endif
+#ifdef HAVE_RHEL7_PCI_DRIVER_RH
+	.pci_driver_rh = &i40e_driver_rh,
+#endif
 };
 
 /**
@@ -15803,12 +18042,18 @@
 	 * since we need to be able to guarantee forward progress even under
 	 * memory pressure.
 	 */
-	i40e_wq = alloc_workqueue("%s", WQ_MEM_RECLAIM, 0, i40e_driver_name);
+	i40e_wq = alloc_workqueue("%s", 0, 0, i40e_driver_name);
 	if (!i40e_wq) {
 		pr_err("%s: Failed to create workqueue\n", i40e_driver_name);
 		return -ENOMEM;
 	}
+#ifdef HAVE_RHEL7_PCI_DRIVER_RH
+	/* The size member must be initialized in the driver via a call to
+	 * set_pci_driver_rh_size before pci_register_driver is called
+	 */
+	set_pci_driver_rh_size(i40e_driver_rh);
 
+#endif
 	i40e_dbg_init();
 	return pci_register_driver(&i40e_driver);
 }
@@ -15825,5 +18070,8 @@
 	pci_unregister_driver(&i40e_driver);
 	destroy_workqueue(i40e_wq);
 	i40e_dbg_exit();
+#ifdef HAVE_KFREE_RCU_BARRIER
+	rcu_barrier();
+#endif
 }
 module_exit(i40e_exit_module);
Binary files linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_main.o and linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_main.o differ
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_nvm.c linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_nvm.c
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_nvm.c	2024-05-10 01:26:45.373079718 -0400
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_nvm.c	2024-05-13 03:58:25.372491940 -0400
@@ -1,22 +1,22 @@
 // SPDX-License-Identifier: GPL-2.0
-/* Copyright(c) 2013 - 2018 Intel Corporation. */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
 
 #include "i40e_prototype.h"
 
 /**
- * i40e_init_nvm_ops - Initialize NVM function pointers
+ * i40e_init_nvm - Initialize NVM function pointers
  * @hw: pointer to the HW structure
  *
  * Setup the function pointers and the NVM info structure. Should be called
  * once per NVM initialization, e.g. inside the i40e_init_shared_code().
  * Please notice that the NVM term is used here (& in all methods covered
  * in this file) as an equivalent of the FLASH part mapped into the SR.
- * We are accessing FLASH always thru the Shadow RAM.
+ * We are accessing FLASH always through the Shadow RAM.
  **/
 i40e_status i40e_init_nvm(struct i40e_hw *hw)
 {
 	struct i40e_nvm_info *nvm = &hw->nvm;
-	i40e_status ret_code = 0;
+	i40e_status ret_code = I40E_SUCCESS;
 	u32 fla, gens;
 	u8 sr_size;
 
@@ -55,7 +55,7 @@
 i40e_status i40e_acquire_nvm(struct i40e_hw *hw,
 				       enum i40e_aq_resource_access_type access)
 {
-	i40e_status ret_code = 0;
+	i40e_status ret_code = I40E_SUCCESS;
 	u64 gtime, timeout;
 	u64 time_left = 0;
 
@@ -73,7 +73,8 @@
 	if (ret_code)
 		i40e_debug(hw, I40E_DEBUG_NVM,
 			   "NVM acquire type %d failed time_left=%llu ret=%d aq_err=%d\n",
-			   access, time_left, ret_code, hw->aq.asq_last_status);
+			   access, (unsigned long long)time_left, ret_code,
+			   hw->aq.asq_last_status);
 
 	if (ret_code && time_left) {
 		/* Poll until the current NVM owner timeouts */
@@ -85,17 +86,18 @@
 							I40E_NVM_RESOURCE_ID,
 							access, 0, &time_left,
 							NULL);
-			if (!ret_code) {
+			if (ret_code == I40E_SUCCESS) {
 				hw->nvm.hw_semaphore_timeout =
 					    I40E_MS_TO_GTIME(time_left) + gtime;
 				break;
 			}
 		}
-		if (ret_code) {
+		if (ret_code != I40E_SUCCESS) {
 			hw->nvm.hw_semaphore_timeout = 0;
 			i40e_debug(hw, I40E_DEBUG_NVM,
 				   "NVM acquire timed out, wait %llu ms before trying again. status=%d aq_err=%d\n",
-				   time_left, ret_code, hw->aq.asq_last_status);
+				   (unsigned long long)time_left, ret_code,
+				   hw->aq.asq_last_status);
 		}
 	}
 
@@ -124,11 +126,10 @@
 	 */
 	while ((ret_code == I40E_ERR_ADMIN_QUEUE_TIMEOUT) &&
 	       (total_delay < hw->aq.asq_cmd_timeout)) {
-		usleep_range(1000, 2000);
-		ret_code = i40e_aq_release_resource(hw,
-						    I40E_NVM_RESOURCE_ID,
-						    0, NULL);
-		total_delay++;
+			usleep_range(1000, 2000);
+			ret_code = i40e_aq_release_resource(hw,
+						I40E_NVM_RESOURCE_ID, 0, NULL);
+			total_delay++;
 	}
 }
 
@@ -147,7 +148,7 @@
 	for (wait_cnt = 0; wait_cnt < I40E_SRRD_SRCTL_ATTEMPTS; wait_cnt++) {
 		srctl = rd32(hw, I40E_GLNVM_SRCTL);
 		if (srctl & I40E_GLNVM_SRCTL_DONE_MASK) {
-			ret_code = 0;
+			ret_code = I40E_SUCCESS;
 			break;
 		}
 		udelay(5);
@@ -165,15 +166,15 @@
  *
  * Reads one 16 bit word from the Shadow RAM using the GLNVM_SRCTL register.
  **/
-static i40e_status i40e_read_nvm_word_srctl(struct i40e_hw *hw, u16 offset,
-					    u16 *data)
+static i40e_status i40e_read_nvm_word_srctl(struct i40e_hw *hw,
+						      u16 offset, u16 *data)
 {
 	i40e_status ret_code = I40E_ERR_TIMEOUT;
 	u32 sr_reg;
 
 	if (offset >= hw->nvm.sr_size) {
 		i40e_debug(hw, I40E_DEBUG_NVM,
-			   "NVM read error: offset %d beyond Shadow RAM limit %d\n",
+			   "NVM read error: Offset %d beyond Shadow RAM limit %d\n",
 			   offset, hw->nvm.sr_size);
 		ret_code = I40E_ERR_PARAM;
 		goto read_nvm_exit;
@@ -181,7 +182,7 @@
 
 	/* Poll the done bit first */
 	ret_code = i40e_poll_sr_srctl_done_bit(hw);
-	if (!ret_code) {
+	if (ret_code == I40E_SUCCESS) {
 		/* Write the address and start reading */
 		sr_reg = ((u32)offset << I40E_GLNVM_SRCTL_ADDR_SHIFT) |
 			 BIT(I40E_GLNVM_SRCTL_START_SHIFT);
@@ -189,14 +190,14 @@
 
 		/* Poll I40E_GLNVM_SRCTL until the done bit is set */
 		ret_code = i40e_poll_sr_srctl_done_bit(hw);
-		if (!ret_code) {
+		if (ret_code == I40E_SUCCESS) {
 			sr_reg = rd32(hw, I40E_GLNVM_SRDATA);
 			*data = (u16)((sr_reg &
 				       I40E_GLNVM_SRDATA_RDDATA_MASK)
 				    >> I40E_GLNVM_SRDATA_RDDATA_SHIFT);
 		}
 	}
-	if (ret_code)
+	if (ret_code != I40E_SUCCESS)
 		i40e_debug(hw, I40E_DEBUG_NVM,
 			   "NVM read error: Couldn't access Shadow RAM address: 0x%x\n",
 			   offset);
@@ -217,9 +218,9 @@
  * Writes a 16 bit words buffer to the Shadow RAM using the admin command.
  **/
 static i40e_status i40e_read_nvm_aq(struct i40e_hw *hw,
-				    u8 module_pointer, u32 offset,
-				    u16 words, void *data,
-				    bool last_command)
+					      u8 module_pointer, u32 offset,
+					      u16 words, void *data,
+					      bool last_command)
 {
 	i40e_status ret_code = I40E_ERR_NVM;
 	struct i40e_asq_cmd_details cmd_details;
@@ -265,18 +266,18 @@
  * Reads one 16 bit word from the Shadow RAM using the AdminQ
  **/
 static i40e_status i40e_read_nvm_word_aq(struct i40e_hw *hw, u16 offset,
-					 u16 *data)
+						   u16 *data)
 {
 	i40e_status ret_code = I40E_ERR_TIMEOUT;
 
 	ret_code = i40e_read_nvm_aq(hw, 0x0, offset, 1, data, true);
-	*data = le16_to_cpu(*(__le16 *)data);
+	*data = LE16_TO_CPU(*(__force __le16 *)data);
 
 	return ret_code;
 }
 
 /**
- * __i40e_read_nvm_word - Reads nvm word, assumes caller does the locking
+ * __i40e_read_nvm_word - Reads NVM word, assumes caller does the locking
  * @hw: pointer to the HW structure
  * @offset: offset of the Shadow RAM word to read (0x000000 - 0x001FFF)
  * @data: word read from the Shadow RAM
@@ -287,8 +288,9 @@
  * taken via i40e_acquire_nvm().
  **/
 static i40e_status __i40e_read_nvm_word(struct i40e_hw *hw,
-					u16 offset, u16 *data)
+						  u16 offset, u16 *data)
 {
+
 	if (hw->flags & I40E_HW_FLAG_AQ_SRCTL_ACCESS_ENABLE)
 		return i40e_read_nvm_word_aq(hw, offset, data);
 
@@ -296,7 +298,7 @@
 }
 
 /**
- * i40e_read_nvm_word - Reads nvm word and acquire lock if necessary
+ * i40e_read_nvm_word - Reads NVM word, acquires lock if necessary
  * @hw: pointer to the HW structure
  * @offset: offset of the Shadow RAM word to read (0x000000 - 0x001FFF)
  * @data: word read from the Shadow RAM
@@ -304,43 +306,43 @@
  * Reads one 16 bit word from the Shadow RAM.
  **/
 i40e_status i40e_read_nvm_word(struct i40e_hw *hw, u16 offset,
-			       u16 *data)
+					 u16 *data)
 {
-	i40e_status ret_code = 0;
+	i40e_status ret_code = I40E_SUCCESS;
 
 	if (hw->flags & I40E_HW_FLAG_NVM_READ_REQUIRES_LOCK)
 		ret_code = i40e_acquire_nvm(hw, I40E_RESOURCE_READ);
+
 	if (ret_code)
 		return ret_code;
-
 	ret_code = __i40e_read_nvm_word(hw, offset, data);
 
 	if (hw->flags & I40E_HW_FLAG_NVM_READ_REQUIRES_LOCK)
 		i40e_release_nvm(hw);
-
 	return ret_code;
 }
 
 /**
  * i40e_read_nvm_module_data - Reads NVM Buffer to specified memory location
- * @hw: pointer to the HW structure
+ * @hw: Pointer to the HW structure
  * @module_ptr: Pointer to module in words with respect to NVM beginning
- * @offset: offset in words from module start
+ * @module_offset: Offset in words from module start
+ * @data_offset: Offset in words from reading data area start
  * @words_data_size: Words to read from NVM
  * @data_ptr: Pointer to memory location where resulting buffer will be stored
  **/
-i40e_status i40e_read_nvm_module_data(struct i40e_hw *hw,
-				      u8 module_ptr, u16 offset,
-				      u16 words_data_size,
-				      u16 *data_ptr)
+enum i40e_status_code
+i40e_read_nvm_module_data(struct i40e_hw *hw, u8 module_ptr, u16 module_offset,
+			  u16 data_offset, u16 words_data_size, u16 *data_ptr)
 {
 	i40e_status status;
+	u16 specific_ptr = 0;
 	u16 ptr_value = 0;
-	u32 flat_offset;
+	u16 offset = 0;
 
 	if (module_ptr != 0) {
 		status = i40e_read_nvm_word(hw, module_ptr, &ptr_value);
-		if (status) {
+		if (status != I40E_SUCCESS) {
 			i40e_debug(hw, I40E_DEBUG_ALL,
 				   "Reading nvm word failed.Error code: %d.\n",
 				   status);
@@ -352,37 +354,36 @@
 
 	/* Pointer not initialized */
 	if (ptr_value == I40E_NVM_INVALID_PTR_VAL ||
-	    ptr_value == I40E_NVM_INVALID_VAL)
+	    ptr_value == I40E_NVM_INVALID_VAL) {
+		i40e_debug(hw, I40E_DEBUG_ALL, "Pointer not initialized.\n");
 		return I40E_ERR_BAD_PTR;
+	}
 
 	/* Check whether the module is in SR mapped area or outside */
 	if (ptr_value & I40E_PTR_TYPE) {
 		/* Pointer points outside of the Shared RAM mapped area */
-		ptr_value &= ~I40E_PTR_TYPE;
+		i40e_debug(hw, I40E_DEBUG_ALL,
+			   "Reading nvm data failed. Pointer points outside of the Shared RAM mapped area.\n");
 
-		/* PtrValue in 4kB units, need to convert to words */
-		ptr_value /= 2;
-		flat_offset = ((u32)ptr_value * 0x1000) + (u32)offset;
-		status = i40e_acquire_nvm(hw, I40E_RESOURCE_READ);
-		if (!status) {
-			status = i40e_aq_read_nvm(hw, 0, 2 * flat_offset,
-						  2 * words_data_size,
-						  data_ptr, true, NULL);
-			i40e_release_nvm(hw);
-			if (status) {
-				i40e_debug(hw, I40E_DEBUG_ALL,
-					   "Reading nvm aq failed.Error code: %d.\n",
-					   status);
-				return I40E_ERR_NVM;
-			}
-		} else {
-			return I40E_ERR_NVM;
-		}
+		return I40E_ERR_PARAM;
 	} else {
 		/* Read from the Shadow RAM */
-		status = i40e_read_nvm_buffer(hw, ptr_value + offset,
-					      &words_data_size, data_ptr);
-		if (status) {
+
+		status = i40e_read_nvm_word(hw, ptr_value + module_offset,
+					    &specific_ptr);
+		if (status != I40E_SUCCESS) {
+			i40e_debug(hw, I40E_DEBUG_ALL,
+				   "Reading nvm word failed.Error code: %d.\n",
+				   status);
+			return I40E_ERR_NVM;
+		}
+
+		offset = ptr_value + module_offset + specific_ptr +
+			data_offset;
+
+		status = i40e_read_nvm_buffer(hw, offset, &words_data_size,
+					      data_ptr);
+		if (status != I40E_SUCCESS) {
 			i40e_debug(hw, I40E_DEBUG_ALL,
 				   "Reading nvm buffer failed.Error code: %d.\n",
 				   status);
@@ -404,16 +405,16 @@
  * and followed by the release.
  **/
 static i40e_status i40e_read_nvm_buffer_srctl(struct i40e_hw *hw, u16 offset,
-					      u16 *words, u16 *data)
+							u16 *words, u16 *data)
 {
-	i40e_status ret_code = 0;
+	i40e_status ret_code = I40E_SUCCESS;
 	u16 index, word;
 
-	/* Loop thru the selected region */
+	/* Loop through the selected region */
 	for (word = 0; word < *words; word++) {
 		index = offset + word;
 		ret_code = i40e_read_nvm_word_srctl(hw, index, &data[word]);
-		if (ret_code)
+		if (ret_code != I40E_SUCCESS)
 			break;
 	}
 
@@ -435,10 +436,10 @@
  * and followed by the release.
  **/
 static i40e_status i40e_read_nvm_buffer_aq(struct i40e_hw *hw, u16 offset,
-					   u16 *words, u16 *data)
+						     u16 *words, u16 *data)
 {
 	i40e_status ret_code;
-	u16 read_size;
+	u16 read_size = *words;
 	bool last_cmd = false;
 	u16 words_read = 0;
 	u16 i = 0;
@@ -462,7 +463,7 @@
 
 		ret_code = i40e_read_nvm_aq(hw, 0x0, offset, read_size,
 					    data + words_read, last_cmd);
-		if (ret_code)
+		if (ret_code != I40E_SUCCESS)
 			goto read_nvm_buffer_aq_exit;
 
 		/* Increment counter for words already read and move offset to
@@ -473,7 +474,7 @@
 	} while (words_read < *words);
 
 	for (i = 0; i < *words; i++)
-		data[i] = le16_to_cpu(((__le16 *)data)[i]);
+		data[i] = LE16_TO_CPU(((__force __le16 *)data)[i]);
 
 read_nvm_buffer_aq_exit:
 	*words = words_read;
@@ -481,7 +482,7 @@
 }
 
 /**
- * __i40e_read_nvm_buffer - Reads nvm buffer, caller must acquire lock
+ * __i40e_read_nvm_buffer - Reads NVM buffer, caller must acquire lock
  * @hw: pointer to the HW structure
  * @offset: offset of the Shadow RAM word to read (0x000000 - 0x001FFF).
  * @words: (in) number of words to read; (out) number of words actually read
@@ -491,8 +492,8 @@
  * method.
  **/
 static i40e_status __i40e_read_nvm_buffer(struct i40e_hw *hw,
-					  u16 offset, u16 *words,
-					  u16 *data)
+						    u16 offset, u16 *words,
+						    u16 *data)
 {
 	if (hw->flags & I40E_HW_FLAG_AQ_SRCTL_ACCESS_ENABLE)
 		return i40e_read_nvm_buffer_aq(hw, offset, words, data);
@@ -512,15 +513,15 @@
  * and followed by the release.
  **/
 i40e_status i40e_read_nvm_buffer(struct i40e_hw *hw, u16 offset,
-				 u16 *words, u16 *data)
+					   u16 *words, u16 *data)
 {
-	i40e_status ret_code = 0;
+	i40e_status ret_code = I40E_SUCCESS;
 
 	if (hw->flags & I40E_HW_FLAG_AQ_SRCTL_ACCESS_ENABLE) {
 		ret_code = i40e_acquire_nvm(hw, I40E_RESOURCE_READ);
 		if (!ret_code) {
 			ret_code = i40e_read_nvm_buffer_aq(hw, offset, words,
-							   data);
+							 data);
 			i40e_release_nvm(hw);
 		}
 	} else {
@@ -542,8 +543,8 @@
  * Writes a 16 bit words buffer to the Shadow RAM using the admin command.
  **/
 static i40e_status i40e_write_nvm_aq(struct i40e_hw *hw, u8 module_pointer,
-				     u32 offset, u16 words, void *data,
-				     bool last_command)
+					       u32 offset, u16 words, void *data,
+					       bool last_command)
 {
 	i40e_status ret_code = I40E_ERR_NVM;
 	struct i40e_asq_cmd_details cmd_details;
@@ -557,20 +558,14 @@
 	 * Firmware will check the module-based model.
 	 */
 	if ((offset + words) > hw->nvm.sr_size)
-		i40e_debug(hw, I40E_DEBUG_NVM,
-			   "NVM write error: offset %d beyond Shadow RAM limit %d\n",
-			   (offset + words), hw->nvm.sr_size);
+		hw_dbg(hw, "NVM write error: offset beyond Shadow RAM limit.\n");
 	else if (words > I40E_SR_SECTOR_SIZE_IN_WORDS)
 		/* We can write only up to 4KB (one sector), in one AQ write */
-		i40e_debug(hw, I40E_DEBUG_NVM,
-			   "NVM write fail error: tried to write %d words, limit is %d.\n",
-			   words, I40E_SR_SECTOR_SIZE_IN_WORDS);
+		hw_dbg(hw, "NVM write fail error: cannot write more than 4KB in a single write.\n");
 	else if (((offset + (words - 1)) / I40E_SR_SECTOR_SIZE_IN_WORDS)
 		 != (offset / I40E_SR_SECTOR_SIZE_IN_WORDS))
 		/* A single write cannot spread over two sectors */
-		i40e_debug(hw, I40E_DEBUG_NVM,
-			   "NVM write error: cannot spread over two sectors in a single write offset=%d words=%d\n",
-			   offset, words);
+		hw_dbg(hw, "NVM write error: cannot spread over two sectors in a single write.\n");
 	else
 		ret_code = i40e_aq_update_nvm(hw, module_pointer,
 					      2 * offset,  /*bytes*/
@@ -594,7 +589,7 @@
 static i40e_status i40e_calc_nvm_checksum(struct i40e_hw *hw,
 						    u16 *checksum)
 {
-	i40e_status ret_code;
+	i40e_status ret_code = I40E_SUCCESS;
 	struct i40e_virt_mem vmem;
 	u16 pcie_alt_module = 0;
 	u16 checksum_local = 0;
@@ -610,7 +605,7 @@
 
 	/* read pointer to VPD area */
 	ret_code = __i40e_read_nvm_word(hw, I40E_SR_VPD_PTR, &vpd_module);
-	if (ret_code) {
+	if (ret_code != I40E_SUCCESS) {
 		ret_code = I40E_ERR_NVM_CHECKSUM;
 		goto i40e_calc_nvm_checksum_exit;
 	}
@@ -618,7 +613,7 @@
 	/* read pointer to PCIe Alt Auto-load module */
 	ret_code = __i40e_read_nvm_word(hw, I40E_SR_PCIE_ALT_AUTO_LOAD_PTR,
 					&pcie_alt_module);
-	if (ret_code) {
+	if (ret_code != I40E_SUCCESS) {
 		ret_code = I40E_ERR_NVM_CHECKSUM;
 		goto i40e_calc_nvm_checksum_exit;
 	}
@@ -632,7 +627,7 @@
 			u16 words = I40E_SR_SECTOR_SIZE_IN_WORDS;
 
 			ret_code = __i40e_read_nvm_buffer(hw, i, &words, data);
-			if (ret_code) {
+			if (ret_code != I40E_SUCCESS) {
 				ret_code = I40E_ERR_NVM_CHECKSUM;
 				goto i40e_calc_nvm_checksum_exit;
 			}
@@ -674,15 +669,16 @@
  **/
 i40e_status i40e_update_nvm_checksum(struct i40e_hw *hw)
 {
-	i40e_status ret_code;
+	i40e_status ret_code = I40E_SUCCESS;
 	u16 checksum;
 	__le16 le_sum;
 
 	ret_code = i40e_calc_nvm_checksum(hw, &checksum);
-	le_sum = cpu_to_le16(checksum);
-	if (!ret_code)
+	if (ret_code == I40E_SUCCESS) {
+		le_sum = CPU_TO_LE16(checksum);
 		ret_code = i40e_write_nvm_aq(hw, 0x00, I40E_SR_SW_CHECKSUM_WORD,
 					     1, &le_sum, true);
+	}
 
 	return ret_code;
 }
@@ -698,7 +694,7 @@
 i40e_status i40e_validate_nvm_checksum(struct i40e_hw *hw,
 						 u16 *checksum)
 {
-	i40e_status ret_code = 0;
+	i40e_status ret_code = I40E_SUCCESS;
 	u16 checksum_sr = 0;
 	u16 checksum_local = 0;
 
@@ -730,51 +726,51 @@
 }
 
 static i40e_status i40e_nvmupd_state_init(struct i40e_hw *hw,
-					  struct i40e_nvm_access *cmd,
-					  u8 *bytes, int *perrno);
+						    struct i40e_nvm_access *cmd,
+						    u8 *bytes, int *perrno);
 static i40e_status i40e_nvmupd_state_reading(struct i40e_hw *hw,
-					     struct i40e_nvm_access *cmd,
-					     u8 *bytes, int *perrno);
+						    struct i40e_nvm_access *cmd,
+						    u8 *bytes, int *perrno);
 static i40e_status i40e_nvmupd_state_writing(struct i40e_hw *hw,
-					     struct i40e_nvm_access *cmd,
-					     u8 *bytes, int *errno);
+						    struct i40e_nvm_access *cmd,
+						    u8 *bytes, int *perrno);
 static enum i40e_nvmupd_cmd i40e_nvmupd_validate_command(struct i40e_hw *hw,
-						struct i40e_nvm_access *cmd,
-						int *perrno);
+						    struct i40e_nvm_access *cmd,
+						    int *perrno);
 static i40e_status i40e_nvmupd_nvm_erase(struct i40e_hw *hw,
-					 struct i40e_nvm_access *cmd,
-					 int *perrno);
+						   struct i40e_nvm_access *cmd,
+						   int *perrno);
 static i40e_status i40e_nvmupd_nvm_write(struct i40e_hw *hw,
-					 struct i40e_nvm_access *cmd,
-					 u8 *bytes, int *perrno);
+						   struct i40e_nvm_access *cmd,
+						   u8 *bytes, int *perrno);
 static i40e_status i40e_nvmupd_nvm_read(struct i40e_hw *hw,
-					struct i40e_nvm_access *cmd,
-					u8 *bytes, int *perrno);
+						  struct i40e_nvm_access *cmd,
+						  u8 *bytes, int *perrno);
 static i40e_status i40e_nvmupd_exec_aq(struct i40e_hw *hw,
-				       struct i40e_nvm_access *cmd,
-				       u8 *bytes, int *perrno);
+						 struct i40e_nvm_access *cmd,
+						 u8 *bytes, int *perrno);
 static i40e_status i40e_nvmupd_get_aq_result(struct i40e_hw *hw,
-					     struct i40e_nvm_access *cmd,
-					     u8 *bytes, int *perrno);
+						    struct i40e_nvm_access *cmd,
+						    u8 *bytes, int *perrno);
 static i40e_status i40e_nvmupd_get_aq_event(struct i40e_hw *hw,
-					    struct i40e_nvm_access *cmd,
-					    u8 *bytes, int *perrno);
-static inline u8 i40e_nvmupd_get_module(u32 val)
+						    struct i40e_nvm_access *cmd,
+						    u8 *bytes, int *perrno);
+static INLINE u8 i40e_nvmupd_get_module(u32 val)
 {
 	return (u8)(val & I40E_NVM_MOD_PNT_MASK);
 }
-static inline u8 i40e_nvmupd_get_transaction(u32 val)
+static INLINE u8 i40e_nvmupd_get_transaction(u32 val)
 {
 	return (u8)((val & I40E_NVM_TRANS_MASK) >> I40E_NVM_TRANS_SHIFT);
 }
 
-static inline u8 i40e_nvmupd_get_preservation_flags(u32 val)
+static INLINE u8 i40e_nvmupd_get_preservation_flags(u32 val)
 {
 	return (u8)((val & I40E_NVM_PRESERVATION_FLAGS_MASK) >>
 		    I40E_NVM_PRESERVATION_FLAGS_SHIFT);
 }
 
-static const char * const i40e_nvm_update_state_str[] = {
+static const char *i40e_nvm_update_state_str[] = {
 	"I40E_NVMUPD_INVALID",
 	"I40E_NVMUPD_READ_CON",
 	"I40E_NVMUPD_READ_SNT",
@@ -792,6 +788,7 @@
 	"I40E_NVMUPD_EXEC_AQ",
 	"I40E_NVMUPD_GET_AQ_RESULT",
 	"I40E_NVMUPD_GET_AQ_EVENT",
+	"I40E_NVMUPD_GET_FEATURES",
 };
 
 /**
@@ -804,8 +801,8 @@
  * Dispatches command depending on what update state is current
  **/
 i40e_status i40e_nvmupd_command(struct i40e_hw *hw,
-				struct i40e_nvm_access *cmd,
-				u8 *bytes, int *perrno)
+					  struct i40e_nvm_access *cmd,
+					  u8 *bytes, int *perrno)
 {
 	i40e_status status;
 	enum i40e_nvmupd_cmd upd_cmd;
@@ -849,7 +846,32 @@
 		if (hw->nvmupd_state == I40E_NVMUPD_STATE_ERROR)
 			hw->nvmupd_state = I40E_NVMUPD_STATE_INIT;
 
-		return 0;
+		return I40E_SUCCESS;
+	}
+
+	/*
+	 * A supported features request returns immediately
+	 * rather than going into state machine
+	 */
+	if (upd_cmd == I40E_NVMUPD_FEATURES) {
+		if (cmd->data_size < hw->nvmupd_features.size) {
+			*perrno = -EFAULT;
+			return I40E_ERR_BUF_TOO_SHORT;
+		}
+
+		/*
+		 * If buffer is bigger than i40e_nvmupd_features structure,
+		 * make sure the trailing bytes are set to 0x0.
+		 */
+		if (cmd->data_size > hw->nvmupd_features.size)
+			i40e_memset(bytes + hw->nvmupd_features.size, 0x0,
+				    cmd->data_size - hw->nvmupd_features.size,
+				    I40E_NONDMA_MEM);
+
+		i40e_memcpy(bytes, &hw->nvmupd_features,
+			    hw->nvmupd_features.size, I40E_NONDMA_MEM);
+
+		return I40E_SUCCESS;
 	}
 
 	/* Clear status even it is not read and log */
@@ -867,7 +889,7 @@
 	 * ~5ms for most commands. However lock is held for ~60ms for
 	 * NVMUPD_CSUM_LCB command.
 	 */
-	mutex_lock(&hw->aq.arq_mutex);
+	i40e_acquire_spinlock(&hw->aq.arq_spinlock);
 	switch (hw->nvmupd_state) {
 	case I40E_NVMUPD_STATE_INIT:
 		status = i40e_nvmupd_state_init(hw, cmd, bytes, perrno);
@@ -888,7 +910,7 @@
 		 */
 		if (cmd->offset == 0xffff) {
 			i40e_nvmupd_clear_wait_state(hw);
-			status = 0;
+			status = I40E_SUCCESS;
 			break;
 		}
 
@@ -905,7 +927,7 @@
 		break;
 	}
 
-	mutex_unlock(&hw->aq.arq_mutex);
+	i40e_release_spinlock(&hw->aq.arq_spinlock);
 	return status;
 }
 
@@ -920,10 +942,10 @@
  * state. Reject all other commands.
  **/
 static i40e_status i40e_nvmupd_state_init(struct i40e_hw *hw,
-					  struct i40e_nvm_access *cmd,
-					  u8 *bytes, int *perrno)
+						    struct i40e_nvm_access *cmd,
+						    u8 *bytes, int *perrno)
 {
-	i40e_status status = 0;
+	i40e_status status = I40E_SUCCESS;
 	enum i40e_nvmupd_cmd upd_cmd;
 
 	upd_cmd = i40e_nvmupd_validate_command(hw, cmd, perrno);
@@ -1059,10 +1081,10 @@
  * change in state; reject all other commands.
  **/
 static i40e_status i40e_nvmupd_state_reading(struct i40e_hw *hw,
-					     struct i40e_nvm_access *cmd,
-					     u8 *bytes, int *perrno)
+						    struct i40e_nvm_access *cmd,
+						    u8 *bytes, int *perrno)
 {
-	i40e_status status = 0;
+	i40e_status status = I40E_SUCCESS;
 	enum i40e_nvmupd_cmd upd_cmd;
 
 	upd_cmd = i40e_nvmupd_validate_command(hw, cmd, perrno);
@@ -1101,10 +1123,10 @@
  * change in state; reject all other commands
  **/
 static i40e_status i40e_nvmupd_state_writing(struct i40e_hw *hw,
-					     struct i40e_nvm_access *cmd,
-					     u8 *bytes, int *perrno)
+						    struct i40e_nvm_access *cmd,
+						    u8 *bytes, int *perrno)
 {
-	i40e_status status = 0;
+	i40e_status status = I40E_SUCCESS;
 	enum i40e_nvmupd_cmd upd_cmd;
 	bool retry_attempt = false;
 
@@ -1257,7 +1279,8 @@
 	u32 aq_desc_len = sizeof(struct i40e_aq_desc);
 
 	if (opcode == hw->nvm_wait_opcode) {
-		memcpy(&hw->nvm_aq_event_desc, desc, aq_desc_len);
+		i40e_memcpy(&hw->nvm_aq_event_desc, desc,
+			    aq_desc_len, I40E_NONDMA_TO_NONDMA);
 		i40e_nvmupd_clear_wait_state(hw);
 	}
 }
@@ -1271,8 +1294,8 @@
  * Return one of the valid command types or I40E_NVMUPD_INVALID
  **/
 static enum i40e_nvmupd_cmd i40e_nvmupd_validate_command(struct i40e_hw *hw,
-						 struct i40e_nvm_access *cmd,
-						 int *perrno)
+						    struct i40e_nvm_access *cmd,
+						    int *perrno)
 {
 	enum i40e_nvmupd_cmd upd_cmd;
 	u8 module, transaction;
@@ -1309,10 +1332,20 @@
 			upd_cmd = I40E_NVMUPD_READ_SA;
 			break;
 		case I40E_NVM_EXEC:
-			if (module == 0xf)
-				upd_cmd = I40E_NVMUPD_STATUS;
-			else if (module == 0)
+			switch (module) {
+			case I40E_NVM_EXEC_GET_AQ_RESULT:
 				upd_cmd = I40E_NVMUPD_GET_AQ_RESULT;
+				break;
+			case I40E_NVM_EXEC_FEATURES:
+				upd_cmd = I40E_NVMUPD_FEATURES;
+				break;
+			case I40E_NVM_EXEC_STATUS:
+				upd_cmd = I40E_NVMUPD_STATUS;
+				break;
+			default:
+				*perrno = -EFAULT;
+				return I40E_NVMUPD_INVALID;
+			}
 			break;
 		case I40E_NVM_AQE:
 			upd_cmd = I40E_NVMUPD_GET_AQ_EVENT;
@@ -1367,8 +1400,8 @@
  * cmd structure contains identifiers and data buffer
  **/
 static i40e_status i40e_nvmupd_exec_aq(struct i40e_hw *hw,
-				       struct i40e_nvm_access *cmd,
-				       u8 *bytes, int *perrno)
+						 struct i40e_nvm_access *cmd,
+						 u8 *bytes, int *perrno)
 {
 	struct i40e_asq_cmd_details cmd_details;
 	i40e_status status;
@@ -1380,7 +1413,7 @@
 
 	i40e_debug(hw, I40E_DEBUG_NVM, "NVMUPD: %s\n", __func__);
 	if (cmd->offset == 0xffff)
-		return 0;
+		return I40E_SUCCESS;
 
 	memset(&cmd_details, 0, sizeof(cmd_details));
 	cmd_details.wb_desc = &hw->nvm_wb_desc;
@@ -1400,7 +1433,7 @@
 
 	/* if data buffer needed, make sure it's ready */
 	aq_data_len = cmd->data_size - aq_desc_len;
-	buff_size = max_t(u32, aq_data_len, le16_to_cpu(aq_desc->datalen));
+	buff_size = max(aq_data_len, (u32)LE16_TO_CPU(aq_desc->datalen));
 	if (buff_size) {
 		if (!hw->nvm_buff.va) {
 			status = i40e_allocate_virt_mem(hw, &hw->nvm_buff,
@@ -1413,7 +1446,8 @@
 
 		if (hw->nvm_buff.va) {
 			buff = hw->nvm_buff.va;
-			memcpy(buff, &bytes[aq_desc_len], aq_data_len);
+			i40e_memcpy(buff, &bytes[aq_desc_len], aq_data_len,
+				I40E_NONDMA_TO_NONDMA);
 		}
 	}
 
@@ -1451,8 +1485,8 @@
  * cmd structure contains identifiers and data buffer
  **/
 static i40e_status i40e_nvmupd_get_aq_result(struct i40e_hw *hw,
-					     struct i40e_nvm_access *cmd,
-					     u8 *bytes, int *perrno)
+						    struct i40e_nvm_access *cmd,
+						    u8 *bytes, int *perrno)
 {
 	u32 aq_total_len;
 	u32 aq_desc_len;
@@ -1462,7 +1496,7 @@
 	i40e_debug(hw, I40E_DEBUG_NVM, "NVMUPD: %s\n", __func__);
 
 	aq_desc_len = sizeof(struct i40e_aq_desc);
-	aq_total_len = aq_desc_len + le16_to_cpu(hw->nvm_wb_desc.datalen);
+	aq_total_len = aq_desc_len + LE16_TO_CPU(hw->nvm_wb_desc.datalen);
 
 	/* check offset range */
 	if (cmd->offset > aq_total_len) {
@@ -1490,13 +1524,13 @@
 			   __func__, cmd->offset, cmd->offset + len);
 
 		buff = ((u8 *)&hw->nvm_wb_desc) + cmd->offset;
-		memcpy(bytes, buff, len);
+		i40e_memcpy(bytes, buff, len, I40E_NONDMA_TO_NONDMA);
 
 		bytes += len;
 		remainder -= len;
 		buff = hw->nvm_buff.va;
 	} else {
-		buff = hw->nvm_buff.va + (cmd->offset - aq_desc_len);
+		buff = (u8 *)hw->nvm_buff.va + (cmd->offset - aq_desc_len);
 	}
 
 	if (remainder > 0) {
@@ -1504,10 +1538,10 @@
 
 		i40e_debug(hw, I40E_DEBUG_NVM, "%s: databuf bytes %d to %d\n",
 			   __func__, start_byte, start_byte + remainder);
-		memcpy(bytes, buff, remainder);
+		i40e_memcpy(bytes, buff, remainder, I40E_NONDMA_TO_NONDMA);
 	}
 
-	return 0;
+	return I40E_SUCCESS;
 }
 
 /**
@@ -1520,8 +1554,8 @@
  * cmd structure contains identifiers and data buffer
  **/
 static i40e_status i40e_nvmupd_get_aq_event(struct i40e_hw *hw,
-					    struct i40e_nvm_access *cmd,
-					    u8 *bytes, int *perrno)
+						    struct i40e_nvm_access *cmd,
+						    u8 *bytes, int *perrno)
 {
 	u32 aq_total_len;
 	u32 aq_desc_len;
@@ -1529,7 +1563,7 @@
 	i40e_debug(hw, I40E_DEBUG_NVM, "NVMUPD: %s\n", __func__);
 
 	aq_desc_len = sizeof(struct i40e_aq_desc);
-	aq_total_len = aq_desc_len + le16_to_cpu(hw->nvm_aq_event_desc.datalen);
+	aq_total_len = aq_desc_len + LE16_TO_CPU(hw->nvm_aq_event_desc.datalen);
 
 	/* check copylength range */
 	if (cmd->data_size > aq_total_len) {
@@ -1539,9 +1573,10 @@
 		cmd->data_size = aq_total_len;
 	}
 
-	memcpy(bytes, &hw->nvm_aq_event_desc, cmd->data_size);
+	i40e_memcpy(bytes, &hw->nvm_aq_event_desc, cmd->data_size,
+		    I40E_NONDMA_TO_NONDMA);
 
-	return 0;
+	return I40E_SUCCESS;
 }
 
 /**
@@ -1554,8 +1589,8 @@
  * cmd structure contains identifiers and data buffer
  **/
 static i40e_status i40e_nvmupd_nvm_read(struct i40e_hw *hw,
-					struct i40e_nvm_access *cmd,
-					u8 *bytes, int *perrno)
+						  struct i40e_nvm_access *cmd,
+						  u8 *bytes, int *perrno)
 {
 	struct i40e_asq_cmd_details cmd_details;
 	i40e_status status;
@@ -1593,10 +1628,10 @@
  * module, offset, data_size and data are in cmd structure
  **/
 static i40e_status i40e_nvmupd_nvm_erase(struct i40e_hw *hw,
-					 struct i40e_nvm_access *cmd,
-					 int *perrno)
+						   struct i40e_nvm_access *cmd,
+						   int *perrno)
 {
-	i40e_status status = 0;
+	i40e_status status = I40E_SUCCESS;
 	struct i40e_asq_cmd_details cmd_details;
 	u8 module, transaction;
 	bool last;
@@ -1633,10 +1668,10 @@
  * module, offset, data_size and data are in cmd structure
  **/
 static i40e_status i40e_nvmupd_nvm_write(struct i40e_hw *hw,
-					 struct i40e_nvm_access *cmd,
-					 u8 *bytes, int *perrno)
+						   struct i40e_nvm_access *cmd,
+						   u8 *bytes, int *perrno)
 {
-	i40e_status status = 0;
+	i40e_status status = I40E_SUCCESS;
 	struct i40e_asq_cmd_details cmd_details;
 	u8 module, transaction;
 	u8 preservation_flags;
Binary files linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_nvm.o and linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_nvm.o differ
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_osdep.h linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_osdep.h
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_osdep.h	2024-05-10 01:26:45.373079718 -0400
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_osdep.h	2024-05-13 03:58:25.372491940 -0400
@@ -1,5 +1,5 @@
 /* SPDX-License-Identifier: GPL-2.0 */
-/* Copyright(c) 2013 - 2018 Intel Corporation. */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
 
 #ifndef _I40E_OSDEP_H_
 #define _I40E_OSDEP_H_
@@ -11,25 +11,48 @@
 #include <linux/pci.h>
 #include <linux/highuid.h>
 
-/* get readq/writeq support for 32 bit kernels, use the low-first version */
-#include <linux/io-64-nonatomic-lo-hi.h>
+#include <linux/io.h>
+#include <asm-generic/int-ll64.h>
+
+#ifndef readq
+static inline __u64 readq(const volatile void __iomem *addr)
+{
+	const volatile u32 __iomem *p = addr;
+	u32 low, high;
+
+	low = readl(p);
+	high = readl(p + 1);
+
+	return low + ((u64)high << 32);
+}
+#endif
+
+#ifndef writeq
+static inline void writeq(__u64 val, volatile void __iomem *addr)
+{
+	writel(val, addr);
+	writel(val >> 32, addr + 4);
+}
+#endif
+#include "kcompat.h"
 
 /* File to be the magic between shared code and
  * actual OS primitives
  */
 
-#define hw_dbg(hw, S, A...)							\
-do {										\
-	dev_dbg(&((struct i40e_pf *)hw->back)->pdev->dev, S, ##A);		\
+#define hw_dbg(h, s, ...) do {					\
+		pr_debug("i40e %02x:%02x.%x " s,		\
+			(h)->bus.bus_id, (h)->bus.device,	\
+			(h)->bus.func, ##__VA_ARGS__);		\
 } while (0)
 
+
 #define wr32(a, reg, value)	writel((value), ((a)->hw_addr + (reg)))
 #define rd32(a, reg)		readl((a)->hw_addr + (reg))
 
 #define wr64(a, reg, value)	writeq((value), ((a)->hw_addr + (reg)))
 #define rd64(a, reg)		readq((a)->hw_addr + (reg))
 #define i40e_flush(a)		readl((a)->hw_addr + I40E_GLGEN_STAT)
-
 /* memory allocation tracking */
 struct i40e_dma_mem {
 	void *va;
@@ -38,7 +61,8 @@
 };
 
 #define i40e_allocate_dma_mem(h, m, unused, s, a) \
-			i40e_allocate_dma_mem_d(h, m, s, a)
+			i40e_allocate_dma_mem_d(h, m, unused, s, a)
+
 #define i40e_free_dma_mem(h, m) i40e_free_dma_mem_d(h, m)
 
 struct i40e_virt_mem {
@@ -57,5 +81,40 @@
 			(h)->bus.func, ##__VA_ARGS__);		\
 } while (0)
 
+/* these things are all directly replaced with sed during the kernel build */
+#define INLINE inline
+
+
+#define CPU_TO_LE16(o) cpu_to_le16(o)
+#define CPU_TO_LE32(s) cpu_to_le32(s)
+#define CPU_TO_LE64(h) cpu_to_le64(h)
+#define LE16_TO_CPU(a) le16_to_cpu(a)
+#define LE32_TO_CPU(c) le32_to_cpu(c)
+#define LE64_TO_CPU(k) le64_to_cpu(k)
+
+/* SW spinlock */
+struct i40e_spinlock {
+	struct mutex spinlock;
+};
+
+static inline void i40e_no_action(struct i40e_spinlock *sp)
+{
+	/* nothing */
+}
+
+/* the locks are initialized in _probe and destroyed in _remove
+ * so make sure NOT to implement init/destroy here, as to
+ * avoid the i40e_init_adminq code trying to reinitialize
+ * the persistent lock memory
+ */
+#define i40e_init_spinlock(_sp)    i40e_no_action(_sp)
+#define i40e_acquire_spinlock(_sp) i40e_acquire_spinlock_d(_sp)
+#define i40e_release_spinlock(_sp) i40e_release_spinlock_d(_sp)
+#define i40e_destroy_spinlock(_sp) i40e_no_action(_sp)
+
+
+#define i40e_memset(a, b, c, d)  memset((a), (b), (c))
+#define i40e_memcpy(a, b, c, d)  memcpy((a), (b), (c))
+
 typedef enum i40e_status_code i40e_status;
 #endif /* _I40E_OSDEP_H_ */
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_prototype.h linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_prototype.h
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_prototype.h	2024-05-10 01:26:45.373079718 -0400
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_prototype.h	2024-05-13 03:58:25.372491940 -0400
@@ -1,12 +1,12 @@
 /* SPDX-License-Identifier: GPL-2.0 */
-/* Copyright(c) 2013 - 2018 Intel Corporation. */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
 
 #ifndef _I40E_PROTOTYPE_H_
 #define _I40E_PROTOTYPE_H_
 
 #include "i40e_type.h"
 #include "i40e_alloc.h"
-#include <linux/avf/virtchnl.h>
+#include "virtchnl.h"
 
 /* Prototypes for shared code functions that are not in
  * the standard function pointer structures.  These are
@@ -22,11 +22,34 @@
 i40e_status i40e_clean_arq_element(struct i40e_hw *hw,
 					     struct i40e_arq_event_info *e,
 					     u16 *events_pending);
-i40e_status i40e_asq_send_command(struct i40e_hw *hw,
+enum i40e_status_code
+i40e_asq_send_command_atomic(struct i40e_hw *hw,
+			     struct i40e_aq_desc *desc,
+			     void *buff, /* can be NULL */
+			     u16  buff_size,
+			     struct i40e_asq_cmd_details *cmd_details,
+			     bool is_atomic_context);
+enum i40e_status_code
+i40e_asq_send_command_atomic_v2(struct i40e_hw *hw,
 				struct i40e_aq_desc *desc,
 				void *buff, /* can be NULL */
 				u16  buff_size,
-				struct i40e_asq_cmd_details *cmd_details);
+				struct i40e_asq_cmd_details *cmd_details,
+				bool is_atomic_context,
+				enum i40e_admin_queue_err *aq_status);
+enum i40e_status_code
+i40e_asq_send_command(struct i40e_hw *hw,
+		      struct i40e_aq_desc *desc,
+		      void *buff, /* can be NULL */
+		      u16  buff_size,
+		      struct i40e_asq_cmd_details *cmd_details);
+enum i40e_status_code
+i40e_asq_send_command_v2(struct i40e_hw *hw,
+			 struct i40e_aq_desc *desc,
+			 void *buff, /* can be NULL */
+			 u16  buff_size,
+			 struct i40e_asq_cmd_details *cmd_details,
+			 enum i40e_admin_queue_err *aq_status);
 
 /* debug function for adminq */
 void i40e_debug_aq(struct i40e_hw *hw, enum i40e_debug_mask mask,
@@ -35,29 +58,39 @@
 void i40e_idle_aq(struct i40e_hw *hw);
 bool i40e_check_asq_alive(struct i40e_hw *hw);
 i40e_status i40e_aq_queue_shutdown(struct i40e_hw *hw, bool unloading);
-const char *i40e_aq_str(struct i40e_hw *hw, enum i40e_admin_queue_err aq_err);
-const char *i40e_stat_str(struct i40e_hw *hw, i40e_status stat_err);
 
 i40e_status i40e_aq_get_rss_lut(struct i40e_hw *hw, u16 seid,
-				bool pf_lut, u8 *lut, u16 lut_size);
+					  bool pf_lut, u8 *lut, u16 lut_size);
 i40e_status i40e_aq_set_rss_lut(struct i40e_hw *hw, u16 seid,
-				bool pf_lut, u8 *lut, u16 lut_size);
+					  bool pf_lut, u8 *lut, u16 lut_size);
 i40e_status i40e_aq_get_rss_key(struct i40e_hw *hw,
-				u16 seid,
-				struct i40e_aqc_get_set_rss_key_data *key);
+				     u16 seid,
+				     struct i40e_aqc_get_set_rss_key_data *key);
 i40e_status i40e_aq_set_rss_key(struct i40e_hw *hw,
-				u16 seid,
-				struct i40e_aqc_get_set_rss_key_data *key);
+				     u16 seid,
+				     struct i40e_aqc_get_set_rss_key_data *key);
+const char *i40e_aq_str(struct i40e_hw *hw, enum i40e_admin_queue_err aq_err);
+const char *i40e_stat_str(struct i40e_hw *hw, i40e_status stat_err);
 
 u32 i40e_led_get(struct i40e_hw *hw);
 void i40e_led_set(struct i40e_hw *hw, u32 mode, bool blink);
 i40e_status i40e_led_set_phy(struct i40e_hw *hw, bool on,
-			     u16 led_addr, u32 mode);
+				       u16 led_addr, u32 mode);
 i40e_status i40e_led_get_phy(struct i40e_hw *hw, u16 *led_addr,
-			     u16 *val);
+				       u16 *val);
 i40e_status i40e_blink_phy_link_led(struct i40e_hw *hw,
-				    u32 time, u32 interval);
-
+					      u32 time, u32 interval);
+i40e_status i40e_get_phy_lpi_status(struct i40e_hw *hw,
+					      struct i40e_hw_port_stats *stats);
+i40e_status i40e_get_lpi_counters(struct i40e_hw *hw, u32 *tx_counter,
+					    u32 *rx_counter, bool *is_clear);
+i40e_status i40e_lpi_stat_update(struct i40e_hw *hw,
+					   bool offset_loaded, u64 *tx_offset,
+					   u64 *tx_stat, u64 *rx_offset,
+					   u64 *rx_stat);
+i40e_status i40e_get_lpi_duration(struct i40e_hw *hw,
+					    struct i40e_hw_port_stats *stat,
+					    u64 *tx_duration, u64 *rx_duration);
 /* admin send queue commands */
 
 i40e_status i40e_aq_get_firmware_version(struct i40e_hw *hw,
@@ -66,8 +99,8 @@
 				u16 *api_major_version, u16 *api_minor_version,
 				struct i40e_asq_cmd_details *cmd_details);
 i40e_status i40e_aq_debug_write_register(struct i40e_hw *hw,
-					u32 reg_addr, u64 reg_val,
-					struct i40e_asq_cmd_details *cmd_details);
+				u32 reg_addr, u64 reg_val,
+				struct i40e_asq_cmd_details *cmd_details);
 i40e_status i40e_aq_debug_read_register(struct i40e_hw *hw,
 				u32  reg_addr, u64 *reg_val,
 				struct i40e_asq_cmd_details *cmd_details);
@@ -76,23 +109,22 @@
 i40e_status i40e_aq_set_default_vsi(struct i40e_hw *hw, u16 vsi_id,
 				struct i40e_asq_cmd_details *cmd_details);
 i40e_status i40e_aq_clear_default_vsi(struct i40e_hw *hw, u16 vsi_id,
-				      struct i40e_asq_cmd_details *cmd_details);
-enum i40e_status_code i40e_aq_get_phy_capabilities(struct i40e_hw *hw,
+				struct i40e_asq_cmd_details *cmd_details);
+i40e_status i40e_aq_get_phy_capabilities(struct i40e_hw *hw,
 			bool qualified_modules, bool report_init,
 			struct i40e_aq_get_phy_abilities_resp *abilities,
 			struct i40e_asq_cmd_details *cmd_details);
-enum i40e_status_code i40e_aq_set_phy_config(struct i40e_hw *hw,
+i40e_status i40e_aq_set_phy_config(struct i40e_hw *hw,
 				struct i40e_aq_set_phy_config *config,
 				struct i40e_asq_cmd_details *cmd_details);
-enum i40e_status_code i40e_set_fc(struct i40e_hw *hw, u8 *aq_failures,
+i40e_status i40e_set_fc(struct i40e_hw *hw, u8 *aq_failures,
 				  bool atomic_reset);
 i40e_status i40e_aq_set_phy_int_mask(struct i40e_hw *hw, u16 mask,
-				     struct i40e_asq_cmd_details *cmd_details);
-i40e_status i40e_aq_clear_pxe_mode(struct i40e_hw *hw,
 				struct i40e_asq_cmd_details *cmd_details);
+i40e_status i40e_aq_clear_pxe_mode(struct i40e_hw *hw,
+			struct i40e_asq_cmd_details *cmd_details);
 i40e_status i40e_aq_set_link_restart_an(struct i40e_hw *hw,
-					bool enable_link,
-					struct i40e_asq_cmd_details *cmd_details);
+		bool enable_link, struct i40e_asq_cmd_details *cmd_details);
 i40e_status i40e_aq_get_link_info(struct i40e_hw *hw,
 				bool enable_lse, struct i40e_link_status *link,
 				struct i40e_asq_cmd_details *cmd_details);
@@ -113,13 +145,14 @@
 		bool rx_only_promisc);
 i40e_status i40e_aq_set_vsi_multicast_promiscuous(struct i40e_hw *hw,
 		u16 vsi_id, bool set, struct i40e_asq_cmd_details *cmd_details);
-enum i40e_status_code i40e_aq_set_vsi_mc_promisc_on_vlan(struct i40e_hw *hw,
-							 u16 seid, bool enable,
-							 u16 vid,
-				struct i40e_asq_cmd_details *cmd_details);
-enum i40e_status_code i40e_aq_set_vsi_uc_promisc_on_vlan(struct i40e_hw *hw,
-							 u16 seid, bool enable,
-							 u16 vid,
+i40e_status i40e_aq_set_vsi_full_promiscuous(struct i40e_hw *hw,
+				u16 seid, bool set,
+				struct i40e_asq_cmd_details *cmd_details);
+i40e_status i40e_aq_set_vsi_mc_promisc_on_vlan(struct i40e_hw *hw,
+				u16 seid, bool enable, u16 vid,
+				struct i40e_asq_cmd_details *cmd_details);
+i40e_status i40e_aq_set_vsi_uc_promisc_on_vlan(struct i40e_hw *hw,
+				u16 seid, bool enable, u16 vid,
 				struct i40e_asq_cmd_details *cmd_details);
 i40e_status i40e_aq_set_vsi_bc_promisc_on_vlan(struct i40e_hw *hw,
 				u16 seid, bool enable, u16 vid,
@@ -146,9 +179,19 @@
 i40e_status i40e_aq_add_macvlan(struct i40e_hw *hw, u16 vsi_id,
 			struct i40e_aqc_add_macvlan_element_data *mv_list,
 			u16 count, struct i40e_asq_cmd_details *cmd_details);
+enum i40e_status_code
+i40e_aq_add_macvlan_v2(struct i40e_hw *hw, u16 seid,
+		       struct i40e_aqc_add_macvlan_element_data *mv_list,
+		       u16 count, struct i40e_asq_cmd_details *cmd_details,
+		       enum i40e_admin_queue_err *aq_status);
 i40e_status i40e_aq_remove_macvlan(struct i40e_hw *hw, u16 vsi_id,
 			struct i40e_aqc_remove_macvlan_element_data *mv_list,
 			u16 count, struct i40e_asq_cmd_details *cmd_details);
+enum i40e_status_code
+i40e_aq_remove_macvlan_v2(struct i40e_hw *hw, u16 seid,
+			  struct i40e_aqc_remove_macvlan_element_data *mv_list,
+			  u16 count, struct i40e_asq_cmd_details *cmd_details,
+			  enum i40e_admin_queue_err *aq_status);
 i40e_status i40e_aq_add_mirrorrule(struct i40e_hw *hw, u16 sw_seid,
 			u16 rule_type, u16 dest_vsi, u16 count, __le16 *mr_list,
 			struct i40e_asq_cmd_details *cmd_details,
@@ -165,9 +208,8 @@
 				struct i40e_aqc_get_switch_config_resp *buf,
 				u16 buf_size, u16 *start_seid,
 				struct i40e_asq_cmd_details *cmd_details);
-enum i40e_status_code i40e_aq_set_switch_config(struct i40e_hw *hw,
-						u16 flags,
-						u16 valid_flags, u8 mode,
+i40e_status i40e_aq_set_switch_config(struct i40e_hw *hw,
+				u16 flags, u16 valid_flags, u8 mode,
 				struct i40e_asq_cmd_details *cmd_details);
 i40e_status i40e_aq_request_resource(struct i40e_hw *hw,
 				enum i40e_aq_resources_ids resource,
@@ -183,8 +225,23 @@
 				bool last_command,
 				struct i40e_asq_cmd_details *cmd_details);
 i40e_status i40e_aq_erase_nvm(struct i40e_hw *hw, u8 module_pointer,
-			      u32 offset, u16 length, bool last_command,
-			      struct i40e_asq_cmd_details *cmd_details);
+				u32 offset, u16 length, bool last_command,
+				struct i40e_asq_cmd_details *cmd_details);
+i40e_status i40e_aq_read_nvm_config(struct i40e_hw *hw,
+				u8 cmd_flags, u32 field_id, void *data,
+				u16 buf_size, u16 *element_count,
+				struct i40e_asq_cmd_details *cmd_details);
+i40e_status i40e_aq_write_nvm_config(struct i40e_hw *hw,
+				u8 cmd_flags, void *data, u16 buf_size,
+				u16 element_count,
+				struct i40e_asq_cmd_details *cmd_details);
+enum i40e_status_code
+i40e_aq_min_rollback_rev_update(struct i40e_hw *hw, u8 mode, u8 module,
+				u32 min_rrev,
+				struct i40e_asq_cmd_details *cmd_details);
+i40e_status i40e_aq_oem_post_update(struct i40e_hw *hw,
+				void *buff, u16 buff_size,
+				struct i40e_asq_cmd_details *cmd_details);
 i40e_status i40e_aq_discover_capabilities(struct i40e_hw *hw,
 				void *buff, u16 buff_size, u16 *data_size,
 				enum i40e_admin_queue_opc list_type_opc,
@@ -194,12 +251,18 @@
 				bool last_command, u8 preservation_flags,
 				struct i40e_asq_cmd_details *cmd_details);
 i40e_status i40e_aq_rearrange_nvm(struct i40e_hw *hw,
-				  u8 rearrange_nvm,
-				  struct i40e_asq_cmd_details *cmd_details);
+				u8 rearrange_nvm,
+				struct i40e_asq_cmd_details *cmd_details);
+i40e_status i40e_aq_nvm_update_in_process(struct i40e_hw *hw,
+				bool update_flow_state,
+				struct i40e_asq_cmd_details *cmd_details);
 i40e_status i40e_aq_get_lldp_mib(struct i40e_hw *hw, u8 bridge_type,
 				u8 mib_type, void *buff, u16 buff_size,
 				u16 *local_len, u16 *remote_len,
 				struct i40e_asq_cmd_details *cmd_details);
+i40e_status i40e_aq_set_lldp_mib(struct i40e_hw *hw,
+				u8 mib_type, void *buff, u16 buff_size,
+				struct i40e_asq_cmd_details *cmd_details);
 i40e_status i40e_aq_cfg_lldp_mib_change_event(struct i40e_hw *hw,
 				bool enable_update,
 				struct i40e_asq_cmd_details *cmd_details);
@@ -207,23 +270,36 @@
 i40e_aq_restore_lldp(struct i40e_hw *hw, u8 *setting, bool restore,
 		     struct i40e_asq_cmd_details *cmd_details);
 i40e_status i40e_aq_stop_lldp(struct i40e_hw *hw, bool shutdown_agent,
-			      bool persist,
+				bool persist,
 				struct i40e_asq_cmd_details *cmd_details);
 i40e_status i40e_aq_set_dcb_parameters(struct i40e_hw *hw,
-				       bool dcb_enable,
-				       struct i40e_asq_cmd_details
-				       *cmd_details);
-i40e_status i40e_aq_start_lldp(struct i40e_hw *hw, bool persist,
-			       struct i40e_asq_cmd_details *cmd_details);
+						 bool dcb_enable,
+						 struct i40e_asq_cmd_details
+						 *cmd_details);
+i40e_status i40e_aq_start_lldp(struct i40e_hw *hw,
+				bool persist,
+				struct i40e_asq_cmd_details *cmd_details);
 i40e_status i40e_aq_get_cee_dcb_config(struct i40e_hw *hw,
-				       void *buff, u16 buff_size,
-				       struct i40e_asq_cmd_details *cmd_details);
+				void *buff, u16 buff_size,
+				struct i40e_asq_cmd_details *cmd_details);
+i40e_status i40e_aq_start_stop_dcbx(struct i40e_hw *hw,
+				bool start_agent,
+				struct i40e_asq_cmd_details *cmd_details);
 i40e_status i40e_aq_add_udp_tunnel(struct i40e_hw *hw,
 				u16 udp_port, u8 protocol_index,
 				u8 *filter_index,
 				struct i40e_asq_cmd_details *cmd_details);
 i40e_status i40e_aq_del_udp_tunnel(struct i40e_hw *hw, u8 index,
 				struct i40e_asq_cmd_details *cmd_details);
+i40e_status i40e_aq_get_switch_resource_alloc(struct i40e_hw *hw,
+			u8 *num_entries,
+			struct i40e_aqc_switch_resource_alloc_element_resp *buf,
+			u16 count,
+			struct i40e_asq_cmd_details *cmd_details);
+i40e_status i40e_aq_set_port_parameters(struct i40e_hw *hw,
+				u16 bad_frame_vsi, bool save_bad_pac,
+				bool pad_short_pac, bool double_vlan,
+				struct i40e_asq_cmd_details *cmd_details);
 i40e_status i40e_aq_delete_element(struct i40e_hw *hw, u16 seid,
 				struct i40e_asq_cmd_details *cmd_details);
 i40e_status i40e_aq_mac_address_write(struct i40e_hw *hw,
@@ -232,6 +308,14 @@
 i40e_status i40e_aq_config_vsi_bw_limit(struct i40e_hw *hw,
 				u16 seid, u16 credit, u8 max_credit,
 				struct i40e_asq_cmd_details *cmd_details);
+i40e_status i40e_aq_config_switch_comp_ets_bw_limit(
+	struct i40e_hw *hw, u16 seid,
+	struct i40e_aqc_configure_switching_comp_ets_bw_limit_data *bw_data,
+	struct i40e_asq_cmd_details *cmd_details);
+i40e_status i40e_aq_config_vsi_ets_sla_bw_limit(struct i40e_hw *hw,
+			u16 seid,
+			struct i40e_aqc_configure_vsi_ets_sla_bw_data *bw_data,
+			struct i40e_asq_cmd_details *cmd_details);
 i40e_status i40e_aq_dcb_updated(struct i40e_hw *hw,
 				struct i40e_asq_cmd_details *cmd_details);
 i40e_status i40e_aq_config_switch_comp_bw_limit(struct i40e_hw *hw,
@@ -270,7 +354,7 @@
 		struct i40e_aqc_query_switching_comp_bw_config_resp *bw_data,
 		struct i40e_asq_cmd_details *cmd_details);
 i40e_status i40e_aq_resume_port_tx(struct i40e_hw *hw,
-				   struct i40e_asq_cmd_details *cmd_details);
+				struct i40e_asq_cmd_details *cmd_details);
 enum i40e_status_code
 i40e_aq_add_cloud_filters_bb(struct i40e_hw *hw, u16 seid,
 			     struct i40e_aqc_cloud_filters_element_bb *filters,
@@ -288,7 +372,15 @@
 			     struct i40e_aqc_cloud_filters_element_bb *filters,
 			     u8 filter_count);
 i40e_status i40e_read_lldp_cfg(struct i40e_hw *hw,
-			       struct i40e_lldp_variables *lldp_cfg);
+					struct i40e_lldp_variables *lldp_cfg);
+i40e_status i40e_aq_suspend_port_tx(struct i40e_hw *hw, u16 seid,
+				struct i40e_asq_cmd_details *cmd_details);
+i40e_status i40e_aq_replace_cloud_filters(struct i40e_hw *hw,
+		struct i40e_aqc_replace_cloud_filters_cmd *filters,
+		struct i40e_aqc_replace_cloud_filters_cmd_buf *cmd_buf);
+i40e_status i40e_aq_alternate_read(struct i40e_hw *hw,
+				u32 reg_addr0, u32 *reg_val0,
+				u32 reg_addr1, u32 *reg_val1);
 /* i40e_common */
 i40e_status i40e_init_shared_code(struct i40e_hw *hw);
 i40e_status i40e_pf_reset(struct i40e_hw *hw);
@@ -298,15 +390,13 @@
 i40e_status i40e_update_link_info(struct i40e_hw *hw);
 i40e_status i40e_get_mac_addr(struct i40e_hw *hw, u8 *mac_addr);
 i40e_status i40e_read_bw_from_alt_ram(struct i40e_hw *hw,
-				      u32 *max_bw, u32 *min_bw, bool *min_valid,
-				      bool *max_valid);
+		u32 *max_bw, u32 *min_bw, bool *min_valid, bool *max_valid);
 i40e_status i40e_aq_configure_partition_bw(struct i40e_hw *hw,
 			struct i40e_aqc_configure_partition_bw_data *bw_data,
 			struct i40e_asq_cmd_details *cmd_details);
 i40e_status i40e_get_port_mac_addr(struct i40e_hw *hw, u8 *mac_addr);
 i40e_status i40e_read_pba_string(struct i40e_hw *hw, u8 *pba_num,
-				 u32 pba_num_size);
-i40e_status i40e_validate_mac_addr(u8 *mac_addr);
+					    u32 pba_num_size);
 void i40e_pre_tx_queue_cfg(struct i40e_hw *hw, u32 queue, bool enable);
 /* prototype for functions used for NVM access */
 i40e_status i40e_init_nvm(struct i40e_hw *hw);
@@ -315,18 +405,17 @@
 void i40e_release_nvm(struct i40e_hw *hw);
 i40e_status i40e_read_nvm_word(struct i40e_hw *hw, u16 offset,
 					 u16 *data);
-i40e_status i40e_read_nvm_module_data(struct i40e_hw *hw,
-				      u8 module_ptr, u16 offset,
-				      u16 words_data_size,
-				      u16 *data_ptr);
+enum i40e_status_code
+i40e_read_nvm_module_data(struct i40e_hw *hw, u8 module_ptr, u16 module_offset,
+			  u16 data_offset, u16 words_data_size, u16 *data_ptr);
 i40e_status i40e_read_nvm_buffer(struct i40e_hw *hw, u16 offset,
-				 u16 *words, u16 *data);
+					   u16 *words, u16 *data);
 i40e_status i40e_update_nvm_checksum(struct i40e_hw *hw);
 i40e_status i40e_validate_nvm_checksum(struct i40e_hw *hw,
 						 u16 *checksum);
 i40e_status i40e_nvmupd_command(struct i40e_hw *hw,
-				struct i40e_nvm_access *cmd,
-				u8 *bytes, int *);
+					  struct i40e_nvm_access *cmd,
+					  u8 *bytes, int *);
 void i40e_nvmupd_check_wait_event(struct i40e_hw *hw, u16 opcode,
 				  struct i40e_aq_desc *desc);
 void i40e_nvmupd_clear_wait_state(struct i40e_hw *hw);
@@ -336,7 +425,7 @@
 
 extern struct i40e_rx_ptype_decoded i40e_ptype_lookup[];
 
-static inline struct i40e_rx_ptype_decoded decode_rx_desc_ptype(u8 ptype)
+static INLINE struct i40e_rx_ptype_decoded decode_rx_desc_ptype(u8 ptype)
 {
 	return i40e_ptype_lookup[ptype];
 }
@@ -350,7 +439,7 @@
  * talking to virtchnl devices. If we can't represent the link speed properly,
  * report LINK_SPEED_UNKNOWN.
  **/
-static inline enum virtchnl_link_speed
+static INLINE enum virtchnl_link_speed
 i40e_virtchnl_link_speed(enum i40e_aq_link_speed link_speed)
 {
 	switch (link_speed) {
@@ -375,8 +464,11 @@
 		return VIRTCHNL_LINK_SPEED_UNKNOWN;
 	}
 }
-
-/* prototype for functions used for SW locks */
+/* prototype for functions used for SW spinlocks */
+void i40e_init_spinlock(struct i40e_spinlock *sp);
+void i40e_acquire_spinlock(struct i40e_spinlock *sp);
+void i40e_release_spinlock(struct i40e_spinlock *sp);
+void i40e_destroy_spinlock(struct i40e_spinlock *sp);
 
 /* i40e_common for VF drivers*/
 void i40e_vf_parse_hw_config(struct i40e_hw *hw,
@@ -395,10 +487,10 @@
 				struct i40e_control_filter_stats *stats,
 				struct i40e_asq_cmd_details *cmd_details);
 i40e_status i40e_aq_debug_dump(struct i40e_hw *hw, u8 cluster_id,
-			       u8 table_id, u32 start_index, u16 buff_size,
-			       void *buff, u16 *ret_buff_size,
-			       u8 *ret_next_table, u32 *ret_next_index,
-			       struct i40e_asq_cmd_details *cmd_details);
+				u8 table_id, u32 start_index, u16 buff_size,
+				void *buff, u16 *ret_buff_size,
+				u8 *ret_next_table, u32 *ret_next_index,
+				struct i40e_asq_cmd_details *cmd_details);
 void i40e_add_filter_to_drop_tx_flow_control_frames(struct i40e_hw *hw,
 						    u16 vsi_seid);
 i40e_status i40e_aq_rx_ctl_read_register(struct i40e_hw *hw,
@@ -409,39 +501,71 @@
 				u32 reg_addr, u32 reg_val,
 				struct i40e_asq_cmd_details *cmd_details);
 void i40e_write_rx_ctl(struct i40e_hw *hw, u32 reg_addr, u32 reg_val);
-i40e_status i40e_aq_set_phy_register(struct i40e_hw *hw,
-				     u8 phy_select, u8 dev_addr,
-				     u32 reg_addr, u32 reg_val,
-				     struct i40e_asq_cmd_details *cmd_details);
-i40e_status i40e_aq_get_phy_register(struct i40e_hw *hw,
-				     u8 phy_select, u8 dev_addr,
-				     u32 reg_addr, u32 *reg_val,
-				     struct i40e_asq_cmd_details *cmd_details);
+enum i40e_status_code
+i40e_aq_set_phy_register_ext(struct i40e_hw *hw,
+			     u8 phy_select, u8 dev_addr, bool page_change,
+			     bool set_mdio, u8 mdio_num,
+			     u32 reg_addr, u32 reg_val,
+			     struct i40e_asq_cmd_details *cmd_details);
+enum i40e_status_code
+i40e_aq_get_phy_register_ext(struct i40e_hw *hw,
+			     u8 phy_select, u8 dev_addr, bool page_change,
+			     bool set_mdio, u8 mdio_num,
+			     u32 reg_addr, u32 *reg_val,
+			     struct i40e_asq_cmd_details *cmd_details);
+
+/* Convenience wrappers for most common use case */
+#define i40e_aq_set_phy_register(hw, ps, da, pc, ra, rv, cd) \
+	i40e_aq_set_phy_register_ext(hw, ps, da, pc, false, 0, ra, rv, cd)
+#define i40e_aq_get_phy_register(hw, ps, da, pc, ra, rv, cd) \
+	i40e_aq_get_phy_register_ext(hw, ps, da, pc, false, 0, ra, rv, cd)
 
+enum i40e_status_code
+i40e_aq_run_phy_activity(struct i40e_hw *hw, u16 activity_id, u32 opcode,
+			 u32 *cmd_status, u32 *data0, u32 *data1,
+			 struct i40e_asq_cmd_details *cmd_details);
+
+i40e_status i40e_aq_set_arp_proxy_config(struct i40e_hw *hw,
+			struct i40e_aqc_arp_proxy_data *proxy_config,
+			struct i40e_asq_cmd_details *cmd_details);
+i40e_status i40e_aq_set_ns_proxy_table_entry(struct i40e_hw *hw,
+			struct i40e_aqc_ns_proxy_data *ns_proxy_table_entry,
+			struct i40e_asq_cmd_details *cmd_details);
+i40e_status i40e_aq_set_clear_wol_filter(struct i40e_hw *hw,
+			u8 filter_index,
+			struct i40e_aqc_set_wol_filter_data *filter,
+			bool set_filter, bool no_wol_tco,
+			bool filter_valid, bool no_wol_tco_valid,
+			struct i40e_asq_cmd_details *cmd_details);
+i40e_status i40e_aq_get_wake_event_reason(struct i40e_hw *hw,
+			u16 *wake_reason,
+			struct i40e_asq_cmd_details *cmd_details);
+i40e_status i40e_aq_clear_all_wol_filters(struct i40e_hw *hw,
+			struct i40e_asq_cmd_details *cmd_details);
 i40e_status i40e_read_phy_register_clause22(struct i40e_hw *hw,
-					    u16 reg, u8 phy_addr, u16 *value);
+					u16 reg, u8 phy_addr, u16 *value);
 i40e_status i40e_write_phy_register_clause22(struct i40e_hw *hw,
-					     u16 reg, u8 phy_addr, u16 value);
+					u16 reg, u8 phy_addr, u16 value);
 i40e_status i40e_read_phy_register_clause45(struct i40e_hw *hw,
 				u8 page, u16 reg, u8 phy_addr, u16 *value);
 i40e_status i40e_write_phy_register_clause45(struct i40e_hw *hw,
 				u8 page, u16 reg, u8 phy_addr, u16 value);
-i40e_status i40e_read_phy_register(struct i40e_hw *hw, u8 page, u16 reg,
-				   u8 phy_addr, u16 *value);
-i40e_status i40e_write_phy_register(struct i40e_hw *hw, u8 page, u16 reg,
-				    u8 phy_addr, u16 value);
+i40e_status i40e_read_phy_register(struct i40e_hw *hw,
+				u8 page, u16 reg, u8 phy_addr, u16 *value);
+i40e_status i40e_write_phy_register(struct i40e_hw *hw,
+				u8 page, u16 reg, u8 phy_addr, u16 value);
 u8 i40e_get_phy_address(struct i40e_hw *hw, u8 dev_num);
 i40e_status i40e_blink_phy_link_led(struct i40e_hw *hw,
-				    u32 time, u32 interval);
+					      u32 time, u32 interval);
 i40e_status i40e_aq_write_ddp(struct i40e_hw *hw, void *buff,
-			      u16 buff_size, u32 track_id,
-			      u32 *error_offset, u32 *error_info,
-			      struct i40e_asq_cmd_details *
-			      cmd_details);
+					u16 buff_size, u32 track_id,
+					u32 *error_offset, u32 *error_info,
+					struct i40e_asq_cmd_details *
+					cmd_details);
 i40e_status i40e_aq_get_ddp_list(struct i40e_hw *hw, void *buff,
-				 u16 buff_size, u8 flags,
-				 struct i40e_asq_cmd_details *
-				 cmd_details);
+					   u16 buff_size, u8 flags,
+					   struct i40e_asq_cmd_details *
+					   cmd_details);
 struct i40e_generic_seg_header *
 i40e_find_segment_in_package(u32 segment_type,
 			     struct i40e_package_header *pkg_header);
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_ptp.c linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_ptp.c
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_ptp.c	2024-05-10 01:26:45.373079718 -0400
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_ptp.c	2024-05-13 03:58:25.372491940 -0400
@@ -1,8 +1,14 @@
 // SPDX-License-Identifier: GPL-2.0
-/* Copyright(c) 2013 - 2018 Intel Corporation. */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
 
+/* this lets the macros that return timespec64 or structs compile cleanly with
+ * W=2
+ */
+#pragma GCC diagnostic ignored "-Waggregate-return"
 #include "i40e.h"
+#ifdef HAVE_PTP_1588_CLOCK
 #include <linux/ptp_classify.h>
+#include <linux/posix-clock.h>
 
 /* The XL710 timesync is very much like Intel's 82599 design when it comes to
  * the fundamental clock design. However, the clock operations are much simpler
@@ -11,40 +17,292 @@
  * operate with the nanosecond field directly without fear of overflow.
  *
  * Much like the 82599, the update period is dependent upon the link speed:
- * At 40Gb link or no link, the period is 1.6ns.
- * At 10Gb link, the period is multiplied by 2. (3.2ns)
+ * At 40Gb, 25Gb, or no link, the period is 1.6ns.
+ * At 10Gb or 5Gb link, the period is multiplied by 2. (3.2ns)
  * At 1Gb link, the period is multiplied by 20. (32ns)
  * 1588 functionality is not supported at 100Mbps.
  */
 #define I40E_PTP_40GB_INCVAL		0x0199999999ULL
 #define I40E_PTP_10GB_INCVAL_MULT	2
+#define I40E_PTP_5GB_INCVAL_MULT	2
 #define I40E_PTP_1GB_INCVAL_MULT	20
+#define I40E_ISGN			0x80000000
 
 #define I40E_PRTTSYN_CTL1_TSYNTYPE_V1  BIT(I40E_PRTTSYN_CTL1_TSYNTYPE_SHIFT)
 #define I40E_PRTTSYN_CTL1_TSYNTYPE_V2  (2 << \
 					I40E_PRTTSYN_CTL1_TSYNTYPE_SHIFT)
+#define I40E_SUBDEV_ID_25G_PTP_PIN	0xB
+#define to_dev(obj) container_of(obj, struct device, kobj)
+
+enum i40e_ptp_pin {
+	SDP3_2 = 0,
+	SDP3_3,
+	GPIO_4
+};
+
+enum i40e_can_set_pins_t {
+	CANT_DO_PINS = -1,
+	CAN_SET_PINS,
+	CAN_DO_PINS
+};
+
+static struct ptp_pin_desc sdp_desc[] = {
+	/* name     idx      func      chan */
+	{"SDP3_2", SDP3_2, PTP_PF_NONE, 0},
+	{"SDP3_3", SDP3_3, PTP_PF_NONE, 1},
+	{"GPIO_4", GPIO_4, PTP_PF_NONE, 1},
+};
+
+#ifndef HAVE_PTP_1588_CLOCK_PINS
+static ssize_t i40e_sysfs_ptp_pins_read(struct kobject *kobj,
+					struct kobj_attribute *attr,
+					char *buf);
+
+static ssize_t i40e_sysfs_ptp_pins_write(struct kobject *kobj,
+					 struct kobj_attribute *attr,
+					 const char *buf, size_t count);
+
+static struct kobj_attribute ptp_pins_attribute = __ATTR(pins, 0660,
+	       i40e_sysfs_ptp_pins_read, i40e_sysfs_ptp_pins_write);
+#endif /* HAVE_PTP_1588_CLOCK_PINS */
+
+enum i40e_ptp_gpio_pin_state {
+	end = -2,
+	invalid,
+	off,
+	in_A,
+	in_B,
+	out_A,
+	out_B,
+};
+
+static const char * const i40e_ptp_gpio_pin_state2str[] = {
+	"off", "in_A", "in_B", "out_A", "out_B"
+};
+
+enum i40e_ptp_led_pin_state {
+	led_end = -2,
+	low = 0,
+	high,
+};
+
+struct i40e_ptp_pins_settings {
+	enum i40e_ptp_gpio_pin_state sdp3_2;
+	enum i40e_ptp_gpio_pin_state sdp3_3;
+	enum i40e_ptp_gpio_pin_state gpio_4;
+	enum i40e_ptp_led_pin_state led2_0;
+	enum i40e_ptp_led_pin_state led2_1;
+	enum i40e_ptp_led_pin_state led3_0;
+	enum i40e_ptp_led_pin_state led3_1;
+};
+
+static const struct i40e_ptp_pins_settings
+	i40e_ptp_pin_led_allowed_states[] = {
+	{off,	off,	off,		high,	high,	high,	high},
+	{off,	in_A,	off,		high,	high,	high,	low},
+	{off,	out_A,	off,		high,	low,	high,	high},
+	{off,	in_B,	off,		high,	high,	high,	low},
+	{off,	out_B,	off,		high,	low,	high,	high},
+	{in_A,	off,	off,		high,	high,	high,	low},
+	{in_A,	in_B,	off,		high,	high,	high,	low},
+	{in_A,	out_B,	off,		high,	low,	high,	high},
+	{out_A,	off,	off,		high,	low,	high,	high},
+	{out_A,	in_B,	off,		high,	low,	high,	high},
+	{in_B,	off,	off,		high,	high,	high,	low},
+	{in_B,	in_A,	off,		high,	high,	high,	low},
+	{in_B,	out_A,	off,		high,	low,	high,	high},
+	{out_B,	off,	off,		high,	low,	high,	high},
+	{out_B,	in_A,	off,		high,	low,	high,	high},
+	{off,	off,	in_A,		high,	high,	low,	high},
+	{off,	out_A,	in_A,		high,	low,	low,	high},
+	{off,	in_B,	in_A,		high,	high,	low,	low},
+	{off,	out_B,	in_A,		high,	low,	low,	high},
+	{out_A,	off,	in_A,		high,	low,	low,	high},
+	{out_A,	in_B,	in_A,		high,	low,	low,	high},
+	{in_B,	off,	in_A,		high,	high,	low,	low},
+	{in_B,	out_A,	in_A,		high,	low,	low,	high},
+	{out_B,	off,	in_A,		high,	low,	low,	high},
+	{off,	off,	out_A,		low,	high,	high,	high},
+	{off,	in_A,	out_A,		low,	high,	high,	low},
+	{off,	in_B,	out_A,		low,	high,	high,	low},
+	{off,	out_B,	out_A,		low,	low,	high,	high},
+	{in_A,	off,	out_A,		low,	high,	high,	low},
+	{in_A,	in_B,	out_A,		low,	high,	high,	low},
+	{in_A,	out_B,	out_A,		low,	low,	high,	high},
+	{in_B,	off,	out_A,		low,	high,	high,	low},
+	{in_B,	in_A,	out_A,		low,	high,	high,	low},
+	{out_B,	off,	out_A,		low,	low,	high,	high},
+	{out_B,	in_A,	out_A,		low,	low,	high,	high},
+	{off,	off,	in_B,		high,	high,	low,	high},
+	{off,	in_A,	in_B,		high,	high,	low,	low},
+	{off,	out_A,	in_B,		high,	low,	low,	high},
+	{off,	out_B,	in_B,		high,	low,	low,	high},
+	{in_A,	off,	in_B,		high,	high,	low,	low},
+	{in_A,	out_B,	in_B,		high,	low,	low,	high},
+	{out_A,	off,	in_B,		high,	low,	low,	high},
+	{out_B,	off,	in_B,		high,	low,	low,	high},
+	{out_B,	in_A,	in_B,		high,	low,	low,	high},
+	{off,	off,	out_B,		low,	high,	high,	high},
+	{off,	in_A,	out_B,		low,	high,	high,	low},
+	{off,	out_A,	out_B,		low,	low,	high,	high},
+	{off,	in_B,	out_B,		low,	high,	high,	low},
+	{in_A,	off,	out_B,		low,	high,	high,	low},
+	{in_A,	in_B,	out_B,		low,	high,	high,	low},
+	{out_A,	off,	out_B,		low,	low,	high,	high},
+	{out_A,	in_B,	out_B,		low,	low,	high,	high},
+	{in_B,	off,	out_B,		low,	high,	high,	low},
+	{in_B,	in_A,	out_B,		low,	high,	high,	low},
+	{in_B,	out_A,	out_B,		low,	low,	high,	high},
+	{end,	end,	end,	led_end, led_end, led_end, led_end}
+};
+
+static int i40e_ptp_set_pins(struct i40e_pf *pf,
+			     struct i40e_ptp_pins_settings *pins);
+
+/**
+ * i40e_ptp_extts0_work - workqueue task function
+ * @work: workqueue task structure
+ *
+ * Service for PTP external clock event
+ **/
+static void i40e_ptp_extts0_work(struct work_struct *work)
+{
+	struct i40e_pf *pf = container_of(work, struct i40e_pf,
+					  ptp_extts0_work);
+	struct i40e_hw *hw = &pf->hw;
+	struct ptp_clock_event event;
+	u32 hi, lo;
+
+	/* Event time is captured by one of the two matched registers
+	 *      PRTTSYN_EVNT_L: 32 LSB of sampled time event
+	 *      PRTTSYN_EVNT_H: 32 MSB of sampled time event
+	 * Event is defined in PRTTSYN_EVNT_0 register
+	 */
+	lo = rd32(hw, I40E_PRTTSYN_EVNT_L(0));
+	hi = rd32(hw, I40E_PRTTSYN_EVNT_H(0));
+
+	event.timestamp = (((u64)hi) << 32) | lo;
+
+	event.type = PTP_CLOCK_EXTTS;
+	event.index = hw->pf_id;
+
+	/* fire event */
+	ptp_clock_event(pf->ptp_clock, &event);
+}
+
+/**
+ * i40e_is_ptp_pin_dev - check if device supports PTP pins
+ * @hw: pointer to the hardware structure
+ *
+ * Return true if device supports PTP pins, false otherwise.
+ **/
+static bool i40e_is_ptp_pin_dev(struct i40e_hw *hw)
+{
+	return hw->device_id == I40E_DEV_ID_25G_SFP28 &&
+	       hw->subsystem_device_id == I40E_SUBDEV_ID_25G_PTP_PIN;
+}
+
+/**
+ * i40e_can_set_pins - check possibility of manipulating the pins
+ * @pf: board private structure
+ *
+ * Check if all conditions are satisfied to manipulate PTP pins.
+ * Return CAN_SET_PINS if pins can be set on a specific PF or
+ * return CAN_DO_PINS if pins can be manipulated within a NIC or
+ * return CANT_DO_PINS otherwise.
+ **/
+static enum i40e_can_set_pins_t i40e_can_set_pins(struct i40e_pf *pf)
+{
+	if (!i40e_is_ptp_pin_dev(&pf->hw)) {
+		dev_warn(&pf->pdev->dev,
+			 "PTP external clock not supported.\n");
+		return CANT_DO_PINS;
+	}
+
+	if (!pf->ptp_pins) {
+		dev_warn(&pf->pdev->dev,
+			 "PTP PIN manipulation not allowed.\n");
+		return CANT_DO_PINS;
+	}
+
+	if (pf->hw.pf_id) {
+		dev_warn(&pf->pdev->dev,
+			 "PTP PINs should be accessed via PF0.\n");
+		return CAN_DO_PINS;
+	}
+
+	return CAN_SET_PINS;
+}
+
+/**
+ * i40_ptp_reset_timing_events - Reset PTP timing events
+ * @pf: Board private structure
+ *
+ * This function resets timing events for pf.
+ **/
+static void i40_ptp_reset_timing_events(struct i40e_pf *pf)
+{
+	u32 i;
+
+	spin_lock_bh(&pf->ptp_rx_lock);
+	for (i = 0; i <= I40E_PRTTSYN_RXTIME_L_MAX_INDEX; i++) {
+		/* reading and automatically clearing timing events registers */
+		rd32(&pf->hw, I40E_PRTTSYN_RXTIME_L(i));
+		rd32(&pf->hw, I40E_PRTTSYN_RXTIME_H(i));
+		pf->latch_events[i] = 0;
+	}
+	/* reading and automatically clearing timing events registers */
+	rd32(&pf->hw, I40E_PRTTSYN_TXTIME_L);
+	rd32(&pf->hw, I40E_PRTTSYN_TXTIME_H);
+
+	pf->tx_hwtstamp_timeouts = 0;
+	pf->tx_hwtstamp_skipped = 0;
+	pf->rx_hwtstamp_cleared = 0;
+	pf->latch_event_flags = 0;
+	spin_unlock_bh(&pf->ptp_rx_lock);
+}
+
+/**
+ * i40e_ptp_verify - check pins
+ * @ptp: ptp clock
+ * @pin: pin index
+ * @func: assigned function
+ * @chan: channel
+ *
+ * Check pins consistency.
+ * Return 0 on success or error on failure.
+ **/
+static int i40e_ptp_verify(struct ptp_clock_info *ptp, unsigned int pin,
+			   enum ptp_pin_function func, unsigned int chan)
+{
+	switch (func) {
+	case PTP_PF_NONE:
+	case PTP_PF_EXTTS:
+	case PTP_PF_PEROUT:
+		break;
+	case PTP_PF_PHYSYNC:
+		return -EOPNOTSUPP;
+	}
+	return 0;
+}
 
 /**
  * i40e_ptp_read - Read the PHC time from the device
  * @pf: Board private structure
  * @ts: timespec structure to hold the current time value
- * @sts: structure to hold the system time before and after reading the PHC
  *
  * This function reads the PRTTSYN_TIME registers and stores them in a
  * timespec. However, since the registers are 64 bits of nanoseconds, we must
  * convert the result to a timespec before we can return.
  **/
-static void i40e_ptp_read(struct i40e_pf *pf, struct timespec64 *ts,
-			  struct ptp_system_timestamp *sts)
+static void i40e_ptp_read(struct i40e_pf *pf, struct timespec64 *ts)
 {
 	struct i40e_hw *hw = &pf->hw;
 	u32 hi, lo;
 	u64 ns;
 
 	/* The timer latches on the lowest register read. */
-	ptp_read_system_prets(sts);
 	lo = rd32(hw, I40E_PRTTSYN_TIME_L);
-	ptp_read_system_postts(sts);
 	hi = rd32(hw, I40E_PRTTSYN_TIME_H);
 
 	ns = (((u64)hi) << 32) | lo;
@@ -69,8 +327,8 @@
 	/* The timer will not update until the high register is written, so
 	 * write the low register first.
 	 */
-	wr32(hw, I40E_PRTTSYN_TIME_L, ns & 0xFFFFFFFF);
-	wr32(hw, I40E_PRTTSYN_TIME_H, ns >> 32);
+	wr32(hw, I40E_PRTTSYN_TIME_L, (u32)ns);
+	wr32(hw, I40E_PRTTSYN_TIME_H, (u32)(ns >> 32));
 }
 
 /**
@@ -129,13 +387,44 @@
 	smp_mb(); /* Force any pending update before accessing. */
 	adj *= READ_ONCE(pf->ptp_adj_mult);
 
-	wr32(hw, I40E_PRTTSYN_INC_L, adj & 0xFFFFFFFF);
-	wr32(hw, I40E_PRTTSYN_INC_H, adj >> 32);
+	wr32(hw, I40E_PRTTSYN_INC_L, (u32)adj);
+	wr32(hw, I40E_PRTTSYN_INC_H, (u32)(adj >> 32));
 
 	return 0;
 }
 
 /**
+ * i40e_ptp_set_1pps_signal_hw - configure 1PPS PTP signal for pins
+ * @pf: the PF private data structure
+ *
+ * Configure 1PPS signal used for PTP pins
+ **/
+static void i40e_ptp_set_1pps_signal_hw(struct i40e_pf *pf)
+{
+	struct i40e_hw *hw = &pf->hw;
+	struct timespec64 now;
+	u64 ns;
+
+	wr32(hw, I40E_PRTTSYN_AUX_0(1), 0);
+	wr32(hw, I40E_PRTTSYN_AUX_1(1), I40E_PRTTSYN_AUX_1_INSTNT);
+	wr32(hw, I40E_PRTTSYN_AUX_0(1), I40E_PRTTSYN_AUX_0_OUT_ENABLE);
+
+	i40e_ptp_read(pf, &now);
+	now.tv_sec += I40E_PTP_2_SEC_DELAY;
+	now.tv_nsec = 0;
+	ns = timespec64_to_ns(&now);
+
+	/* I40E_PRTTSYN_TGT_L(1) */
+	wr32(hw, I40E_PRTTSYN_TGT_L(1), ns & 0xFFFFFFFF);
+	/* I40E_PRTTSYN_TGT_H(1) */
+	wr32(hw, I40E_PRTTSYN_TGT_H(1), ns >> 32);
+	wr32(hw, I40E_PRTTSYN_CLKO(1), I40E_PTP_HALF_SECOND);
+	wr32(hw, I40E_PRTTSYN_AUX_1(1), I40E_PRTTSYN_AUX_1_INSTNT);
+	wr32(hw, I40E_PRTTSYN_AUX_0(1),
+	     I40E_PRTTSYN_AUX_0_OUT_ENABLE_CLK_MOD);
+}
+
+/**
  * i40e_ptp_adjtime - Adjust the PHC time
  * @ptp: The PTP clock structure
  * @delta: Offset in nanoseconds to adjust the PHC time by
@@ -145,45 +434,62 @@
 static int i40e_ptp_adjtime(struct ptp_clock_info *ptp, s64 delta)
 {
 	struct i40e_pf *pf = container_of(ptp, struct i40e_pf, ptp_caps);
-	struct timespec64 now, then;
+	struct i40e_hw *hw = &pf->hw;
 
-	then = ns_to_timespec64(delta);
 	mutex_lock(&pf->tmreg_lock);
 
-	i40e_ptp_read(pf, &now, NULL);
-	now = timespec64_add(now, then);
-	i40e_ptp_write(pf, (const struct timespec64 *)&now);
+	if (delta > -999999900LL && delta < 999999900LL) {
+		int neg_adj = 0;
+		u32 timadj;
+		u64 tohw;
+
+		if (delta < 0) {
+			neg_adj = 1;
+			tohw = -delta;
+		} else {
+			tohw = delta;
+		}
 
-	mutex_unlock(&pf->tmreg_lock);
+		timadj = tohw & 0x3FFFFFFF;
+		if (neg_adj)
+			timadj |= I40E_ISGN;
+		wr32(hw, I40E_PRTTSYN_ADJ, timadj);
+	} else {
+		struct timespec64 then, now;
+
+		then = ns_to_timespec64(delta);
+		i40e_ptp_read(pf, &now);
+		now = timespec64_add(now, then);
+		i40e_ptp_write(pf, (const struct timespec64 *)&now);
+		i40e_ptp_set_1pps_signal_hw(pf);
+	}
 
+	mutex_unlock(&pf->tmreg_lock);
 	return 0;
 }
 
 /**
- * i40e_ptp_gettimex - Get the time of the PHC
+ * i40e_ptp_gettime - Get the time of the PHC
  * @ptp: The PTP clock structure
- * @ts: timespec structure to hold the current time value
- * @sts: structure to hold the system time before and after reading the PHC
+ * @ts: timespec64 structure to hold the current time value
  *
  * Read the device clock and return the correct value on ns, after converting it
  * into a timespec struct.
  **/
-static int i40e_ptp_gettimex(struct ptp_clock_info *ptp, struct timespec64 *ts,
-			     struct ptp_system_timestamp *sts)
+static int i40e_ptp_gettime(struct ptp_clock_info *ptp, struct timespec64 *ts)
 {
 	struct i40e_pf *pf = container_of(ptp, struct i40e_pf, ptp_caps);
 
 	mutex_lock(&pf->tmreg_lock);
-	i40e_ptp_read(pf, ts, sts);
+	i40e_ptp_read(pf, ts);
 	mutex_unlock(&pf->tmreg_lock);
-
 	return 0;
 }
 
 /**
  * i40e_ptp_settime - Set the time of the PHC
  * @ptp: The PTP clock structure
- * @ts: timespec structure that holds the new time value
+ * @ts: timespec64 structure that holds the new time value
  *
  * Set the device clock to the user input value. The conversion from timespec
  * to ns happens in the write function.
@@ -196,27 +502,192 @@
 	mutex_lock(&pf->tmreg_lock);
 	i40e_ptp_write(pf, ts);
 	mutex_unlock(&pf->tmreg_lock);
+	return 0;
+}
+
+#ifndef HAVE_PTP_CLOCK_INFO_GETTIME64
+/**
+ * i40e_ptp_gettime32 - Get the time of the PHC
+ * @ptp: The PTP clock structure
+ * @ts: timespec structure to hold the current time value
+ *
+ * Read the device clock and return the correct value on ns, after converting it
+ * into a timespec struct.
+ **/
+static int i40e_ptp_gettime32(struct ptp_clock_info *ptp, struct timespec *ts)
+{
+	struct timespec64 ts64;
+	int err;
 
+	err = i40e_ptp_gettime(ptp, &ts64);
+	if (err)
+		return err;
+
+	*ts = timespec64_to_timespec(ts64);
 	return 0;
 }
 
 /**
- * i40e_ptp_feature_enable - Enable/disable ancillary features of the PHC subsystem
+ * i40e_ptp_settime32 - Set the time of the PHC
  * @ptp: The PTP clock structure
- * @rq: The requested feature to change
- * @on: Enable/disable flag
+ * @ts: timespec structure that holds the new time value
  *
- * The XL710 does not support any of the ancillary features of the PHC
- * subsystem, so this function may just return.
+ * Set the device clock to the user input value. The conversion from timespec
+ * to ns happens in the write function.
+ **/
+static int i40e_ptp_settime32(struct ptp_clock_info *ptp,
+			      const struct timespec *ts)
+{
+	struct timespec64 ts64 = timespec_to_timespec64(*ts);
+
+	return i40e_ptp_settime(ptp, &ts64);
+}
+#endif
+
+/**
+ * i40e_pps_configure - configure PPS events
+ * @ptp: ptp clock
+ * @rq: clock request
+ * @on: status
+ *
+ * Configure PPS events for external clock source.
+ * Return 0 on success or error on failure.
+ **/
+static int i40e_pps_configure(struct ptp_clock_info *ptp,
+			      struct ptp_clock_request *rq,
+			      int on)
+{
+	struct i40e_pf *pf = container_of(ptp, struct i40e_pf, ptp_caps);
+
+	if (!!on)
+		i40e_ptp_set_1pps_signal_hw(pf);
+
+	return 0;
+}
+
+/**
+ * i40e_pin_state - determine PIN state
+ * @index: PIN index
+ * @func: function assigned to PIN
+ *
+ * Determine PIN state based on PIN index and function assigned.
+ * Return PIN state.
+ **/
+static enum i40e_ptp_gpio_pin_state i40e_pin_state(int index, int func)
+{
+	enum i40e_ptp_gpio_pin_state state = off;
+
+	if (index == 0 && func == PTP_PF_EXTTS)
+		state = in_A;
+	if (index == 1 && func == PTP_PF_EXTTS)
+		state = in_B;
+	if (index == 0 && func == PTP_PF_PEROUT)
+		state = out_A;
+	if (index == 1 && func == PTP_PF_PEROUT)
+		state = out_B;
+
+	return state;
+}
+
+/**
+ * i40e_ptp_enable_pin - enable PINs.
+ * @pf: private board structure
+ * @chan: channel
+ * @func: PIN function
+ * @on: state
+ *
+ * Enable PTP pins for external clock source.
+ * Return 0 on success or error code on failure.
+ **/
+static int i40e_ptp_enable_pin(struct i40e_pf *pf, unsigned int chan,
+			       enum ptp_pin_function func, int on)
+{
+	enum i40e_ptp_gpio_pin_state *pin = NULL;
+	struct i40e_ptp_pins_settings pins;
+	int pin_index;
+
+	/* Use PF0 to set pins. Return success for user space tools */
+	if (pf->hw.pf_id)
+		return 0;
+
+	/* Preserve previous state of pins that we don't touch */
+	pins.sdp3_2 = pf->ptp_pins->sdp3_2;
+	pins.sdp3_3 = pf->ptp_pins->sdp3_3;
+	pins.gpio_4 = pf->ptp_pins->gpio_4;
+
+	/* To turn on the pin - find the corresponding one based on
+	 * the given index. To to turn the function off - find
+	 * which pin had it assigned. Don't use ptp_find_pin here
+	 * because it tries to lock the pincfg_mux which is locked by
+	 * ptp_pin_store() that calls here.
+	 */
+	if (on) {
+		pin_index = ptp_find_pin(pf->ptp_clock, func, chan);
+		if (pin_index < 0)
+			return -EBUSY;
+
+		switch (pin_index) {
+		case SDP3_2:
+			pin = &pins.sdp3_2;
+			break;
+		case SDP3_3:
+			pin = &pins.sdp3_3;
+			break;
+		case GPIO_4:
+			pin = &pins.gpio_4;
+			break;
+		default:
+			return -EINVAL;
+		}
+
+		*pin = i40e_pin_state(chan, func);
+	} else {
+		pins.sdp3_2 = off;
+		pins.sdp3_3 = off;
+		pins.gpio_4 = off;
+	}
+
+	return i40e_ptp_set_pins(pf, &pins) ? -EINVAL : 0;
+}
+
+/**
+ * i40e_ptp_feature_enable - Enable external clock pins
+ * @ptp: The PTP clock structure
+ * @rq: The PTP clock request structure
+ * @on: To turn feature on/off
+ *
+ * Setting on/off PTP PPS feature for pin.
  **/
 static int i40e_ptp_feature_enable(struct ptp_clock_info *ptp,
-				   struct ptp_clock_request *rq, int on)
+				   struct ptp_clock_request *rq,
+				   int on)
 {
-	return -EOPNOTSUPP;
+	struct i40e_pf *pf = container_of(ptp, struct i40e_pf, ptp_caps);
+
+	enum ptp_pin_function func;
+	unsigned int chan;
+
+	/* TODO: Implement flags handling for EXTTS and PEROUT */
+	switch (rq->type) {
+	case PTP_CLK_REQ_EXTTS:
+		func = PTP_PF_EXTTS;
+		chan = rq->extts.index;
+		break;
+	case PTP_CLK_REQ_PEROUT:
+		func = PTP_PF_PEROUT;
+		chan = rq->perout.index;
+		break;
+	case PTP_CLK_REQ_PPS:
+		return i40e_pps_configure(ptp, rq, on);
+	default:
+		return -EOPNOTSUPP;
+	}
+
+	return i40e_ptp_enable_pin(pf, chan, func, on);
 }
 
 /**
- * i40e_ptp_update_latch_events - Read I40E_PRTTSYN_STAT_1 and latch events
+ * i40e_ptp_get_rx_events - Read I40E_PRTTSYN_STAT_1 and latch events
  * @pf: the PF data structure
  *
  * This function reads I40E_PRTTSYN_STAT_1 and updates the corresponding timers
@@ -259,7 +730,6 @@
 /**
  * i40e_ptp_rx_hang - Detect error case when Rx timestamp registers are hung
  * @pf: The PF private data structure
- * @vsi: The VSI with the rings relevant to 1588
  *
  * This watchdog task is scheduled to detect error case where hardware has
  * dropped an Rx packet that was timestamped when the ring is full. The
@@ -309,7 +779,7 @@
 	 */
 	if (cleared > 2)
 		dev_dbg(&pf->pdev->dev,
-			"Dropped %d missed RXTIME timestamp events\n",
+			"Dropped %d missed RXTIME timestamp events.\n",
 			cleared);
 
 	/* Finally, update the rx_hwtstamp_cleared counter */
@@ -444,6 +914,66 @@
 }
 
 /**
+ * i40e_ptp_get_link_speed_hw - get the link speed
+ * @pf: Board private structure
+ *
+ * Calculate link speed depending on the link status.
+ * Return the link speed.
+ **/
+static enum i40e_aq_link_speed i40e_ptp_get_link_speed_hw(struct i40e_pf *pf)
+{
+	bool link_up = pf->hw.phy.link_info.link_info & I40E_AQ_LINK_UP;
+	enum i40e_aq_link_speed link_speed = I40E_LINK_SPEED_UNKNOWN;
+	struct i40e_hw *hw = &pf->hw;
+
+	if (link_up) {
+		struct i40e_link_status *hw_link_info = &hw->phy.link_info;
+
+		i40e_aq_get_link_info(hw, true, NULL, NULL);
+		link_speed = hw_link_info->link_speed;
+	} else {
+		enum i40e_prt_mac_link_speed prtmac_linksta;
+		u64 prtmac_pcs_linksta;
+
+		prtmac_linksta = (rd32(hw, I40E_PRTMAC_LINKSTA(0))
+			& I40E_PRTMAC_LINKSTA_MAC_LINK_SPEED_MASK)
+			>> I40E_PRTMAC_LINKSTA_MAC_LINK_SPEED_SHIFT;
+
+		if (prtmac_linksta == I40E_PRT_MAC_LINK_SPEED_40GB) {
+			link_speed = I40E_LINK_SPEED_40GB;
+		} else {
+			i40e_aq_debug_read_register(hw,
+						    I40E_PRTMAC_PCS_LINK_STATUS1(0),
+						    &prtmac_pcs_linksta,
+						    NULL);
+
+			prtmac_pcs_linksta = (prtmac_pcs_linksta
+			& I40E_PRTMAC_PCS_LINK_STATUS1_LINK_SPEED_MASK)
+			>> I40E_PRTMAC_PCS_LINK_STATUS1_LINK_SPEED_SHIFT;
+
+			switch (prtmac_pcs_linksta) {
+			case I40E_PRT_MAC_PCS_LINK_SPEED_100MB:
+				link_speed = I40E_LINK_SPEED_100MB;
+				break;
+			case I40E_PRT_MAC_PCS_LINK_SPEED_1GB:
+				link_speed = I40E_LINK_SPEED_1GB;
+				break;
+			case I40E_PRT_MAC_PCS_LINK_SPEED_10GB:
+				link_speed = I40E_LINK_SPEED_10GB;
+				break;
+			case I40E_PRT_MAC_PCS_LINK_SPEED_20GB:
+				link_speed = I40E_LINK_SPEED_20GB;
+				break;
+			default:
+				link_speed = I40E_LINK_SPEED_UNKNOWN;
+			}
+		}
+	}
+
+	return link_speed;
+}
+
+/**
  * i40e_ptp_set_increment - Utility function to update clock increment rate
  * @pf: Board private structure
  *
@@ -453,19 +983,20 @@
  **/
 void i40e_ptp_set_increment(struct i40e_pf *pf)
 {
-	struct i40e_link_status *hw_link_info;
+	enum i40e_aq_link_speed link_speed;
 	struct i40e_hw *hw = &pf->hw;
 	u64 incval;
 	u32 mult;
 
-	hw_link_info = &hw->phy.link_info;
-
-	i40e_aq_get_link_info(&pf->hw, true, NULL, NULL);
+	link_speed = i40e_ptp_get_link_speed_hw(pf);
 
-	switch (hw_link_info->link_speed) {
+	switch (link_speed) {
 	case I40E_LINK_SPEED_10GB:
 		mult = I40E_PTP_10GB_INCVAL_MULT;
 		break;
+	case I40E_LINK_SPEED_5GB:
+		mult = I40E_PTP_5GB_INCVAL_MULT;
+		break;
 	case I40E_LINK_SPEED_1GB:
 		mult = I40E_PTP_1GB_INCVAL_MULT;
 		break;
@@ -496,12 +1027,13 @@
 	 * hardware will not update the clock until both registers have been
 	 * written.
 	 */
-	wr32(hw, I40E_PRTTSYN_INC_L, incval & 0xFFFFFFFF);
-	wr32(hw, I40E_PRTTSYN_INC_H, incval >> 32);
+	wr32(hw, I40E_PRTTSYN_INC_L, (u32)incval);
+	wr32(hw, I40E_PRTTSYN_INC_H, (u32)(incval >> 32));
 
 	/* Update the base adjustement value. */
 	WRITE_ONCE(pf->ptp_adj_mult, mult);
 	smp_mb(); /* Force the above update. */
+	i40e_ptp_set_1pps_signal_hw(pf);
 }
 
 /**
@@ -525,6 +1057,272 @@
 }
 
 /**
+ * i40e_ptp_free_pins - free memory used by PTP pins
+ * @pf: Board private structure
+ *
+ * Release memory allocated for PTP pins.
+ **/
+static void i40e_ptp_free_pins(struct i40e_pf *pf)
+{
+	if (i40e_is_ptp_pin_dev(&pf->hw)) {
+		kfree(pf->ptp_pins);
+		kfree(pf->ptp_caps.pin_config);
+		pf->ptp_pins = NULL;
+		kobject_put(pf->ptp_kobj);
+	}
+}
+
+/**
+ * i40e_ptp_set_pin_hw - Set HW GPIO pin
+ * @hw: pointer to the hardware structure
+ * @pin: pin index
+ * @state: pin state
+ *
+ * Set status of GPIO pin for external clock handling.
+ **/
+static void i40e_ptp_set_pin_hw(struct i40e_hw *hw,
+				unsigned int pin,
+				enum i40e_ptp_gpio_pin_state state)
+{
+	switch (state) {
+	case off:
+		wr32(hw, I40E_GLGEN_GPIO_CTL(pin), 0);
+		break;
+	case in_A:
+		wr32(hw, I40E_GLGEN_GPIO_CTL(pin),
+		     I40E_GLGEN_GPIO_CTL_PORT_0_IN_TIMESYNC_0);
+		break;
+	case in_B:
+		wr32(hw, I40E_GLGEN_GPIO_CTL(pin),
+		     I40E_GLGEN_GPIO_CTL_PORT_1_IN_TIMESYNC_0);
+		break;
+	case out_A:
+		wr32(hw, I40E_GLGEN_GPIO_CTL(pin),
+		     I40E_GLGEN_GPIO_CTL_PORT_0_OUT_TIMESYNC_1);
+		break;
+	case out_B:
+		wr32(hw, I40E_GLGEN_GPIO_CTL(pin),
+		     I40E_GLGEN_GPIO_CTL_PORT_1_OUT_TIMESYNC_1);
+		break;
+	default:
+		break;
+	}
+}
+
+/**
+ * i40e_ptp_set_led_hw - Set HW GPIO led
+ * @hw: pointer to the hardware structure
+ * @led: led index
+ * @state: led state
+ *
+ * Set status of GPIO led for external clock handling.
+ **/
+static void i40e_ptp_set_led_hw(struct i40e_hw *hw,
+				unsigned int led,
+				enum i40e_ptp_led_pin_state state)
+{
+	switch (state) {
+	case low:
+		wr32(hw, I40E_GLGEN_GPIO_SET,
+		     I40E_GLGEN_GPIO_SET_DRV_SDP_DATA | led);
+		break;
+	case high:
+		wr32(hw, I40E_GLGEN_GPIO_SET,
+		     I40E_GLGEN_GPIO_SET_DRV_SDP_DATA |
+		     I40E_GLGEN_GPIO_SET_SDP_DATA_HI | led);
+		break;
+	default:
+		break;
+	}
+}
+
+/**
+ * i40e_ptp_init_leds_hw - init LEDs
+ * @hw: pointer to a hardware structure
+ *
+ * Set initial state of LEDs
+ **/
+static void i40e_ptp_init_leds_hw(struct i40e_hw *hw)
+{
+	wr32(hw, I40E_GLGEN_GPIO_CTL(I40E_LED2_0),
+	     I40E_GLGEN_GPIO_CTL_LED_INIT);
+	wr32(hw, I40E_GLGEN_GPIO_CTL(I40E_LED2_1),
+	     I40E_GLGEN_GPIO_CTL_LED_INIT);
+	wr32(hw, I40E_GLGEN_GPIO_CTL(I40E_LED3_0),
+	     I40E_GLGEN_GPIO_CTL_LED_INIT);
+	wr32(hw, I40E_GLGEN_GPIO_CTL(I40E_LED3_1),
+	     I40E_GLGEN_GPIO_CTL_LED_INIT);
+}
+
+/**
+ * i40e_ptp_set_pins_hw - Set HW GPIO pins
+ * @pf: Board private structure
+ *
+ * This function sets GPIO pins for PTP
+ **/
+static void i40e_ptp_set_pins_hw(struct i40e_pf *pf)
+{
+	const struct i40e_ptp_pins_settings *pins = pf->ptp_pins;
+	struct i40e_hw *hw = &pf->hw;
+
+	/* pin must be disabled before it may be used */
+	i40e_ptp_set_pin_hw(hw, I40E_SDP3_2, off);
+	i40e_ptp_set_pin_hw(hw, I40E_SDP3_3, off);
+	i40e_ptp_set_pin_hw(hw, I40E_GPIO_4, off);
+
+	i40e_ptp_set_pin_hw(hw, I40E_SDP3_2, pins->sdp3_2);
+	i40e_ptp_set_pin_hw(hw, I40E_SDP3_3, pins->sdp3_3);
+	i40e_ptp_set_pin_hw(hw, I40E_GPIO_4, pins->gpio_4);
+
+	i40e_ptp_set_led_hw(hw, I40E_LED2_0, pins->led2_0);
+	i40e_ptp_set_led_hw(hw, I40E_LED2_1, pins->led2_1);
+	i40e_ptp_set_led_hw(hw, I40E_LED3_0, pins->led3_0);
+	i40e_ptp_set_led_hw(hw, I40E_LED3_1, pins->led3_1);
+
+	dev_info(&pf->pdev->dev,
+		 "PTP configuration set to: SDP3_2: %s,  SDP3_3: %s,  GPIO_4: %s.\n",
+		 i40e_ptp_gpio_pin_state2str[pins->sdp3_2],
+		 i40e_ptp_gpio_pin_state2str[pins->sdp3_3],
+		 i40e_ptp_gpio_pin_state2str[pins->gpio_4]);
+}
+
+/**
+ * i40e_ptp_set_pins - set PTP pins in HW
+ * @pf: Board private structure
+ * @pins: PTP pins to be applied
+ *
+ * Validate and set PTP pins in HW for specific PF.
+ * Return 0 on success or negative value on error.
+ **/
+static int i40e_ptp_set_pins(struct i40e_pf *pf,
+			     struct i40e_ptp_pins_settings *pins)
+{
+	enum i40e_can_set_pins_t pin_caps = i40e_can_set_pins(pf);
+	int i = 0;
+
+	if (pin_caps == CANT_DO_PINS)
+		return -EOPNOTSUPP;
+	else if (pin_caps == CAN_DO_PINS)
+		return 0;
+
+	if (pins->sdp3_2 == invalid)
+		pins->sdp3_2 = pf->ptp_pins->sdp3_2;
+	if (pins->sdp3_3 == invalid)
+		pins->sdp3_3 = pf->ptp_pins->sdp3_3;
+	if (pins->gpio_4 == invalid)
+		pins->gpio_4 = pf->ptp_pins->gpio_4;
+	while (i40e_ptp_pin_led_allowed_states[i].sdp3_2 != end) {
+		if (pins->sdp3_2 == i40e_ptp_pin_led_allowed_states[i].sdp3_2 &&
+		    pins->sdp3_3 == i40e_ptp_pin_led_allowed_states[i].sdp3_3 &&
+		    pins->gpio_4 == i40e_ptp_pin_led_allowed_states[i].gpio_4) {
+			pins->led2_0 =
+				i40e_ptp_pin_led_allowed_states[i].led2_0;
+			pins->led2_1 =
+				i40e_ptp_pin_led_allowed_states[i].led2_1;
+			pins->led3_0 =
+				i40e_ptp_pin_led_allowed_states[i].led3_0;
+			pins->led3_1 =
+				i40e_ptp_pin_led_allowed_states[i].led3_1;
+			break;
+		}
+		i++;
+	}
+	if (i40e_ptp_pin_led_allowed_states[i].sdp3_2 == end) {
+		dev_warn(&pf->pdev->dev,
+			 "Unsupported PTP pin configuration: SDP3_2: %s,  SDP3_3: %s,  GPIO_4: %s.\n",
+			 i40e_ptp_gpio_pin_state2str[pins->sdp3_2],
+			 i40e_ptp_gpio_pin_state2str[pins->sdp3_3],
+			 i40e_ptp_gpio_pin_state2str[pins->gpio_4]);
+
+		return -EPERM;
+	}
+	memcpy(pf->ptp_pins, pins, sizeof(*pins));
+	i40e_ptp_set_pins_hw(pf);
+	i40_ptp_reset_timing_events(pf);
+
+	return 0;
+}
+
+/**
+ * i40e_ptp_set_pins_ioctl - ioctl interface to set the HW timestamping
+ *                           gpio pins
+ * @pf: board private structure
+ * @ifr: ioctl data
+ *
+ * Set the current hardware timestamping pins for current PF.
+ **/
+int i40e_ptp_set_pins_ioctl(struct i40e_pf *pf, struct ifreq *ifr)
+{
+	struct i40e_ptp_pins_settings pins;
+	int err;
+
+	if (i40e_can_set_pins(pf) != CAN_SET_PINS)
+		return -EOPNOTSUPP;
+
+	err = copy_from_user(&pins, ifr->ifr_data, sizeof(pins));
+	if (err) {
+		dev_warn(&pf->pdev->dev, "Cannot read user data during SIOCSPINS ioctl.\n");
+		return -EIO;
+	}
+
+	return i40e_ptp_set_pins(pf, &pins);
+}
+
+/**
+ * i40e_ptp_alloc_pins - allocate PTP pins structure
+ * @pf: Board private structure
+ *
+ * allocate PTP pins structure
+ **/
+int i40e_ptp_alloc_pins(struct i40e_pf *pf)
+{
+	if (!i40e_is_ptp_pin_dev(&pf->hw))
+		return 0;
+
+	pf->ptp_pins =
+		kzalloc(sizeof(struct i40e_ptp_pins_settings), GFP_KERNEL);
+
+	if (!pf->ptp_pins) {
+		dev_warn(&pf->pdev->dev, "Cannot allocate memory for PTP pins structure.\n");
+		return -I40E_ERR_NO_MEMORY;
+	}
+
+	pf->ptp_pins->sdp3_2 = off;
+	pf->ptp_pins->sdp3_3 = off;
+	pf->ptp_pins->gpio_4 = off;
+	pf->ptp_pins->led2_0 = high;
+	pf->ptp_pins->led2_1 = high;
+	pf->ptp_pins->led3_0 = high;
+	pf->ptp_pins->led3_1 = high;
+
+	/* Use PF0 to set pins in HW. Return success for user space tools */
+	if (pf->hw.pf_id)
+		return 0;
+
+	i40e_ptp_init_leds_hw(&pf->hw);
+	i40e_ptp_set_pins_hw(pf);
+
+	return 0;
+}
+
+/**
+ * i40e_ptp_get_pins - ioctl interface to read the HW timestamping gpio pins
+ * @pf: Board private structure
+ * @ifr: ioctl data
+ *
+ * Obtain the current hardware timestamping gpio pins settigs as requested.
+ **/
+int i40e_ptp_get_pins(struct i40e_pf *pf, struct ifreq *ifr)
+{
+	if (i40e_can_set_pins(pf) != CAN_SET_PINS)
+		return -EOPNOTSUPP;
+
+	return copy_to_user(ifr->ifr_data, pf->ptp_pins,
+			    sizeof(*pf->ptp_pins))
+	       ? -EFAULT : 0;
+}
+
+/**
  * i40e_ptp_set_timestamp_mode - setup hardware for requested timestamp mode
  * @pf: Board private structure
  * @config: hwtstamp settings requested or saved
@@ -542,6 +1340,21 @@
 	struct i40e_hw *hw = &pf->hw;
 	u32 tsyntype, regval;
 
+	/* Selects external trigger to cause event */
+	regval = rd32(hw, I40E_PRTTSYN_AUX_0(0));
+	/* Bit 17:16 is EVNTLVL, 01B rising edge */
+	regval &= 0;
+	regval |= (1 << I40E_PRTTSYN_AUX_0_EVNTLVL_SHIFT);
+	/* regval: 0001 0000 0000 0000 0000 */
+	wr32(hw, I40E_PRTTSYN_AUX_0(0), regval);
+
+	/* Enabel interrupts */
+	regval = rd32(hw, I40E_PRTTSYN_CTL0);
+	regval |= 1 << I40E_PRTTSYN_CTL0_EVENT_INT_ENA_SHIFT;
+	wr32(hw, I40E_PRTTSYN_CTL0, regval);
+
+	INIT_WORK(&pf->ptp_extts0_work, i40e_ptp_extts0_work);
+
 	/* Reserved for future extensions. */
 	if (config->flags)
 		return -EINVAL;
@@ -600,7 +1413,9 @@
 			config->rx_filter = HWTSTAMP_FILTER_PTP_V2_L2_EVENT;
 		}
 		break;
+#ifdef HAVE_HWTSTAMP_FILTER_NTP_ALL
 	case HWTSTAMP_FILTER_NTP_ALL:
+#endif /* HAVE_HWTSTAMP_FILTER_NTP_ALL */
 	case HWTSTAMP_FILTER_ALL:
 	default:
 		return -ERANGE;
@@ -685,6 +1500,45 @@
 }
 
 /**
+ * i40e_init_pin_config - initialize pins.
+ * @pf: private board structure
+ *
+ * Initialize pins for external clock source.
+ * Return 0 on success or error code on failure.
+ **/
+static int i40e_init_pin_config(struct i40e_pf *pf)
+{
+	int i;
+
+	pf->ptp_caps.n_pins = 3;
+	pf->ptp_caps.n_ext_ts = 2;
+	pf->ptp_caps.pps = 1;
+	pf->ptp_caps.n_per_out = 2;
+
+	pf->ptp_caps.pin_config = kcalloc(pf->ptp_caps.n_pins,
+					  sizeof(*pf->ptp_caps.pin_config),
+					  GFP_KERNEL);
+	if (!pf->ptp_caps.pin_config)
+		return -ENOMEM;
+
+	for (i = 0; i < pf->ptp_caps.n_pins; i++) {
+		snprintf(pf->ptp_caps.pin_config[i].name,
+			 sizeof(pf->ptp_caps.pin_config[i].name),
+			 "%s", sdp_desc[i].name);
+		pf->ptp_caps.pin_config[i].index = sdp_desc[i].index;
+		pf->ptp_caps.pin_config[i].func = PTP_PF_NONE;
+		pf->ptp_caps.pin_config[i].chan = sdp_desc[i].chan;
+	}
+
+	pf->ptp_caps.verify = i40e_ptp_verify;
+	pf->ptp_caps.enable = i40e_ptp_feature_enable;
+
+	pf->ptp_caps.pps = 1;
+
+	return 0;
+}
+
+/**
  * i40e_ptp_create_clock - Create PTP clock device for userspace
  * @pf: Board private structure
  *
@@ -704,13 +1558,21 @@
 		sizeof(pf->ptp_caps.name) - 1);
 	pf->ptp_caps.owner = THIS_MODULE;
 	pf->ptp_caps.max_adj = 999999999;
-	pf->ptp_caps.n_ext_ts = 0;
-	pf->ptp_caps.pps = 0;
 	pf->ptp_caps.adjfreq = i40e_ptp_adjfreq;
 	pf->ptp_caps.adjtime = i40e_ptp_adjtime;
-	pf->ptp_caps.gettimex64 = i40e_ptp_gettimex;
+#ifdef HAVE_PTP_CLOCK_INFO_GETTIME64
+	pf->ptp_caps.gettime64 = i40e_ptp_gettime;
 	pf->ptp_caps.settime64 = i40e_ptp_settime;
-	pf->ptp_caps.enable = i40e_ptp_feature_enable;
+#else
+	pf->ptp_caps.gettime = i40e_ptp_gettime32;
+	pf->ptp_caps.settime = i40e_ptp_settime32;
+#endif
+	if (i40e_is_ptp_pin_dev(&pf->hw)) {
+		int err = i40e_init_pin_config(pf);
+
+		if (err)
+			return err;
+	}
 
 	/* Attempt to register the clock before enabling the hardware. */
 	pf->ptp_clock = ptp_clock_register(&pf->ptp_caps, &pf->pdev->dev);
@@ -746,7 +1608,7 @@
 	if (!(pf->flags & I40E_FLAG_PTP))
 		return;
 
-	i40e_ptp_gettimex(&pf->ptp_caps, &pf->ptp_prev_hw_time, NULL);
+	i40e_ptp_gettime(&pf->ptp_caps, &pf->ptp_prev_hw_time);
 	/* Get a monotonic starting time for this reset */
 	pf->ptp_reset_start = ktime_get();
 }
@@ -774,6 +1636,150 @@
 	i40e_ptp_settime(&pf->ptp_caps, &pf->ptp_prev_hw_time);
 }
 
+#ifndef HAVE_PTP_1588_CLOCK_PINS
+/**
+ * __get_pf_pdev - helper function to get the pdev
+ * @kobj:	kobject passed
+ * @pdev:	PCI device information struct
+ */
+static int __get_pf_pdev(struct kobject *kobj, struct pci_dev **pdev)
+{
+	struct device *dev;
+
+	if (!kobj->parent)
+		return -EINVAL;
+
+	/* get pdev */
+	dev = to_dev(kobj->parent);
+	*pdev = to_pci_dev(dev);
+
+	return 0;
+}
+
+/**
+ * i40e_ptp_pins_to_num - convert PTP pins to integer number
+ * @pf:	PCI physical function
+ *
+ * Return PTP pins states from pf as integer number.
+ **/
+static unsigned int i40e_ptp_pins_to_num(struct i40e_pf *pf)
+{
+	return pf->ptp_pins->gpio_4 +
+	       pf->ptp_pins->sdp3_3 * 10 +
+	       pf->ptp_pins->sdp3_2 * 100;
+}
+
+/**
+ * i40e_ptp_set_pins_str - wrapper to set PTP pins in HW from string
+ * @pf: Board private structure
+ * @buf: string with PTP pins to be applied
+ * @count: length of a buf argument
+ *
+ * Set the current hardware timestamping pins for current PF.
+ * Return 0 on success and negative value on error.
+ **/
+static int i40e_ptp_set_pins_str(struct i40e_pf *pf, const char *buf,
+				 int count)
+{
+	struct i40e_ptp_pins_settings pins;
+	const int PIN_STR_LEN = 4;
+	unsigned long res;
+
+	if (count != PIN_STR_LEN || kstrtoul(buf, 10, &res))
+		return -EINVAL;
+
+	pins.sdp3_2 = res / 100 % 10;
+	pins.sdp3_3 = res / 10 % 10;
+	pins.gpio_4 = res % 10;
+
+	if (pins.sdp3_2 > out_B ||
+	    pins.sdp3_3 > out_B ||
+	    pins.gpio_4 > out_B)
+		return -EINVAL;
+
+	return i40e_ptp_set_pins(pf, &pins) ? -EINVAL : count;
+}
+
+/**
+ * i40e_sysfs_ptp_pins_read - sysfs interface for reading PTP pins status
+ * @kobj:  sysfs node
+ * @attr:  sysfs node attributes
+ * @buf:   string representing PTP pins
+ *
+ * Return number of bytes read on success or negative value on failure.
+ **/
+static ssize_t i40e_sysfs_ptp_pins_read(struct kobject *kobj,
+					struct kobj_attribute *attr,
+					char *buf)
+{
+	struct pci_dev *pdev;
+	struct i40e_pf *pf;
+	unsigned int pins;
+
+	if (__get_pf_pdev(kobj, &pdev))
+		return -EPERM;
+
+	pf = pci_get_drvdata(pdev);
+	pins = i40e_ptp_pins_to_num(pf);
+
+	dev_info(&pf->pdev->dev,
+		 "PTP pins: SDP3_2: %s, SDP3_3: %s, GPIO_4: %s.\n",
+		 i40e_ptp_gpio_pin_state2str[pf->ptp_pins->sdp3_2],
+		 i40e_ptp_gpio_pin_state2str[pf->ptp_pins->sdp3_3],
+		 i40e_ptp_gpio_pin_state2str[pf->ptp_pins->gpio_4]);
+
+	return sprintf(buf, "%.3d\n", pins);
+}
+
+/**
+ * i40e_sysfs_ptp_pins_write - sysfs interface for setting PTP pins in HW
+ * @kobj:  sysfs node
+ * @attr:  sysfs node attributes
+ * @buf:   string representing PTP pins
+ * @count: length of a 'buf' string
+ *
+ * Return number of bytes written on success or negative value on failure.
+ **/
+static ssize_t i40e_sysfs_ptp_pins_write(struct kobject *kobj,
+					 struct kobj_attribute *attr,
+					 const char *buf, size_t count)
+{
+	struct pci_dev *pdev;
+	struct i40e_pf *pf;
+
+	if (__get_pf_pdev(kobj, &pdev))
+		return -EPERM;
+
+	pf = pci_get_drvdata(pdev);
+
+	return i40e_ptp_set_pins_str(pf, buf, count);
+}
+
+/**
+ * i40e_ptp_pins_sysfs_init - initialize sysfs for PTP pins
+ * @pf: board private structure
+ *
+ * Initialize sysfs for handling PTP timestamping pins in HW.
+ **/
+static void i40e_ptp_pins_sysfs_init(struct i40e_pf *pf)
+{
+	if (!i40e_is_ptp_pin_dev(&pf->hw))
+		return;
+
+	pf->ptp_kobj = kobject_create_and_add("ptp_pins", &pf->pdev->dev.kobj);
+	if (!pf->ptp_kobj) {
+		dev_info(&pf->pdev->dev, "Failed to create ptp_pins kobject.\n");
+		return;
+	}
+
+	if (sysfs_create_file(pf->ptp_kobj, &ptp_pins_attribute.attr)) {
+		dev_info(&pf->pdev->dev, "Failed to create PTP pins kobject.\n");
+		kobject_put(pf->ptp_kobj);
+		return;
+	}
+}
+#endif /* HAVE_PTP_1588_CLOCK_PINS */
+
 /**
  * i40e_ptp_init - Initialize the 1588 support after device probe or reset
  * @pf: Board private structure
@@ -789,7 +1795,6 @@
  **/
 void i40e_ptp_init(struct i40e_pf *pf)
 {
-	struct net_device *netdev = pf->vsi[pf->lan_vsi]->netdev;
 	struct i40e_hw *hw = &pf->hw;
 	u32 pf_id;
 	long err;
@@ -801,9 +1806,7 @@
 		I40E_PRTTSYN_CTL0_PF_ID_SHIFT;
 	if (hw->pf_id != pf_id) {
 		pf->flags &= ~I40E_FLAG_PTP;
-		dev_info(&pf->pdev->dev, "%s: PTP not supported on %s\n",
-			 __func__,
-			 netdev->name);
+		dev_info(&pf->pdev->dev, "PTP not supported on this device.\n");
 		return;
 	}
 
@@ -814,13 +1817,13 @@
 	err = i40e_ptp_create_clock(pf);
 	if (err) {
 		pf->ptp_clock = NULL;
-		dev_err(&pf->pdev->dev, "%s: ptp_clock_register failed\n",
-			__func__);
-	} else if (pf->ptp_clock) {
+		dev_err(&pf->pdev->dev,
+			"PTP clock register failed: %ld.\n", err);
+	} else {
 		u32 regval;
 
 		if (pf->hw.debug_mask & I40E_DEBUG_LAN)
-			dev_info(&pf->pdev->dev, "PHC enabled\n");
+			dev_info(&pf->pdev->dev, "PHC enabled.\n");
 		pf->flags |= I40E_FLAG_PTP;
 
 		/* Ensure the clocks are running. */
@@ -840,6 +1843,12 @@
 		/* Restore the clock time based on last known value */
 		i40e_ptp_restore_hw_time(pf);
 	}
+
+#ifndef HAVE_PTP_1588_CLOCK_PINS
+	i40e_ptp_pins_sysfs_init(pf);
+#endif /* HAVE_PTP_1588_CLOCK_PINS */
+
+	i40e_ptp_set_1pps_signal_hw(pf);
 }
 
 /**
@@ -851,6 +1860,9 @@
  **/
 void i40e_ptp_stop(struct i40e_pf *pf)
 {
+	struct i40e_hw *hw = &pf->hw;
+	u32 regval;
+
 	pf->flags &= ~I40E_FLAG_PTP;
 	pf->ptp_tx = false;
 	pf->ptp_rx = false;
@@ -866,7 +1878,25 @@
 	if (pf->ptp_clock) {
 		ptp_clock_unregister(pf->ptp_clock);
 		pf->ptp_clock = NULL;
-		dev_info(&pf->pdev->dev, "%s: removed PHC on %s\n", __func__,
+		dev_info(&pf->pdev->dev, "removed PHC from %s.\n",
 			 pf->vsi[pf->lan_vsi]->netdev->name);
 	}
+
+	if (i40e_is_ptp_pin_dev(&pf->hw)) {
+		i40e_ptp_set_pin_hw(hw, I40E_SDP3_2, off);
+		i40e_ptp_set_pin_hw(hw, I40E_SDP3_3, off);
+		i40e_ptp_set_pin_hw(hw, I40E_GPIO_4, off);
+	}
+
+	regval = rd32(hw, I40E_PRTTSYN_AUX_0(0));
+	regval &= ~I40E_PRTTSYN_AUX_0_PTPFLAG_MASK;
+	wr32(hw, I40E_PRTTSYN_AUX_0(0), regval);
+
+	/* Disable interrupts */
+	regval = rd32(hw, I40E_PRTTSYN_CTL0);
+	regval &= ~I40E_PRTTSYN_CTL0_EVENT_INT_ENA_MASK;
+	wr32(hw, I40E_PRTTSYN_CTL0, regval);
+
+	i40e_ptp_free_pins(pf);
 }
+#endif /* HAVE_PTP_1588_CLOCK */
Binary files linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_ptp.o and linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_ptp.o differ
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_register.h linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_register.h
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_register.h	2024-05-10 01:26:45.389079797 -0400
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_register.h	2024-05-13 03:58:25.372491940 -0400
@@ -1,28 +1,28 @@
 /* SPDX-License-Identifier: GPL-2.0 */
-/* Copyright(c) 2013 - 2018 Intel Corporation. */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
 
 #ifndef _I40E_REGISTER_H_
 #define _I40E_REGISTER_H_
 
-#define I40E_GL_ARQBAH 0x000801C0 /* Reset: EMPR */
+#define I40E_GL_ARQBAH              0x000801C0 /* Reset: EMPR */
 #define I40E_GL_ARQBAH_ARQBAH_SHIFT 0
-#define I40E_GL_ARQBAH_ARQBAH_MASK I40E_MASK(0xFFFFFFFF, I40E_GL_ARQBAH_ARQBAH_SHIFT)
-#define I40E_GL_ARQBAL 0x000800C0 /* Reset: EMPR */
+#define I40E_GL_ARQBAH_ARQBAH_MASK  I40E_MASK(0xFFFFFFFF, I40E_GL_ARQBAH_ARQBAH_SHIFT)
+#define I40E_GL_ARQBAL              0x000800C0 /* Reset: EMPR */
 #define I40E_GL_ARQBAL_ARQBAL_SHIFT 0
-#define I40E_GL_ARQBAL_ARQBAL_MASK I40E_MASK(0xFFFFFFFF, I40E_GL_ARQBAL_ARQBAL_SHIFT)
-#define I40E_GL_ARQH 0x000803C0 /* Reset: EMPR */
+#define I40E_GL_ARQBAL_ARQBAL_MASK  I40E_MASK(0xFFFFFFFF, I40E_GL_ARQBAL_ARQBAL_SHIFT)
+#define I40E_GL_ARQH            0x000803C0 /* Reset: EMPR */
 #define I40E_GL_ARQH_ARQH_SHIFT 0
-#define I40E_GL_ARQH_ARQH_MASK I40E_MASK(0x3FF, I40E_GL_ARQH_ARQH_SHIFT)
-#define I40E_GL_ARQT 0x000804C0 /* Reset: EMPR */
+#define I40E_GL_ARQH_ARQH_MASK  I40E_MASK(0x3FF, I40E_GL_ARQH_ARQH_SHIFT)
+#define I40E_GL_ARQT            0x000804C0 /* Reset: EMPR */
 #define I40E_GL_ARQT_ARQT_SHIFT 0
-#define I40E_GL_ARQT_ARQT_MASK I40E_MASK(0x3FF, I40E_GL_ARQT_ARQT_SHIFT)
-#define I40E_GL_ATQBAH 0x00080140 /* Reset: EMPR */
+#define I40E_GL_ARQT_ARQT_MASK  I40E_MASK(0x3FF, I40E_GL_ARQT_ARQT_SHIFT)
+#define I40E_GL_ATQBAH              0x00080140 /* Reset: EMPR */
 #define I40E_GL_ATQBAH_ATQBAH_SHIFT 0
-#define I40E_GL_ATQBAH_ATQBAH_MASK I40E_MASK(0xFFFFFFFF, I40E_GL_ATQBAH_ATQBAH_SHIFT)
-#define I40E_GL_ATQBAL 0x00080040 /* Reset: EMPR */
+#define I40E_GL_ATQBAH_ATQBAH_MASK  I40E_MASK(0xFFFFFFFF, I40E_GL_ATQBAH_ATQBAH_SHIFT)
+#define I40E_GL_ATQBAL              0x00080040 /* Reset: EMPR */
 #define I40E_GL_ATQBAL_ATQBAL_SHIFT 0
-#define I40E_GL_ATQBAL_ATQBAL_MASK I40E_MASK(0xFFFFFFFF, I40E_GL_ATQBAL_ATQBAL_SHIFT)
-#define I40E_GL_ATQH 0x00080340 /* Reset: EMPR */
+#define I40E_GL_ATQBAL_ATQBAL_MASK  I40E_MASK(0xFFFFFFFF, I40E_GL_ATQBAL_ATQBAL_SHIFT)
+#define I40E_GL_ATQH            0x00080340 /* Reset: EMPR */
 #define I40E_GL_ATQH_ATQH_SHIFT 0
 #define I40E_GL_ATQH_ATQH_MASK I40E_MASK(0x3FF, I40E_GL_ATQH_ATQH_SHIFT)
 #define I40E_GL_ATQLEN 0x00080240 /* Reset: EMPR */
@@ -200,6 +200,9 @@
 #define I40E_VFCM_PE_ERRINFO1_RLU_ERROR_CNT_MASK I40E_MASK(0xFF, I40E_VFCM_PE_ERRINFO1_RLU_ERROR_CNT_SHIFT)
 #define I40E_VFCM_PE_ERRINFO1_RLS_ERROR_CNT_SHIFT 24
 #define I40E_VFCM_PE_ERRINFO1_RLS_ERROR_CNT_MASK I40E_MASK(0xFF, I40E_VFCM_PE_ERRINFO1_RLS_ERROR_CNT_SHIFT)
+#define I40E_PRT_SWR_PM_THR 0x0026CD00 /* Reset: CORER */
+#define I40E_PRT_SWR_PM_THR_THRESHOLD_SHIFT 0
+#define I40E_PRT_SWR_PM_THR_THRESHOLD_MASK I40E_MASK(0xFF, I40E_PRT_SWR_PM_THR_THRESHOLD_SHIFT)
 #define I40E_GLDCB_GENC 0x00083044 /* Reset: CORER */
 #define I40E_GLDCB_GENC_PCIRTT_SHIFT 0
 #define I40E_GLDCB_GENC_PCIRTT_MASK I40E_MASK(0xFFFF, I40E_GLDCB_GENC_PCIRTT_SHIFT)
@@ -363,6 +366,8 @@
 #define I40E_GL_FWSTS_FWRI_MASK I40E_MASK(0x1, I40E_GL_FWSTS_FWRI_SHIFT)
 #define I40E_GL_FWSTS_FWS1B_SHIFT 16
 #define I40E_GL_FWSTS_FWS1B_MASK I40E_MASK(0xFF, I40E_GL_FWSTS_FWS1B_SHIFT)
+#define I40E_GL_FWSTS_FWS1B_EMPR_0 I40E_MASK(0x20, I40E_GL_FWSTS_FWS1B_SHIFT)
+#define I40E_GL_FWSTS_FWS1B_EMPR_10 I40E_MASK(0x2A, I40E_GL_FWSTS_FWS1B_SHIFT)
 #define I40E_XL710_GL_FWSTS_FWS1B_REC_MOD_CORER_MASK I40E_MASK(0x30, I40E_GL_FWSTS_FWS1B_SHIFT)
 #define I40E_XL710_GL_FWSTS_FWS1B_REC_MOD_GLOBR_MASK I40E_MASK(0x31, I40E_GL_FWSTS_FWS1B_SHIFT)
 #define I40E_XL710_GL_FWSTS_FWS1B_REC_MOD_TRANSITION_MASK I40E_MASK(0x32, I40E_GL_FWSTS_FWS1B_SHIFT)
@@ -1400,6 +1405,10 @@
 #define I40E_PRTMAC_PCS_XAUI_SWAP_B_SWAP_RX_LANE1_MASK I40E_MASK(0x3, I40E_PRTMAC_PCS_XAUI_SWAP_B_SWAP_RX_LANE1_SHIFT)
 #define I40E_PRTMAC_PCS_XAUI_SWAP_B_SWAP_RX_LANE0_SHIFT 14
 #define I40E_PRTMAC_PCS_XAUI_SWAP_B_SWAP_RX_LANE0_MASK I40E_MASK(0x3, I40E_PRTMAC_PCS_XAUI_SWAP_B_SWAP_RX_LANE0_SHIFT)
+/* _i=0...3 */ /* Reset: GLOBR */
+#define I40E_PRTMAC_PCS_LINK_STATUS1(_i) (0x0008C200 + ((_i) * 4))
+#define I40E_PRTMAC_PCS_LINK_STATUS1_LINK_SPEED_SHIFT 24
+#define I40E_PRTMAC_PCS_LINK_STATUS1_LINK_SPEED_MASK I40E_MASK(0x7, I40E_PRTMAC_PCS_LINK_STATUS1_LINK_SPEED_SHIFT)
 #define I40E_GL_FWRESETCNT 0x00083100 /* Reset: POR */
 #define I40E_GL_FWRESETCNT_FWRESETCNT_SHIFT 0
 #define I40E_GL_FWRESETCNT_FWRESETCNT_MASK I40E_MASK(0xFFFFFFFF, I40E_GL_FWRESETCNT_FWRESETCNT_SHIFT)
@@ -2897,6 +2906,9 @@
 #define I40E_PRTTSYN_AUX_0_PULSEW_MASK I40E_MASK(0xF, I40E_PRTTSYN_AUX_0_PULSEW_SHIFT)
 #define I40E_PRTTSYN_AUX_0_EVNTLVL_SHIFT 16
 #define I40E_PRTTSYN_AUX_0_EVNTLVL_MASK I40E_MASK(0x3, I40E_PRTTSYN_AUX_0_EVNTLVL_SHIFT)
+#define I40E_PRTTSYN_AUX_0_PTPFLAG_SHIFT 17
+#define I40E_PRTTSYN_AUX_0_PTPFLAG_MASK I40E_MASK(0x1, I40E_PRTTSYN_AUX_0_PTPFLAG_SHIFT)
+#define I40E_PRTTSYN_AUX_0_PTP_OUT_SYNC_CLK_IO 0xF
 #define I40E_PRTTSYN_AUX_1(_i) (0x001E42E0 + ((_i) * 32)) /* _i=0...1 */ /* Reset: GLOBR */
 #define I40E_PRTTSYN_AUX_1_MAX_INDEX 1
 #define I40E_PRTTSYN_AUX_1_INSTNT_SHIFT 0
@@ -3854,6 +3866,10 @@
 #define I40E_PRTMAC_LINK_DOWN_COUNTER 0x001E2440 /* Reset: GLOBR */
 #define I40E_PRTMAC_LINK_DOWN_COUNTER_LINK_DOWN_COUNTER_SHIFT 0
 #define I40E_PRTMAC_LINK_DOWN_COUNTER_LINK_DOWN_COUNTER_MASK I40E_MASK(0xFFFF, I40E_PRTMAC_LINK_DOWN_COUNTER_LINK_DOWN_COUNTER_SHIFT)
+/* _i=0...3 */ /* Reset: GLOBR */
+#define I40E_PRTMAC_LINKSTA(_i) (0x001E2420 + ((_i) * 4))
+#define I40E_PRTMAC_LINKSTA_MAC_LINK_SPEED_SHIFT 27
+#define I40E_PRTMAC_LINKSTA_MAC_LINK_SPEED_MASK I40E_MASK(0x7, I40E_PRTMAC_LINKSTA_MAC_LINK_SPEED_SHIFT)
 #define I40E_GLNVM_AL_REQ 0x000B6164 /* Reset: POR */
 #define I40E_GLNVM_AL_REQ_POR_SHIFT 0
 #define I40E_GLNVM_AL_REQ_POR_MASK I40E_MASK(0x1, I40E_GLNVM_AL_REQ_POR_SHIFT)
@@ -5273,6 +5289,87 @@
 #define I40E_GLGEN_STAT_HALT 0x00390000 /* Reset: CORER */
 #define I40E_GLGEN_STAT_HALT_HALT_CELLS_SHIFT 0
 #define I40E_GLGEN_STAT_HALT_HALT_CELLS_MASK I40E_MASK(0x3FFFFFFF, I40E_GLGEN_STAT_HALT_HALT_CELLS_SHIFT)
+/* Flow Director */
+#define I40E_REG_INSET_L2_DMAC_SHIFT 60
+#define I40E_REG_INSET_L2_DMAC_MASK I40E_MASK(0xEULL, I40E_REG_INSET_L2_DMAC_SHIFT)
+#define I40E_REG_INSET_L2_SMAC_SHIFT 56
+#define I40E_REG_INSET_L2_SMAC_MASK I40E_MASK(0x1CULL, I40E_REG_INSET_L2_SMAC_SHIFT)
+#define I40E_REG_INSET_L2_OUTER_VLAN_SHIFT 26
+#define I40E_REG_INSET_L2_OUTER_VLAN_MASK I40E_MASK(0x1ULL, I40E_REG_INSET_L2_OUTER_VLAN_SHIFT)
+#define I40E_REG_INSET_L2_INNER_VLAN_SHIFT 55
+#define I40E_REG_INSET_L2_INNER_VLAN_MASK I40E_MASK(0x1ULL, I40E_REG_INSET_L2_INNER_VLAN_SHIFT)
+#define I40E_REG_INSET_TUNNEL_VLAN_SHIFT 56
+#define I40E_REG_INSET_TUNNEL_VLAN_MASK I40E_MASK(0x1ULL, I40E_REG_INSET_TUNNEL_VLAN_SHIFT)
+#define I40E_REG_INSET_L3_SRC_IP4_SHIFT 47
+#define I40E_REG_INSET_L3_SRC_IP4_MASK I40E_MASK(0x3ULL, I40E_REG_INSET_L3_SRC_IP4_SHIFT)
+#define I40E_REG_INSET_L3_DST_IP4_SHIFT 35
+#define I40E_REG_INSET_L3_DST_IP4_MASK I40E_MASK(0x3ULL, I40E_REG_INSET_L3_DST_IP4_SHIFT)
+#define I40E_X722_REG_INSET_L3_SRC_IP4_SHIFT 49
+#define I40E_X722_REG_INSET_L3_SRC_IP4_MASK I40E_MASK(0x3ULL, I40E_X722_REG_INSET_L3_SRC_IP4_SHIFT)
+#define I40E_X722_REG_INSET_L3_DST_IP4_SHIFT 41
+#define I40E_X722_REG_INSET_L3_DST_IP4_MASK I40E_MASK(0x3ULL, I40E_X722_REG_INSET_L3_DST_IP4_SHIFT)
+#define I40E_X722_REG_INSET_L3_IP4_PROTO_SHIFT 52
+#define I40E_X722_REG_INSET_L3_IP4_PROTO_MASK I40E_MASK(0x1ULL, I40E_X722_REG_INSET_L3_IP4_PROTO_SHIFT)
+#define I40E_X722_REG_INSET_L3_IP4_TTL_SHIFT 52
+#define I40E_X722_REG_INSET_L3_IP4_TTL_MASK I40E_MASK(0x1ULL, I40E_X722_REG_INSET_L3_IP4_TTL_SHIFT)
+#define I40E_REG_INSET_L3_IP4_TOS_SHIFT 54
+#define I40E_REG_INSET_L3_IP4_TOS_MASK I40E_MASK(0x1ULL, I40E_REG_INSET_L3_IP4_TOS_SHIFT)
+#define I40E_REG_INSET_L3_IP4_PROTO_SHIFT 50
+#define I40E_REG_INSET_L3_IP4_PROTO_MASK I40E_MASK(0x1ULL, I40E_REG_INSET_L3_IP4_PROTO_SHIFT)
+#define I40E_REG_INSET_L3_IP4_TTL_SHIFT 50
+#define I40E_REG_INSET_L3_IP4_TTL_MASK I40E_MASK(0x1ULL, I40E_REG_INSET_L3_IP4_TTL_SHIFT)
+#define I40E_REG_INSET_L3_SRC_IP6_SHIFT 43
+#define I40E_REG_INSET_L3_SRC_IP6_MASK I40E_MASK(0xFFULL, I40E_REG_INSET_L3_SRC_IP6_SHIFT)
+#define I40E_REG_INSET_L3_DST_IP6_SHIFT 35
+#define I40E_REG_INSET_L3_DST_IP6_MASK I40E_MASK(0xFFULL, I40E_REG_INSET_L3_DST_IP6_SHIFT)
+#define I40E_REG_INSET_L3_IP6_TC_SHIFT 54
+#define I40E_REG_INSET_L3_IP6_TC_MASK I40E_MASK(0x1ULL, I40E_REG_INSET_L3_IP6_TC_SHIFT)
+#define I40E_REG_INSET_L3_IP6_NEXT_HDR_SHIFT 51
+#define I40E_REG_INSET_L3_IP6_NEXT_HDR_MASK I40E_MASK(0x1ULL, I40E_REG_INSET_L3_IP6_NEXT_HDR_SHIFT)
+#define I40E_REG_INSET_L3_IP6_HOP_LIMIT_SHIFT 51
+#define I40E_REG_INSET_L3_IP6_HOP_LIMIT_MASK I40E_MASK(0x1ULL, I40E_REG_INSET_L3_IP6_HOP_LIMIT_SHIFT)
+#define I40E_REG_INSET_L4_SRC_PORT_SHIFT 34
+#define I40E_REG_INSET_L4_SRC_PORT_MASK I40E_MASK(0x1ULL, I40E_REG_INSET_L4_SRC_PORT_SHIFT)
+#define I40E_REG_INSET_L4_DST_PORT_SHIFT 33
+#define I40E_REG_INSET_L4_DST_PORT_MASK I40E_MASK(0x1ULL, I40E_REG_INSET_L4_DST_PORT_SHIFT)
+#define I40E_REG_INSET_L4_SCTP_VERIFICATION_TAG_SHIFT 31
+#define I40E_REG_INSET_L4_SCTP_VERIFICATION_TAG_MASK I40E_MASK(0x3ULL, I40E_REG_INSET_L4_SCTP_VERIFICATION_TAG_SHIFT)
+#define I40E_REG_INSET_TUNNEL_L2_INNER_DST_MAC_SHIFT 22
+#define I40E_REG_INSET_TUNNEL_L2_INNER_DST_MAC_MASK I40E_MASK(0x7ULL, I40E_REG_INSET_TUNNEL_L2_INNER_DST_MAC_SHIFT)
+#define I40E_REG_INSET_TUNNEL_L2_INNER_SRC_MAC_SHIFT 11
+#define I40E_REG_INSET_TUNNEL_L2_INNER_SRC_MAC_MASK I40E_MASK(0x7ULL, I40E_REG_INSET_TUNNEL_L2_INNER_SRC_MAC_SHIFT)
+#define I40E_REG_INSET_TUNNEL_L4_UDP_DST_PORT_SHIFT 21
+#define I40E_REG_INSET_TUNNEL_L4_UDP_DST_PORT_MASK I40E_MASK(0x1ULL, I40E_REG_INSET_TUNNEL_L4_UDP_DST_PORT_SHIFT)
+#define I40E_REG_INSET_TUNNEL_ID_SHIFT 18
+#define I40E_REG_INSET_TUNNEL_ID_MASK I40E_MASK(0x3ULL, I40E_REG_INSET_TUNNEL_ID_SHIFT)
+#define I40E_REG_INSET_LAST_ETHER_TYPE_SHIFT 14
+#define I40E_REG_INSET_LAST_ETHER_TYPE_MASK I40E_MASK(0x1ULL, I40E_REG_INSET_LAST_ETHER_TYPE_SHIFT)
+#define I40E_REG_INSET_TUNNEL_L3_SRC_IP4_SHIFT 8
+#define I40E_REG_INSET_TUNNEL_L3_SRC_IP4_MASK I40E_MASK(0x3ULL, I40E_REG_INSET_TUNNEL_L3_SRC_IP4_SHIFT)
+#define I40E_REG_INSET_TUNNEL_L3_DST_IP4_SHIFT 6
+#define I40E_REG_INSET_TUNNEL_L3_DST_IP4_MASK I40E_MASK(0x3ULL, I40E_REG_INSET_TUNNEL_L3_DST_IP4_SHIFT)
+#define I40E_REG_INSET_TUNNEL_L3_DST_IP6_SHIFT 6
+#define I40E_REG_INSET_TUNNEL_L3_DST_IP6_MASK I40E_MASK(0xFFULL, I40E_REG_INSET_TUNNEL_L3_DST_IP6_SHIFT)
+#define I40E_REG_INSET_FLEX_PAYLOAD_WORD1_SHIFT 13
+#define I40E_REG_INSET_FLEX_PAYLOAD_WORD1_MASK I40E_MASK(0x1ULL, I40E_REG_INSET_FLEX_PAYLOAD_WORD1_SHIFT)
+#define I40E_REG_INSET_FLEX_PAYLOAD_WORD2_SHIFT 12
+#define I40E_REG_INSET_FLEX_PAYLOAD_WORD2_MASK I40E_MASK(0x1ULL, I40E_REG_INSET_FLEX_PAYLOAD_WORD2_SHIFT)
+#define I40E_REG_INSET_FLEX_PAYLOAD_WORD3_SHIFT 11
+#define I40E_REG_INSET_FLEX_PAYLOAD_WORD3_MASK I40E_MASK(0x1ULL, I40E_REG_INSET_FLEX_PAYLOAD_WORD3_SHIFT)
+#define I40E_REG_INSET_FLEX_PAYLOAD_WORD4_SHIFT 10
+#define I40E_REG_INSET_FLEX_PAYLOAD_WORD4_MASK I40E_MASK(0x1ULL, I40E_REG_INSET_FLEX_PAYLOAD_WORD4_SHIFT)
+#define I40E_REG_INSET_FLEX_PAYLOAD_WORD5_SHIFT 9
+#define I40E_REG_INSET_FLEX_PAYLOAD_WORD5_MASK I40E_MASK(0x1ULL, I40E_REG_INSET_FLEX_PAYLOAD_WORD5_SHIFT)
+#define I40E_REG_INSET_FLEX_PAYLOAD_WORD6_SHIFT 8
+#define I40E_REG_INSET_FLEX_PAYLOAD_WORD6_MASK I40E_MASK(0x1ULL, I40E_REG_INSET_FLEX_PAYLOAD_WORD6_SHIFT)
+#define I40E_REG_INSET_FLEX_PAYLOAD_WORD7_SHIFT 7
+#define I40E_REG_INSET_FLEX_PAYLOAD_WORD7_MASK I40E_MASK(0x1ULL, I40E_REG_INSET_FLEX_PAYLOAD_WORD7_SHIFT)
+#define I40E_REG_INSET_FLEX_PAYLOAD_WORD8_SHIFT 6
+#define I40E_REG_INSET_FLEX_PAYLOAD_WORD8_MASK I40E_MASK(0x1ULL, I40E_REG_INSET_FLEX_PAYLOAD_WORD8_SHIFT)
+#define I40E_REG_INSET_FLEX_PAYLOAD_WORDS_SHIFT 6
+#define I40E_REG_INSET_FLEX_PAYLOAD_WORDS_MASK I40E_MASK(0xFFULL, I40E_REG_INSET_FLEX_PAYLOAD_WORDS_SHIFT)
+#define I40E_REG_INSET_MASK_DEFAULT 0x0000000000000000ULL
+
 #define I40E_VFINT_DYN_CTL01_WB_ON_ITR_SHIFT 30
 #define I40E_VFINT_DYN_CTL01_WB_ON_ITR_MASK I40E_MASK(0x1, I40E_VFINT_DYN_CTL01_WB_ON_ITR_SHIFT)
 #define I40E_VFINT_DYN_CTLN1_WB_ON_ITR_SHIFT 30
@@ -5333,4 +5430,5 @@
 #define I40E_VFPE_WQEALLOC1_PEQPID_MASK I40E_MASK(0x3FFFF, I40E_VFPE_WQEALLOC1_PEQPID_SHIFT)
 #define I40E_VFPE_WQEALLOC1_WQE_DESC_INDEX_SHIFT 20
 #define I40E_VFPE_WQEALLOC1_WQE_DESC_INDEX_MASK I40E_MASK(0xFFF, I40E_VFPE_WQEALLOC1_WQE_DESC_INDEX_SHIFT)
+
 #endif /* _I40E_REGISTER_H_ */
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_status.h linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_status.h
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_status.h	2024-05-10 01:26:45.389079797 -0400
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_status.h	2024-05-13 03:58:25.372491940 -0400
@@ -1,5 +1,5 @@
 /* SPDX-License-Identifier: GPL-2.0 */
-/* Copyright(c) 2013 - 2018 Intel Corporation. */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
 
 #ifndef _I40E_STATUS_H_
 #define _I40E_STATUS_H_
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_trace.h linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_trace.h
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_trace.h	2024-05-10 01:26:45.389079797 -0400
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_trace.h	2024-05-13 03:58:25.372491940 -0400
@@ -1,9 +1,23 @@
 /* SPDX-License-Identifier: GPL-2.0 */
-/* Copyright(c) 2013 - 2018 Intel Corporation. */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
 
-/* Modeled on trace-events-sample.h */
+#ifndef CONFIG_TRACEPOINTS
+#if !defined(_I40E_TRACE_H_)
+#define _I40E_TRACE_H_
+/* If the Linux kernel tracepoints are not available then the i40e_trace*
+ * macros become nops.
+ */
 
-/* The trace subsystem name for i40e will be "i40e".
+#define i40e_trace(trace_name, args...)
+#define i40e_trace_enabled(trace_name) (0)
+#endif /* !defined(_I40E_TRACE_H_) */
+#else /* CONFIG_TRACEPOINTS */
+/*
+ * Modeled on trace-events-sample.h
+ */
+
+/*
+ * The trace subsystem name for i40e will be "i40e".
  *
  * This file is named i40e_trace.h.
  *
@@ -14,7 +28,8 @@
 #undef TRACE_SYSTEM
 #define TRACE_SYSTEM i40e
 
-/* See trace-events-sample.h for a detailed description of why this
+/*
+ * See trace-events-sample.h for a detailed description of why this
  * guard clause is different from most normal include files.
  */
 #if !defined(_I40E_TRACE_H_) || defined(TRACE_HEADER_MULTI_READ)
@@ -49,7 +64,8 @@
 
 #define i40e_trace_enabled(trace_name) I40E_TRACE_NAME(trace_name##_enabled)()
 
-/* Events common to PF and VF. Corresponding versions will be defined
+/*
+ * Events common to PF and VF. Corresponding versions will be defined
  * for both, named trace_i40e_* and trace_i40evf_*. The i40e_trace()
  * macro above will select the right trace point name for the driver
  * being built from shared code.
@@ -65,7 +81,8 @@
 
 	TP_ARGS(ring, desc, buf),
 
-	/* The convention here is to make the first fields in the
+	/*
+	 * The convention here is to make the first fields in the
 	 * TP_STRUCT match the TP_PROTO exactly. This enables the use
 	 * of the args struct generated by the tplist tool (from the
 	 * bcc-tools package) to be used for those fields. To access
@@ -112,7 +129,7 @@
 	i40e_rx_template,
 
 	TP_PROTO(struct i40e_ring *ring,
-		 union i40e_32byte_rx_desc *desc,
+		 union i40e_rx_desc *desc,
 		 struct sk_buff *skb),
 
 	TP_ARGS(ring, desc, skb),
@@ -140,7 +157,7 @@
 DEFINE_EVENT(
 	i40e_rx_template, i40e_clean_rx_irq,
 	TP_PROTO(struct i40e_ring *ring,
-		 union i40e_32byte_rx_desc *desc,
+		 union i40e_rx_desc *desc,
 		 struct sk_buff *skb),
 
 	TP_ARGS(ring, desc, skb));
@@ -148,7 +165,7 @@
 DEFINE_EVENT(
 	i40e_rx_template, i40e_clean_rx_irq_rx,
 	TP_PROTO(struct i40e_ring *ring,
-		 union i40e_32byte_rx_desc *desc,
+		 union i40e_rx_desc *desc,
 		 struct sk_buff *skb),
 
 	TP_ARGS(ring, desc, skb));
@@ -193,7 +210,9 @@
 
 	TP_ARGS(skb, ring));
 
-/* Events unique to the PF. */
+/*
+ * Events unique to the PF.
+ */
 
 #endif /* _I40E_TRACE_H_ */
 /* This must be outside ifdef _I40E_TRACE_H */
@@ -207,3 +226,4 @@
 #undef TRACE_INCLUDE_FILE
 #define TRACE_INCLUDE_FILE i40e_trace
 #include <trace/define_trace.h>
+#endif /* CONFIG_TRACEPOINTS */
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_txrx.c linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_txrx.c
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_txrx.c	2024-05-10 01:26:45.389079797 -0400
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_txrx.c	2024-05-13 03:58:25.372491940 -0400
@@ -1,14 +1,24 @@
 // SPDX-License-Identifier: GPL-2.0
-/* Copyright(c) 2013 - 2018 Intel Corporation. */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
 
 #include <linux/prefetch.h>
-#include <linux/bpf_trace.h>
+#ifdef HAVE_XDP_SUPPORT
 #include <net/xdp.h>
+#endif
 #include "i40e.h"
 #include "i40e_trace.h"
 #include "i40e_prototype.h"
 #include "i40e_txrx_common.h"
-#include "i40e_xsk.h"
+
+static inline __le64 build_ctob(u32 td_cmd, u32 td_offset, unsigned int size,
+				u32 td_tag)
+{
+	return cpu_to_le64(I40E_TX_DESC_DTYPE_DATA |
+			   ((u64)td_cmd  << I40E_TXD_QW1_CMD_SHIFT) |
+			   ((u64)td_offset << I40E_TXD_QW1_OFFSET_SHIFT) |
+			   ((u64)size  << I40E_TXD_QW1_TX_BUF_SZ_SHIFT) |
+			   ((u64)td_tag  << I40E_TXD_QW1_L2TAG1_SHIFT));
+}
 
 #define I40E_TXD_CMD (I40E_TX_DESC_CMD_EOP | I40E_TX_DESC_CMD_RS)
 /**
@@ -37,14 +47,11 @@
 		     (fdata->q_index << I40E_TXD_FLTR_QW0_QINDEX_SHIFT);
 
 	flex_ptype |= I40E_TXD_FLTR_QW0_FLEXOFF_MASK &
-		      (fdata->flex_off << I40E_TXD_FLTR_QW0_FLEXOFF_SHIFT);
+		      (fdata->flex_offset << I40E_TXD_FLTR_QW0_FLEXOFF_SHIFT);
 
 	flex_ptype |= I40E_TXD_FLTR_QW0_PCTYPE_MASK &
 		      (fdata->pctype << I40E_TXD_FLTR_QW0_PCTYPE_SHIFT);
 
-	flex_ptype |= I40E_TXD_FLTR_QW0_PCTYPE_MASK &
-		      (fdata->flex_offset << I40E_TXD_FLTR_QW0_FLEXOFF_SHIFT);
-
 	/* Use LAN VSI Id if not programmed by user */
 	flex_ptype |= I40E_TXD_FLTR_QW0_DEST_VSI_MASK &
 		      ((u32)(fdata->dest_vsi ? : pf->vsi[pf->lan_vsi]->id) <<
@@ -121,6 +128,7 @@
 	/* grab the next descriptor */
 	i = tx_ring->next_to_use;
 	first = &tx_ring->tx_bi[i];
+
 	i40e_fdir(tx_ring, fdir_data, add);
 
 	/* Now program a dummy descriptor */
@@ -160,59 +168,182 @@
 	return -1;
 }
 
-#define IP_HEADER_OFFSET 14
-#define I40E_UDPIP_DUMMY_PACKET_LEN 42
 /**
- * i40e_add_del_fdir_udpv4 - Add/Remove UDPv4 filters
- * @vsi: pointer to the targeted VSI
- * @fd_data: the flow director data required for the FDir descriptor
- * @add: true adds a filter, false removes it
+ * i40e_create_dummy_packet - Constructs dummy packet for HW
+ * @dummy_packet: preallocated space for dummy packet
+ * @ipv4: is layer 3 packet of version 4 or 6
+ * @l4proto: next level protocol used in data portion of l3
+ * @data: filter data
+ *
+ * Returns address of layer 4 protocol dummy packet.
+ **/
+static char *i40e_create_dummy_packet(u8 *dummy_packet, bool ipv4, u8 l4proto,
+				      struct i40e_fdir_filter *data)
+{
+	bool is_vlan = !!data->vlan_tag;
+	struct vlan_hdr vlan;
+	struct ipv6hdr ipv6;
+	struct ethhdr eth;
+	struct iphdr ip;
+	u8 *tmp;
+
+	if (ipv4) {
+		eth.h_proto = cpu_to_be16(ETH_P_IP);
+		ip.protocol = l4proto;
+		ip.version = 0x4;
+		ip.ihl = 0x5;
+
+		ip.daddr = data->dst_ip;
+		ip.saddr = data->src_ip;
+	} else {
+		eth.h_proto = cpu_to_be16(ETH_P_IPV6);
+		ipv6.nexthdr = l4proto;
+		ipv6.version = 0x6;
+
+		memcpy(&ipv6.saddr.in6_u.u6_addr32, data->src_ip6,
+		       sizeof(__be32) * 4);
+		memcpy(&ipv6.daddr.in6_u.u6_addr32, data->dst_ip6,
+		       sizeof(__be32) * 4);
+	}
+	if (is_vlan) {
+		vlan.h_vlan_TCI = data->vlan_tag;
+		vlan.h_vlan_encapsulated_proto = eth.h_proto;
+		eth.h_proto = data->vlan_etype;
+	}
+
+	tmp = dummy_packet;
+	memcpy(tmp, &eth, sizeof(eth));
+	tmp += sizeof(eth);
+
+	if (is_vlan) {
+		memcpy(tmp, &vlan, sizeof(vlan));
+		tmp += sizeof(vlan);
+	}
+
+	if (ipv4) {
+		memcpy(tmp, &ip, sizeof(ip));
+		tmp += sizeof(ip);
+	} else {
+		memcpy(tmp, &ipv6, sizeof(ipv6));
+		tmp += sizeof(ipv6);
+	}
+
+	return tmp;
+}
+
+/**
+ * i40e_create_dummy_udp_packet - helper function to create UDP packet
+ * @raw_packet: preallocated space for dummy packet
+ * @ipv4: is layer 3 packet of version 4 or 6
+ * @l4proto: next level protocol used in data portion of l3
+ * @data: filter data
  *
- * Returns 0 if the filters were successfully added or removed
+ * Helper function to populate udp fields.
  **/
-static int i40e_add_del_fdir_udpv4(struct i40e_vsi *vsi,
-				   struct i40e_fdir_filter *fd_data,
-				   bool add)
+static void i40e_create_dummy_udp_packet(u8 *raw_packet, bool ipv4, u8 l4proto,
+					 struct i40e_fdir_filter *data)
 {
-	struct i40e_pf *pf = vsi->back;
 	struct udphdr *udp;
-	struct iphdr *ip;
-	u8 *raw_packet;
-	int ret;
-	static char packet[] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0x08, 0,
-		0x45, 0, 0, 0x1c, 0, 0, 0x40, 0, 0x40, 0x11, 0, 0, 0, 0, 0, 0,
-		0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};
+	u8 *tmp;
 
-	raw_packet = kzalloc(I40E_FDIR_MAX_RAW_PACKET_SIZE, GFP_KERNEL);
-	if (!raw_packet)
-		return -ENOMEM;
-	memcpy(raw_packet, packet, I40E_UDPIP_DUMMY_PACKET_LEN);
+	tmp = i40e_create_dummy_packet(raw_packet, ipv4, IPPROTO_UDP, data);
+	udp = (struct udphdr *)(tmp);
+	udp->dest = data->dst_port;
+	udp->source = data->src_port;
+}
+
+/**
+ * i40e_create_dummy_tcp_packet - helper function to create TCP packet
+ * @raw_packet: preallocated space for dummy packet
+ * @ipv4: is layer 3 packet of version 4 or 6
+ * @l4proto: next level protocol used in data portion of l3
+ * @data: filter data
+ *
+ * Helper function to populate tcp fields.
+ **/
+static void i40e_create_dummy_tcp_packet(u8 *raw_packet, bool ipv4, u8 l4proto,
+					 struct i40e_fdir_filter *data)
+{
+	struct tcphdr *tcp;
+	u8 *tmp;
+	/* Dummy tcp packet */
+	static const char tcp_packet[] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+		0x50, 0x11, 0x0, 0x72, 0, 0, 0, 0};
+
+	tmp = i40e_create_dummy_packet(raw_packet, ipv4, IPPROTO_TCP, data);
+
+	tcp = (struct tcphdr *)tmp;
+	memcpy(tcp, tcp_packet, sizeof(tcp_packet));
+	tcp->dest = data->dst_port;
+	tcp->source = data->src_port;
+}
 
-	ip = (struct iphdr *)(raw_packet + IP_HEADER_OFFSET);
-	udp = (struct udphdr *)(raw_packet + IP_HEADER_OFFSET
-	      + sizeof(struct iphdr));
-
-	ip->daddr = fd_data->dst_ip;
-	udp->dest = fd_data->dst_port;
-	ip->saddr = fd_data->src_ip;
-	udp->source = fd_data->src_port;
+/**
+ * i40e_create_dummy_sctp_packet - helper function to create SCTP packet
+ * @raw_packet: preallocated space for dummy packet
+ * @ipv4: is layer 3 packet of version 4 or 6
+ * @l4proto: next level protocol used in data portion of l3
+ * @data: filter data
+ *
+ * Helper function to populate sctp fields.
+ **/
+static void i40e_create_dummy_sctp_packet(u8 *raw_packet, bool ipv4,
+					  u8 l4proto,
+					  struct i40e_fdir_filter *data)
+{
+	struct sctphdr *sctp;
+	u8 *tmp;
+
+	tmp = i40e_create_dummy_packet(raw_packet, ipv4, IPPROTO_SCTP, data);
+
+	sctp = (struct sctphdr *)tmp;
+	sctp->dest = data->dst_port;
+	sctp->source = data->src_port;
+}
+
+/**
+ * i40e_prepare_fdir_filter - Prepare and program fdir filter
+ * @pf: physical function to attach filter to
+ * @fd_data: filter data
+ * @add: add or delete filter
+ * @packet_addr: address of dummy packet, used in filtering
+ * @payload_offset: offset from dummy packet address to user defined data
+ * @pctype: Packet type for which filter is used
+ *
+ * Helper function to offset data of dummy packet, program it and
+ * handle errors.
+ **/
+static int i40e_prepare_fdir_filter(struct i40e_pf *pf,
+				    struct i40e_fdir_filter *fd_data,
+				    bool add, char *packet_addr,
+				    int payload_offset, u8 pctype)
+{
+	int ret = 0;
 
 	if (fd_data->flex_filter) {
-		u8 *payload = raw_packet + I40E_UDPIP_DUMMY_PACKET_LEN;
+		u8 *payload;
 		__be16 pattern = fd_data->flex_word;
 		u16 off = fd_data->flex_offset;
 
+		payload = packet_addr + payload_offset;
+
+		/* If user provided vlan, offset payload by vlan
+		 * header length
+		 */
+		if (!!fd_data->vlan_tag)
+			payload += VLAN_HLEN;
+
 		*((__force __be16 *)(payload + off)) = pattern;
 	}
 
-	fd_data->pctype = I40E_FILTER_PCTYPE_NONF_IPV4_UDP;
-	ret = i40e_program_fdir_filter(fd_data, raw_packet, pf, add);
+	fd_data->pctype = pctype;
+	ret = i40e_program_fdir_filter(fd_data, packet_addr, pf, add);
+
 	if (ret) {
 		dev_info(&pf->pdev->dev,
 			 "PCTYPE:%d, Filter command send failed for fd_id:%d (ret = %d)\n",
 			 fd_data->pctype, fd_data->fd_id, ret);
 		/* Free the packet buffer since it wasn't added to the ring */
-		kfree(raw_packet);
 		return -EOPNOTSUPP;
 	} else if (I40E_DEBUG_FD & pf->hw.debug_mask) {
 		if (add)
@@ -225,238 +356,226 @@
 				 fd_data->pctype, fd_data->fd_id);
 	}
 
-	if (add)
-		pf->fd_udp4_filter_cnt++;
-	else
-		pf->fd_udp4_filter_cnt--;
+	return ret;
+}
 
-	return 0;
+static void i40e_change_filter_num(bool ipv4, bool add, u16 *ipv4_filter_num,
+				   u16 *ipv6_filter_num)
+{
+	if (add) {
+		if (ipv4)
+			(*ipv4_filter_num)++;
+		else
+			(*ipv6_filter_num)++;
+	} else {
+		if (ipv4)
+			(*ipv4_filter_num)--;
+		else
+			(*ipv6_filter_num)--;
+	}
 }
 
-#define I40E_TCPIP_DUMMY_PACKET_LEN 54
+#define IP_HEADER_OFFSET 14
 /**
- * i40e_add_del_fdir_tcpv4 - Add/Remove TCPv4 filters
+ * i40e_add_del_fdir_udp - Add/Remove UDPv4 filters
  * @vsi: pointer to the targeted VSI
  * @fd_data: the flow director data required for the FDir descriptor
  * @add: true adds a filter, false removes it
+ * @ipv4: true is v4, false is v6
  *
  * Returns 0 if the filters were successfully added or removed
  **/
-static int i40e_add_del_fdir_tcpv4(struct i40e_vsi *vsi,
-				   struct i40e_fdir_filter *fd_data,
-				   bool add)
+static int i40e_add_del_fdir_udp(struct i40e_vsi *vsi,
+				 struct i40e_fdir_filter *fd_data,
+				 bool add,
+				 bool ipv4)
 {
 	struct i40e_pf *pf = vsi->back;
-	struct tcphdr *tcp;
-	struct iphdr *ip;
 	u8 *raw_packet;
 	int ret;
-	/* Dummy packet */
-	static char packet[] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0x08, 0,
-		0x45, 0, 0, 0x28, 0, 0, 0x40, 0, 0x40, 0x6, 0, 0, 0, 0, 0, 0,
-		0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0x80, 0x11,
-		0x0, 0x72, 0, 0, 0, 0};
 
 	raw_packet = kzalloc(I40E_FDIR_MAX_RAW_PACKET_SIZE, GFP_KERNEL);
 	if (!raw_packet)
 		return -ENOMEM;
-	memcpy(raw_packet, packet, I40E_TCPIP_DUMMY_PACKET_LEN);
+	i40e_create_dummy_udp_packet(raw_packet, ipv4, IPPROTO_UDP, fd_data);
 
-	ip = (struct iphdr *)(raw_packet + IP_HEADER_OFFSET);
-	tcp = (struct tcphdr *)(raw_packet + IP_HEADER_OFFSET
-	      + sizeof(struct iphdr));
-
-	ip->daddr = fd_data->dst_ip;
-	tcp->dest = fd_data->dst_port;
-	ip->saddr = fd_data->src_ip;
-	tcp->source = fd_data->src_port;
+	if (ipv4)
+		ret = i40e_prepare_fdir_filter
+			(pf, fd_data, add, raw_packet,
+			 I40E_UDPIP_DUMMY_PACKET_LEN,
+			 I40E_FILTER_PCTYPE_NONF_IPV4_UDP);
+	else
+		ret = i40e_prepare_fdir_filter
+			(pf, fd_data, add, raw_packet,
+			 I40E_UDPIP6_DUMMY_PACKET_LEN,
+			 I40E_FILTER_PCTYPE_NONF_IPV6_UDP);
 
-	if (fd_data->flex_filter) {
-		u8 *payload = raw_packet + I40E_TCPIP_DUMMY_PACKET_LEN;
-		__be16 pattern = fd_data->flex_word;
-		u16 off = fd_data->flex_offset;
+	if (ret)
+		goto err;
 
-		*((__force __be16 *)(payload + off)) = pattern;
-	}
+	i40e_change_filter_num(ipv4, add, &pf->fd_udp4_filter_cnt,
+			       &pf->fd_udp6_filter_cnt);
 
-	fd_data->pctype = I40E_FILTER_PCTYPE_NONF_IPV4_TCP;
-	ret = i40e_program_fdir_filter(fd_data, raw_packet, pf, add);
-	if (ret) {
-		dev_info(&pf->pdev->dev,
-			 "PCTYPE:%d, Filter command send failed for fd_id:%d (ret = %d)\n",
-			 fd_data->pctype, fd_data->fd_id, ret);
-		/* Free the packet buffer since it wasn't added to the ring */
-		kfree(raw_packet);
-		return -EOPNOTSUPP;
-	} else if (I40E_DEBUG_FD & pf->hw.debug_mask) {
-		if (add)
-			dev_info(&pf->pdev->dev, "Filter OK for PCTYPE %d loc = %d)\n",
-				 fd_data->pctype, fd_data->fd_id);
-		else
-			dev_info(&pf->pdev->dev,
-				 "Filter deleted for PCTYPE %d loc = %d\n",
-				 fd_data->pctype, fd_data->fd_id);
-	}
+	return 0;
+err:
+	kfree(raw_packet);
+	return ret;
+}
+
+/**
+ * i40e_add_del_fdir_tcp - Add/Remove TCPv4 filters
+ * @vsi: pointer to the targeted VSI
+ * @fd_data: the flow director data required for the FDir descriptor
+ * @add: true adds a filter, false removes it
+ * @ipv4: true is v4, false is v6
+ *
+ * Returns 0 if the filters were successfully added or removed
+ **/
+static int i40e_add_del_fdir_tcp(struct i40e_vsi *vsi,
+				 struct i40e_fdir_filter *fd_data,
+				 bool add,
+				 bool ipv4)
+{
+	struct i40e_pf *pf = vsi->back;
+	u8 *raw_packet;
+	int ret;
+
+	raw_packet = kzalloc(I40E_FDIR_MAX_RAW_PACKET_SIZE, GFP_KERNEL);
+	if (!raw_packet)
+		return -ENOMEM;
+	i40e_create_dummy_tcp_packet(raw_packet, ipv4, IPPROTO_TCP, fd_data);
+	if (ipv4)
+		ret = i40e_prepare_fdir_filter
+			(pf, fd_data, add, raw_packet,
+			 I40E_TCPIP_DUMMY_PACKET_LEN,
+			 I40E_FILTER_PCTYPE_NONF_IPV4_TCP);
+	else
+		ret = i40e_prepare_fdir_filter
+			(pf, fd_data, add, raw_packet,
+			 I40E_TCPIP6_DUMMY_PACKET_LEN,
+			 I40E_FILTER_PCTYPE_NONF_IPV6_TCP);
+
+	if (ret)
+		goto err;
 
+	i40e_change_filter_num(ipv4, add, &pf->fd_tcp4_filter_cnt,
+			       &pf->fd_tcp6_filter_cnt);
 	if (add) {
-		pf->fd_tcp4_filter_cnt++;
 		if ((pf->flags & I40E_FLAG_FD_ATR_ENABLED) &&
 		    I40E_DEBUG_FD & pf->hw.debug_mask)
 			dev_info(&pf->pdev->dev, "Forcing ATR off, sideband rules for TCP/IPv4 flow being applied\n");
 		set_bit(__I40E_FD_ATR_AUTO_DISABLED, pf->state);
-	} else {
-		pf->fd_tcp4_filter_cnt--;
 	}
-
 	return 0;
+err:
+	kfree(raw_packet);
+	return ret;
 }
 
-#define I40E_SCTPIP_DUMMY_PACKET_LEN 46
 /**
- * i40e_add_del_fdir_sctpv4 - Add/Remove SCTPv4 Flow Director filters for
+ * i40e_add_del_fdir_sctp - Add/Remove SCTPv4 Flow Director filters for
  * a specific flow spec
  * @vsi: pointer to the targeted VSI
  * @fd_data: the flow director data required for the FDir descriptor
  * @add: true adds a filter, false removes it
+ * @ipv4: true is v4, false is v6
  *
  * Returns 0 if the filters were successfully added or removed
  **/
-static int i40e_add_del_fdir_sctpv4(struct i40e_vsi *vsi,
-				    struct i40e_fdir_filter *fd_data,
-				    bool add)
+static int i40e_add_del_fdir_sctp(struct i40e_vsi *vsi,
+				  struct i40e_fdir_filter *fd_data,
+				  bool add,
+				  bool ipv4)
 {
 	struct i40e_pf *pf = vsi->back;
-	struct sctphdr *sctp;
-	struct iphdr *ip;
 	u8 *raw_packet;
 	int ret;
-	/* Dummy packet */
-	static char packet[] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0x08, 0,
-		0x45, 0, 0, 0x20, 0, 0, 0x40, 0, 0x40, 0x84, 0, 0, 0, 0, 0, 0,
-		0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};
 
 	raw_packet = kzalloc(I40E_FDIR_MAX_RAW_PACKET_SIZE, GFP_KERNEL);
 	if (!raw_packet)
 		return -ENOMEM;
-	memcpy(raw_packet, packet, I40E_SCTPIP_DUMMY_PACKET_LEN);
 
-	ip = (struct iphdr *)(raw_packet + IP_HEADER_OFFSET);
-	sctp = (struct sctphdr *)(raw_packet + IP_HEADER_OFFSET
-	      + sizeof(struct iphdr));
-
-	ip->daddr = fd_data->dst_ip;
-	sctp->dest = fd_data->dst_port;
-	ip->saddr = fd_data->src_ip;
-	sctp->source = fd_data->src_port;
+	i40e_create_dummy_sctp_packet(raw_packet, ipv4, IPPROTO_SCTP, fd_data);
 
-	if (fd_data->flex_filter) {
-		u8 *payload = raw_packet + I40E_SCTPIP_DUMMY_PACKET_LEN;
-		__be16 pattern = fd_data->flex_word;
-		u16 off = fd_data->flex_offset;
-
-		*((__force __be16 *)(payload + off)) = pattern;
-	}
+	if (ipv4)
+		ret = i40e_prepare_fdir_filter
+			(pf, fd_data, add, raw_packet,
+			 I40E_SCTPIP_DUMMY_PACKET_LEN,
+			 I40E_FILTER_PCTYPE_NONF_IPV4_SCTP);
+	else
+		ret = i40e_prepare_fdir_filter
+			(pf, fd_data, add, raw_packet,
+			 I40E_SCTPIP6_DUMMY_PACKET_LEN,
+			 I40E_FILTER_PCTYPE_NONF_IPV6_SCTP);
 
-	fd_data->pctype = I40E_FILTER_PCTYPE_NONF_IPV4_SCTP;
-	ret = i40e_program_fdir_filter(fd_data, raw_packet, pf, add);
-	if (ret) {
-		dev_info(&pf->pdev->dev,
-			 "PCTYPE:%d, Filter command send failed for fd_id:%d (ret = %d)\n",
-			 fd_data->pctype, fd_data->fd_id, ret);
-		/* Free the packet buffer since it wasn't added to the ring */
-		kfree(raw_packet);
-		return -EOPNOTSUPP;
-	} else if (I40E_DEBUG_FD & pf->hw.debug_mask) {
-		if (add)
-			dev_info(&pf->pdev->dev,
-				 "Filter OK for PCTYPE %d loc = %d\n",
-				 fd_data->pctype, fd_data->fd_id);
-		else
-			dev_info(&pf->pdev->dev,
-				 "Filter deleted for PCTYPE %d loc = %d\n",
-				 fd_data->pctype, fd_data->fd_id);
-	}
+	if (ret)
+		goto err;
 
-	if (add)
-		pf->fd_sctp4_filter_cnt++;
-	else
-		pf->fd_sctp4_filter_cnt--;
+	i40e_change_filter_num(ipv4, add, &pf->fd_sctp4_filter_cnt,
+			       &pf->fd_sctp6_filter_cnt);
 
 	return 0;
+err:
+	kfree(raw_packet);
+	return ret;
 }
 
-#define I40E_IP_DUMMY_PACKET_LEN 34
 /**
- * i40e_add_del_fdir_ipv4 - Add/Remove IPv4 Flow Director filters for
- * a specific flow spec
+ * i40e_add_del_fdir_ip - Add/Remove IPv4/v6 Flow Director filter
  * @vsi: pointer to the targeted VSI
  * @fd_data: the flow director data required for the FDir descriptor
  * @add: true adds a filter, false removes it
+ * @ipv4: true is v4, false is v6
  *
- * Returns 0 if the filters were successfully added or removed
+ * Adds or removes IPv4 or IPv6 filters for a specific flow spec.
+ * Returns 0 if the filters were successfully added or removed.
  **/
-static int i40e_add_del_fdir_ipv4(struct i40e_vsi *vsi,
-				  struct i40e_fdir_filter *fd_data,
-				  bool add)
+static int i40e_add_del_fdir_ip(struct i40e_vsi *vsi,
+				struct i40e_fdir_filter *fd_data,
+				bool add,
+				bool ipv4)
 {
 	struct i40e_pf *pf = vsi->back;
-	struct iphdr *ip;
 	u8 *raw_packet;
+	int iter_start;
+	int iter_end;
 	int ret;
 	int i;
-	static char packet[] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0x08, 0,
-		0x45, 0, 0, 0x14, 0, 0, 0x40, 0, 0x40, 0x10, 0, 0, 0, 0, 0, 0,
-		0, 0, 0, 0};
 
-	for (i = I40E_FILTER_PCTYPE_NONF_IPV4_OTHER;
-	     i <= I40E_FILTER_PCTYPE_FRAG_IPV4;	i++) {
+	if (ipv4) {
+		iter_start = I40E_FILTER_PCTYPE_NONF_IPV4_OTHER;
+		iter_end = I40E_FILTER_PCTYPE_FRAG_IPV4;
+	} else {
+		iter_start = I40E_FILTER_PCTYPE_NONF_IPV6_OTHER;
+		iter_end = I40E_FILTER_PCTYPE_FRAG_IPV6;
+	}
+
+	for (i = iter_start; i <= iter_end; i++) {
+		int payload_offset;
+
 		raw_packet = kzalloc(I40E_FDIR_MAX_RAW_PACKET_SIZE, GFP_KERNEL);
 		if (!raw_packet)
 			return -ENOMEM;
-		memcpy(raw_packet, packet, I40E_IP_DUMMY_PACKET_LEN);
-		ip = (struct iphdr *)(raw_packet + IP_HEADER_OFFSET);
-
-		ip->saddr = fd_data->src_ip;
-		ip->daddr = fd_data->dst_ip;
-		ip->protocol = 0;
-
-		if (fd_data->flex_filter) {
-			u8 *payload = raw_packet + I40E_IP_DUMMY_PACKET_LEN;
-			__be16 pattern = fd_data->flex_word;
-			u16 off = fd_data->flex_offset;
 
-			*((__force __be16 *)(payload + off)) = pattern;
-		}
-
-		fd_data->pctype = i;
-		ret = i40e_program_fdir_filter(fd_data, raw_packet, pf, add);
-		if (ret) {
-			dev_info(&pf->pdev->dev,
-				 "PCTYPE:%d, Filter command send failed for fd_id:%d (ret = %d)\n",
-				 fd_data->pctype, fd_data->fd_id, ret);
-			/* The packet buffer wasn't added to the ring so we
-			 * need to free it now.
-			 */
-			kfree(raw_packet);
-			return -EOPNOTSUPP;
-		} else if (I40E_DEBUG_FD & pf->hw.debug_mask) {
-			if (add)
-				dev_info(&pf->pdev->dev,
-					 "Filter OK for PCTYPE %d loc = %d\n",
-					 fd_data->pctype, fd_data->fd_id);
-			else
-				dev_info(&pf->pdev->dev,
-					 "Filter deleted for PCTYPE %d loc = %d\n",
-					 fd_data->pctype, fd_data->fd_id);
-		}
+		/* IPv6 no header option differs from IPv4 */
+		(void)i40e_create_dummy_packet
+			(raw_packet, ipv4, (ipv4) ? IPPROTO_IP : IPPROTO_NONE,
+			 fd_data);
+
+		payload_offset = (ipv4) ? I40E_IP_DUMMY_PACKET_LEN :
+			I40E_IP6_DUMMY_PACKET_LEN;
+		ret = i40e_prepare_fdir_filter(pf, fd_data, add, raw_packet,
+					       payload_offset, i);
+		if (ret)
+			goto err;
 	}
 
-	if (add)
-		pf->fd_ip4_filter_cnt++;
-	else
-		pf->fd_ip4_filter_cnt--;
-
+	i40e_change_filter_num(ipv4, add, &pf->fd_ip4_filter_cnt,
+			       &pf->fd_ip6_filter_cnt);
 	return 0;
+err:
+	kfree(raw_packet);
+	return ret;
 }
 
 /**
@@ -469,40 +588,73 @@
 int i40e_add_del_fdir(struct i40e_vsi *vsi,
 		      struct i40e_fdir_filter *input, bool add)
 {
+	enum ip_ver { ipv6 = 0, ipv4 = 1 };
 	struct i40e_pf *pf = vsi->back;
 	int ret;
 
-	switch (input->flow_type & ~FLOW_EXT) {
+	switch (input->flow_type) {
 	case TCP_V4_FLOW:
-		ret = i40e_add_del_fdir_tcpv4(vsi, input, add);
+		ret = i40e_add_del_fdir_tcp(vsi, input, add, ipv4);
 		break;
 	case UDP_V4_FLOW:
-		ret = i40e_add_del_fdir_udpv4(vsi, input, add);
+		ret = i40e_add_del_fdir_udp(vsi, input, add, ipv4);
 		break;
 	case SCTP_V4_FLOW:
-		ret = i40e_add_del_fdir_sctpv4(vsi, input, add);
+		ret = i40e_add_del_fdir_sctp(vsi, input, add, ipv4);
+		break;
+	case TCP_V6_FLOW:
+		ret = i40e_add_del_fdir_tcp(vsi, input, add, ipv6);
+		break;
+	case UDP_V6_FLOW:
+		ret = i40e_add_del_fdir_udp(vsi, input, add, ipv6);
+		break;
+	case SCTP_V6_FLOW:
+		ret = i40e_add_del_fdir_sctp(vsi, input, add, ipv6);
 		break;
 	case IP_USER_FLOW:
-		switch (input->ip4_proto) {
+		switch (input->ipl4_proto) {
 		case IPPROTO_TCP:
-			ret = i40e_add_del_fdir_tcpv4(vsi, input, add);
+			ret = i40e_add_del_fdir_tcp(vsi, input, add, ipv4);
 			break;
 		case IPPROTO_UDP:
-			ret = i40e_add_del_fdir_udpv4(vsi, input, add);
+			ret = i40e_add_del_fdir_udp(vsi, input, add, ipv4);
 			break;
 		case IPPROTO_SCTP:
-			ret = i40e_add_del_fdir_sctpv4(vsi, input, add);
+			ret = i40e_add_del_fdir_sctp(vsi, input, add, ipv4);
 			break;
 		case IPPROTO_IP:
-			ret = i40e_add_del_fdir_ipv4(vsi, input, add);
+			ret = i40e_add_del_fdir_ip(vsi, input, add, ipv4);
 			break;
 		default:
 			/* We cannot support masking based on protocol */
 			dev_info(&pf->pdev->dev, "Unsupported IPv4 protocol 0x%02x\n",
-				 input->ip4_proto);
+				 input->ipl4_proto);
+			return -EINVAL;
+		}
+		break;
+#ifdef HAVE_ETHTOOL_FLOW_UNION_IP6_SPEC
+	case IPV6_USER_FLOW:
+		switch (input->ipl4_proto) {
+		case IPPROTO_TCP:
+			ret = i40e_add_del_fdir_tcp(vsi, input, add, ipv6);
+			break;
+		case IPPROTO_UDP:
+			ret = i40e_add_del_fdir_udp(vsi, input, add, ipv6);
+			break;
+		case IPPROTO_SCTP:
+			ret = i40e_add_del_fdir_sctp(vsi, input, add, ipv6);
+			break;
+		case IPPROTO_IP:
+			ret = i40e_add_del_fdir_ip(vsi, input, add, ipv6);
+			break;
+		default:
+			/* We cannot support masking based on protocol */
+			dev_info(&pf->pdev->dev, "Unsupported IPv6 protocol 0x%02x\n",
+				 input->ipl4_proto);
 			return -EINVAL;
 		}
 		break;
+#endif /* HAVE_ETHTOOL_FLOW_UNION_IP6_SPEC */
 	default:
 		dev_info(&pf->pdev->dev, "Unsupported flow type 0x%02x\n",
 			 input->flow_type);
@@ -604,8 +756,14 @@
 	if (tx_buffer->skb) {
 		if (tx_buffer->tx_flags & I40E_TX_FLAGS_FD_SB)
 			kfree(tx_buffer->raw_buf);
+#ifdef HAVE_XDP_SUPPORT
 		else if (ring_is_xdp(ring))
+#ifdef HAVE_XDP_FRAME_STRUCT
 			xdp_return_frame(tx_buffer->xdpf);
+#else
+			page_frag_free(tx_buffer->raw_buf);
+#endif
+#endif
 		else
 			dev_kfree_skb_any(tx_buffer->skb);
 		if (dma_unmap_len(tx_buffer, len))
@@ -635,18 +793,13 @@
 	unsigned long bi_size;
 	u16 i;
 
-	if (ring_is_xdp(tx_ring) && tx_ring->xsk_umem) {
-		i40e_xsk_clean_tx_ring(tx_ring);
-	} else {
-		/* ring already cleared, nothing to do */
-		if (!tx_ring->tx_bi)
-			return;
+	/* ring already cleared, nothing to do */
+	if (!tx_ring->tx_bi)
+		return;
 
-		/* Free all the Tx ring sk_buffs */
-		for (i = 0; i < tx_ring->count; i++)
-			i40e_unmap_and_free_tx_resource(tx_ring,
-							&tx_ring->tx_bi[i]);
-	}
+	/* Free all the Tx ring sk_buffs */
+	for (i = 0; i < tx_ring->count; i++)
+		i40e_unmap_and_free_tx_resource(tx_ring, &tx_ring->tx_bi[i]);
 
 	bi_size = sizeof(struct i40e_tx_buffer) * tx_ring->count;
 	memset(tx_ring->tx_bi, 0, bi_size);
@@ -763,6 +916,8 @@
 	}
 }
 
+#define WB_STRIDE 4
+
 /**
  * i40e_clean_tx_irq - Reclaim resources after transmit completes
  * @vsi: the VSI we care about
@@ -810,9 +965,15 @@
 		total_packets += tx_buf->gso_segs;
 
 		/* free the skb/XDP data */
+#ifdef HAVE_XDP_SUPPORT
 		if (ring_is_xdp(tx_ring))
+#ifdef HAVE_XDP_FRAME_STRUCT
 			xdp_return_frame(tx_buf->xdpf);
+#else
+			page_frag_free(tx_buf->raw_buf);
+#endif
 		else
+#endif
 			napi_consume_skb(tx_buf->skb, napi_budget);
 
 		/* unmap skb header data */
@@ -867,8 +1028,27 @@
 
 	i += tx_ring->count;
 	tx_ring->next_to_clean = i;
-	i40e_update_tx_stats(tx_ring, total_packets, total_bytes);
-	i40e_arm_wb(tx_ring, vsi, budget);
+	u64_stats_update_begin(&tx_ring->syncp);
+	tx_ring->stats.bytes += total_bytes;
+	tx_ring->stats.packets += total_packets;
+	u64_stats_update_end(&tx_ring->syncp);
+	tx_ring->q_vector->tx.total_bytes += total_bytes;
+	tx_ring->q_vector->tx.total_packets += total_packets;
+
+	if (tx_ring->flags & I40E_TXR_FLAGS_WB_ON_ITR) {
+		/* check to see if there are < 4 descriptors
+		 * waiting to be written back, then kick the hardware to force
+		 * them to be written back in case we stay in NAPI.
+		 * In this mode on X722 we do not enable Interrupt.
+		 */
+		unsigned int j = i40e_get_tx_pending(tx_ring, false);
+
+		if (budget &&
+		    ((j / WB_STRIDE) == 0) && (j > 0) &&
+		    !test_bit(__I40E_VSI_DOWN, vsi->state) &&
+		    (I40E_DESC_UNUSED(tx_ring) != tx_ring->count))
+			tx_ring->arm_wb = true;
+	}
 
 	if (ring_is_xdp(tx_ring))
 		return !!budget;
@@ -919,8 +1099,8 @@
 		      I40E_PFINT_DYN_CTLN_ITR_INDX_MASK; /* set noitr */
 
 		wr32(&vsi->back->hw,
-		     I40E_PFINT_DYN_CTLN(q_vector->reg_idx),
-		     val);
+		    I40E_PFINT_DYN_CTLN(q_vector->reg_idx),
+		    val);
 	} else {
 		val = I40E_PFINT_DYN_CTL0_WB_ON_ITR_MASK |
 		      I40E_PFINT_DYN_CTL0_ITR_INDX_MASK; /* set noitr */
@@ -1195,11 +1375,7 @@
 	rc->total_packets = 0;
 }
 
-static struct i40e_rx_buffer *i40e_rx_bi(struct i40e_ring *rx_ring, u32 idx)
-{
-	return &rx_ring->rx_bi[idx];
-}
-
+#ifndef CONFIG_I40E_DISABLE_PACKET_SPLIT
 /**
  * i40e_reuse_rx_page - page flip buffer and store it back on the ring
  * @rx_ring: rx descriptor ring to store buffers on
@@ -1213,7 +1389,7 @@
 	struct i40e_rx_buffer *new_buff;
 	u16 nta = rx_ring->next_to_alloc;
 
-	new_buff = i40e_rx_bi(rx_ring, nta);
+	new_buff = &rx_ring->rx_bi[nta];
 
 	/* update, and store next to alloc */
 	nta++;
@@ -1231,27 +1407,39 @@
 	old_buff->page = NULL;
 }
 
+#endif /* CONFIG_I40E_DISABLE_PACKET_SPLIT */
+#ifdef CONFIG_I40E_DISABLE_PACKET_SPLIT
 /**
- * i40e_rx_is_programming_status - check for programming status descriptor
- * @qw: qword representing status_error_len in CPU ordering
+ * i40e_reuse_rx_skb - Recycle unused skb and store it back on the ring
+ * @rx_ring: rx descriptor ring to store buffers on
+ * @old_buff: donor buffer to have skb reused
  *
- * The value of in the descriptor length field indicate if this
- * is a programming status descriptor for flow director or FCoE
- * by the value of I40E_RX_PROG_STATUS_DESC_LENGTH, otherwise
- * it is a packet descriptor.
+ * Synchronizes skb for reuse by the adapter
  **/
-static inline bool i40e_rx_is_programming_status(u64 qw)
+static void i40e_reuse_rx_skb(struct i40e_ring *rx_ring,
+			      struct i40e_rx_buffer *old_buff)
 {
-	/* The Rx filter programming status and SPH bit occupy the same
-	 * spot in the descriptor. Since we don't support packet split we
-	 * can just reuse the bit as an indication that this is a
-	 * programming status descriptor.
-	 */
-	return qw & I40E_RXD_QW1_LENGTH_SPH_MASK;
+	struct i40e_rx_buffer *new_buff;
+	u16 nta = rx_ring->next_to_alloc;
+
+	new_buff = &rx_ring->rx_bi[nta];
+
+	/* update, and store next to alloc */
+	nta++;
+	rx_ring->next_to_alloc = (nta < rx_ring->count) ? nta : 0;
+
+	/* transfer page from old buffer to new buffer */
+	new_buff->dma		= old_buff->dma;
+	new_buff->skb		= old_buff->skb;
+
+	rx_buffer->skb = NULL;
 }
 
+#endif /* CONFIG_I40E_DISABLE_PACKET_SPLIT */
+
 /**
- * i40e_clean_programming_status - try clean the programming status descriptor
+ * i40e_clean_programming_status - try to clean the programming status
+ * descriptor
  * @rx_ring: the rx ring that has this descriptor
  * @rx_desc: the rx descriptor written back by HW
  * @qw: qword representing status_error_len in CPU ordering
@@ -1262,10 +1450,10 @@
  *
  * Returns an i40e_rx_buffer to reuse if the cleanup occurred, otherwise NULL.
  **/
-struct i40e_rx_buffer *i40e_clean_programming_status(
-	struct i40e_ring *rx_ring,
-	union i40e_rx_desc *rx_desc,
-	u64 qw)
+struct i40e_rx_buffer *i40e_clean_programming_status
+	(struct i40e_ring *rx_ring,
+	 union i40e_rx_desc *rx_desc,
+	 u64 qw)
 {
 	struct i40e_rx_buffer *rx_buffer;
 	u32 ntc;
@@ -1277,7 +1465,7 @@
 	ntc = rx_ring->next_to_clean;
 
 	/* fetch, update, and store next to clean */
-	rx_buffer = i40e_rx_bi(rx_ring, ntc++);
+	rx_buffer = &rx_ring->rx_bi[ntc++];
 	ntc = (ntc < rx_ring->count) ? ntc : 0;
 	rx_ring->next_to_clean = ntc;
 
@@ -1313,8 +1501,6 @@
 	if (!tx_ring->tx_bi)
 		goto err;
 
-	u64_stats_init(&tx_ring->syncp);
-
 	/* round up to nearest 4K */
 	tx_ring->size = tx_ring->count * sizeof(struct i40e_tx_desc);
 	/* add u32 for head writeback, align after this takes care of
@@ -1359,15 +1545,19 @@
 		rx_ring->skb = NULL;
 	}
 
-	if (rx_ring->xsk_umem) {
-		i40e_xsk_clean_rx_ring(rx_ring);
-		goto skip_free;
-	}
-
 	/* Free all the Rx ring sk_buffs */
 	for (i = 0; i < rx_ring->count; i++) {
-		struct i40e_rx_buffer *rx_bi = i40e_rx_bi(rx_ring, i);
+		struct i40e_rx_buffer *rx_bi = &rx_ring->rx_bi[i];
+
+#ifdef CONFIG_I40E_DISABLE_PACKET_SPLIT
+		if (!rx_bi->skb)
+			continue;
 
+		dma_unmap_single(rx_ring->dev, rx_bi->dma,
+				 rx_ring->rx_buf_len, DMA_FROM_DEVICE);
+		dev_kfree_skb(rx_bi->skb);
+		rx_bi->skb = NULL;
+#else /* CONFIG_I40E_DISABLE_PACKET_SPLIT */
 		if (!rx_bi->page)
 			continue;
 
@@ -1390,9 +1580,9 @@
 
 		rx_bi->page = NULL;
 		rx_bi->page_offset = 0;
+#endif /* CONFIG_I40E_DISABLE_PACKET_SPLIT */
 	}
 
-skip_free:
 	bi_size = sizeof(struct i40e_rx_buffer) * rx_ring->count;
 	memset(rx_ring->rx_bi, 0, bi_size);
 
@@ -1413,8 +1603,10 @@
 void i40e_free_rx_resources(struct i40e_ring *rx_ring)
 {
 	i40e_clean_rx_ring(rx_ring);
+#ifdef HAVE_XDP_BUFF_RXQ
 	if (rx_ring->vsi->type == I40E_VSI_MAIN)
 		xdp_rxq_info_unreg(&rx_ring->xdp_rxq);
+#endif
 	rx_ring->xdp_prog = NULL;
 	kfree(rx_ring->rx_bi);
 	rx_ring->rx_bi = NULL;
@@ -1444,11 +1636,13 @@
 	rx_ring->rx_bi = kzalloc(bi_size, GFP_KERNEL);
 	if (!rx_ring->rx_bi)
 		goto err;
+#ifdef HAVE_NDO_GET_STATS64
 
 	u64_stats_init(&rx_ring->syncp);
+#endif /* HAVE_NDO_GET_STATS64 */
 
 	/* Round up to nearest 4K */
-	rx_ring->size = rx_ring->count * sizeof(union i40e_32byte_rx_desc);
+	rx_ring->size = rx_ring->count * sizeof(union i40e_rx_desc);
 	rx_ring->size = ALIGN(rx_ring->size, 4096);
 	rx_ring->desc = dma_alloc_coherent(dev, rx_ring->size,
 					   &rx_ring->dma, GFP_KERNEL);
@@ -1462,17 +1656,17 @@
 	rx_ring->next_to_alloc = 0;
 	rx_ring->next_to_clean = 0;
 	rx_ring->next_to_use = 0;
-
+#ifdef HAVE_XDP_BUFF_RXQ
 	/* XDP RX-queue info only needed for RX rings exposed to XDP */
 	if (rx_ring->vsi->type == I40E_VSI_MAIN) {
 		err = xdp_rxq_info_reg(&rx_ring->xdp_rxq, rx_ring->netdev,
-				       rx_ring->queue_index);
+				       rx_ring->queue_index, rx_ring->q_vector->napi.napi_id);
 		if (err < 0)
 			goto err;
 	}
 
 	rx_ring->xdp_prog = rx_ring->vsi->xdp_prog;
-
+#endif
 	return 0;
 err:
 	kfree(rx_ring->rx_bi);
@@ -1501,6 +1695,72 @@
 	writel(val, rx_ring->tail);
 }
 
+#ifdef HAVE_XDP_BUFF_FRAME_SZ
+/**
+ * i40e_rx_frame_truesize - Returns an actual size of Rx frame in memory
+ * @rx_ring: Rx ring we are requesting the frame size of
+ * @size: Packet length from rx_desc
+ *
+ * Returns an actual size of Rx frame in memory, considering page size
+ * and SKB data alignment.
+ */
+static unsigned int i40e_rx_frame_truesize(struct i40e_ring *rx_ring,
+					   unsigned int size)
+{
+	unsigned int truesize;
+
+#if (PAGE_SIZE < 8192)
+	truesize = i40e_rx_pg_size(rx_ring) / 2; /* Must be power-of-2 */
+#else
+	truesize = i40e_rx_offset(rx_ring) ?
+		SKB_DATA_ALIGN(size + i40e_rx_offset(rx_ring)) +
+		SKB_DATA_ALIGN(sizeof(struct skb_shared_info)) :
+		SKB_DATA_ALIGN(size);
+#endif
+	return truesize;
+}
+#endif /* HAVE_XDP_BUFF_FRAME_SZ */
+
+#ifdef CONFIG_I40E_DISABLE_PACKET_SPLIT
+static bool i40e_alloc_mapped_skb(struct i40e_ring *rx_ring,
+				  struct i40e_rx_buffer *bi)
+{
+	struct sk_buff *skb = bi->skb;
+	dma_addr_t dma;
+
+	if (unlikely(skb))
+		return true;
+
+	if (likely(!skb)) {
+		skb = __napi_alloc_skb(&rx_ring->q_vector->napi,
+				       rx_ring->rx_buf_len,
+				       GFP_ATOMIC | __GFP_NOWARN);
+		if (unlikely(!skb)) {
+			rx_ring->rx_stats.alloc_buff_failed++;
+			return false;
+		}
+	}
+
+	dma = dma_map_single(rx_ring->dev, skb->data,
+			     rx_ring->rx_buf_len, DMA_FROM_DEVICE);
+
+	/*
+	 * if mapping failed free memory back to system since
+	 * there isn't much point in holding memory we can't use
+	 */
+	if (dma_mapping_error(rx_ring->dev, dma)) {
+		dev_kfree_skb_any(skb);
+		rx_ring->rx_stats.alloc_buff_failed++;
+		return false;
+	}
+
+	bi->skb = skb;
+	bi->dma = dma;
+
+	return true;
+}
+
+#else /* CONFIG_I40E_DISABLE_PACKET_SPLIT */
 /**
  * i40e_rx_offset - Return expected offset into page to access data
  * @rx_ring: Ring we are requesting offset of
@@ -1557,12 +1817,58 @@
 	bi->dma = dma;
 	bi->page = page;
 	bi->page_offset = i40e_rx_offset(rx_ring);
+
+#ifdef HAVE_PAGE_COUNT_BULK_UPDATE
 	page_ref_add(page, USHRT_MAX - 1);
 	bi->pagecnt_bias = USHRT_MAX;
+#else
+	/* initialize pagecnt_bias to 1 representing we fully own page */
+	bi->pagecnt_bias = 1;
+#endif
 
 	return true;
 }
 
+#endif /* CONFIG_I40E_DISABLE_PACKET_SPLIT */
+/**
+ * i40e_receive_skb - Send a completed packet up the stack
+ * @rx_ring:  rx ring in play
+ * @skb: packet to send up
+ * @vlan_tag: vlan tag for packet
+ * @vlan_tpid: vlan tpid for packet
+ **/
+void i40e_receive_skb(struct i40e_ring *rx_ring,
+		      struct sk_buff *skb, u16 vlan_tag, u16 vlan_tpid)
+{
+	struct i40e_q_vector *q_vector = rx_ring->q_vector;
+#ifdef HAVE_VLAN_RX_REGISTER
+	struct i40e_vsi *vsi = rx_ring->vsi;
+#endif
+
+#ifdef HAVE_VLAN_RX_REGISTER
+	if (vlan_tag & VLAN_VID_MASK) {
+		if (!vsi->vlgrp)
+			dev_kfree_skb_any(skb);
+		else
+			vlan_gro_receive(&q_vector->napi, vsi->vlgrp,
+					 vlan_tag, skb);
+	} else {
+		napi_gro_receive(&q_vector->napi, skb);
+	}
+#else /* HAVE_VLAN_RX_REGISTER */
+#ifdef NETIF_F_HW_VLAN_CTAG_RX
+	if ((rx_ring->netdev->features & NETIF_F_HW_VLAN_CTAG_RX) &&
+	    (vlan_tag & VLAN_VID_MASK))
+#else
+	if ((rx_ring->netdev->features & NETIF_F_HW_VLAN_RX) &&
+	    (vlan_tag & VLAN_VID_MASK))
+#endif /* NETIF_F_HW_VLAN_CTAG_RX */
+		__vlan_hwaccel_put_tag(skb, htons(vlan_tpid), vlan_tag);
+
+	napi_gro_receive(&q_vector->napi, skb);
+#endif /* HAVE_VLAN_RX_REGISTER */
+}
+
 /**
  * i40e_alloc_rx_buffers - Replace used receive buffers
  * @rx_ring: ring to place buffers on
@@ -1581,9 +1887,15 @@
 		return false;
 
 	rx_desc = I40E_RX_DESC(rx_ring, ntu);
-	bi = i40e_rx_bi(rx_ring, ntu);
+	bi = &rx_ring->rx_bi[ntu];
 
 	do {
+#ifdef CONFIG_I40E_DISABLE_PACKET_SPLIT
+		if (!i40e_alloc_mapped_skb(rx_ring, bi))
+			goto no_buffers;
+
+		rx_desc->read.pkt_addr = cpu_to_le64(bi->dma);
+#else
 		if (!i40e_alloc_mapped_page(rx_ring, bi))
 			goto no_buffers;
 
@@ -1597,13 +1909,14 @@
 		 * because each write-back erases this info.
 		 */
 		rx_desc->read.pkt_addr = cpu_to_le64(bi->dma + bi->page_offset);
+#endif /* CONFIG_I40E_DISABLE_PACKET_SPLIT */
 
 		rx_desc++;
 		bi++;
 		ntu++;
 		if (unlikely(ntu == rx_ring->count)) {
 			rx_desc = I40E_RX_DESC(rx_ring, 0);
-			bi = i40e_rx_bi(rx_ring, 0);
+			bi = rx_ring->rx_bi;
 			ntu = 0;
 		}
 
@@ -1629,6 +1942,59 @@
 }
 
 /**
+ * i40e_xdp_ring_update_tail - Updates the XDP Tx ring tail register
+ * @xdp_ring: XDP Tx ring
+ *
+ * This function updates the XDP Tx ring tail register.
+ **/
+void i40e_xdp_ring_update_tail(struct i40e_ring *xdp_ring)
+{
+	/* Force memory writes to complete before letting h/w
+	 * know there are new descriptors to fetch.
+	 */
+	wmb();
+	writel_relaxed(xdp_ring->next_to_use, xdp_ring->tail);
+}
+
+#ifdef I40E_ADD_PROBES
+static void i40e_rx_extra_counters(struct i40e_vsi *vsi, u32 rx_error,
+				   const struct i40e_rx_ptype_decoded decoded)
+{
+	bool ipv4;
+
+	ipv4 = (decoded.outer_ip == I40E_RX_PTYPE_OUTER_IP) &&
+	       (decoded.outer_ip_ver == I40E_RX_PTYPE_OUTER_IPV4);
+
+	if (ipv4 &&
+	    (rx_error & (BIT(I40E_RX_DESC_ERROR_IPE_SHIFT) |
+			 BIT(I40E_RX_DESC_ERROR_EIPE_SHIFT))))
+		vsi->back->rx_ip4_cso_err++;
+
+	if (rx_error & BIT(I40E_RX_DESC_ERROR_L4E_SHIFT)) {
+		if (decoded.inner_prot == I40E_RX_PTYPE_INNER_PROT_TCP)
+			vsi->back->rx_tcp_cso_err++;
+		else if (decoded.inner_prot == I40E_RX_PTYPE_INNER_PROT_UDP)
+			vsi->back->rx_udp_cso_err++;
+		else if (decoded.inner_prot == I40E_RX_PTYPE_INNER_PROT_SCTP)
+			vsi->back->rx_sctp_cso_err++;
+	}
+
+	if ((decoded.outer_ip == I40E_RX_PTYPE_OUTER_IP) &&
+	    (decoded.outer_ip_ver == I40E_RX_PTYPE_OUTER_IPV4))
+		vsi->back->rx_ip4_cso++;
+	if (decoded.inner_prot == I40E_RX_PTYPE_INNER_PROT_TCP)
+		vsi->back->rx_tcp_cso++;
+	else if (decoded.inner_prot == I40E_RX_PTYPE_INNER_PROT_UDP)
+		vsi->back->rx_udp_cso++;
+	else if (decoded.inner_prot == I40E_RX_PTYPE_INNER_PROT_SCTP)
+		vsi->back->rx_sctp_cso++;
+}
+
+#endif /* I40E_ADD_PROBES */
+#if defined(HAVE_VXLAN_RX_OFFLOAD) || defined(HAVE_GENEVE_RX_OFFLOAD) || defined(HAVE_UDP_ENC_RX_OFFLOAD)
+#define I40E_TUNNEL_SUPPORT
+#endif
+/**
  * i40e_rx_checksum - Indicate in skb if hw indicated a good cksum
  * @vsi: the VSI we care about
  * @skb: skb currently being received and modified
@@ -1657,8 +2023,13 @@
 	skb_checksum_none_assert(skb);
 
 	/* Rx csum enabled and ip headers found? */
+#ifdef HAVE_NDO_SET_FEATURES
 	if (!(vsi->netdev->features & NETIF_F_RXCSUM))
 		return;
+#else
+	if (!(vsi->back->flags & I40E_FLAG_RX_CSUM_ENABLED))
+		return;
+#endif
 
 	/* did the hardware decode the packet and checksum? */
 	if (!(rx_status & BIT(I40E_RX_DESC_STATUS_L3L4P_SHIFT)))
@@ -1667,12 +2038,19 @@
 	/* both known and outer_ip must be set for the below code to work */
 	if (!(decoded.known && decoded.outer_ip))
 		return;
+#ifdef I40E_ADD_PROBES
+	vsi->back->hw_csum_rx_outer++;
+#endif
 
 	ipv4 = (decoded.outer_ip == I40E_RX_PTYPE_OUTER_IP) &&
 	       (decoded.outer_ip_ver == I40E_RX_PTYPE_OUTER_IPV4);
 	ipv6 = (decoded.outer_ip == I40E_RX_PTYPE_OUTER_IP) &&
 	       (decoded.outer_ip_ver == I40E_RX_PTYPE_OUTER_IPV6);
 
+#ifdef I40E_ADD_PROBES
+	i40e_rx_extra_counters(vsi, rx_error, decoded);
+
+#endif /* I40E_ADD_PROBES */
 	if (ipv4 &&
 	    (rx_error & (BIT(I40E_RX_DESC_ERROR_IPE_SHIFT) |
 			 BIT(I40E_RX_DESC_ERROR_EIPE_SHIFT))))
@@ -1695,12 +2073,18 @@
 	if (rx_error & BIT(I40E_RX_DESC_ERROR_PPRS_SHIFT))
 		return;
 
+#ifdef I40E_TUNNEL_SUPPORT
 	/* If there is an outer header present that might contain a checksum
 	 * we need to bump the checksum level by 1 to reflect the fact that
 	 * we are indicating we validated the inner checksum.
 	 */
 	if (decoded.tunnel_type >= I40E_RX_PTYPE_TUNNEL_IP_GRENAT)
+#ifdef HAVE_SKBUFF_CSUM_LEVEL
 		skb->csum_level = 1;
+#else
+		skb->encapsulation = 1;
+#endif
+#endif /* I40E_TUNNEL_SUPPORT */
 
 	/* Only report checksum unnecessary for TCP, UDP, or SCTP */
 	switch (decoded.inner_prot) {
@@ -1708,11 +2092,10 @@
 	case I40E_RX_PTYPE_INNER_PROT_UDP:
 	case I40E_RX_PTYPE_INNER_PROT_SCTP:
 		skb->ip_summed = CHECKSUM_UNNECESSARY;
-		/* fall though */
+		/* fall through */
 	default:
 		break;
 	}
-
 	return;
 
 checksum_fail:
@@ -1725,7 +2108,7 @@
  *
  * Returns a hash type to be used by skb_set_hash
  **/
-static inline int i40e_ptype_to_htype(u8 ptype)
+static inline enum pkt_hash_types i40e_ptype_to_htype(u8 ptype)
 {
 	struct i40e_rx_ptype_decoded decoded = decode_rx_desc_ptype(ptype);
 
@@ -1754,6 +2137,7 @@
 				struct sk_buff *skb,
 				u8 rx_ptype)
 {
+#ifdef NETIF_F_RXHASH
 	u32 hash;
 	const __le64 rss_mask =
 		cpu_to_le64((u64)I40E_RX_DESC_FLTSTAT_RSS_HASH <<
@@ -1766,6 +2150,7 @@
 		hash = le32_to_cpu(rx_desc->wb.qword0.hi_dword.rss);
 		skb_set_hash(skb, hash, i40e_ptype_to_htype(rx_ptype));
 	}
+#endif /* NETIF_F_RXHASH */
 }
 
 /**
@@ -1780,19 +2165,20 @@
  * other fields within the skb.
  **/
 void i40e_process_skb_fields(struct i40e_ring *rx_ring,
-			     union i40e_rx_desc *rx_desc, struct sk_buff *skb)
+			     union i40e_rx_desc *rx_desc, struct sk_buff *skb,
+			     u8 rx_ptype)
 {
+#ifdef HAVE_PTP_1588_CLOCK
 	u64 qword = le64_to_cpu(rx_desc->wb.qword1.status_error_len);
 	u32 rx_status = (qword & I40E_RXD_QW1_STATUS_MASK) >>
 			I40E_RXD_QW1_STATUS_SHIFT;
 	u32 tsynvalid = rx_status & I40E_RXD_QW1_STATUS_TSYNVALID_MASK;
 	u32 tsyn = (rx_status & I40E_RXD_QW1_STATUS_TSYNINDX_MASK) >>
 		   I40E_RXD_QW1_STATUS_TSYNINDX_SHIFT;
-	u8 rx_ptype = (qword & I40E_RXD_QW1_PTYPE_MASK) >>
-		      I40E_RXD_QW1_PTYPE_SHIFT;
 
 	if (unlikely(tsynvalid))
 		i40e_ptp_rx_hwtstamp(rx_ring->vsi->back, skb, tsyn);
+#endif /* HAVE_PTP_1588_CLOCK */
 
 	i40e_rx_hash(rx_ring, rx_desc, skb, rx_ptype);
 
@@ -1800,13 +2186,6 @@
 
 	skb_record_rx_queue(skb, rx_ring->queue_index);
 
-	if (qword & BIT(I40E_RX_DESC_STATUS_L2TAG1P_SHIFT)) {
-		u16 vlan_tag = rx_desc->wb.qword0.lo_dword.l2tag1;
-
-		__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q),
-				       le16_to_cpu(vlan_tag));
-	}
-
 	/* modifies the skb - consumes the enet header */
 	skb->protocol = eth_type_trans(skb, rx_ring->netdev);
 }
@@ -1827,7 +2206,6 @@
  **/
 static bool i40e_cleanup_headers(struct i40e_ring *rx_ring, struct sk_buff *skb,
 				 union i40e_rx_desc *rx_desc)
-
 {
 	/* XDP packets use error pointer so abort at this point */
 	if (IS_ERR(skb))
@@ -1851,6 +2229,51 @@
 	return false;
 }
 
+#ifdef CONFIG_I40E_DISABLE_PACKET_SPLIT
+/**
+ * i40e_get_rx_buffer - Fetch Rx buffer and synchronize data for use
+ * @rx_ring: rx descriptor ring to transact packets on
+ * @size: size of buffer to add to skb
+ *
+ * This function will pull an Rx buffer from the ring and synchronize it
+ * for use by the CPU.
+ *
+ * ONE-BUFF version
+ */
+static struct i40e_rx_buffer *i40e_get_rx_buffer(struct i40e_ring *rx_ring,
+						 const unsigned int size)
+{
+	struct i40e_rx_buffer *rx_buffer;
+
+	rx_buffer = &rx_ring->rx_bi[rx_ring->next_to_clean];
+
+	/* we are reusing so sync this buffer for CPU use */
+	dma_unmap_single(rx_ring->dev, rx_buffer->dma,
+			 rx_ring->rx_buf_len, DMA_FROM_DEVICE);
+
+	prefetch(rx_buffer->skb->data);
+
+	return rx_buffer;
+}
+
+/**
+ * i40e_put_rx_buffer - Clean up used buffer and either recycle or free
+ * @rx_ring: rx descriptor ring to transact packets on
+ * @rx_buffer: rx buffer to pull data from
+ *
+ * This function will clean up the contents of the rx_buffer.  It will
+ * either recycle the bufer or unmap it and free the associated resources.
+ *
+ * ONE-BUFF version
+ */
+static void i40e_put_rx_buffer(struct i40e_ring *rx_ring,
+			       struct i40e_rx_buffer *rx_buffer)
+{
+	/* clear contents of buffer_info */
+	rx_buffer->skb = NULL;
+}
+
+#else /* CONFIG_I40E_DISABLE_PACKET_SPLIT */
 /**
  * i40e_page_is_reusable - check if any reuse is possible
  * @page: page struct to check
@@ -1865,16 +2288,13 @@
 }
 
 /**
- * i40e_can_reuse_rx_page - Determine if this page can be reused by
- * the adapter for another receive
- *
+ * i40e_can_reuse_rx_page - Determine if this page can be reused
  * @rx_buffer: buffer containing the page
- * @rx_buffer_pgcnt: buffer page refcount pre xdp_do_redirect() call
  *
  * If page is reusable, rx_buffer->page_offset is adjusted to point to
  * an unused region in the page.
  *
- * For small pages, @truesize will be a constant value, half the size
+ * For small pages, arg truesize will be a constant value, half the size
  * of the memory at page.  We'll attempt to alternate between high and
  * low halves of the page, with one half ready for use by the hardware
  * and the other half being consumed by the stack.  We use the page
@@ -1883,7 +2303,7 @@
  * the page ref count is >1, we'll assume the "other" half page is
  * still busy, and this page cannot be reused.
  *
- * For larger pages, @truesize will be the actual space used by the
+ * For larger pages, arg truesize will be the actual space used by the
  * received packet (adjusted upward to an even multiple of the cache
  * line size).  This will advance through the page by the amount
  * actually consumed by the received packets while there is still
@@ -1892,8 +2312,7 @@
  *
  * In either case, if the page is reusable its refcount is increased.
  **/
-static bool i40e_can_reuse_rx_page(struct i40e_rx_buffer *rx_buffer,
-				   int rx_buffer_pgcnt)
+static bool i40e_can_reuse_rx_page(struct i40e_rx_buffer *rx_buffer)
 {
 	unsigned int pagecnt_bias = rx_buffer->pagecnt_bias;
 	struct page *page = rx_buffer->page;
@@ -1904,7 +2323,7 @@
 
 #if (PAGE_SIZE < 8192)
 	/* if we are only owner of page we can reuse it */
-	if (unlikely((rx_buffer_pgcnt - pagecnt_bias) > 1))
+	if (unlikely((page_count(page) - pagecnt_bias) > 1))
 		return false;
 #else
 #define I40E_LAST_OFFSET \
@@ -1917,10 +2336,17 @@
 	 * the pagecnt_bias and page count so that we fully restock the
 	 * number of references the driver holds.
 	 */
+#ifdef HAVE_PAGE_COUNT_BULK_UPDATE
 	if (unlikely(pagecnt_bias == 1)) {
 		page_ref_add(page, USHRT_MAX - 1);
 		rx_buffer->pagecnt_bias = USHRT_MAX;
 	}
+#else
+	if (likely(!pagecnt_bias)) {
+		get_page(page);
+		rx_buffer->pagecnt_bias = 1;
+	}
+#endif
 
 	return true;
 }
@@ -1945,7 +2371,7 @@
 #if (PAGE_SIZE < 8192)
 	unsigned int truesize = i40e_rx_pg_size(rx_ring) / 2;
 #else
-	unsigned int truesize = SKB_DATA_ALIGN(size + i40e_rx_offset(rx_ring));
+	unsigned int truesize =	SKB_DATA_ALIGN(size + i40e_rx_offset(rx_ring));
 #endif
 
 	skb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags, rx_buffer->page,
@@ -1963,25 +2389,17 @@
  * i40e_get_rx_buffer - Fetch Rx buffer and synchronize data for use
  * @rx_ring: rx descriptor ring to transact packets on
  * @size: size of buffer to add to skb
- * @rx_buffer_pgcnt: buffer page refcount
  *
  * This function will pull an Rx buffer from the ring and synchronize it
  * for use by the CPU.
  */
 static struct i40e_rx_buffer *i40e_get_rx_buffer(struct i40e_ring *rx_ring,
-						 const unsigned int size,
-						 int *rx_buffer_pgcnt)
+						 const unsigned int size)
 {
 	struct i40e_rx_buffer *rx_buffer;
 
-	rx_buffer = i40e_rx_bi(rx_ring, rx_ring->next_to_clean);
-	*rx_buffer_pgcnt =
-#if (PAGE_SIZE < 8192)
-		page_count(rx_buffer->page);
-#else
-		0;
-#endif
-	prefetch_page_address(rx_buffer->page);
+	rx_buffer = &rx_ring->rx_bi[rx_ring->next_to_clean];
+	prefetchw(rx_buffer->page);
 
 	/* we are reusing so sync this buffer for CPU use */
 	dma_sync_single_range_for_cpu(rx_ring->dev,
@@ -2010,11 +2428,13 @@
 					  struct i40e_rx_buffer *rx_buffer,
 					  struct xdp_buff *xdp)
 {
-	unsigned int size = xdp->data_end - xdp->data;
+	unsigned int size = (u8 *)xdp->data_end - (u8 *)xdp->data;
+
 #if (PAGE_SIZE < 8192)
 	unsigned int truesize = i40e_rx_pg_size(rx_ring) / 2;
 #else
-	unsigned int truesize = SKB_DATA_ALIGN(size);
+	unsigned int truesize = SKB_DATA_ALIGN(sizeof(struct skb_shared_info)) +
+				SKB_DATA_ALIGN(I40E_SKB_PAD + size);
 #endif
 	unsigned int headlen;
 	struct sk_buff *skb;
@@ -2022,23 +2442,8 @@
 	/* prefetch first cache line of first page */
 	prefetch(xdp->data);
 #if L1_CACHE_BYTES < 128
-	prefetch(xdp->data + L1_CACHE_BYTES);
+	prefetch((void *)((u8 *)xdp->data + L1_CACHE_BYTES));
 #endif
-	/* Note, we get here by enabling legacy-rx via:
-	 *
-	 *    ethtool --set-priv-flags <dev> legacy-rx on
-	 *
-	 * In this mode, we currently get 0 extra XDP headroom as
-	 * opposed to having legacy-rx off, where we process XDP
-	 * packets going to stack via i40e_build_skb(). The latter
-	 * provides us currently with 192 bytes of headroom.
-	 *
-	 * For i40e_construct_skb() mode it means that the
-	 * xdp->data_meta will always point to xdp->data, since
-	 * the helper cannot expand the head. Should this ever
-	 * change in future for legacy-rx mode on, then lets also
-	 * add xdp->data_meta handling here.
-	 */
 
 	/* allocate a skb to store the frags */
 	skb = __napi_alloc_skb(&rx_ring->q_vector->napi,
@@ -2078,10 +2483,11 @@
 	return skb;
 }
 
+#ifdef HAVE_SWIOTLB_SKIP_CPU_SYNC
 /**
  * i40e_build_skb - Build skb around an existing buffer
- * @rx_ring: Rx descriptor ring to transact packets on
- * @rx_buffer: Rx buffer to pull data from
+ * @rx_ring: rx descriptor ring to transact packets on
+ * @rx_buffer: rx buffer to pull data from
  * @xdp: xdp_buff pointing to the data
  *
  * This function builds an skb around an existing Rx buffer, taking care
@@ -2091,7 +2497,8 @@
 				      struct i40e_rx_buffer *rx_buffer,
 				      struct xdp_buff *xdp)
 {
-	unsigned int metasize = xdp->data - xdp->data_meta;
+	unsigned int size = (u8 *)xdp->data_end - (u8 *)xdp->data;
+
 #if (PAGE_SIZE < 8192)
 	unsigned int truesize = i40e_rx_pg_size(rx_ring) / 2;
 #else
@@ -2101,14 +2508,10 @@
 #endif
 	struct sk_buff *skb;
 
-	/* Prefetch first cache line of first page. If xdp->data_meta
-	 * is unused, this points exactly as xdp->data, otherwise we
-	 * likely have a consumer accessing first few bytes of meta
-	 * data, and then actual data.
-	 */
-	prefetch(xdp->data_meta);
+	/* prefetch first cache line of first page */
+	prefetch(xdp->data);
 #if L1_CACHE_BYTES < 128
-	prefetch(xdp->data_meta + L1_CACHE_BYTES);
+	prefetch(xdp->data + L1_CACHE_BYTES);
 #endif
 	/* build an skb around the page buffer */
 	skb = build_skb(xdp->data_hard_start, truesize);
@@ -2117,9 +2520,7 @@
 
 	/* update pointers within the skb to store the data */
 	skb_reserve(skb, xdp->data - xdp->data_hard_start);
-	__skb_put(skb, xdp->data_end - xdp->data);
-	if (metasize)
-		skb_metadata_set(skb, metasize);
+	__skb_put(skb, size);
 
 	/* buffer is used by skb, update page_offset */
 #if (PAGE_SIZE < 8192)
@@ -2131,22 +2532,22 @@
 	return skb;
 }
 
+#endif /* HAVE_SWIOTLB_SKIP_CPU_SYNC */
 /**
  * i40e_put_rx_buffer - Clean up used buffer and either recycle or free
  * @rx_ring: rx descriptor ring to transact packets on
  * @rx_buffer: rx buffer to pull data from
- * @rx_buffer_pgcnt: rx buffer page refcount pre xdp_do_redirect() call
  *
  * This function will clean up the contents of the rx_buffer.  It will
  * either recycle the buffer or unmap it and free the associated resources.
  */
 static void i40e_put_rx_buffer(struct i40e_ring *rx_ring,
-			       struct i40e_rx_buffer *rx_buffer,
-			       int rx_buffer_pgcnt)
+			       struct i40e_rx_buffer *rx_buffer)
 {
-	if (i40e_can_reuse_rx_page(rx_buffer, rx_buffer_pgcnt)) {
+	if (i40e_can_reuse_rx_page(rx_buffer)) {
 		/* hand second half of page back to the ring */
 		i40e_reuse_rx_page(rx_ring, rx_buffer);
+		rx_ring->rx_stats.page_reuse_count++;
 	} else {
 		/* we are not reusing the buffer so unmap it */
 		dma_unmap_page_attrs(rx_ring->dev, rx_buffer->dma,
@@ -2154,11 +2555,13 @@
 				     DMA_FROM_DEVICE, I40E_RX_DMA_ATTR);
 		__page_frag_cache_drain(rx_buffer->page,
 					rx_buffer->pagecnt_bias);
-		/* clear contents of buffer_info */
-		rx_buffer->page = NULL;
 	}
+
+	/* clear contents of buffer_info */
+	rx_buffer->page = NULL;
 }
 
+#endif /* CONFIG_I40E_DISABLE_PACKET_SPLIT */
 /**
  * i40e_is_non_eop - process handling of non-EOP buffers
  * @rx_ring: Rx ring being processed
@@ -2192,18 +2595,26 @@
 	return true;
 }
 
-static int i40e_xmit_xdp_ring(struct xdp_frame *xdpf,
+#ifdef HAVE_XDP_SUPPORT
+#ifdef HAVE_XDP_FRAME_STRUCT
+static int i40e_xmit_xdp_ring(struct xdp_frame *xdp,
 			      struct i40e_ring *xdp_ring);
 
-int i40e_xmit_xdp_tx_ring(struct xdp_buff *xdp, struct i40e_ring *xdp_ring)
+int i40e_xmit_xdp_tx_ring(struct xdp_buff *xdp,
+			  struct i40e_ring *xdp_ring)
 {
-	struct xdp_frame *xdpf = convert_to_xdp_frame(xdp);
+	struct xdp_frame *xdpf = xdp_convert_buff_to_frame(xdp);
 
 	if (unlikely(!xdpf))
 		return I40E_XDP_CONSUMED;
 
 	return i40e_xmit_xdp_ring(xdpf, xdp_ring);
 }
+#else
+static int i40e_xmit_xdp_ring(struct xdp_buff *xdp,
+			      struct i40e_ring *xdp_ring);
+#endif
+#endif
 
 /**
  * i40e_run_xdp - run an XDP program
@@ -2213,10 +2624,12 @@
 static struct sk_buff *i40e_run_xdp(struct i40e_ring *rx_ring,
 				    struct xdp_buff *xdp)
 {
-	int err, result = I40E_XDP_PASS;
+	int result = I40E_XDP_PASS;
+#ifdef HAVE_XDP_SUPPORT
 	struct i40e_ring *xdp_ring;
 	struct bpf_prog *xdp_prog;
 	u32 act;
+	int err;
 
 	rcu_read_lock();
 	xdp_prog = READ_ONCE(rx_ring->xdp_prog);
@@ -2229,28 +2642,41 @@
 	act = bpf_prog_run_xdp(xdp_prog, xdp);
 	switch (act) {
 	case XDP_PASS:
+		rx_ring->xdp_stats.xdp_pass++;
 		break;
 	case XDP_TX:
 		xdp_ring = rx_ring->vsi->xdp_rings[rx_ring->queue_index];
+#ifdef HAVE_XDP_FRAME_STRUCT
 		result = i40e_xmit_xdp_tx_ring(xdp, xdp_ring);
+#else
+		result = i40e_xmit_xdp_ring(xdp, xdp_ring);
+#endif
+		rx_ring->xdp_stats.xdp_tx++;
 		break;
 	case XDP_REDIRECT:
 		err = xdp_do_redirect(rx_ring->netdev, xdp, xdp_prog);
 		result = !err ? I40E_XDP_REDIR : I40E_XDP_CONSUMED;
+		if (!err)
+			rx_ring->xdp_stats.xdp_redirect++;
+		else
+			rx_ring->xdp_stats.xdp_redirect_fail++;
 		break;
 	default:
 		bpf_warn_invalid_xdp_action(act);
-		/* fall through */
+		/* fallthrough -- abort and drop */
 	case XDP_ABORTED:
 		trace_xdp_exception(rx_ring->netdev, xdp_prog, act);
-		/* fall through -- handle aborts by dropping packet */
+		rx_ring->xdp_stats.xdp_unknown++;
+		/* fallthrough -- handle aborts by dropping packet */
 	case XDP_DROP:
 		result = I40E_XDP_CONSUMED;
+		rx_ring->xdp_stats.xdp_drop++;
 		break;
 	}
 xdp_out:
 	rcu_read_unlock();
-	return ERR_PTR(-result);
+#endif /* HAVE_XDP_SUPPORT */
+	return (struct sk_buff *)ERR_PTR(-result);
 }
 
 /**
@@ -2263,33 +2689,24 @@
 				struct i40e_rx_buffer *rx_buffer,
 				unsigned int size)
 {
+#ifdef HAVE_XDP_BUFF_FRAME_SZ
+	unsigned int truesize = i40e_rx_frame_truesize(rx_ring, size);
+#else /* HAVE_XDP_BUFF_FRAME_SZ */
 #if (PAGE_SIZE < 8192)
 	unsigned int truesize = i40e_rx_pg_size(rx_ring) / 2;
-
-	rx_buffer->page_offset ^= truesize;
 #else
 	unsigned int truesize = SKB_DATA_ALIGN(i40e_rx_offset(rx_ring) + size);
+#endif
+#endif /* HAVE_XDP_BUFF_FRAME_SZ */
 
+#if (PAGE_SIZE < 8192)
+	rx_buffer->page_offset ^= truesize;
+#else
 	rx_buffer->page_offset += truesize;
 #endif
 }
 
 /**
- * i40e_xdp_ring_update_tail - Updates the XDP Tx ring tail register
- * @xdp_ring: XDP Tx ring
- *
- * This function updates the XDP Tx ring tail register.
- **/
-void i40e_xdp_ring_update_tail(struct i40e_ring *xdp_ring)
-{
-	/* Force memory writes to complete before letting h/w
-	 * know there are new descriptors to fetch.
-	 */
-	wmb();
-	writel_relaxed(xdp_ring->next_to_use, xdp_ring->tail);
-}
-
-/**
  * i40e_update_rx_stats - Update Rx ring statistics
  * @rx_ring: rx descriptor ring
  * @total_rx_bytes: number of bytes received
@@ -2318,7 +2735,8 @@
  * should be called when a batch of packets has been processed in the
  * napi loop.
  **/
-void i40e_finalize_xdp_rx(struct i40e_ring *rx_ring, unsigned int xdp_res)
+void i40e_finalize_xdp_rx(struct i40e_ring *rx_ring,
+			  unsigned int xdp_res)
 {
 	if (xdp_res & I40E_XDP_REDIR)
 		xdp_do_flush_map();
@@ -2351,14 +2769,28 @@
 	unsigned int xdp_xmit = 0;
 	bool failure = false;
 	struct xdp_buff xdp;
+	u16 tpid;
 
+#ifdef HAVE_XDP_BUFF_FRAME_SZ
+#if (PAGE_SIZE < 8192)
+	xdp.frame_sz = i40e_rx_frame_truesize(rx_ring, 0);
+#endif
+#endif /* HAVE_XDP_BUFF_FRAME_SZ */
+
+#ifdef HAVE_XDP_BUFF_RXQ
 	xdp.rxq = &rx_ring->xdp_rxq;
+#endif /* HAVE_XDP_BUFF_RXQ */
+	tpid = rx_ring->vsi->back->hw.second_tag;
+
+	if (i40e_is_double_vlan(&rx_ring->vsi->back->hw))
+		tpid = rx_ring->vsi->back->hw.first_tag;
 
 	while (likely(total_rx_packets < (unsigned int)budget)) {
 		struct i40e_rx_buffer *rx_buffer;
 		union i40e_rx_desc *rx_desc;
-		int rx_buffer_pgcnt;
 		unsigned int size;
+		u16 vlan_tag;
+		u8 rx_ptype;
 		u64 qword;
 
 		/* return some buffers to hardware, one at a time is too slow */
@@ -2382,11 +2814,14 @@
 		 * verified the descriptor has been written back.
 		 */
 		dma_rmb();
-
 		rx_buffer = i40e_clean_programming_status(rx_ring, rx_desc,
 							  qword);
 		if (unlikely(rx_buffer)) {
+#ifdef CONFIG_I40E_DISABLE_PACKET_SPLIT
+			i40e_reuse_rx_skb(rx_ring, rx_buffer);
+#else
 			i40e_reuse_rx_page(rx_ring, rx_buffer);
+#endif /* CONFIG_I40E_DISABLE_PACKET_SPLIT */
 			cleaned_count++;
 			continue;
 		}
@@ -2397,17 +2832,30 @@
 			break;
 
 		i40e_trace(clean_rx_irq, rx_ring, rx_desc, skb);
-		rx_buffer = i40e_get_rx_buffer(rx_ring, size, &rx_buffer_pgcnt);
+		rx_buffer = i40e_get_rx_buffer(rx_ring, size);
 
 		/* retrieve a buffer from the ring */
+#ifdef CONFIG_I40E_DISABLE_PACKET_SPLIT
+		/* we are leaking memory if an skb is already present */
+		WARN_ON(skb);
+		skb = rx_buffer->skb;
+		__skb_put(skb, size);
+#else
 		if (!skb) {
 			xdp.data = page_address(rx_buffer->page) +
 				   rx_buffer->page_offset;
-			xdp.data_meta = xdp.data;
-			xdp.data_hard_start = xdp.data -
-					      i40e_rx_offset(rx_ring);
-			xdp.data_end = xdp.data + size;
-
+#ifdef HAVE_XDP_BUFF_DATA_META
+			xdp_set_data_meta_invalid(&xdp);
+#endif
+			xdp.data_hard_start = (void *)((u8 *)xdp.data -
+					      i40e_rx_offset(rx_ring));
+			xdp.data_end = (void *)((u8 *)xdp.data + size);
+#ifdef HAVE_XDP_BUFF_FRAME_SZ
+#if (PAGE_SIZE > 4096)
+			/* At larger PAGE_SIZE, frame_sz depend on len size */
+			xdp.frame_sz = i40e_rx_frame_truesize(rx_ring, size);
+#endif
+#endif /* HAVE_XDP_BUFF_FRAME_SZ */
 			skb = i40e_run_xdp(rx_ring, &xdp);
 		}
 
@@ -2424,8 +2872,10 @@
 			total_rx_packets++;
 		} else if (skb) {
 			i40e_add_rx_frag(rx_ring, rx_buffer, skb, size);
+#ifdef HAVE_SWIOTLB_SKIP_CPU_SYNC
 		} else if (ring_uses_build_skb(rx_ring)) {
 			skb = i40e_build_skb(rx_ring, rx_buffer, &xdp);
+#endif
 		} else {
 			skb = i40e_construct_skb(rx_ring, rx_buffer, &xdp);
 		}
@@ -2436,8 +2886,9 @@
 			rx_buffer->pagecnt_bias++;
 			break;
 		}
+#endif /* CONFIG_I40E_DISABLE_PACKET_SPLIT */
 
-		i40e_put_rx_buffer(rx_ring, rx_buffer, rx_buffer_pgcnt);
+		i40e_put_rx_buffer(rx_ring, rx_buffer);
 		cleaned_count++;
 
 		if (i40e_is_non_eop(rx_ring, rx_desc, skb))
@@ -2451,11 +2902,18 @@
 		/* probably a little skewed due to removing CRC */
 		total_rx_bytes += skb->len;
 
+		qword = le64_to_cpu(rx_desc->wb.qword1.status_error_len);
+		rx_ptype = (qword & I40E_RXD_QW1_PTYPE_MASK) >>
+			   I40E_RXD_QW1_PTYPE_SHIFT;
+
 		/* populate checksum, VLAN, and protocol */
-		i40e_process_skb_fields(rx_ring, rx_desc, skb);
+		i40e_process_skb_fields(rx_ring, rx_desc, skb, rx_ptype);
+
+		vlan_tag = (qword & BIT(I40E_RX_DESC_STATUS_L2TAG1P_SHIFT)) ?
+			   le16_to_cpu(rx_desc->wb.qword0.lo_dword.l2tag1) : 0;
 
 		i40e_trace(clean_rx_irq_rx, rx_ring, rx_desc, skb);
-		napi_gro_receive(&rx_ring->q_vector->napi, skb);
+		i40e_receive_skb(rx_ring, skb, vlan_tag, tpid);
 		skb = NULL;
 
 		/* update budget accounting */
@@ -2589,6 +3047,7 @@
 			       container_of(napi, struct i40e_q_vector, napi);
 	struct i40e_vsi *vsi = q_vector->vsi;
 	struct i40e_ring *ring;
+	u64 flags = vsi->back->flags;
 	bool clean_complete = true;
 	bool arm_wb = false;
 	int budget_per_ring;
@@ -2603,11 +3062,7 @@
 	 * budget and be more aggressive about cleaning up the Tx descriptors.
 	 */
 	i40e_for_each_ring(ring, q_vector->tx) {
-		bool wd = ring->xsk_umem ?
-			  i40e_clean_xdp_tx_irq(vsi, ring, budget) :
-			  i40e_clean_tx_irq(vsi, ring, budget);
-
-		if (!wd) {
+		if (!i40e_clean_tx_irq(vsi, ring, budget)) {
 			clean_complete = false;
 			continue;
 		}
@@ -2625,9 +3080,7 @@
 	budget_per_ring = max(budget/q_vector->num_ringpairs, 1);
 
 	i40e_for_each_ring(ring, q_vector->rx) {
-		int cleaned = ring->xsk_umem ?
-			      i40e_clean_rx_irq_zc(ring, budget_per_ring) :
-			      i40e_clean_rx_irq(ring, budget_per_ring);
+		int cleaned = i40e_clean_rx_irq(ring, budget_per_ring);
 
 		work_done += cleaned;
 		/* if we clean as many as budgeted, we must not be done */
@@ -2635,8 +3088,15 @@
 			clean_complete = false;
 	}
 
+#ifndef HAVE_NETDEV_NAPI_LIST
+	/* if netdev is disabled we need to stop polling */
+	if (!netif_running(vsi->netdev))
+		clean_complete = true;
+
+#endif
 	/* If work not completed, return budget and polling will return */
 	if (!clean_complete) {
+#ifdef HAVE_IRQ_AFFINITY_NOTIFY
 		int cpu_id = smp_processor_id();
 
 		/* It is possible that the interrupt affinity has changed but,
@@ -2656,6 +3116,7 @@
 			/* Return budget-1 so that polling stops */
 			return budget - 1;
 		}
+#endif /* HAVE_IRQ_AFFINITY_NOTIFY */
 tx_only:
 		if (arm_wb) {
 			q_vector->tx.ring[0].tx_stats.tx_force_wb++;
@@ -2664,14 +3125,13 @@
 		return budget;
 	}
 
-	if (vsi->back->flags & I40E_TXR_FLAGS_WB_ON_ITR)
+	if (flags & I40E_TXR_FLAGS_WB_ON_ITR)
 		q_vector->arm_wb_state = false;
 
-	/* Exit the polling mode, but don't re-enable interrupts if stack might
-	 * poll us due to busy-polling
-	 */
-	if (likely(napi_complete_done(napi, work_done)))
-		i40e_update_enable_itr(vsi, q_vector);
+	/* Work is done so exit the polling mode and re-enable the interrupt */
+	napi_complete_done(napi, work_done);
+
+	i40e_update_enable_itr(vsi, q_vector);
 
 	return min(work_done, budget - 1);
 }
@@ -2714,8 +3174,12 @@
 		return;
 
 	/* snag network header to get L4 type and address */
-	hdr.network = (tx_flags & I40E_TX_FLAGS_UDP_TUNNEL) ?
+#ifdef HAVE_SKB_INNER_NETWORK_HEADER
+	hdr.network = (tx_flags & I40E_TX_FLAGS_TUNNEL) ?
 		      skb_inner_network_header(skb) : skb_network_header(skb);
+#else
+	hdr.network = skb_network_header(skb);
+#endif /* HAVE_SKB_INNER_NETWORK_HEADER */
 
 	/* Note: tx_flags gets modified to reflect inner protocols in
 	 * tx_enable_csum function if encap is enabled.
@@ -2730,8 +3194,7 @@
 		unsigned int h_offset = inner_hlen;
 
 		/* this function updates h_offset to the end of the header */
-		l4_proto =
-		  ipv6_find_hdr(skb, &h_offset, IPPROTO_TCP, NULL, NULL);
+		l4_proto = ipv6_find_hdr(skb, &h_offset, IPPROTO_TCP, NULL, NULL);
 		/* hlen will contain our best estimate of the tcp header */
 		hlen = h_offset - inner_hlen;
 	}
@@ -2744,6 +3207,7 @@
 	/* Due to lack of space, no more new filters can be programmed */
 	if (th->syn && test_bit(__I40E_FD_ATR_AUTO_DISABLED, pf->state))
 		return;
+
 	if (pf->flags & I40E_FLAG_HW_ATR_EVICT_ENABLED) {
 		/* HW ATR eviction will take care of removing filters on FIN
 		 * and RST packets.
@@ -2795,7 +3259,7 @@
 		     I40E_TXD_FLTR_QW1_FD_STATUS_SHIFT;
 
 	dtype_cmd |= I40E_TXD_FLTR_QW1_CNT_ENA_MASK;
-	if (!(tx_flags & I40E_TX_FLAGS_UDP_TUNNEL))
+	if (!(tx_flags & I40E_TX_FLAGS_TUNNEL))
 		dtype_cmd |=
 			((u32)I40E_FD_ATR_STAT_IDX(pf->hw.pf_id) <<
 			I40E_TXD_FLTR_QW1_CNTINDEX_SHIFT) &
@@ -2817,9 +3281,10 @@
 
 /**
  * i40e_tx_prepare_vlan_flags - prepare generic TX VLAN tagging flags for HW
- * @skb:     send buffer
- * @tx_ring: ring to send buffer on
- * @flags:   the tx flags to be set
+ * @skb:        send buffer
+ * @tx_ring:    ring to send buffer on
+ * @flags:      the tx flags to be set
+ * @outer_vlan: VLAN tag returned in case of double vlan
  *
  * Checks the skb and set up correspondingly several generic transmit flags
  * related to VLAN tagging for the HW, such as VLAN, DCB, etc.
@@ -2829,13 +3294,19 @@
  **/
 static inline int i40e_tx_prepare_vlan_flags(struct sk_buff *skb,
 					     struct i40e_ring *tx_ring,
-					     u32 *flags)
+					     u32 *flags,
+					     u32 *outer_vlan)
 {
 	__be16 protocol = skb->protocol;
 	u32  tx_flags = 0;
 
+#ifdef NETIF_F_HW_VLAN_CTAG_RX
 	if (protocol == htons(ETH_P_8021Q) &&
 	    !(tx_ring->netdev->features & NETIF_F_HW_VLAN_CTAG_TX)) {
+#else
+	if (protocol == htons(ETH_P_8021Q) &&
+	    !(tx_ring->netdev->features & NETIF_F_HW_VLAN_TX)) {
+#endif
 		/* When HW VLAN acceleration is turned off by the user the
 		 * stack sets the protocol to 8021q so that the driver
 		 * can take any steps required to support the SW only
@@ -2849,8 +3320,15 @@
 
 	/* if we have a HW VLAN tag being added, default to the HW one */
 	if (skb_vlan_tag_present(skb)) {
-		tx_flags |= skb_vlan_tag_get(skb) << I40E_TX_FLAGS_VLAN_SHIFT;
-		tx_flags |= I40E_TX_FLAGS_HW_VLAN;
+		/* Only offload on outer, if there is a vlan header in skb */
+		if (i40e_is_double_vlan(&tx_ring->vsi->back->hw) &&
+		    eth_type_vlan(skb->protocol)) {
+			*outer_vlan = skb_vlan_tag_get(skb);
+		} else {
+			tx_flags |= skb_vlan_tag_get(skb) <<
+				I40E_TX_FLAGS_VLAN_SHIFT;
+			tx_flags |= I40E_TX_FLAGS_HW_VLAN;
+		}
 	/* else if it is a SW VLAN, check the next protocol and store the tag */
 	} else if (protocol == htons(ETH_P_8021Q)) {
 		struct vlan_hdr *vhdr, _vhdr;
@@ -2893,6 +3371,14 @@
 	return 0;
 }
 
+#ifndef HAVE_ENCAP_TSO_OFFLOAD
+#define inner_ip_hdr(skb) 0
+#define inner_tcp_hdr(skb) 0
+#define inner_ipv6_hdr(skb) 0
+#define inner_tcp_hdrlen(skb) 0
+#define inner_tcp_hdrlen(skb) 0
+#define skb_inner_transport_header(skb) ((skb)->data)
+#endif /* HAVE_ENCAP_TSO_OFFLOAD */
 /**
  * i40e_tso - set up the tso context descriptor
  * @first:    pointer to first Tx buffer for xmit
@@ -2941,14 +3427,32 @@
 		ip.v6->payload_len = 0;
 	}
 
+#ifdef HAVE_ENCAP_TSO_OFFLOAD
 	if (skb_shinfo(skb)->gso_type & (SKB_GSO_GRE |
+#ifdef NETIF_F_GSO_PARTIAL
 					 SKB_GSO_GRE_CSUM |
+#endif
+#ifdef NETIF_F_GSO_IPXIP4
 					 SKB_GSO_IPXIP4 |
+#ifdef NETIF_F_GSO_IPXIP6
 					 SKB_GSO_IPXIP6 |
+#endif
+#else
+#ifdef NETIF_F_GSO_IPIP
+					 SKB_GSO_IPIP |
+#endif
+#ifdef NETIF_F_GSO_SIT
+					 SKB_GSO_SIT |
+#endif
+#endif
 					 SKB_GSO_UDP_TUNNEL |
 					 SKB_GSO_UDP_TUNNEL_CSUM)) {
+#ifndef NETIF_F_GSO_PARTIAL
+		if (skb_shinfo(skb)->gso_type & SKB_GSO_UDP_TUNNEL_CSUM) {
+#else
 		if (!(skb_shinfo(skb)->gso_type & SKB_GSO_PARTIAL) &&
 		    (skb_shinfo(skb)->gso_type & SKB_GSO_UDP_TUNNEL_CSUM)) {
+#endif
 			l4.udp->len = 0;
 
 			/* determine offset of outer transport header */
@@ -2973,6 +3477,7 @@
 		}
 	}
 
+#endif /* HAVE_ENCAP_TSO_OFFLOAD */
 	/* determine offset of inner transport header */
 	l4_offset = l4.hdr - skb->data;
 
@@ -2987,7 +3492,14 @@
 	gso_size = skb_shinfo(skb)->gso_size;
 	gso_segs = skb_shinfo(skb)->gso_segs;
 
-	/* update GSO size and bytecount with header size */
+#ifndef HAVE_NDO_FEATURES_CHECK
+	/* too small a TSO segment size causes problems */
+	if (gso_size < 64) {
+		gso_size = 64;
+		gso_segs = DIV_ROUND_UP(skb->len - *hdr_len, 64);
+	}
+#endif
+	/* update gso size and bytecount with header size */
 	first->gso_segs = gso_segs;
 	first->bytecount += (first->gso_segs - 1) * *hdr_len;
 
@@ -3001,6 +3513,7 @@
 	return 1;
 }
 
+#ifdef HAVE_PTP_1588_CLOCK
 /**
  * i40e_tsyn - set up the tsyn context descriptor
  * @tx_ring:  ptr to the ring to send
@@ -3015,7 +3528,11 @@
 {
 	struct i40e_pf *pf;
 
+#ifdef SKB_SHARED_TX_IS_UNION
+	if (likely(!(skb_tx(skb)->hardware)))
+#else
 	if (likely(!(skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP)))
+#endif
 		return 0;
 
 	/* Tx timestamps cannot be sampled when doing TSO */
@@ -3031,7 +3548,11 @@
 
 	if (pf->ptp_tx &&
 	    !test_and_set_bit_lock(__I40E_PTP_TX_IN_PROGRESS, pf->state)) {
+#ifdef SKB_SHARED_TX_IS_UNION
+		skb_tx(skb)->in_progress = 1;
+#else
 		skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
+#endif
 		pf->ptp_tx_start = jiffies;
 		pf->ptp_tx_skb = skb_get(skb);
 	} else {
@@ -3045,6 +3566,7 @@
 	return 1;
 }
 
+#endif /* HAVE_PTP_1588_CLOCK */
 /**
  * i40e_tx_enable_csum - Enable Tx checksum offloads
  * @skb: send buffer
@@ -3083,6 +3605,7 @@
 	/* compute outer L2 header size */
 	offset = ((ip.hdr - skb->data) / 2) << I40E_TX_DESC_LENGTH_MACLEN_SHIFT;
 
+#ifdef HAVE_ENCAP_CSUM_OFFLOAD
 	if (skb->encapsulation) {
 		u32 tunnel = 0;
 		/* define outer network header type */
@@ -3093,30 +3616,45 @@
 
 			l4_proto = ip.v4->protocol;
 		} else if (*tx_flags & I40E_TX_FLAGS_IPV6) {
+			int ret;
+
 			tunnel |= I40E_TX_CTX_EXT_IP_IPV6;
 
 			exthdr = ip.hdr + sizeof(*ip.v6);
 			l4_proto = ip.v6->nexthdr;
-			if (l4.hdr != exthdr)
-				ipv6_skip_exthdr(skb, exthdr - skb->data,
-						 &l4_proto, &frag_off);
+			ret = ipv6_skip_exthdr(skb, exthdr - skb->data,
+					       &l4_proto, &frag_off);
+			if (ret < 0)
+				return -1;
 		}
 
 		/* define outer transport */
 		switch (l4_proto) {
 		case IPPROTO_UDP:
 			tunnel |= I40E_TXD_CTX_UDP_TUNNELING;
-			*tx_flags |= I40E_TX_FLAGS_UDP_TUNNEL;
+			*tx_flags |= I40E_TX_FLAGS_TUNNEL;
 			break;
+#ifdef HAVE_GRE_ENCAP_OFFLOAD
 		case IPPROTO_GRE:
 			tunnel |= I40E_TXD_CTX_GRE_TUNNELING;
-			*tx_flags |= I40E_TX_FLAGS_UDP_TUNNEL;
+			*tx_flags |= I40E_TX_FLAGS_TUNNEL;
+			/* There was a long-standing issue in GRE where GSO
+			 * was not setting the outer transport header unless
+			 * a GRE checksum was requested. This was fixed in
+			 * the 4.6 version of the kernel.  In the 4.7 kernel
+			 * support for GRE over IPv6 was added to GSO.  So we
+			 * can assume this workaround for all IPv4 headers
+			 * without impacting later versions of the GRE.
+			 */
+			if (ip.v4->version == 4)
+				l4.hdr = ip.hdr + (ip.v4->ihl * 4);
 			break;
 		case IPPROTO_IPIP:
 		case IPPROTO_IPV6:
-			*tx_flags |= I40E_TX_FLAGS_UDP_TUNNEL;
+			*tx_flags |= I40E_TX_FLAGS_TUNNEL;
 			l4.hdr = skb_inner_network_header(skb);
 			break;
+#endif
 		default:
 			if (*tx_flags & I40E_TX_FLAGS_TSO)
 				return -1;
@@ -3125,6 +3663,11 @@
 			return 0;
 		}
 
+#ifdef I40E_ADD_PROBES
+		if (*tx_flags & I40E_TX_FLAGS_IPV4)
+			if (*tx_flags & I40E_TX_FLAGS_TSO)
+				tx_ring->vsi->back->tx_ip4_cso++;
+#endif
 		/* compute outer L3 header size */
 		tunnel |= ((l4.hdr - ip.hdr) / 4) <<
 			  I40E_TXD_CTX_QW0_EXT_IPLEN_SHIFT;
@@ -3138,7 +3681,9 @@
 
 		/* indicate if we need to offload outer UDP header */
 		if ((*tx_flags & I40E_TX_FLAGS_TSO) &&
+#ifdef NETIF_F_GSO_PARTIAL
 		    !(skb_shinfo(skb)->gso_type & SKB_GSO_PARTIAL) &&
+#endif
 		    (skb_shinfo(skb)->gso_type & SKB_GSO_UDP_TUNNEL_CSUM))
 			tunnel |= I40E_TXD_CTX_QW0_L4T_CS_MASK;
 
@@ -3156,16 +3701,22 @@
 		if (ip.v6->version == 6)
 			*tx_flags |= I40E_TX_FLAGS_IPV6;
 	}
+#endif /* HAVE_ENCAP_CSUM_OFFLOAD */
 
 	/* Enable IP checksum offloads */
 	if (*tx_flags & I40E_TX_FLAGS_IPV4) {
 		l4_proto = ip.v4->protocol;
+#ifdef I40E_ADD_PROBES
+		if (*tx_flags & I40E_TX_FLAGS_TSO)
+			tx_ring->vsi->back->tx_ip4_cso++;
+#endif
 		/* the stack computes the IP header already, the only time we
 		 * need the hardware to recompute it is in the case of TSO.
 		 */
 		cmd |= (*tx_flags & I40E_TX_FLAGS_TSO) ?
 		       I40E_TX_DESC_CMD_IIPT_IPV4_CSUM :
 		       I40E_TX_DESC_CMD_IIPT_IPV4;
+#ifdef NETIF_F_IPV6_CSUM
 	} else if (*tx_flags & I40E_TX_FLAGS_IPV6) {
 		cmd |= I40E_TX_DESC_CMD_IIPT_IPV6;
 
@@ -3174,6 +3725,7 @@
 		if (l4.hdr != exthdr)
 			ipv6_skip_exthdr(skb, exthdr - skb->data,
 					 &l4_proto, &frag_off);
+#endif
 	}
 
 	/* compute inner L3 header size */
@@ -3185,18 +3737,29 @@
 		/* enable checksum offloads */
 		cmd |= I40E_TX_DESC_CMD_L4T_EOFT_TCP;
 		offset |= l4.tcp->doff << I40E_TX_DESC_LENGTH_L4_FC_LEN_SHIFT;
+#ifdef I40E_ADD_PROBES
+		tx_ring->vsi->back->tx_tcp_cso++;
+#endif
 		break;
 	case IPPROTO_SCTP:
 		/* enable SCTP checksum offload */
+#ifdef HAVE_SCTP
 		cmd |= I40E_TX_DESC_CMD_L4T_EOFT_SCTP;
 		offset |= (sizeof(struct sctphdr) >> 2) <<
 			  I40E_TX_DESC_LENGTH_L4_FC_LEN_SHIFT;
+#ifdef I40E_ADD_PROBES
+			tx_ring->vsi->back->tx_sctp_cso++;
+#endif
+#endif /* HAVE_SCTP */
 		break;
 	case IPPROTO_UDP:
 		/* enable UDP checksum offload */
 		cmd |= I40E_TX_DESC_CMD_L4T_EOFT_UDP;
 		offset |= (sizeof(struct udphdr) >> 2) <<
 			  I40E_TX_DESC_LENGTH_L4_FC_LEN_SHIFT;
+#ifdef I40E_ADD_PROBES
+			tx_ring->vsi->back->tx_udp_cso++;
+#endif
 		break;
 	default:
 		if (*tx_flags & I40E_TX_FLAGS_TSO)
@@ -3212,7 +3775,7 @@
 }
 
 /**
- * i40e_create_tx_ctx Build the Tx context descriptor
+ * i40e_create_tx_ctx - Build the Tx context descriptor
  * @tx_ring:  ring to create the descriptor on
  * @cd_type_cmd_tso_mss: Quad Word 1
  * @cd_tunneling: Quad Word 0 - bits 0-31
@@ -3240,6 +3803,9 @@
 	context_desc->l2tag2 = cpu_to_le16(cd_l2tag2);
 	context_desc->rsvd = cpu_to_le16(0);
 	context_desc->type_cmd_tso_mss = cpu_to_le64(cd_type_cmd_tso_mss);
+	if (context_desc->l2tag2)
+		context_desc->type_cmd_tso_mss |= I40E_TX_CTX_DESC_IL2TAG2 <<
+			I40E_TXD_CTX_QW1_CMD_SHIFT;
 }
 
 /**
@@ -3359,7 +3925,7 @@
  * @td_cmd:   the command field in the descriptor
  * @td_offset: offset for checksum or crc
  *
- * Returns 0 on success, -1 on failure to DMA
+ * Returns 0 on success, negative error code on DMA failure.
  **/
 static inline int i40e_tx_map(struct i40e_ring *tx_ring, struct sk_buff *skb,
 			      struct i40e_tx_buffer *first, u32 tx_flags,
@@ -3381,6 +3947,11 @@
 			 I40E_TX_FLAGS_VLAN_SHIFT;
 	}
 
+#ifdef I40E_ADD_PROBES
+	if (tx_flags & (I40E_TX_FLAGS_TSO | I40E_TX_FLAGS_FSO))
+		tx_ring->vsi->back->tcp_segs += first->gso_segs;
+
+#endif
 	first->tx_flags = tx_flags;
 
 	dma = dma_map_single(tx_ring->dev, skb->data, size, DMA_TO_DEVICE);
@@ -3474,6 +4045,9 @@
 	tx_desc->cmd_type_offset_bsz =
 			build_ctob(td_cmd, td_offset, size, td_tag);
 
+	/* timestamp the skb as late as possible, just prior to notifying
+	 * the MAC that it should transmit this packet
+	 */
 	skb_tx_timestamp(skb);
 
 	/* Force memory writes to complete before letting h/w know there
@@ -3488,9 +4062,37 @@
 	first->next_to_watch = tx_desc;
 
 	/* notify HW of packet */
+#ifdef HAVE_SKB_XMIT_MORE
 	if (netif_xmit_stopped(txring_txq(tx_ring)) || !netdev_xmit_more()) {
 		writel(i, tx_ring->tail);
+
+#ifndef SPIN_UNLOCK_IMPLIES_MMIOWB
+		/* We need this mmiowb on IA64/Altix systems where wmb() isn't
+		 * guaranteed to synchronize I/O.
+		 *
+		 * Note that mmiowb() only provides a guarantee about ordering
+		 * when in conjunction with a spin_unlock(). This barrier is
+		 * used to guarantee the I/O ordering with respect to a spin
+		 * lock in the networking core code.
+		 */
+		mmiowb();
+#endif
 	}
+#else
+	writel(i, tx_ring->tail);
+
+#ifndef SPIN_UNLOCK_IMPLIES_MMIOWB
+	/* We need this mmiowb on IA64/Altix systems where wmb() isn't
+	 * guaranteed to synchronize I/O.
+	 *
+	 * Note that mmiowb() only provides a guarantee about ordering when in
+	 * conjunction with a spin_unlock(). This barrier is used to guarantee
+	 * the I/O ordering with respect to a spin lock in the networking core
+	 * code.
+	 */
+	mmiowb();
+#endif
+#endif /* HAVE_XMIT_MORE */
 
 	return 0;
 
@@ -3510,23 +4112,170 @@
 
 	tx_ring->next_to_use = i;
 
-	return -1;
+	return -EIO;
 }
 
+#ifdef HAVE_NETDEV_SELECT_QUEUE
+static u16 i40e_swdcb_skb_tx_hash(struct net_device *dev,
+				  const struct sk_buff *skb,
+				  u16 num_tx_queues)
+{
+	u32 jhash_initval_salt = 0xd631614b; /* same as in COMPAT */
+	u32 hash;
+
+	if (skb->sk && skb->sk->sk_hash)
+		hash = skb->sk->sk_hash;
+	else
+#ifdef NETIF_F_RXHASH
+#ifdef HAVE_SKBUFF_RXHASH
+		hash = (__force u16)skb->protocol ^ skb->rxhash;
+#else /* HAVE_SKBUFF_RXHASH */
+		hash = (__force u16)skb->protocol ^ skb->hash;
+#endif /* HAVE_SKBUFF_RXHASH */
+#else /* NETIF_F_RXHASH */
+		hash = skb->protocol;
+#endif /* NETIF_F_RXHASH */
+
+	hash = jhash_1word(hash, jhash_initval_salt);
+
+	return (u16)(((u64)hash * num_tx_queues) >> 32);
+}
+
+#ifndef HAVE_NDO_SELECT_QUEUE_SB_DEV
+#if defined(HAVE_NDO_SELECT_QUEUE_ACCEL) || \
+defined(HAVE_NDO_SELECT_QUEUE_ACCEL_FALLBACK)
+#ifndef HAVE_NDO_SELECT_QUEUE_FALLBACK_REMOVED
+/**
+ * i40e_lan_select_queue - Select the right Tx queue for the skb for LAN VSI
+ * @netdev: network interface device structure
+ * @skb: send buffer
+ * @accel_priv: unused
+ * @fallback: unused
+ *
+ * Returns the index of the selected Tx queue
+ **/
+u16 i40e_lan_select_queue(struct net_device *netdev,
+			  struct sk_buff *skb,
+			  void __always_unused *accel_priv,
+			  select_queue_fallback_t __always_unused fallback)
+{
+#else /* HAVE_NDO_SELECT_QUEUE_FALLBACK_REMOVED */
+/**
+ * i40e_lan_select_queue - Select the right Tx queue for the skb for LAN VSI
+ * @netdev: network interface device structure
+ * @skb: send buffer
+ * @accel_priv: unused
+ *
+ * Returns the index of the selected Tx queue
+ **/
+u16 i40e_lan_select_queue(struct net_device *netdev,
+			  struct sk_buff *skb,
+			  void __always_unused *accel_priv)
+{
+#endif /* HAVE_NDO_SELECT_QUEUE_FALLBACK_REMOVED */
+#else /* HAVE_NDO_SELECT_QUEUE_ACCEL || HAVE_NDO_SELECT_QUEUE_ACCEL_FALLBACK */
+/**
+ * i40e_lan_select_queue - Select the right Tx queue for the skb for LAN VSI
+ * @netdev: network interface device structure
+ * @skb: send buffer
+ *
+ * Returns the index of the selected Tx queue
+ **/
+u16 i40e_lan_select_queue(struct net_device *netdev,
+			  struct sk_buff *skb)
+{
+#endif /*HAVE_NDO_SELECT_QUEUE_ACCEL || HAVE_NDO_SELECT_QUEUE_ACCEL_FALLBACK */
+#else /* HAVE_NDO_SELECT_QUEUE_SB_DEV */
+#ifdef HAVE_NDO_SELECT_QUEUE_FALLBACK_REMOVED
+/**
+ * i40e_lan_select_queue - Select the right Tx queue for the skb for LAN VSI
+ * @netdev: network interface device structure
+ * @skb: send buffer
+ * @sb_dev: unused
+ *
+ * Returns the index of the selected Tx queue
+ **/
+u16 i40e_lan_select_queue(struct net_device *netdev,
+			  struct sk_buff *skb,
+			  struct net_device *sb_dev)
+{
+#else /* HAVE_NDO_SELECT_QUEUE_FALLBACK_REMOVED */
+/**
+ * i40e_lan_select_queue - Select the right Tx queue for the skb for LAN VSI
+ * @netdev: network interface device structure
+ * @skb: send buffer
+ * @sb_dev: unused
+ * @fallback: unused
+ *
+ * Returns the index of the selected Tx queue
+ **/
+u16 i40e_lan_select_queue(struct net_device *netdev,
+			  struct sk_buff *skb,
+			  struct net_device *sb_dev,
+			  select_queue_fallback_t __always_unused fallback)
+{
+#endif /* HAVE_NDO_SELECT_QUEUE_FALLBACK_REMOVED */
+#endif /* HAVE_NDO_SELECT_QUEUE_SB_DEV */
+	struct i40e_netdev_priv *np = netdev_priv(netdev);
+	struct i40e_vsi *vsi = np->vsi;
+	struct i40e_hw *hw;
+	u16 qoffset;
+	u16 qcount;
+	u8 tclass;
+	u16 hash;
+	u8 prio;
+
+	/* is DCB enabled at all? */
+	if (vsi->tc_config.numtc == 1)
+#if defined(HAVE_NDO_SELECT_QUEUE_FALLBACK_REMOVED)
+		return netdev_pick_tx(netdev, skb, sb_dev);
+#elif defined(HAVE_NDO_SELECT_QUEUE_SB_DEV)
+		return fallback(netdev, skb, sb_dev);
+#elif defined(HAVE_NDO_SELECT_QUEUE_ACCEL_FALLBACK)
+		return fallback(netdev, skb);
+#else
+		return __netdev_pick_tx(netdev, skb);
+#endif
+
+	prio = skb->priority;
+	hw = &vsi->back->hw;
+	tclass = hw->local_dcbx_config.etscfg.prioritytable[prio];
+	/* sanity check */
+	if (unlikely(!(vsi->tc_config.enabled_tc & BIT(tclass))))
+		tclass = 0;
+
+	/* select a queue assigned for the given TC */
+	qcount = vsi->tc_config.tc_info[tclass].qcount;
+	hash = i40e_swdcb_skb_tx_hash(netdev, skb, qcount);
+
+	qoffset = vsi->tc_config.tc_info[tclass].qoffset;
+	return qoffset + hash;
+}
+#endif /* HAVE_NETDEV_SELECT_QUEUE */
+
+#ifdef HAVE_XDP_SUPPORT
 /**
  * i40e_xmit_xdp_ring - transmits an XDP buffer to an XDP Tx ring
- * @xdp: data to transmit
+ * @xdp: frame data to transmit
  * @xdp_ring: XDP Tx ring
  **/
-static int i40e_xmit_xdp_ring(struct xdp_frame *xdpf,
+#ifdef HAVE_XDP_FRAME_STRUCT
+static int i40e_xmit_xdp_ring(struct xdp_frame *xdp,
 			      struct i40e_ring *xdp_ring)
+#else
+static int i40e_xmit_xdp_ring(struct xdp_buff *xdp,
+			      struct i40e_ring *xdp_ring)
+#endif
 {
 	u16 i = xdp_ring->next_to_use;
 	struct i40e_tx_buffer *tx_bi;
 	struct i40e_tx_desc *tx_desc;
-	void *data = xdpf->data;
-	u32 size = xdpf->len;
 	dma_addr_t dma;
+	void *data;
+	u32 size;
+
+	size = xdp_get_len(xdp);
+	data = xdp->data;
 
 	if (!unlikely(I40E_DESC_UNUSED(xdp_ring))) {
 		xdp_ring->tx_stats.tx_busy++;
@@ -3535,11 +4284,14 @@
 	dma = dma_map_single(xdp_ring->dev, data, size, DMA_TO_DEVICE);
 	if (dma_mapping_error(xdp_ring->dev, dma))
 		return I40E_XDP_CONSUMED;
-
 	tx_bi = &xdp_ring->tx_bi[i];
 	tx_bi->bytecount = size;
 	tx_bi->gso_segs = 1;
-	tx_bi->xdpf = xdpf;
+#ifdef HAVE_XDP_FRAME_STRUCT
+	tx_bi->xdpf = xdp;
+#else
+	tx_bi->raw_buf = data;
+#endif
 
 	/* record length, and DMA address */
 	dma_unmap_len_set(tx_bi, len, size);
@@ -3559,12 +4311,11 @@
 	i++;
 	if (i == xdp_ring->count)
 		i = 0;
-
 	tx_bi->next_to_watch = tx_desc;
 	xdp_ring->next_to_use = i;
-
 	return I40E_XDP_TX;
 }
+#endif
 
 /**
  * i40e_xmit_frame_ring - Sends buffer on Tx ring
@@ -3585,7 +4336,9 @@
 	u32 td_cmd = 0;
 	u8 hdr_len = 0;
 	int tso, count;
+#ifdef HAVE_PTP_1588_CLOCK
 	int tsyn;
+#endif /* HAVE_PTP_1588_CLOCK */
 
 	/* prefetch the data, we'll need it later */
 	prefetch(skb->data);
@@ -3620,7 +4373,7 @@
 	first->gso_segs = 1;
 
 	/* prepare the xmit flags */
-	if (i40e_tx_prepare_vlan_flags(skb, tx_ring, &tx_flags))
+	if (i40e_tx_prepare_vlan_flags(skb, tx_ring, &tx_flags, &cd_l2tag2))
 		goto out_drop;
 
 	/* obtain protocol of skb */
@@ -3645,11 +4398,14 @@
 	if (tso < 0)
 		goto out_drop;
 
+#ifdef HAVE_PTP_1588_CLOCK
 	tsyn = i40e_tsyn(tx_ring, skb, tx_flags, &cd_type_cmd_tso_mss);
 
 	if (tsyn)
 		tx_flags |= I40E_TX_FLAGS_TSYN;
 
+#endif /* HAVE_PTP_1588_CLOCK */
+
 	/* always enable CRC insertion offload */
 	td_cmd |= I40E_TX_DESC_CMD_ICRC;
 
@@ -3662,16 +4418,25 @@
 	 */
 	i40e_atr(tx_ring, skb, tx_flags);
 
+#ifdef HAVE_PTP_1588_CLOCK
 	if (i40e_tx_map(tx_ring, skb, first, tx_flags, hdr_len,
 			td_cmd, td_offset))
 		goto cleanup_tx_tstamp;
+#else
+	i40e_tx_map(tx_ring, skb, first, tx_flags, hdr_len,
+		    td_cmd, td_offset);
+#endif
 
+#ifndef HAVE_TRANS_START_IN_QUEUE
+	tx_ring->netdev->trans_start = jiffies;
+#endif
 	return NETDEV_TX_OK;
 
 out_drop:
 	i40e_trace(xmit_frame_ring_drop, first->skb, tx_ring);
 	dev_kfree_skb_any(first->skb);
 	first->skb = NULL;
+#ifdef HAVE_PTP_1588_CLOCK
 cleanup_tx_tstamp:
 	if (unlikely(tx_flags & I40E_TX_FLAGS_TSYN)) {
 		struct i40e_pf *pf = i40e_netdev_to_pf(tx_ring->netdev);
@@ -3680,7 +4445,7 @@
 		pf->ptp_tx_skb = NULL;
 		clear_bit_unlock(__I40E_PTP_TX_IN_PROGRESS, pf->state);
 	}
-
+#endif
 	return NETDEV_TX_OK;
 }
 
@@ -3706,10 +4471,13 @@
 	return i40e_xmit_frame_ring(skb, tx_ring);
 }
 
+#ifdef HAVE_XDP_SUPPORT
 /**
  * i40e_xdp_xmit - Implements ndo_xdp_xmit
  * @dev: netdev
- * @xdp: XDP buffer
+ * @n: amount of frames
+ * @frames: XDP frames
+ * @flags: XDP xmit flags
  *
  * Returns number of frames successfully sent. Frames that fail are
  * free'ed via XDP return API.
@@ -3717,16 +4485,23 @@
  * For error cases, a negative errno code is returned and no-frames
  * are transmitted (caller must handle freeing frames).
  **/
+#ifdef HAVE_XDP_FRAME_STRUCT
 int i40e_xdp_xmit(struct net_device *dev, int n, struct xdp_frame **frames,
 		  u32 flags)
+#else
+int i40e_xdp_xmit(struct net_device *dev, struct xdp_buff *xdp)
+#endif
 {
 	struct i40e_netdev_priv *np = netdev_priv(dev);
 	unsigned int queue_index = smp_processor_id();
 	struct i40e_vsi *vsi = np->vsi;
 	struct i40e_pf *pf = vsi->back;
+#ifdef HAVE_XDP_FRAME_STRUCT
 	struct i40e_ring *xdp_ring;
 	int drops = 0;
 	int i;
+#endif
+	int err;
 
 	if (test_bit(__I40E_VSI_DOWN, vsi->state))
 		return -ENETDOWN;
@@ -3734,7 +4509,7 @@
 	if (!i40e_enabled_xdp_vsi(vsi) || queue_index >= vsi->num_queue_pairs ||
 	    test_bit(__I40E_CONFIG_BUSY, pf->state))
 		return -ENXIO;
-
+#ifdef HAVE_XDP_FRAME_STRUCT
 	if (unlikely(flags & ~XDP_XMIT_FLAGS_MASK))
 		return -EINVAL;
 
@@ -3742,7 +4517,6 @@
 
 	for (i = 0; i < n; i++) {
 		struct xdp_frame *xdpf = frames[i];
-		int err;
 
 		err = i40e_xmit_xdp_ring(xdpf, xdp_ring);
 		if (err != I40E_XDP_TX) {
@@ -3755,4 +4529,33 @@
 		i40e_xdp_ring_update_tail(xdp_ring);
 
 	return n - drops;
+#else
+	err = i40e_xmit_xdp_ring(xdp, vsi->xdp_rings[queue_index]);
+
+	if (err != I40E_XDP_TX)
+		return -ENOSPC;
+
+	return 0;
+#endif
 }
+
+/**
+ * i40e_xdp_flush - Implements ndo_xdp_flush
+ * @dev: netdev
+ **/
+void i40e_xdp_flush(struct net_device *dev)
+{
+	struct i40e_netdev_priv *np = netdev_priv(dev);
+	unsigned int queue_index = smp_processor_id();
+	struct i40e_vsi *vsi = np->vsi;
+
+	if (test_bit(__I40E_VSI_DOWN, vsi->state))
+		return;
+
+	if (!i40e_enabled_xdp_vsi(vsi) || queue_index >= vsi->num_queue_pairs)
+		return;
+
+	i40e_xdp_ring_update_tail(vsi->xdp_rings[queue_index]);
+}
+#endif
+
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_txrx_common.h linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_txrx_common.h
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_txrx_common.h	2024-05-10 01:26:45.389079797 -0400
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_txrx_common.h	2024-05-13 03:58:25.372491940 -0400
@@ -1,18 +1,23 @@
 /* SPDX-License-Identifier: GPL-2.0 */
-/* Copyright(c) 2018 Intel Corporation. */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
 
-#ifndef I40E_TXRX_COMMON_
-#define I40E_TXRX_COMMON_
+#ifndef _I40E_TXRX_COMMON_H_
+#define _I40E_TXRX_COMMON_H_
 
 void i40e_fd_handle_status(struct i40e_ring *rx_ring,
 			   union i40e_rx_desc *rx_desc, u8 prog_id);
 int i40e_xmit_xdp_tx_ring(struct xdp_buff *xdp, struct i40e_ring *xdp_ring);
-struct i40e_rx_buffer *i40e_clean_programming_status(
-	struct i40e_ring *rx_ring,
-	union i40e_rx_desc *rx_desc,
-	u64 qw);
+struct i40e_rx_buffer *i40e_clean_programming_status
+	(struct i40e_ring *rx_ring,
+	 union i40e_rx_desc *rx_desc,
+	 u64 qw);
 void i40e_process_skb_fields(struct i40e_ring *rx_ring,
-			     union i40e_rx_desc *rx_desc, struct sk_buff *skb);
+			     union i40e_rx_desc *rx_desc,
+			     struct sk_buff *skb,
+			     u8 rx_ptype);
+void i40e_receive_skb(struct i40e_ring *rx_ring,
+		      struct sk_buff *skb, u16 vlan_tag, u16 vlan_tpid);
+
 void i40e_xdp_ring_update_tail(struct i40e_ring *xdp_ring);
 void i40e_update_rx_stats(struct i40e_ring *rx_ring,
 			  unsigned int total_rx_bytes,
@@ -20,72 +25,28 @@
 void i40e_finalize_xdp_rx(struct i40e_ring *rx_ring, unsigned int xdp_res);
 void i40e_release_rx_desc(struct i40e_ring *rx_ring, u32 val);
 
-#define I40E_XDP_PASS		0
-#define I40E_XDP_CONSUMED	BIT(0)
-#define I40E_XDP_TX		BIT(1)
-#define I40E_XDP_REDIR		BIT(2)
-
-/**
- * build_ctob - Builds the Tx descriptor (cmd, offset and type) qword
- **/
-static inline __le64 build_ctob(u32 td_cmd, u32 td_offset, unsigned int size,
-				u32 td_tag)
-{
-	return cpu_to_le64(I40E_TX_DESC_DTYPE_DATA |
-			   ((u64)td_cmd  << I40E_TXD_QW1_CMD_SHIFT) |
-			   ((u64)td_offset << I40E_TXD_QW1_OFFSET_SHIFT) |
-			   ((u64)size  << I40E_TXD_QW1_TX_BUF_SZ_SHIFT) |
-			   ((u64)td_tag  << I40E_TXD_QW1_L2TAG1_SHIFT));
-}
-
-/**
- * i40e_update_tx_stats - Update the egress statistics for the Tx ring
- * @tx_ring: Tx ring to update
- * @total_packet: total packets sent
- * @total_bytes: total bytes sent
- **/
-static inline void i40e_update_tx_stats(struct i40e_ring *tx_ring,
-					unsigned int total_packets,
-					unsigned int total_bytes)
-{
-	u64_stats_update_begin(&tx_ring->syncp);
-	tx_ring->stats.bytes += total_bytes;
-	tx_ring->stats.packets += total_packets;
-	u64_stats_update_end(&tx_ring->syncp);
-	tx_ring->q_vector->tx.total_bytes += total_bytes;
-	tx_ring->q_vector->tx.total_packets += total_packets;
-}
-
-#define WB_STRIDE 4
-
 /**
- * i40e_arm_wb - (Possibly) arms Tx write-back
- * @tx_ring: Tx ring to update
- * @vsi: the VSI
- * @budget: the NAPI budget left
+ * i40e_rx_is_programming_status - check for programming status descriptor
+ * @qw: qword representing status_error_len in CPU ordering
+ *
+ * The value of in the descriptor length field indicate if this
+ * is a programming status descriptor for flow director or FCoE
+ * by the value of I40E_RX_PROG_STATUS_DESC_LENGTH, otherwise
+ * it is a packet descriptor.
  **/
-static inline void i40e_arm_wb(struct i40e_ring *tx_ring,
-			       struct i40e_vsi *vsi,
-			       int budget)
+static inline bool i40e_rx_is_programming_status(u64 qw)
 {
-	if (tx_ring->flags & I40E_TXR_FLAGS_WB_ON_ITR) {
-		/* check to see if there are < 4 descriptors
-		 * waiting to be written back, then kick the hardware to force
-		 * them to be written back in case we stay in NAPI.
-		 * In this mode on X722 we do not enable Interrupt.
-		 */
-		unsigned int j = i40e_get_tx_pending(tx_ring, false);
-
-		if (budget &&
-		    ((j / WB_STRIDE) == 0) && j > 0 &&
-		    !test_bit(__I40E_VSI_DOWN, vsi->state) &&
-		    (I40E_DESC_UNUSED(tx_ring) != tx_ring->count))
-			tx_ring->arm_wb = true;
-	}
+/* The Rx filter programming status and SPH bit occupy the same
+ * spot in the descriptor. Since we don't support packet split we
+ * can just reuse the bit as an indication that this is a
+ * programming status descriptor.
+ */
+	return qw & I40E_RXD_QW1_LENGTH_SPH_MASK;
 }
 
-void i40e_xsk_clean_rx_ring(struct i40e_ring *rx_ring);
-void i40e_xsk_clean_tx_ring(struct i40e_ring *tx_ring);
-bool i40e_xsk_any_rx_ring_enabled(struct i40e_vsi *vsi);
+#define I40E_XDP_PASS          0
+#define I40E_XDP_CONSUMED      BIT(0)
+#define I40E_XDP_TX            BIT(1)
+#define I40E_XDP_REDIR         BIT(2)
 
-#endif /* I40E_TXRX_COMMON_ */
+#endif /* _I40E_TXRX_COMMON_H_ */
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_txrx.h linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_txrx.h
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_txrx.h	2024-05-10 01:26:45.389079797 -0400
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_txrx.h	2024-05-13 03:58:25.372491940 -0400
@@ -1,11 +1,9 @@
 /* SPDX-License-Identifier: GPL-2.0 */
-/* Copyright(c) 2013 - 2018 Intel Corporation. */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
 
 #ifndef _I40E_TXRX_H_
 #define _I40E_TXRX_H_
 
-#include <net/xdp.h>
-
 /* Interrupt Throttling and Rate Limiting Goodies */
 #define I40E_DEFAULT_IRQ_WORK      256
 
@@ -18,7 +16,7 @@
 #define I40E_ITR_DYNAMIC	0x8000	/* use top bit as a flag */
 #define I40E_ITR_MASK		0x1FFE	/* mask for ITR register value */
 #define I40E_MIN_ITR		     2	/* reg uses 2 usec resolution */
-#define I40E_ITR_100K		    10	/* all values below must be even */
+#define I40E_ITR_100K		    10  /* all values below must be even */
 #define I40E_ITR_50K		    20
 #define I40E_ITR_20K		    50
 #define I40E_ITR_18K		    60
@@ -28,8 +26,8 @@
 #define ITR_REG_ALIGN(setting) __ALIGN_MASK(setting, ~I40E_ITR_MASK)
 #define ITR_IS_DYNAMIC(setting) (!!((setting) & I40E_ITR_DYNAMIC))
 
-#define I40E_ITR_RX_DEF		(I40E_ITR_20K | I40E_ITR_DYNAMIC)
-#define I40E_ITR_TX_DEF		(I40E_ITR_20K | I40E_ITR_DYNAMIC)
+#define I40E_ITR_RX_DEF            (I40E_ITR_20K | I40E_ITR_DYNAMIC)
+#define I40E_ITR_TX_DEF            (I40E_ITR_20K | I40E_ITR_DYNAMIC)
 
 /* 0x40 is the enable bit for interrupt rate limiting, and must be set if
  * the value of the rate limit is non-zero
@@ -117,10 +115,18 @@
  */
 #define I40E_RX_HDR_SIZE I40E_RXBUFFER_256
 #define I40E_PACKET_HDR_PAD (ETH_HLEN + ETH_FCS_LEN + (VLAN_HLEN * 2))
+#ifdef I40E_32BYTE_RX
 #define i40e_rx_desc i40e_32byte_rx_desc
+#else
+#define i40e_rx_desc i40e_16byte_rx_desc
+#endif
 
+#ifdef HAVE_STRUCT_DMA_ATTRS
+#define I40E_RX_DMA_ATTR NULL
+#else
 #define I40E_RX_DMA_ATTR \
 	(DMA_ATTR_SKIP_CPU_SYNC | DMA_ATTR_WEAK_ORDERING)
+#endif
 
 /* Attempt to maximize the headroom available for incoming frames.  We
  * use a 2K buffer for receives and need 1536/1534 to store the data for
@@ -271,9 +277,11 @@
 #define I40E_TX_FLAGS_IPV6		BIT(5)
 #define I40E_TX_FLAGS_FCCRC		BIT(6)
 #define I40E_TX_FLAGS_FSO		BIT(7)
+#ifdef HAVE_PTP_1588_CLOCK
 #define I40E_TX_FLAGS_TSYN		BIT(8)
+#endif /* HAVE_PTP_1588_CLOCK */
 #define I40E_TX_FLAGS_FD_SB		BIT(9)
-#define I40E_TX_FLAGS_UDP_TUNNEL	BIT(10)
+#define I40E_TX_FLAGS_TUNNEL		BIT(10)
 #define I40E_TX_FLAGS_VLAN_MASK		0xffff0000
 #define I40E_TX_FLAGS_VLAN_PRIO_MASK	0xe0000000
 #define I40E_TX_FLAGS_VLAN_PRIO_SHIFT	29
@@ -296,17 +304,17 @@
 
 struct i40e_rx_buffer {
 	dma_addr_t dma;
-	union {
-		struct {
-			struct page *page;
-			__u32 page_offset;
-			__u16 pagecnt_bias;
-		};
-		struct {
-			void *addr;
-			u64 handle;
-		};
-	};
+#ifdef CONFIG_I40E_DISABLE_PACKET_SPLIT
+	struct sk_buff *skb;
+#else
+	struct page *page;
+#if (BITS_PER_LONG > 32) || (PAGE_SIZE >= 65536)
+	__u32 page_offset;
+#else
+	__u16 page_offset;
+#endif
+	__u16 pagecnt_bias;
+#endif /* CONFIG_I40E_DISABLE_PACKET_SPLIT */
 };
 
 struct i40e_queue_stats {
@@ -314,6 +322,17 @@
 	u64 bytes;
 };
 
+#ifdef HAVE_XDP_SUPPORT
+struct i40e_xdp_stats {
+	u64 xdp_pass;
+	u64 xdp_drop;
+	u64 xdp_tx;
+	u64 xdp_unknown;
+	u64 xdp_redirect;
+	u64 xdp_redirect_fail;
+};
+#endif
+
 struct i40e_tx_queue_stats {
 	u64 restart_queue;
 	u64 tx_busy;
@@ -393,7 +412,12 @@
 
 	/* stats structs */
 	struct i40e_queue_stats	stats;
+#ifdef HAVE_NDO_GET_STATS64
 	struct u64_stats_sync syncp;
+#endif
+#ifdef HAVE_XDP_SUPPORT
+	struct i40e_xdp_stats xdp_stats;
+#endif
 	union {
 		struct i40e_tx_queue_stats tx_stats;
 		struct i40e_rx_queue_stats rx_stats;
@@ -417,9 +441,9 @@
 					 */
 
 	struct i40e_channel *ch;
+#ifdef HAVE_XDP_BUFF_RXQ
 	struct xdp_rxq_info xdp_rxq;
-	struct xdp_umem *xsk_umem;
-	struct zero_copy_allocator zca; /* ZC allocator anchor */
+#endif
 } ____cacheline_internodealigned_in_smp;
 
 static inline bool ring_uses_build_skb(struct i40e_ring *ring)
@@ -437,6 +461,13 @@
 	ring->flags &= ~I40E_RXR_FLAGS_BUILD_SKB_ENABLED;
 }
 
+#define I40E_ITR_ADAPTIVE_MIN_INC       0x0002
+#define I40E_ITR_ADAPTIVE_MIN_USECS     0x0002
+#define I40E_ITR_ADAPTIVE_MAX_USECS     0x007e
+#define I40E_ITR_ADAPTIVE_LATENCY       0x8000
+#define I40E_ITR_ADAPTIVE_BULK          0x0000
+#define ITR_IS_BULK(x) (!((x) & I40E_ITR_ADAPTIVE_LATENCY))
+
 static inline bool ring_is_xdp(struct i40e_ring *ring)
 {
 	return !!(ring->flags & I40E_TXR_FLAGS_XDP);
@@ -447,13 +478,6 @@
 	ring->flags |= I40E_TXR_FLAGS_XDP;
 }
 
-#define I40E_ITR_ADAPTIVE_MIN_INC	0x0002
-#define I40E_ITR_ADAPTIVE_MIN_USECS	0x0002
-#define I40E_ITR_ADAPTIVE_MAX_USECS	0x007e
-#define I40E_ITR_ADAPTIVE_LATENCY	0x8000
-#define I40E_ITR_ADAPTIVE_BULK		0x0000
-#define ITR_IS_BULK(x) (!((x) & I40E_ITR_ADAPTIVE_LATENCY))
-
 struct i40e_ring_container {
 	struct i40e_ring *ring;		/* pointer to linked list of ring(s) */
 	unsigned long next_update;	/* jiffies value of next update */
@@ -481,6 +505,31 @@
 
 bool i40e_alloc_rx_buffers(struct i40e_ring *rxr, u16 cleaned_count);
 netdev_tx_t i40e_lan_xmit_frame(struct sk_buff *skb, struct net_device *netdev);
+#ifdef HAVE_NETDEV_SELECT_QUEUE
+#ifndef HAVE_NDO_SELECT_QUEUE_SB_DEV
+#if defined(HAVE_NDO_SELECT_QUEUE_ACCEL) || \
+defined(HAVE_NDO_SELECT_QUEUE_ACCEL_FALLBACK)
+#ifndef HAVE_NDO_SELECT_QUEUE_FALLBACK_REMOVED
+u16 i40e_lan_select_queue(struct net_device *netdev, struct sk_buff *skb,
+			  void *accel_priv, select_queue_fallback_t fallback);
+#else /* HAVE_NDO_SELECT_QUEUE_FALLBACK_REMOVED */
+u16 i40e_lan_select_queue(struct net_device *netdev, struct sk_buff *skb,
+			  void *accel_priv);
+#endif /* HAVE_NDO_SELECT_QUEUE_FALLBACK_REMOVED */
+#else /* HAVE_NDO_SELECT_QUEUE_ACCEL || HAVE_NDO_SELECT_QUEUE_ACCEL_FALLBACK */
+u16 i40e_lan_select_queue(struct net_device *netdev, struct sk_buff *skb);
+#endif /*HAVE_NDO_SELECT_QUEUE_ACCEL || HAVE_NDO_SELECT_QUEUE_ACCEL_FALLBACK */
+#else /* HAVE_NDO_SELECT_QUEUE_SB_DEV */
+#ifdef HAVE_NDO_SELECT_QUEUE_FALLBACK_REMOVED
+u16 i40e_lan_select_queue(struct net_device *netdev, struct sk_buff *skb,
+			  struct net_device *sb_dev);
+#else /* HAVE_NDO_SELECT_QUEUE_FALLBACK_REMOVED */
+u16 i40e_lan_select_queue(struct net_device *netdev, struct sk_buff *skb,
+			  struct net_device *sb_dev,
+			  select_queue_fallback_t fallback);
+#endif /* HAVE_NDO_SELECT_QUEUE_FALLBACK_REMOVED */
+#endif /* HAVE_NDO_SELECT_QUEUE_SB_DEV */
+#endif /* HAVE_NETDEV_SELECT_QUEUE */
 void i40e_clean_tx_ring(struct i40e_ring *tx_ring);
 void i40e_clean_rx_ring(struct i40e_ring *rx_ring);
 int i40e_setup_tx_descriptors(struct i40e_ring *tx_ring);
@@ -493,8 +542,27 @@
 void i40e_detect_recover_hung(struct i40e_vsi *vsi);
 int __i40e_maybe_stop_tx(struct i40e_ring *tx_ring, int size);
 bool __i40e_chk_linearize(struct sk_buff *skb);
+#ifdef HAVE_XDP_FRAME_STRUCT
 int i40e_xdp_xmit(struct net_device *dev, int n, struct xdp_frame **frames,
 		  u32 flags);
+#else
+int i40e_xdp_xmit(struct net_device *dev, struct xdp_buff *xdp);
+#endif
+void i40e_xdp_flush(struct net_device *dev);
+
+#ifdef HAVE_XDP_SUPPORT
+#ifdef HAVE_XDP_FRAME_STRUCT
+static inline u32 xdp_get_len(struct xdp_frame *xdp)
+{
+	return xdp->len;
+}
+#else
+static inline u32 xdp_get_len(struct xdp_buff *xdp)
+{
+	return xdp->data_end - xdp->data;
+}
+#endif
+#endif
 
 /**
  * i40e_get_head - Retrieve head from head writeback
@@ -513,7 +581,6 @@
 /**
  * i40e_xmit_descriptor_count - calculate number of Tx descriptors needed
  * @skb:     send buffer
- * @tx_ring: ring to send buffer on
  *
  * Returns number of data descriptors needed for this skb. Returns 0 to indicate
  * there is not enough descriptors available in this ring since we need at least
Binary files linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_txrx.o and linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_txrx.o differ
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_type.h linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_type.h
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_type.h	2024-05-10 01:26:45.389079797 -0400
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_type.h	2024-05-13 03:58:25.372491940 -0400
@@ -1,5 +1,5 @@
 /* SPDX-License-Identifier: GPL-2.0 */
-/* Copyright(c) 2013 - 2018 Intel Corporation. */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
 
 #ifndef _I40E_TYPE_H_
 #define _I40E_TYPE_H_
@@ -13,7 +13,7 @@
 #include "i40e_devids.h"
 
 /* I40E_MASK is a macro used on 32 bit registers */
-#define I40E_MASK(mask, shift) ((u32)(mask) << (shift))
+#define I40E_MASK(mask, shift) (mask << shift)
 
 #define I40E_MAX_VSI_QP			16
 #define I40E_MAX_VF_VSI			4
@@ -33,6 +33,9 @@
 struct i40e_hw;
 typedef void (*I40E_ADMINQ_CALLBACK)(struct i40e_hw *, struct i40e_aq_desc *);
 
+#ifndef ETH_ALEN
+#define ETH_ALEN	6
+#endif
 /* Data type manipulation macros. */
 
 #define I40E_DESC_UNUSED(R)	\
@@ -59,7 +62,9 @@
 	I40E_DEBUG_DIAG			= 0x00000800,
 	I40E_DEBUG_FD			= 0x00001000,
 	I40E_DEBUG_PACKAGE		= 0x00002000,
+
 	I40E_DEBUG_IWARP		= 0x00F00000,
+
 	I40E_DEBUG_AQ_MESSAGE		= 0x01000000,
 	I40E_DEBUG_AQ_DESCRIPTOR	= 0x02000000,
 	I40E_DEBUG_AQ_DESC_BUFFER	= 0x04000000,
@@ -85,16 +90,16 @@
 #define I40E_MDIO_CLAUSE45_OPCODE_WRITE_MASK	I40E_MASK(1, \
 						  I40E_GLGEN_MSCA_OPCODE_SHIFT)
 #define I40E_MDIO_CLAUSE45_OPCODE_READ_INC_ADDR_MASK	I40E_MASK(2, \
-						I40E_GLGEN_MSCA_OPCODE_SHIFT)
+						  I40E_GLGEN_MSCA_OPCODE_SHIFT)
 #define I40E_MDIO_CLAUSE45_OPCODE_READ_MASK	I40E_MASK(3, \
-						I40E_GLGEN_MSCA_OPCODE_SHIFT)
+						  I40E_GLGEN_MSCA_OPCODE_SHIFT)
 
-#define I40E_PHY_COM_REG_PAGE                   0x1E
-#define I40E_PHY_LED_LINK_MODE_MASK             0xF0
-#define I40E_PHY_LED_MANUAL_ON                  0x100
-#define I40E_PHY_LED_PROV_REG_1                 0xC430
-#define I40E_PHY_LED_MODE_MASK                  0xFFFF
-#define I40E_PHY_LED_MODE_ORIG                  0x80000000
+#define I40E_PHY_COM_REG_PAGE			0x1E
+#define I40E_PHY_LED_LINK_MODE_MASK		0xF0
+#define I40E_PHY_LED_MANUAL_ON			0x100
+#define I40E_PHY_LED_PROV_REG_1			0xC430
+#define I40E_PHY_LED_MODE_MASK			0xFFFF
+#define I40E_PHY_LED_MODE_ORIG			0x80000000
 
 /* These are structs for managing the hardware information and the operations.
  * The structures of function pointers are filled out at init time when we
@@ -160,6 +165,14 @@
 	I40E_QUEUE_TYPE_UNKNOWN
 };
 
+enum i40e_prt_mac_link_speed {
+	I40E_PRT_MAC_LINK_SPEED_100MB = 0,
+	I40E_PRT_MAC_LINK_SPEED_1GB,
+	I40E_PRT_MAC_LINK_SPEED_10GB,
+	I40E_PRT_MAC_LINK_SPEED_40GB,
+	I40E_PRT_MAC_LINK_SPEED_20GB
+};
+
 struct i40e_link_status {
 	enum i40e_aq_phy_type phy_type;
 	enum i40e_aq_link_speed link_speed;
@@ -233,7 +246,8 @@
 #define I40E_CAP_PHY_TYPE_1000BASE_T_OPTICAL \
 				BIT_ULL(I40E_PHY_TYPE_1000BASE_T_OPTICAL)
 #define I40E_CAP_PHY_TYPE_20GBASE_KR2 BIT_ULL(I40E_PHY_TYPE_20GBASE_KR2)
-/* Defining the macro I40E_TYPE_OFFSET to implement a bit shift for some
+/*
+ * Defining the macro I40E_TYPE_OFFSET to implement a bit shift for some
  * PHY types. There is an unused bit (31) in the I40E_CAP_PHY_TYPE_* bit
  * fields but no corresponding gap in the i40e_aq_phy_type enumeration. So,
  * a shift is needed to adjust for this with values larger than 31. The
@@ -252,13 +266,18 @@
 					     I40E_PHY_TYPE_OFFSET)
 #define I40E_CAP_PHY_TYPE_25GBASE_ACC BIT_ULL(I40E_PHY_TYPE_25GBASE_ACC + \
 					     I40E_PHY_TYPE_OFFSET)
-/* Offset for 2.5G/5G PHY Types value to bit number conversion */
-#define I40E_PHY_TYPE_OFFSET2 (-10)
-#define I40E_CAP_PHY_TYPE_2_5GBASE_T BIT_ULL(I40E_PHY_TYPE_2_5GBASE_T + \
-					     I40E_PHY_TYPE_OFFSET2)
-#define I40E_CAP_PHY_TYPE_5GBASE_T BIT_ULL(I40E_PHY_TYPE_5GBASE_T + \
-					     I40E_PHY_TYPE_OFFSET2)
+#define I40E_CAP_PHY_TYPE_2_5GBASE_T BIT_ULL(I40E_PHY_TYPE_2_5GBASE_T)
+#define I40E_CAP_PHY_TYPE_5GBASE_T BIT_ULL(I40E_PHY_TYPE_5GBASE_T)
 #define I40E_HW_CAP_MAX_GPIO			30
+enum i40e_acpi_programming_method {
+	I40E_ACPI_PROGRAMMING_METHOD_HW_FVL = 0,
+	I40E_ACPI_PROGRAMMING_METHOD_AQC_FPK = 1
+};
+
+#define I40E_WOL_SUPPORT_MASK			0x1
+#define I40E_ACPI_PROGRAMMING_METHOD_MASK	0x2
+#define I40E_PROXY_SUPPORT_MASK			0x4
+
 /* Capabilities of a PF or a VF or the whole device */
 struct i40e_hw_capabilities {
 	u32  switch_mode;
@@ -336,6 +355,10 @@
 	u32 enabled_tcmap;
 	u32 maxtc;
 	u64 wr_csr_prot;
+	bool dis_unused_ports;
+	bool apm_wol_support;
+	enum i40e_acpi_programming_method acpi_prog_method;
+	bool proxy_support;
 };
 
 struct i40e_mac_info {
@@ -386,6 +409,7 @@
 	I40E_NVMUPD_EXEC_AQ,
 	I40E_NVMUPD_GET_AQ_RESULT,
 	I40E_NVMUPD_GET_AQ_EVENT,
+	I40E_NVMUPD_FEATURES,
 };
 
 enum i40e_nvmupd_state {
@@ -421,8 +445,12 @@
 #define I40E_NVM_AQE				0xe
 #define I40E_NVM_EXEC				0xf
 
+#define I40E_NVM_EXEC_GET_AQ_RESULT		0x0
+#define I40E_NVM_EXEC_FEATURES			0xe
+#define I40E_NVM_EXEC_STATUS			0xf
+
 #define I40E_NVM_ADAPT_SHIFT	16
-#define I40E_NVM_ADAPT_MASK	(0xffff << I40E_NVM_ADAPT_SHIFT)
+#define I40E_NVM_ADAPT_MASK	(0xffffULL << I40E_NVM_ADAPT_SHIFT)
 
 #define I40E_NVMUPD_MAX_DATA	4096
 #define I40E_NVMUPD_IFACE_TIMEOUT 2 /* seconds */
@@ -435,6 +463,20 @@
 	u8 data[1];
 };
 
+/* NVMUpdate features API */
+#define I40E_NVMUPD_FEATURES_API_VER_MAJOR		0
+#define I40E_NVMUPD_FEATURES_API_VER_MINOR		14
+#define I40E_NVMUPD_FEATURES_API_FEATURES_ARRAY_LEN	12
+
+#define I40E_NVMUPD_FEATURE_FLAT_NVM_SUPPORT		BIT(0)
+
+struct i40e_nvmupd_features {
+	u8 major;
+	u8 minor;
+	u16 size;
+	u8 features[I40E_NVMUPD_FEATURES_API_FEATURES_ARRAY_LEN];
+};
+
 /* (Q)SFP module access definitions */
 #define I40E_I2C_EEPROM_DEV_ADDR	0xA0
 #define I40E_I2C_EEPROM_DEV_ADDR2	0xA2
@@ -443,7 +485,7 @@
 #define I40E_MODULE_SFF_8472_COMP	0x5E
 #define I40E_MODULE_SFF_8472_SWAP	0x5C
 #define I40E_MODULE_SFF_ADDR_MODE	0x04
-#define I40E_MODULE_SFF_DDM_IMPLEMENTED 0x40
+#define I40E_MODULE_SFF_DIAG_CAPAB	0x40
 #define I40E_MODULE_TYPE_QSFP_PLUS	0x0D
 #define I40E_MODULE_TYPE_QSFP28		0x11
 #define I40E_MODULE_QSFP_MAX_LEN	640
@@ -618,26 +660,37 @@
 	struct i40e_dcbx_config remote_dcbx_config; /* Peer Cfg */
 	struct i40e_dcbx_config desired_dcbx_config; /* CEE Desired Cfg */
 
+	/* WoL and proxy support */
+	u16 num_wol_proxy_filters;
+	u16 wol_proxy_vsi_seid;
+
 #define I40E_HW_FLAG_AQ_SRCTL_ACCESS_ENABLE BIT_ULL(0)
 #define I40E_HW_FLAG_802_1AD_CAPABLE        BIT_ULL(1)
 #define I40E_HW_FLAG_AQ_PHY_ACCESS_CAPABLE  BIT_ULL(2)
 #define I40E_HW_FLAG_NVM_READ_REQUIRES_LOCK BIT_ULL(3)
-#define I40E_HW_FLAG_FW_LLDP_STOPPABLE      BIT_ULL(4)
+#define I40E_HW_FLAG_FW_LLDP_STOPPABLE	    BIT_ULL(4)
 #define I40E_HW_FLAG_FW_LLDP_PERSISTENT     BIT_ULL(5)
-#define I40E_HW_FLAG_DROP_MODE              BIT_ULL(7)
+#define I40E_HW_FLAG_AQ_PHY_ACCESS_EXTENDED BIT_ULL(6)
+#define I40E_HW_FLAG_DROP_MODE		    BIT_ULL(7)
+#define I40E_HW_FLAG_X722_FEC_REQUEST_CAPABLE BIT_ULL(8)
 	u64 flags;
 
 	/* Used in set switch config AQ command */
 	u16 switch_tag;
 	u16 first_tag;
 	u16 second_tag;
+	bool is_double_vlan;
+	bool is_outer_vlan_processing;
+
+	/* NVMUpdate features */
+	struct i40e_nvmupd_features nvmupd_features;
 
 	/* debug mask */
 	u32 debug_mask;
 	char err_str[16];
 };
 
-static inline bool i40e_is_vf(struct i40e_hw *hw)
+static INLINE bool i40e_is_vf(struct i40e_hw *hw)
 {
 	return (hw->mac.type == I40E_MAC_VF ||
 		hw->mac.type == I40E_MAC_X722_VF);
@@ -688,7 +741,7 @@
 		__le64  rsvd2;
 	} read;
 	struct {
-		struct {
+		struct i40e_32b_rx_wb_qw0 {
 			struct {
 				union {
 					__le16 mirroring_status;
@@ -726,6 +779,9 @@
 			} hi_dword;
 		} qword3;
 	} wb;  /* writeback */
+	struct {
+		u64 qword[4];
+	} raw;
 };
 
 enum i40e_rx_desc_status_bits {
@@ -737,32 +793,28 @@
 	I40E_RX_DESC_STATUS_CRCP_SHIFT		= 4,
 	I40E_RX_DESC_STATUS_TSYNINDX_SHIFT	= 5, /* 2 BITS */
 	I40E_RX_DESC_STATUS_TSYNVALID_SHIFT	= 7,
-	/* Note: Bit 8 is reserved in X710 and XL710 */
 	I40E_RX_DESC_STATUS_EXT_UDP_0_SHIFT	= 8,
+
 	I40E_RX_DESC_STATUS_UMBCAST_SHIFT	= 9, /* 2 BITS */
 	I40E_RX_DESC_STATUS_FLM_SHIFT		= 11,
 	I40E_RX_DESC_STATUS_FLTSTAT_SHIFT	= 12, /* 2 BITS */
 	I40E_RX_DESC_STATUS_LPBK_SHIFT		= 14,
 	I40E_RX_DESC_STATUS_IPV6EXADD_SHIFT	= 15,
-	I40E_RX_DESC_STATUS_RESERVED_SHIFT	= 16, /* 2 BITS */
-	/* Note: For non-tunnel packets INT_UDP_0 is the right status for
-	 * UDP header
-	 */
+	I40E_RX_DESC_STATUS_RESERVED2_SHIFT	= 16, /* 2 BITS */
 	I40E_RX_DESC_STATUS_INT_UDP_0_SHIFT	= 18,
 	I40E_RX_DESC_STATUS_LAST /* this entry must be last!!! */
 };
 
 #define I40E_RXD_QW1_STATUS_SHIFT	0
-#define I40E_RXD_QW1_STATUS_MASK	((BIT(I40E_RX_DESC_STATUS_LAST) - 1) \
-					 << I40E_RXD_QW1_STATUS_SHIFT)
+#define I40E_RXD_QW1_STATUS_MASK	((BIT(I40E_RX_DESC_STATUS_LAST) - 1) << \
+					 I40E_RXD_QW1_STATUS_SHIFT)
 
 #define I40E_RXD_QW1_STATUS_TSYNINDX_SHIFT   I40E_RX_DESC_STATUS_TSYNINDX_SHIFT
 #define I40E_RXD_QW1_STATUS_TSYNINDX_MASK	(0x3UL << \
 					     I40E_RXD_QW1_STATUS_TSYNINDX_SHIFT)
 
 #define I40E_RXD_QW1_STATUS_TSYNVALID_SHIFT  I40E_RX_DESC_STATUS_TSYNVALID_SHIFT
-#define I40E_RXD_QW1_STATUS_TSYNVALID_MASK \
-				    BIT_ULL(I40E_RXD_QW1_STATUS_TSYNVALID_SHIFT)
+#define I40E_RXD_QW1_STATUS_TSYNVALID_MASK   BIT_ULL(I40E_RXD_QW1_STATUS_TSYNVALID_SHIFT)
 
 enum i40e_rx_desc_fltstat_values {
 	I40E_RX_DESC_FLTSTAT_NO_DATA	= 0,
@@ -825,7 +877,8 @@
 	I40E_RX_PTYPE_GRENAT4_MAC_PAY3			= 58,
 	I40E_RX_PTYPE_GRENAT4_MACVLAN_IPV6_ICMP_PAY4	= 87,
 	I40E_RX_PTYPE_GRENAT6_MAC_PAY3			= 124,
-	I40E_RX_PTYPE_GRENAT6_MACVLAN_IPV6_ICMP_PAY4	= 153
+	I40E_RX_PTYPE_GRENAT6_MACVLAN_IPV6_ICMP_PAY4	= 153,
+	I40E_RX_PTYPE_PARSER_ABORTED			= 255
 };
 
 struct i40e_rx_ptype_decoded {
@@ -1076,8 +1129,7 @@
 #define I40E_TXD_CTX_GRE_TUNNELING	(0x2ULL << I40E_TXD_CTX_QW0_NATT_SHIFT)
 
 #define I40E_TXD_CTX_QW0_EIP_NOINC_SHIFT	11
-#define I40E_TXD_CTX_QW0_EIP_NOINC_MASK \
-				       BIT_ULL(I40E_TXD_CTX_QW0_EIP_NOINC_SHIFT)
+#define I40E_TXD_CTX_QW0_EIP_NOINC_MASK	BIT_ULL(I40E_TXD_CTX_QW0_EIP_NOINC_SHIFT)
 
 #define I40E_TXD_CTX_EIP_NOINC_IPID_CONST	I40E_TXD_CTX_QW0_EIP_NOINC_MASK
 
@@ -1183,10 +1235,6 @@
 					 I40E_TXD_FLTR_QW1_CMD_SHIFT)
 #define I40E_TXD_FLTR_QW1_ATR_MASK	BIT_ULL(I40E_TXD_FLTR_QW1_ATR_SHIFT)
 
-#define I40E_TXD_FLTR_QW1_ATR_SHIFT	(0xEULL + \
-					 I40E_TXD_FLTR_QW1_CMD_SHIFT)
-#define I40E_TXD_FLTR_QW1_ATR_MASK	BIT_ULL(I40E_TXD_FLTR_QW1_ATR_SHIFT)
-
 #define I40E_TXD_FLTR_QW1_CNTINDEX_SHIFT 20
 #define I40E_TXD_FLTR_QW1_CNTINDEX_MASK	(0x1FFUL << \
 					 I40E_TXD_FLTR_QW1_CNTINDEX_SHIFT)
@@ -1300,6 +1348,8 @@
 	u32 rx_lpi_status;
 	u64 tx_lpi_count;		/* etlpic */
 	u64 rx_lpi_count;		/* erlpic */
+	u64 tx_lpi_duration;
+	u64 rx_lpi_duration;
 };
 
 /* Checksum and Shadow RAM pointers */
@@ -1313,6 +1363,10 @@
 #define I40E_SR_NVM_DEV_STARTER_VERSION		0x18
 #define I40E_SR_NVM_WAKE_ON_LAN			0x19
 #define I40E_SR_ALTERNATE_SAN_MAC_ADDRESS_PTR	0x27
+#define I40E_SR_PERMANENT_SAN_MAC_ADDRESS_PTR	0x28
+#define I40E_SR_NVM_MAP_VERSION			0x29
+#define I40E_SR_NVM_IMAGE_VERSION		0x2A
+#define I40E_SR_NVM_STRUCTURE_VERSION		0x2B
 #define I40E_SR_NVM_EETRACK_LO			0x2D
 #define I40E_SR_NVM_EETRACK_HI			0x2E
 #define I40E_SR_VPD_PTR				0x2F
@@ -1443,9 +1497,27 @@
 	I40E_RESET_EMPR		= 3,
 };
 
+/* EMP Settings Module Header Section */
+struct i40e_emp_settings_module {
+	u16 length;
+	u16 fw_params;
+	u16 reserved;
+	u16 features;
+	u16 oem_cfg;
+	u16 pfalloc_ptr;
+	u16 eee_variables;
+	u16 lldp_cfg_ptr;
+	u16 ltr_max_snoop;
+	u16 ltr_max_no_snoop;
+	u16 ltr_delta;
+	u16 ltr_grade_value;
+	u16 lldp_tlv_ptr;
+	u16 crc8;
+};
+
 /* IEEE 802.1AB LLDP Agent Variables from NVM */
-#define I40E_NVM_LLDP_CFG_PTR	0x06
-#define I40E_SR_LLDP_CFG_PTR	0x31
+#define I40E_NVM_LLDP_CFG_PTR   0x06
+#define I40E_SR_LLDP_CFG_PTR    0x31
 struct i40e_lldp_variables {
 	u16 length;
 	u16 adminstatus;
@@ -1487,6 +1559,8 @@
 #define I40E_L4_DST_MASK		(0x1ULL << I40E_L4_DST_SHIFT)
 #define I40E_VERIFY_TAG_SHIFT		31
 #define I40E_VERIFY_TAG_MASK		(0x3ULL << I40E_VERIFY_TAG_SHIFT)
+#define I40E_VLAN_SRC_SHIFT		55
+#define I40E_VLAN_SRC_MASK		(0x1ULL << I40E_VLAN_SRC_SHIFT)
 
 #define I40E_FLEX_50_SHIFT		13
 #define I40E_FLEX_50_MASK		(0x1ULL << I40E_FLEX_50_SHIFT)
@@ -1539,6 +1613,8 @@
 	struct i40e_ddp_version version;
 #define I40E_DDP_TRACKID_RDONLY		0
 #define I40E_DDP_TRACKID_INVALID	0xFFFFFFFF
+#define I40E_DDP_TRACKID_GRP_MSK	0x00FF0000
+#define I40E_DDP_TRACKID_GRP_COMP_ALL	0xFF
 	u32 track_id;
 	char name[I40E_DDP_NAME_SIZE];
 };
@@ -1606,4 +1682,10 @@
 	u8 reserved[7];
 	u8 name[I40E_DDP_NAME_SIZE];
 };
+
+#define I40E_BCM_PHY_PCS_STATUS1_PAGE	0x3
+#define I40E_BCM_PHY_PCS_STATUS1_REG	0x0001
+#define I40E_BCM_PHY_PCS_STATUS1_RX_LPI	BIT(8)
+#define I40E_BCM_PHY_PCS_STATUS1_TX_LPI	BIT(9)
+
 #endif /* _I40E_TYPE_H_ */
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_virtchnl_pf.c linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_virtchnl_pf.c
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_virtchnl_pf.c	2024-05-10 01:26:45.389079797 -0400
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_virtchnl_pf.c	2024-05-13 03:58:58.100242665 -0400
@@ -1,5 +1,5 @@
 // SPDX-License-Identifier: GPL-2.0
-/* Copyright(c) 2013 - 2018 Intel Corporation. */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
 
 #include "i40e.h"
 
@@ -40,6 +40,73 @@
 }
 
 /**
+ * i40e_vc_link_speed2mbps - Convert AdminQ link_speed bit represented
+ * to integer value of Mbps
+ * @link_speed: the speed to convert
+ *
+ * Returns the speed as direct value of Mbps.
+ **/
+static INLINE u32
+i40e_vc_link_speed2mbps(enum i40e_aq_link_speed link_speed)
+{
+	switch (link_speed) {
+	case I40E_LINK_SPEED_100MB:
+		return SPEED_100;
+	case I40E_LINK_SPEED_1GB:
+		return SPEED_1000;
+	case I40E_LINK_SPEED_2_5GB:
+		return SPEED_2500;
+	case I40E_LINK_SPEED_5GB:
+		return SPEED_5000;
+	case I40E_LINK_SPEED_10GB:
+		return SPEED_10000;
+	case I40E_LINK_SPEED_20GB:
+		return SPEED_20000;
+	case I40E_LINK_SPEED_25GB:
+		return SPEED_25000;
+	case I40E_LINK_SPEED_40GB:
+		return SPEED_40000;
+	case I40E_LINK_SPEED_UNKNOWN:
+		return SPEED_UNKNOWN;
+	}
+	return SPEED_UNKNOWN;
+}
+
+/**
+ * i40e_set_vf_link_state
+ * @vf: pointer to the VF structure
+ * @pfe: pointer to PF event structure
+ * @ls: pointer to link status structure
+ *
+ * set a link state on a single vf
+ **/
+static void
+i40e_set_vf_link_state(struct i40e_vf *vf,
+		       struct virtchnl_pf_event *pfe,
+		       struct i40e_link_status *ls)
+{
+	u8 link_status = ls->link_info & I40E_AQ_LINK_UP;
+
+#ifdef HAVE_NDO_SET_VF_LINK_STATE
+	if (vf->link_forced)
+		link_status = vf->link_up;
+#endif
+
+	if (vf->driver_caps & VIRTCHNL_VF_CAP_ADV_LINK_SPEED) {
+		pfe->event_data.link_event_adv.link_speed =
+			link_status ? i40e_vc_link_speed2mbps(ls->link_speed) :
+				0;
+		pfe->event_data.link_event_adv.link_status = link_status;
+	} else {
+		pfe->event_data.link_event.link_speed =
+			link_status ?
+				i40e_virtchnl_link_speed(ls->link_speed) :
+				VIRTCHNL_LINK_SPEED_UNKNOWN;
+		pfe->event_data.link_event.link_status = link_status;
+	}
+}
+
+/**
  * i40e_vc_notify_vf_link_state
  * @vf: pointer to the VF structure
  *
@@ -56,23 +123,10 @@
 	pfe.event = VIRTCHNL_EVENT_LINK_CHANGE;
 	pfe.severity = PF_EVENT_SEVERITY_INFO;
 
-	/* Always report link is down if the VF queues aren't enabled */
-	if (!vf->queues_enabled) {
-		pfe.event_data.link_event.link_status = false;
-		pfe.event_data.link_event.link_speed = 0;
-	} else if (vf->link_forced) {
-		pfe.event_data.link_event.link_status = vf->link_up;
-		pfe.event_data.link_event.link_speed =
-			(vf->link_up ? VIRTCHNL_LINK_SPEED_40GB : 0);
-	} else {
-		pfe.event_data.link_event.link_status =
-			ls->link_info & I40E_AQ_LINK_UP;
-		pfe.event_data.link_event.link_speed =
-			i40e_virtchnl_link_speed(ls->link_speed);
-	}
+	i40e_set_vf_link_state(vf, &pfe, ls);
 
 	i40e_aq_send_msg_to_vf(hw, abs_vf_id, VIRTCHNL_OP_EVENT,
-			       0, (u8 *)&pfe, sizeof(pfe), NULL);
+			       I40E_SUCCESS, (u8 *)&pfe, sizeof(pfe), NULL);
 }
 
 /**
@@ -101,11 +155,43 @@
 
 	pfe.event = VIRTCHNL_EVENT_RESET_IMPENDING;
 	pfe.severity = PF_EVENT_SEVERITY_CERTAIN_DOOM;
-	i40e_vc_vf_broadcast(pf, VIRTCHNL_OP_EVENT, 0,
+	i40e_vc_vf_broadcast(pf, VIRTCHNL_OP_EVENT, I40E_SUCCESS,
 			     (u8 *)&pfe, sizeof(struct virtchnl_pf_event));
 }
 
 /**
+ * i40e_restore_all_vfs_msi_state - restore VF MSI state after PF FLR
+ * @pdev: pointer to a pci_dev structure
+ *
+ * Called when recovering from a PF FLR to restore interrupt capability to
+ * the VFs.
+ */
+void i40e_restore_all_vfs_msi_state(struct pci_dev *pdev)
+{
+	struct pci_dev *vfdev;
+	u16 vf_id;
+	int pos;
+
+	/* Continue only if this is a PF */
+	if (!pdev->is_physfn)
+		return;
+
+	if (!pci_num_vf(pdev))
+		return;
+
+	pos = pci_find_ext_capability(pdev, PCI_EXT_CAP_ID_SRIOV);
+	if (pos) {
+		pci_read_config_word(pdev, pos + PCI_SRIOV_VF_DID, &vf_id);
+		vfdev = pci_get_device(pdev->vendor, vf_id, NULL);
+		while (vfdev) {
+			if (vfdev->is_virtfn && pci_physfn(pdev) == vfdev)
+				pci_restore_msi_state(vfdev);
+			vfdev = pci_get_device(pdev->vendor, vf_id, vfdev);
+		}
+	}
+}
+
+/**
  * i40e_vc_notify_vf_reset
  * @vf: pointer to the VF structure
  *
@@ -125,27 +211,33 @@
 	    !test_bit(I40E_VF_STATE_ACTIVE, &vf->vf_states))
 		return;
 
+	if (ktime_get_ns() - vf->reset_timestamp < I40E_VF_RESET_TIME_MIN)
+		usleep_range(30000, 60000);
+
 	abs_vf_id = vf->vf_id + (int)vf->pf->hw.func_caps.vf_base_id;
 
 	pfe.event = VIRTCHNL_EVENT_RESET_IMPENDING;
 	pfe.severity = PF_EVENT_SEVERITY_CERTAIN_DOOM;
 	i40e_aq_send_msg_to_vf(&vf->pf->hw, abs_vf_id, VIRTCHNL_OP_EVENT,
-			       0, (u8 *)&pfe,
+			       I40E_SUCCESS, (u8 *)&pfe,
 			       sizeof(struct virtchnl_pf_event), NULL);
 }
 /***********************misc routines*****************************/
 
 /**
- * i40e_vc_disable_vf
+ * i40e_vc_reset_vf
  * @vf: pointer to the VF info
+ * @notify_vf: notify vf about reset or not
  *
- * Disable the VF through a SW reset.
+ * Reset VF handler.
  **/
-static inline void i40e_vc_disable_vf(struct i40e_vf *vf)
+static inline void i40e_vc_reset_vf(struct i40e_vf *vf, bool notify_vf)
 {
+	struct i40e_pf *pf = vf->pf;
 	int i;
 
-	i40e_vc_notify_vf_reset(vf);
+	if (notify_vf)
+		i40e_vc_notify_vf_reset(vf);
 
 	/* We want to ensure that an actual reset occurs initiated after this
 	 * function was called. However, we do not want to wait forever, so
@@ -153,14 +245,26 @@
 	 * ensure a reset.
 	 */
 	for (i = 0; i < 20; i++) {
+		/* If pf is in vfs releasing state reset vf is impossible,
+		 * so leave it.
+		 */
+		if (test_bit(__I40E_VFS_RELEASING, pf->state))
+			return;
+
 		if (i40e_reset_vf(vf, false))
 			return;
+
 		usleep_range(10000, 20000);
 	}
 
-	dev_warn(&vf->pf->pdev->dev,
-		 "Failed to initiate reset for VF %d after 200 milliseconds\n",
-		 vf->vf_id);
+	if (notify_vf)
+		dev_warn(&vf->pf->pdev->dev,
+			 "Failed to initiate reset for VF %d after 200 milliseconds\n",
+			 vf->vf_id);
+	else
+		dev_dbg(&vf->pf->pdev->dev,
+			"Failed to initiate reset for VF %d after 200 milliseconds\n",
+			vf->vf_id);
 }
 
 /**
@@ -266,8 +370,8 @@
 			 * given VSI.
 			 */
 			queue_id -= vf->ch[i].num_qps;
-			}
 		}
+	}
 
 	return i40e_vc_get_pf_queue_id(vf, vsi_id, queue_id);
 }
@@ -353,7 +457,7 @@
 							   vsi_queue_id);
 		} else {
 			pf_queue_id = I40E_QUEUE_END_OF_LIST;
-			qtype = 0;
+			qtype = I40E_QUEUE_TYPE_RX;
 		}
 
 		/* format for the RQCTL & TQCTL regs is same */
@@ -382,156 +486,6 @@
 }
 
 /**
- * i40e_release_iwarp_qvlist
- * @vf: pointer to the VF.
- *
- **/
-static void i40e_release_iwarp_qvlist(struct i40e_vf *vf)
-{
-	struct i40e_pf *pf = vf->pf;
-	struct virtchnl_iwarp_qvlist_info *qvlist_info = vf->qvlist_info;
-	u32 msix_vf;
-	u32 i;
-
-	if (!vf->qvlist_info)
-		return;
-
-	msix_vf = pf->hw.func_caps.num_msix_vectors_vf;
-	for (i = 0; i < qvlist_info->num_vectors; i++) {
-		struct virtchnl_iwarp_qv_info *qv_info;
-		u32 next_q_index, next_q_type;
-		struct i40e_hw *hw = &pf->hw;
-		u32 v_idx, reg_idx, reg;
-
-		qv_info = &qvlist_info->qv_info[i];
-		if (!qv_info)
-			continue;
-		v_idx = qv_info->v_idx;
-		if (qv_info->ceq_idx != I40E_QUEUE_INVALID_IDX) {
-			/* Figure out the queue after CEQ and make that the
-			 * first queue.
-			 */
-			reg_idx = (msix_vf - 1) * vf->vf_id + qv_info->ceq_idx;
-			reg = rd32(hw, I40E_VPINT_CEQCTL(reg_idx));
-			next_q_index = (reg & I40E_VPINT_CEQCTL_NEXTQ_INDX_MASK)
-					>> I40E_VPINT_CEQCTL_NEXTQ_INDX_SHIFT;
-			next_q_type = (reg & I40E_VPINT_CEQCTL_NEXTQ_TYPE_MASK)
-					>> I40E_VPINT_CEQCTL_NEXTQ_TYPE_SHIFT;
-
-			reg_idx = ((msix_vf - 1) * vf->vf_id) + (v_idx - 1);
-			reg = (next_q_index &
-			       I40E_VPINT_LNKLSTN_FIRSTQ_INDX_MASK) |
-			       (next_q_type <<
-			       I40E_VPINT_LNKLSTN_FIRSTQ_TYPE_SHIFT);
-
-			wr32(hw, I40E_VPINT_LNKLSTN(reg_idx), reg);
-		}
-	}
-	kfree(vf->qvlist_info);
-	vf->qvlist_info = NULL;
-}
-
-/**
- * i40e_config_iwarp_qvlist
- * @vf: pointer to the VF info
- * @qvlist_info: queue and vector list
- *
- * Return 0 on success or < 0 on error
- **/
-static int i40e_config_iwarp_qvlist(struct i40e_vf *vf,
-				    struct virtchnl_iwarp_qvlist_info *qvlist_info)
-{
-	struct i40e_pf *pf = vf->pf;
-	struct i40e_hw *hw = &pf->hw;
-	struct virtchnl_iwarp_qv_info *qv_info;
-	u32 v_idx, i, reg_idx, reg;
-	u32 next_q_idx, next_q_type;
-	u32 msix_vf;
-	int ret = 0;
-
-	msix_vf = pf->hw.func_caps.num_msix_vectors_vf;
-
-	if (qvlist_info->num_vectors > msix_vf) {
-		dev_warn(&pf->pdev->dev,
-			 "Incorrect number of iwarp vectors %u. Maximum %u allowed.\n",
-			 qvlist_info->num_vectors,
-			 msix_vf);
-		ret = -EINVAL;
-		goto err_out;
-	}
-
-	kfree(vf->qvlist_info);
-	vf->qvlist_info = kzalloc(struct_size(vf->qvlist_info, qv_info,
-					      qvlist_info->num_vectors - 1),
-				  GFP_KERNEL);
-	if (!vf->qvlist_info) {
-		ret = -ENOMEM;
-		goto err_out;
-	}
-	vf->qvlist_info->num_vectors = qvlist_info->num_vectors;
-
-	msix_vf = pf->hw.func_caps.num_msix_vectors_vf;
-	for (i = 0; i < qvlist_info->num_vectors; i++) {
-		qv_info = &qvlist_info->qv_info[i];
-		if (!qv_info)
-			continue;
-
-		/* Validate vector id belongs to this vf */
-		if (!i40e_vc_isvalid_vector_id(vf, qv_info->v_idx)) {
-			ret = -EINVAL;
-			goto err_free;
-		}
-
-		v_idx = qv_info->v_idx;
-
-		vf->qvlist_info->qv_info[i] = *qv_info;
-
-		reg_idx = ((msix_vf - 1) * vf->vf_id) + (v_idx - 1);
-		/* We might be sharing the interrupt, so get the first queue
-		 * index and type, push it down the list by adding the new
-		 * queue on top. Also link it with the new queue in CEQCTL.
-		 */
-		reg = rd32(hw, I40E_VPINT_LNKLSTN(reg_idx));
-		next_q_idx = ((reg & I40E_VPINT_LNKLSTN_FIRSTQ_INDX_MASK) >>
-				I40E_VPINT_LNKLSTN_FIRSTQ_INDX_SHIFT);
-		next_q_type = ((reg & I40E_VPINT_LNKLSTN_FIRSTQ_TYPE_MASK) >>
-				I40E_VPINT_LNKLSTN_FIRSTQ_TYPE_SHIFT);
-
-		if (qv_info->ceq_idx != I40E_QUEUE_INVALID_IDX) {
-			reg_idx = (msix_vf - 1) * vf->vf_id + qv_info->ceq_idx;
-			reg = (I40E_VPINT_CEQCTL_CAUSE_ENA_MASK |
-			(v_idx << I40E_VPINT_CEQCTL_MSIX_INDX_SHIFT) |
-			(qv_info->itr_idx << I40E_VPINT_CEQCTL_ITR_INDX_SHIFT) |
-			(next_q_type << I40E_VPINT_CEQCTL_NEXTQ_TYPE_SHIFT) |
-			(next_q_idx << I40E_VPINT_CEQCTL_NEXTQ_INDX_SHIFT));
-			wr32(hw, I40E_VPINT_CEQCTL(reg_idx), reg);
-
-			reg_idx = ((msix_vf - 1) * vf->vf_id) + (v_idx - 1);
-			reg = (qv_info->ceq_idx &
-			       I40E_VPINT_LNKLSTN_FIRSTQ_INDX_MASK) |
-			       (I40E_QUEUE_TYPE_PE_CEQ <<
-			       I40E_VPINT_LNKLSTN_FIRSTQ_TYPE_SHIFT);
-			wr32(hw, I40E_VPINT_LNKLSTN(reg_idx), reg);
-		}
-
-		if (qv_info->aeq_idx != I40E_QUEUE_INVALID_IDX) {
-			reg = (I40E_VPINT_AEQCTL_CAUSE_ENA_MASK |
-			(v_idx << I40E_VPINT_AEQCTL_MSIX_INDX_SHIFT) |
-			(qv_info->itr_idx << I40E_VPINT_AEQCTL_ITR_INDX_SHIFT));
-
-			wr32(hw, I40E_VPINT_AEQCTL(vf->vf_id), reg);
-		}
-	}
-
-	return 0;
-err_free:
-	kfree(vf->qvlist_info);
-	vf->qvlist_info = NULL;
-err_out:
-	return ret;
-}
-
-/**
  * i40e_config_vsi_tx_queue
  * @vf: pointer to the VF info
  * @vsi_id: id of VSI as provided by the FW
@@ -549,8 +503,8 @@
 	struct i40e_hmc_obj_txq tx_ctx;
 	struct i40e_vsi *vsi;
 	u16 pf_queue_id;
+	int ret = 0, i;
 	u32 qtx_ctl;
-	int ret = 0;
 
 	if (!i40e_vc_isvalid_vsi_id(vf, info->vsi_id)) {
 		ret = -ENOENT;
@@ -569,7 +523,30 @@
 	/* only set the required fields */
 	tx_ctx.base = info->dma_ring_addr / 128;
 	tx_ctx.qlen = info->ring_len;
-	tx_ctx.rdylist = le16_to_cpu(vsi->info.qs_handle[0]);
+
+	if (vsi->tc_config.enabled_tc == 1) {
+		tx_ctx.rdylist = le16_to_cpu(vsi->info.qs_handle[0]);
+	} else {
+		for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
+			/* If queue is assigned to this TC */
+			if (vsi->tc_config.tc_info[i].qoffset <=
+			    vsi_queue_id && vsi_queue_id <
+			    vsi->tc_config.tc_info[i].qoffset +
+			    vsi->tc_config.tc_info[i].qcount)
+				break;
+		}
+
+		/* If queue was somehow assigned to nonextisting queue set,
+		 * or queue did not find it's TC, assign it to queue set 0
+		 */
+		if (i >= I40E_MAX_TRAFFIC_CLASS ||
+		    le16_to_cpu(vsi->info.qs_handle[i]) ==
+		    I40E_AQ_VSI_QS_HANDLE_INVALID)
+			tx_ctx.rdylist = le16_to_cpu(vsi->info.qs_handle[0]);
+		else
+			tx_ctx.rdylist = le16_to_cpu(vsi->info.qs_handle[i]);
+	}
+
 	tx_ctx.rdylist_act = 0;
 	tx_ctx.head_wb_ena = info->headwb_enabled;
 	tx_ctx.head_wb_addr = info->dma_headwb_addr;
@@ -624,6 +601,7 @@
 	struct i40e_pf *pf = vf->pf;
 	struct i40e_hw *hw = &pf->hw;
 	struct i40e_hmc_obj_rxq rx_ctx;
+	struct i40e_vsi *vsi = pf->vsi[vf->lan_vsi_idx];
 	u16 pf_queue_id;
 	int ret = 0;
 
@@ -648,7 +626,7 @@
 		}
 		rx_ctx.hbuff = info->hdr_size >> I40E_RXQ_CTX_HBUFF_SHIFT;
 
-		/* set split mode 10b */
+		/* set splitalways mode 10b */
 		rx_ctx.dtype = I40E_RX_DTYPE_HEADER_SPLIT;
 	}
 
@@ -666,6 +644,10 @@
 	}
 	rx_ctx.rxmax = info->max_pkt_size;
 
+	/* if port/outer VLAN is configured increase the max packet size */
+	if (i40e_is_vid(&vsi->info))
+		rx_ctx.rxmax += VLAN_HLEN;
+
 	/* enable 32bytes desc always */
 	rx_ctx.dsize = 1;
 
@@ -700,6 +682,797 @@
 }
 
 /**
+ * i40e_validate_vf
+ * @pf: the physical function
+ * @vf_id: VF identifier
+ *
+ * Check that the VF is enabled and the vsi exists.
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_validate_vf(struct i40e_pf *pf, int vf_id)
+{
+	struct i40e_vsi *vsi;
+	struct i40e_vf *vf;
+	int ret = 0;
+
+	if (vf_id >= pf->num_alloc_vfs) {
+		dev_err(&pf->pdev->dev,
+			"Invalid VF Identifier %d\n", vf_id);
+		ret = -EINVAL;
+		goto err_out;
+	}
+	vf = &pf->vf[vf_id];
+	vsi = i40e_find_vsi_from_id(pf, vf->lan_vsi_id);
+	if (!vsi)
+		ret = -EINVAL;
+err_out:
+	return ret;
+}
+
+#ifdef HAVE_NDO_SET_VF_LINK_STATE
+
+/**
+ * i40e_set_spoof_settings
+ * @vsi: VF VSI to configure
+ * @sec_flag: the spoof check flag to enable or disable
+ * @enable: enable or disable
+ *
+ * This function sets the spoof check settings
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_set_spoof_settings(struct i40e_vsi *vsi, u8 sec_flag,
+				   bool enable)
+{
+	struct i40e_pf *pf = vsi->back;
+	struct i40e_hw *hw = &pf->hw;
+	struct i40e_vsi_context ctxt;
+	int ret = 0;
+
+	vsi->info.valid_sections = CPU_TO_LE16(I40E_AQ_VSI_PROP_SECURITY_VALID);
+	if (enable)
+		vsi->info.sec_flags |= sec_flag;
+	else
+		vsi->info.sec_flags &= ~sec_flag;
+
+	memset(&ctxt, 0, sizeof(ctxt));
+	ctxt.seid = vsi->seid;
+	ctxt.pf_num = vsi->back->hw.pf_id;
+	ctxt.info = vsi->info;
+	ret = i40e_aq_update_vsi_params(hw, &ctxt, NULL);
+	if (ret) {
+		dev_err(&pf->pdev->dev, "Error %d updating VSI parameters\n",
+			ret);
+		ret = -EIO;
+	}
+	return ret;
+}
+
+/**
+ * i40e_configure_vf_loopback
+ * @vsi: VF VSI to configure
+ * @vf_id: VF identifier
+ * @enable: enable or disable
+ *
+ * This function configures the VF VSI with the loopback settings
+ *
+ * Returns 0 on success, negative on failure
+ *
+ **/
+static int i40e_configure_vf_loopback(struct i40e_vsi *vsi, int vf_id,
+				      bool enable)
+{
+	struct i40e_pf *pf = vsi->back;
+	struct i40e_vsi_context ctxt;
+	int ret = 0;
+
+	vsi->info.valid_sections = CPU_TO_LE16(I40E_AQ_VSI_PROP_SWITCH_VALID);
+	if (enable)
+		vsi->info.switch_id |=
+				CPU_TO_LE16(I40E_AQ_VSI_SW_ID_FLAG_ALLOW_LB);
+	else
+		vsi->info.switch_id &=
+				~CPU_TO_LE16(I40E_AQ_VSI_SW_ID_FLAG_ALLOW_LB);
+
+	memset(&ctxt, 0, sizeof(ctxt));
+	ctxt.seid = vsi->seid;
+	ctxt.pf_num = vsi->back->hw.pf_id;
+	ctxt.info = vsi->info;
+	ret = i40e_aq_update_vsi_params(&pf->hw, &ctxt, NULL);
+	if (ret) {
+		dev_err(&pf->pdev->dev, "Error %d configuring loopback for VF %d\n",
+			ret, vf_id);
+		ret = -EIO;
+	}
+	return ret;
+}
+
+/**
+ * i40e_configure_vf_outer_vlan_stripping
+ * @vsi: VF VSI to configure
+ * @vf_id: VF identifier
+ * @enable: enable or disable
+ *
+ * This function enables or disables outer vlan stripping on the VF
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_configure_vf_outer_vlan_stripping(struct i40e_vsi *vsi,
+						  int vf_id,
+						  bool enable)
+{
+	struct i40e_pf *pf = vsi->back;
+	struct i40e_vsi_context ctxt;
+	int ret = 0;
+	u8 flag;
+
+	vsi->info.valid_sections = cpu_to_le16(I40E_AQ_VSI_PROP_VLAN_VALID);
+	if (enable) {
+		/* Don't enable vlan stripping if outer vlan is set */
+		if (vsi->info.outer_vlan) {
+			dev_err(&pf->pdev->dev,
+				"Cannot enable vlan stripping when port VLAN is set\n");
+			ret = -EINVAL;
+			goto err_out;
+		}
+		flag = I40E_AQ_VSI_OVLAN_EMOD_SHOW_ALL;
+	} else {
+		flag = I40E_AQ_VSI_OVLAN_EMOD_NOTHING;
+	}
+	vsi->info.outer_vlan_flags = I40E_AQ_VSI_OVLAN_MODE_ALL |
+		(flag << I40E_AQ_VSI_OVLAN_EMOD_SHIFT) |
+		(I40E_AQ_VSI_OVLAN_CTRL_ENA << I40E_AQ_VSI_OVLAN_EMOD_SHIFT);
+	ctxt.seid = vsi->seid;
+	ctxt.info = vsi->info;
+	ret = i40e_aq_update_vsi_params(&pf->hw, &ctxt, NULL);
+	if (ret) {
+		dev_err(&pf->pdev->dev, "Error %d configuring vlan stripping for VF %d\n",
+			ret, vf_id);
+		ret = -EIO;
+	}
+err_out:
+	return ret;
+}
+
+/**
+ * i40e_configure_vf_vlan_stripping
+ * @vsi: VF VSI to configure
+ * @vf_id: VF identifier
+ * @enable: enable or disable
+ *
+ * This function enables or disables vlan stripping on the VF
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_configure_vf_vlan_stripping(struct i40e_vsi *vsi, int vf_id,
+					    bool enable)
+{
+	struct i40e_pf *pf = vsi->back;
+	struct i40e_vsi_context ctxt;
+	int ret = 0;
+	u8 flag;
+
+	if (i40e_is_double_vlan(&pf->hw))
+		return i40e_configure_vf_outer_vlan_stripping(vsi, vf_id,
+							      enable);
+
+	vsi->info.valid_sections = cpu_to_le16(I40E_AQ_VSI_PROP_VLAN_VALID);
+	if (enable) {
+		/* Don't enable vlan stripping if port vlan is set */
+		if (vsi->info.pvid) {
+			dev_err(&pf->pdev->dev,
+				"Cannot enable vlan stripping when port VLAN is set\n");
+			ret = -EINVAL;
+			goto err_out;
+		}
+		flag = I40E_AQ_VSI_PVLAN_EMOD_STR_BOTH;
+	} else {
+		flag = I40E_AQ_VSI_PVLAN_EMOD_NOTHING;
+	}
+	vsi->info.port_vlan_flags = I40E_AQ_VSI_PVLAN_MODE_ALL | flag;
+	ctxt.seid = vsi->seid;
+	ctxt.info = vsi->info;
+	ret = i40e_aq_update_vsi_params(&pf->hw, &ctxt, NULL);
+	if (ret) {
+		dev_err(&pf->pdev->dev, "Error %d configuring vlan stripping for VF %d\n",
+			ret, vf_id);
+		ret = -EIO;
+	}
+err_out:
+	return ret;
+}
+
+/**
+ * i40e_configure_vf_promisc_mode
+ * @vf: VF
+ * @vsi: VF VSI to configure
+ * @promisc_mode: promisc mode to configure
+ *
+ * This function configures the requested promisc mode for a vf
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_configure_vf_promisc_mode(struct i40e_vf *vf,
+					  struct i40e_vsi *vsi,
+					  u8 promisc_mode)
+{
+	struct i40e_pf *pf = vsi->back;
+	int ret = 0;
+
+	if (promisc_mode & VFD_PROMISC_MULTICAST) {
+		ret = i40e_aq_set_vsi_multicast_promiscuous(&pf->hw, vsi->seid,
+							    true, NULL);
+		if (ret)
+			goto err;
+		vf->promisc_mode |= VFD_PROMISC_MULTICAST;
+	} else {
+		ret = i40e_aq_set_vsi_multicast_promiscuous(&pf->hw, vsi->seid,
+							    false, NULL);
+		if (ret)
+			goto err;
+		vf->promisc_mode &= ~VFD_PROMISC_MULTICAST;
+	}
+	if (promisc_mode & VFD_PROMISC_UNICAST) {
+		ret = i40e_aq_set_vsi_unicast_promiscuous(&pf->hw, vsi->seid,
+							  true, NULL, true);
+		if (ret)
+			goto err;
+		vf->promisc_mode |= VFD_PROMISC_UNICAST;
+	} else {
+		ret = i40e_aq_set_vsi_unicast_promiscuous(&pf->hw, vsi->seid,
+							  false, NULL, true);
+		if (ret)
+			goto err;
+		vf->promisc_mode &= ~VFD_PROMISC_UNICAST;
+	}
+err:
+	if (ret)
+		dev_err(&pf->pdev->dev, "Error %d configuring promisc mode for VF %d\n",
+			ret, vf->vf_id);
+
+	return ret;
+}
+
+/**
+ * i40e_add_ingress_egress_mirror
+ * @src_vsi: VSI to mirror from
+ * @mirror_vsi: VSI to mirror to
+ * @rule_type: rule type to configure
+ * @rule_id: rule id to store
+ *
+ * This function adds the requested ingress/egress mirror for a vsi
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_add_ingress_egress_mirror(struct i40e_vsi *src_vsi,
+					  struct i40e_vsi *mirror_vsi,
+					  u16 rule_type, u16 *rule_id)
+{
+	u16 dst_seid, rules_used, rules_free, sw_seid;
+	struct i40e_pf *pf = src_vsi->back;
+	int ret, num = 0, cnt = 1;
+	int *vsi_ingress_vlan;
+	int *vsi_egress_vlan;
+	__le16 *mr_list;
+
+	mr_list = kcalloc(cnt, sizeof(__le16), GFP_KERNEL);
+	if (!mr_list) {
+		ret = -ENOMEM;
+		goto err_out;
+	}
+
+	if (src_vsi->type == I40E_VSI_MAIN) {
+		vsi_ingress_vlan = &pf->ingress_vlan;
+		vsi_egress_vlan = &pf->egress_vlan;
+	} else {
+		vsi_ingress_vlan = &pf->vf[src_vsi->vf_id].ingress_vlan;
+		vsi_egress_vlan = &pf->vf[src_vsi->vf_id].egress_vlan;
+	}
+
+	if (I40E_IS_MIRROR_VLAN_ID_VALID(*vsi_ingress_vlan)) {
+		if (src_vsi->type == I40E_VSI_MAIN)
+			dev_err(&pf->pdev->dev,
+				"PF already has an ingress mirroring configured, only one rule per PF is supported!\n");
+		else
+			dev_err(&pf->pdev->dev,
+				"VF=%d already has an ingress mirroring configured, only one rule per VF is supported!\n",
+				src_vsi->vf_id);
+		ret = -EPERM;
+		goto err_out;
+	} else if (I40E_IS_MIRROR_VLAN_ID_VALID(*vsi_egress_vlan)) {
+		if (src_vsi->type == I40E_VSI_MAIN)
+			dev_err(&pf->pdev->dev,
+				"PF already has an egress mirroring configured, only one rule per PF is supported!\n");
+		else
+			dev_err(&pf->pdev->dev,
+				"VF=%d already has an egress mirroring configured, only one rule per VF is supported!\n",
+				src_vsi->vf_id);
+		ret = -EPERM;
+		goto err_out;
+	}
+
+	sw_seid = src_vsi->uplink_seid;
+	dst_seid = mirror_vsi->seid;
+	mr_list[num] = CPU_TO_LE16(src_vsi->seid);
+	ret = i40e_aq_add_mirrorrule(&pf->hw, sw_seid,
+				     rule_type, dst_seid,
+				     cnt, mr_list, NULL,
+				     rule_id, &rules_used,
+				     &rules_free);
+	kfree(mr_list);
+err_out:
+	return ret;
+}
+
+/**
+ * i40e_del_ingress_egress_mirror
+ * @src_vsi: the mirrored VSI
+ * @rule_type: rule type to configure
+ * @rule_id : rule id to delete
+ *
+ * This function deletes the ingress/egress mirror on a VSI
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_del_ingress_egress_mirror(struct i40e_vsi *src_vsi,
+					  u16 rule_type, u16 rule_id)
+{
+	u16 rules_used, rules_free, sw_seid;
+	struct i40e_pf *pf = src_vsi->back;
+	int ret;
+
+	sw_seid = src_vsi->uplink_seid;
+	ret = i40e_aq_delete_mirrorrule(&pf->hw, sw_seid, rule_type,
+					rule_id, 0, NULL, NULL,
+					&rules_used, &rules_free);
+	return ret;
+}
+
+/**
+ * i40e_restore_ingress_egress_mirror
+ * @src_vsi: the mirrored VSI
+ * @mirror: VSI to mirror to
+ * @rule_type: rule type to configure
+ * @rule_id : rule id to delete
+ *
+ * This function restores the configured ingress/egress mirrors
+ *
+ * Returns 0 on success, negative on failure
+ **/
+int i40e_restore_ingress_egress_mirror(struct i40e_vsi *src_vsi,
+				       int mirror, u16 rule_type, u16 *rule_id)
+{
+	struct i40e_vsi *mirror_vsi;
+	struct i40e_vf *mirror_vf;
+	struct i40e_pf *pf;
+	int ret = 0;
+
+	pf = src_vsi->back;
+
+	/* validate the mirror */
+	ret = i40e_validate_vf(pf, mirror);
+	if (ret)
+		goto err_out;
+	mirror_vf = &pf->vf[mirror];
+	mirror_vsi = pf->vsi[mirror_vf->lan_vsi_idx];
+	ret = i40e_add_ingress_egress_mirror(src_vsi, mirror_vsi, rule_type,
+					     rule_id);
+
+err_out:
+	return ret;
+}
+
+/**
+ * i40e_configure_vf_link
+ * @vf: VF
+ * @link: link state to configure
+ *
+ * This function configures the requested link state for a VF
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_configure_vf_link(struct i40e_vf *vf, u8 link)
+{
+	struct virtchnl_pf_event pfe;
+	struct i40e_link_status *ls;
+	struct i40e_pf *pf = vf->pf;
+	struct i40e_hw *hw;
+	int abs_vf_id;
+	int ret = 0;
+
+	hw = &pf->hw;
+	abs_vf_id = vf->vf_id + hw->func_caps.vf_base_id;
+	pfe.event = VIRTCHNL_EVENT_LINK_CHANGE;
+	pfe.severity = PF_EVENT_SEVERITY_INFO;
+	ls = &pf->hw.phy.link_info;
+	switch (link) {
+	case VFD_LINKSTATE_AUTO:
+		vf->link_forced = false;
+		i40e_set_vf_link_state(vf, &pfe, ls);
+		break;
+	case VFD_LINKSTATE_ON:
+		vf->link_forced = true;
+		vf->link_up = true;
+		i40e_set_vf_link_state(vf, &pfe, ls);
+		break;
+	case VFD_LINKSTATE_OFF:
+		vf->link_forced = true;
+		vf->link_up = false;
+		i40e_set_vf_link_state(vf, &pfe, ls);
+		break;
+	default:
+		ret = -EINVAL;
+		goto error_out;
+	}
+
+	/* Notify the VF of its new link state */
+	i40e_aq_send_msg_to_vf(hw, abs_vf_id, VIRTCHNL_OP_EVENT,
+			       I40E_SUCCESS, (u8 *)&pfe, sizeof(pfe), NULL);
+error_out:
+	return ret;
+}
+
+/**
+ * i40e_vf_del_vlan_mirror
+ * @vf: pointer to the VF structure
+ * @vsi: pointer to the VSI structure
+ *
+ * Delete configured mirror vlans
+ *
+ * Returns 0 on success, negative on failure
+ *
+ **/
+static int i40e_vf_del_vlan_mirror(struct i40e_vf *vf, struct i40e_vsi *vsi)
+{
+	u16 rules_used, rules_free, vid;
+	struct i40e_pf *pf = vf->pf;
+	int ret = 0, num = 0, cnt;
+	__le16 *mr_list;
+
+	cnt = bitmap_weight(vf->mirror_vlans, VLAN_N_VID);
+	if (cnt) {
+		mr_list = kcalloc(cnt, sizeof(__le16), GFP_KERNEL);
+		if (!mr_list)
+			return -ENOMEM;
+
+		for_each_set_bit(vid, vf->mirror_vlans, VLAN_N_VID) {
+			mr_list[num] = CPU_TO_LE16(vid);
+			num++;
+		}
+
+		ret = i40e_aq_delete_mirrorrule(&pf->hw, vsi->uplink_seid,
+						I40E_AQC_MIRROR_RULE_TYPE_VLAN,
+						vf->vlan_rule_id, cnt, mr_list,
+						NULL, &rules_used,
+						&rules_free);
+
+		vf->vlan_rule_id = 0;
+		kfree(mr_list);
+	}
+
+	return ret;
+}
+
+/**
+ * i40e_apply_vsi_tc_bw
+ * @vf: pointer to the VF structure
+ * @share: array representing 8 elements of vfd share
+ *
+ * Apply VSI BW credits per TC.
+ *
+ * Returns 0 on success, negative on failure
+ *
+ **/
+static int i40e_apply_vsi_tc_bw(struct i40e_vf *vf, u8 *share)
+{
+	struct i40e_aqc_configure_vsi_tc_bw_data bw_data = {0};
+	struct i40e_pf *pf = vf->pf;
+	struct i40e_vsi *vsi = pf->vsi[vf->lan_vsi_idx];
+	int ret, i;
+
+	if (!share)
+		return -EINVAL;
+
+	/* Reapply share option */
+	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
+		if (BIT(i) & vsi->tc_config.enabled_tc) {
+			bw_data.tc_valid_bits |= BIT(i);
+			bw_data.tc_bw_credits[i] = 1;
+			if (share[i])
+				bw_data.tc_bw_credits[i] = share[i];
+		}
+	}
+
+	if (unlikely(bw_data.tc_valid_bits == 0)) {
+		/* This shouldn't happen, log this */
+		dev_info(&pf->pdev->dev,
+			 "No valid bits provided for VF %d, can't change share settings",
+			 vf->vf_id);
+		ret = -EINVAL;
+		goto err;
+	}
+
+	ret = i40e_aq_config_vsi_tc_bw(&pf->hw, vsi->seid,
+				       &bw_data, NULL);
+	if (ret) {
+		dev_info(&pf->pdev->dev,
+			 "AQ command Config VSI BW allocation per TC failed = %d\n",
+			 ret);
+		goto err;
+	}
+
+	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
+		vsi->info.qs_handle[i] = bw_data.qs_handles[i];
+		vsi->tc_config.tc_info[i].tc_bw_credits =
+			vf->tc_info.max_tc_tx_rate[i];
+	}
+	i40e_vsi_get_bw_info(vsi);
+err:
+	return ret;
+}
+
+/**
+ * i40e_restore_vfd_config
+ * @vf: pointer to the VF structure
+ * @vsi: VF VSI to be configured
+ *
+ * Restore the VF-d config as per the stored configuration
+ *
+ * Returns 0 on success, negative on failure
+ *
+ **/
+static int i40e_restore_vfd_config(struct i40e_vf *vf, struct i40e_vsi *vsi)
+{
+	struct i40e_pf *pf = vf->pf;
+	int ret = 0, cnt = 0;
+	u8 sec_flag;
+	u16 vid;
+
+	/* Restore all VF-d configuration on reset */
+	for_each_set_bit(vid, vf->trunk_vlans, VLAN_N_VID) {
+		ret = i40e_vsi_add_vlan(vsi, vid);
+		if (ret)
+			goto err_out;
+	}
+
+	cnt = bitmap_weight(vf->mirror_vlans, VLAN_N_VID);
+	if (cnt) {
+		u16 rule_type = I40E_AQC_MIRROR_RULE_TYPE_VLAN;
+		u16 rule_id, rules_used, rules_free;
+		u16 sw_seid = vsi->uplink_seid;
+		u16 dst_seid = vsi->seid;
+		__le16 *mr_list;
+		int num = 0;
+
+		mr_list = kcalloc(cnt, sizeof(__le16), GFP_KERNEL);
+		if (!mr_list)
+			return -ENOMEM;
+		for_each_set_bit(vid, vf->mirror_vlans, VLAN_N_VID) {
+			mr_list[num] = CPU_TO_LE16(vid);
+			num++;
+		}
+		ret = i40e_aq_add_mirrorrule(&pf->hw, sw_seid, rule_type,
+					     dst_seid, cnt, mr_list, NULL,
+					     &rule_id, &rules_used,
+					     &rules_free);
+		if (!ret)
+			vf->vlan_rule_id = rule_id;
+		kfree(mr_list);
+	}
+
+	sec_flag = I40E_AQ_VSI_SEC_FLAG_ENABLE_MAC_CHK;
+	ret = i40e_set_spoof_settings(vsi, sec_flag, vf->mac_anti_spoof);
+	if (ret)
+		goto err_out;
+
+	if (vf->vlan_anti_spoof) {
+		sec_flag = I40E_AQ_VSI_SEC_FLAG_ENABLE_VLAN_CHK;
+		ret = i40e_set_spoof_settings(vsi, sec_flag, true);
+		if (ret)
+			goto err_out;
+	}
+
+	ret = i40e_configure_vf_loopback(vsi, vf->vf_id, vf->loopback);
+	if (ret) {
+		vf->loopback = false;
+		goto err_out;
+	}
+
+	if (!vf->vlan_stripping) {
+		ret = i40e_configure_vf_vlan_stripping(vsi, vf->vf_id, false);
+		if (ret) {
+			vf->vlan_stripping = true;
+			goto err_out;
+		}
+	}
+
+	if (vf->promisc_mode) {
+		ret = i40e_configure_vf_promisc_mode(vf, vsi, vf->promisc_mode);
+		if (ret) {
+			vf->promisc_mode = VFD_PROMISC_OFF;
+			goto err_out;
+		}
+	}
+
+	if (vf->link_forced) {
+		u8 link;
+
+		link = (vf->link_up ? VFD_LINKSTATE_ON : VFD_LINKSTATE_OFF);
+		ret = i40e_configure_vf_link(vf, link);
+		if (ret) {
+			vf->link_forced = false;
+			goto err_out;
+		}
+	}
+
+	if (vf->bw_share_applied && vf->bw_share) {
+		struct i40e_aqc_configure_vsi_tc_bw_data bw_data = {0};
+		int i;
+
+		bw_data.tc_valid_bits = 1;
+		bw_data.tc_bw_credits[0] = vf->bw_share;
+
+		ret = i40e_aq_config_vsi_tc_bw(&pf->hw, vsi->seid, &bw_data,
+					       NULL);
+		if (ret) {
+			dev_info(&pf->pdev->dev,
+				 "AQ command Config VSI BW allocation per TC failed = %d\n",
+				 pf->hw.aq.asq_last_status);
+			vf->bw_share_applied = false;
+			goto err_out;
+		}
+
+		for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++)
+			vsi->info.qs_handle[i] = bw_data.qs_handles[i];
+	}
+
+	if (vf->tc_info.applied) {
+		i40e_apply_vsi_tc_bw(vf, vf->tc_info.applied_tc_share);
+
+		ret = i40e_vsi_configure_tc_max_bw(vsi);
+		if (ret)
+			dev_info(&pf->pdev->dev,
+				 "AQ command Config VSI BW allocation per TC failed = %d\n",
+				 ret);
+	}
+
+err_out:
+	return ret;
+}
+
+/**
+ * i40e_copy_mac_list_sync
+ * @vsi: pointer to the vsi structure
+ * @mac_list: pointer to head of mac list
+ *
+ * This function copy mac addresses to mac_list
+ **/
+static int i40e_copy_mac_list_sync(struct i40e_vsi *vsi,
+				   struct list_head *mac_list)
+{
+	struct i40e_mac_filter *f;
+	struct vfd_macaddr *elem;
+	int ret = 0, bkt;
+
+	spin_lock_bh(&vsi->mac_filter_hash_lock);
+	hash_for_each(vsi->mac_filter_hash, bkt, f, hlist) {
+		elem = kzalloc(sizeof(*elem), GFP_ATOMIC);
+		if (!elem) {
+			ret = -ENOMEM;
+			goto error_unlock;
+		}
+		INIT_LIST_HEAD(&elem->list);
+		ether_addr_copy(elem->mac, f->macaddr);
+		list_add_tail(&elem->list, mac_list);
+	}
+error_unlock:
+	spin_unlock_bh(&vsi->mac_filter_hash_lock);
+
+	return ret;
+}
+
+static bool i40e_find_vmmac_on_list(struct i40e_vf *vf, const u8 *macaddr);
+
+/**
+ * i40e_retain_mac_list
+ * @pf: pointer to the PF structure
+ * @vf_id: VF identifier
+ * @vsi_idx: vsi idx
+ *
+ * This function do backup of vf mac_list without broadcast and default
+ * lan address before vsi release
+ **/
+static int i40e_retain_mac_list(struct i40e_pf *pf, int vf_id, u16 vsi_idx)
+{
+	struct vfd_macaddr *tmp, *pos;
+	struct list_head *mac_list;
+	u8 broadcast[ETH_ALEN];
+	struct i40e_vsi *vsi;
+	struct i40e_vf *vf;
+	int ret;
+
+	vf = &pf->vf[vf_id];
+	vsi = pf->vsi[vsi_idx];
+	mac_list = &pf->mac_list[vf_id];
+	eth_broadcast_addr(broadcast);
+	INIT_LIST_HEAD(mac_list);
+
+	ret = i40e_copy_mac_list_sync(vsi, mac_list);
+	if (ret)
+		goto err_copy;
+
+	list_for_each_entry_safe(tmp, pos, mac_list, list) {
+		if (ether_addr_equal(tmp->mac, broadcast) ||
+		    ether_addr_equal(tmp->mac, vf->default_lan_addr.addr) ||
+		    i40e_find_vmmac_on_list(vf, tmp->mac)) {
+			list_del(&tmp->list);
+			kfree(tmp);
+		}
+	}
+err_copy:
+	return ret;
+}
+
+/**
+ * i40e_merge_macs
+ * @vf: pointer to the VF info
+ * @vsi: pointer to the vsi structure
+ * @mac_list: pointer to head of mac list
+ * @force: true if continue merge if any problem occurred, otherwise false
+ *
+ * This function merge mac addresses from mac_list to vsi
+ **/
+static int i40e_merge_macs(struct i40e_vf *vf, struct i40e_vsi *vsi,
+			   struct list_head *mac_list, bool force)
+{
+	struct i40e_pf *pf = vf->pf;
+	struct i40e_mac_filter *f;
+	struct vfd_macaddr *elem;
+	int ret = 0;
+
+	spin_lock_bh(&vsi->mac_filter_hash_lock);
+	list_for_each_entry(elem, mac_list, list) {
+		f = i40e_find_mac(vsi, elem->mac);
+		if (!f) {
+			f = i40e_add_mac_filter(vsi, elem->mac);
+			if (!f) {
+				if (force) {
+					dev_info(&pf->pdev->dev,
+						 "Unable to add MAC filter %pM for VF %d\n",
+						 elem->mac, vf->vf_id);
+				} else {
+					dev_err(&pf->pdev->dev,
+						"Unable to add MAC filter %pM for VF %d\n",
+						elem->mac, vf->vf_id);
+					ret = I40E_ERR_PARAM;
+					break;
+				}
+			}
+		}
+	}
+	spin_unlock_bh(&vsi->mac_filter_hash_lock);
+	return ret;
+}
+
+/**
+ * i40e_free_macs
+ * @mac_list: pointer to head of mac list
+ *
+ * This function release mac addresses list
+ **/
+static void i40e_free_macs(struct list_head *mac_list)
+{
+	struct vfd_macaddr *elem, *tmp;
+
+	list_for_each_entry_safe(elem, tmp, mac_list, list) {
+		list_del(&elem->list);
+		kfree(elem);
+	}
+}
+#endif /* HAVE_NDO_SET_VF_LINK_STATE */
+
+/**
  * i40e_alloc_vsi_res
  * @vf: pointer to the VF info
  * @idx: VSI index, applies only for ADq mode, zero otherwise
@@ -727,8 +1500,14 @@
 
 	if (!idx) {
 		u64 hena = i40e_pf_get_default_rss_hena(pf);
+		bool trunk_conf = false;
 		u8 broadcast[ETH_ALEN];
+		u16 vid;
 
+		for_each_set_bit(vid, vf->trunk_vlans, VLAN_N_VID) {
+			if (vid != vf->port_vlan_id)
+				trunk_conf = true;
+		}
 		vf->lan_vsi_idx = vsi->idx;
 		vf->lan_vsi_id = vsi->id;
 		/* If the port VLAN has been configured and then the
@@ -737,7 +1516,7 @@
 		 * a port VLAN and restore the VSI configuration if
 		 * needed.
 		 */
-		if (vf->port_vlan_id)
+		if (vf->port_vlan_id && !trunk_conf)
 			i40e_vsi_add_pvid(vsi, vf->port_vlan_id);
 
 		spin_lock_bh(&vsi->mac_filter_hash_lock);
@@ -746,7 +1525,7 @@
 						vf->default_lan_addr.addr);
 			if (!f)
 				dev_info(&pf->pdev->dev,
-					 "Could not add MAC filter %pM for VF %d\n",
+					"Could not add MAC filter %pM for VF %d\n",
 					vf->default_lan_addr.addr, vf->vf_id);
 		}
 		eth_broadcast_addr(broadcast);
@@ -754,7 +1533,13 @@
 		if (!f)
 			dev_info(&pf->pdev->dev,
 				 "Could not allocate VF broadcast filter\n");
+
 		spin_unlock_bh(&vsi->mac_filter_hash_lock);
+#ifdef HAVE_NDO_SET_VF_LINK_STATE
+		/* restore pre-reset mac_list */
+		i40e_merge_macs(vf, vsi, &pf->mac_list[vf->vf_id], true);
+		i40e_free_macs(&pf->mac_list[vf->vf_id]);
+#endif /* HAVE_NDO_SET_VF_LINK_STATE */
 		wr32(&pf->hw, I40E_VFQF_HENA1(0, vf->vf_id), (u32)hena);
 		wr32(&pf->hw, I40E_VFQF_HENA1(1, vf->vf_id), (u32)(hena >> 32));
 		/* program mac filter only for VF VSI */
@@ -784,6 +1569,12 @@
 			dev_err(&pf->pdev->dev, "Unable to set tx rate, VF %d, error code %d.\n",
 				vf->vf_id, ret);
 	}
+#ifdef HAVE_NDO_SET_VF_LINK_STATE
+	ret = i40e_restore_vfd_config(vf, vsi);
+	if (ret)
+		dev_err(&pf->pdev->dev,
+			"Failed to restore VF-d config error %d\n", ret);
+#endif /* HAVE_NDO_SET_VF_LINK_STATE */
 
 error_alloc_vsi_res:
 	return ret;
@@ -856,6 +1647,9 @@
 		num_tc = vf->num_tc;
 
 	for (i = 0; i < num_tc; i++) {
+		const u32 queue_mapping_size = ARRAY_SIZE
+			(pf->vsi[vf->lan_vsi_idx]->info.queue_mapping);
+
 		if (vf->adq_enabled) {
 			qps = vf->ch[i].num_qps;
 			vsi_id =  vf->ch[i].vsi_id;
@@ -864,6 +1658,8 @@
 			vsi_id = vf->lan_vsi_id;
 		}
 
+		qps = min(queue_mapping_size, qps);
+
 		for (j = 0; j < qps; j++) {
 			qid = i40e_vc_get_pf_queue_id(vf, vsi_id, j);
 
@@ -925,6 +1721,162 @@
 }
 
 /**
+ * i40e_add_vmvlan_to_list
+ * @vf: pointer to the VF info
+ * @vfl:  pointer to the VF VLAN tag filters list
+ * @vlan_idx: vlan_id index in VLAN tag filters list
+ *
+ * add VLAN tag into the VLAN list for VM
+ **/
+static i40e_status
+i40e_add_vmvlan_to_list(struct i40e_vf *vf,
+			struct virtchnl_vlan_filter_list *vfl,
+			u16 vlan_idx)
+{
+	struct i40e_vm_vlan *vlan_elem;
+
+	vlan_elem = kzalloc(sizeof(*vlan_elem), GFP_KERNEL);
+	if (!vlan_elem)
+		return I40E_ERR_NO_MEMORY;
+	vlan_elem->vlan = vfl->vlan_id[vlan_idx];
+	vlan_elem->vsi_id = vfl->vsi_id;
+	INIT_LIST_HEAD(&vlan_elem->list);
+	vf->num_vlan++;
+	list_add(&vlan_elem->list, &vf->vm_vlan_list);
+	return I40E_SUCCESS;
+}
+
+/**
+ * i40e_del_vmvlan_from_list
+ * @vsi: pointer to the VSI structure
+ * @vf: pointer to the VF info
+ * @vlan: VLAN tag to be removed from the list
+ *
+ * delete VLAN tag from the VLAN list for VM
+ **/
+static void i40e_del_vmvlan_from_list(struct i40e_vsi *vsi,
+				      struct i40e_vf *vf, u16 vlan)
+{
+	struct i40e_vm_vlan *entry, *tmp;
+
+	list_for_each_entry_safe(entry, tmp,
+				 &vf->vm_vlan_list, list) {
+		if (vlan == entry->vlan) {
+			i40e_vsi_kill_vlan(vsi, vlan);
+			vf->num_vlan--;
+			list_del(&entry->list);
+			kfree(entry);
+			break;
+		}
+	}
+}
+
+/**
+ * i40e_free_vmvlan_list
+ * @vsi: pointer to the VSI structure
+ * @vf: pointer to the VF info
+ *
+ * remove whole list of VLAN tags for VM
+ **/
+static void i40e_free_vmvlan_list(struct i40e_vsi *vsi, struct i40e_vf *vf)
+{
+	struct i40e_vm_vlan *entry, *tmp;
+
+	if (list_empty(&vf->vm_vlan_list))
+		return;
+
+	list_for_each_entry_safe(entry, tmp,
+				 &vf->vm_vlan_list, list) {
+		if (vsi)
+			i40e_vsi_kill_vlan(vsi, entry->vlan);
+		list_del(&entry->list);
+		kfree(entry);
+	}
+	vf->num_vlan = 0;
+}
+
+/**
+ * i40e_add_vmmac_to_list
+ * @vf: pointer to the VF info
+ * @macaddr: pointer to the MAC address
+ *
+ * add MAC address into the MAC list for VM
+ **/
+static i40e_status i40e_add_vmmac_to_list(struct i40e_vf *vf,
+					  const u8 *macaddr)
+{
+	struct i40e_vm_mac *mac_elem;
+
+	mac_elem = kzalloc(sizeof(*mac_elem), GFP_ATOMIC);
+
+	if (!mac_elem)
+		return I40E_ERR_NO_MEMORY;
+	ether_addr_copy(mac_elem->macaddr, macaddr);
+	INIT_LIST_HEAD(&mac_elem->list);
+	list_add(&mac_elem->list, &vf->vm_mac_list);
+	return I40E_SUCCESS;
+}
+
+/**
+ * i40e_del_vmmac_from_list
+ * @vf: pointer to the VF info
+ * @macaddr: pointer to the MAC address
+ *
+ * delete MAC address from the MAC list for VM
+ **/
+static void i40e_del_vmmac_from_list(struct i40e_vf *vf, const u8 *macaddr)
+{
+	struct i40e_vm_mac *entry, *tmp;
+
+	list_for_each_entry_safe(entry, tmp, &vf->vm_mac_list, list) {
+		if (ether_addr_equal(macaddr, entry->macaddr)) {
+			list_del(&entry->list);
+			kfree(entry);
+			break;
+		}
+	}
+}
+
+#ifdef HAVE_NDO_SET_VF_LINK_STATE
+/**
+ * i40e_find_vmmac_on_list
+ * @vf: pointer to the VF info
+ * @macaddr: pointer to the MAC address
+ *
+ * Search MAC address on MAC list
+ **/
+static bool i40e_find_vmmac_on_list(struct i40e_vf *vf, const u8 *macaddr)
+{
+	struct i40e_vm_mac *entry, *tmp;
+
+	list_for_each_entry_safe(entry, tmp, &vf->vm_mac_list, list) {
+		if (ether_addr_equal(macaddr, entry->macaddr))
+			return true;
+	}
+	return false;
+}
+#endif /* HAVE_NDO_SET_VF_LINK_STATE */
+
+/**
+ * i40e_free_vmmac_list
+ * @vf: pointer to the VF info
+ *
+ * remove whole list of MAC addresses for VM
+ **/
+static void i40e_free_vmmac_list(struct i40e_vf *vf)
+{
+	struct i40e_vm_mac *entry, *tmp;
+
+	if (list_empty(&vf->vm_mac_list))
+		return;
+
+	list_for_each_entry_safe(entry, tmp, &vf->vm_mac_list, list) {
+		list_del(&entry->list);
+		kfree(entry);
+	}
+}
+
+/**
  * i40e_free_vf_res
  * @vf: pointer to the VF info
  *
@@ -942,12 +1894,21 @@
 	 */
 	clear_bit(I40E_VF_STATE_INIT, &vf->vf_states);
 
+#ifdef HAVE_NDO_SET_VF_LINK_STATE
+	/* Release vlan mirror */
+	if (vf->lan_vsi_idx) {
+		i40e_vf_del_vlan_mirror(vf, pf->vsi[vf->lan_vsi_idx]);
+		if (!test_bit(__I40E_VFS_RELEASING, pf->state))
+			i40e_retain_mac_list(pf, vf->vf_id, vf->lan_vsi_idx);
+	}
+#endif /* HAVE_NDO_SET_VF_LINK_STATE */
+
 	/* It's possible the VF had requeuested more queues than the default so
 	 * do the accounting here when we're about to free them.
 	 */
 	if (vf->num_queue_pairs > I40E_DEFAULT_QUEUES_PER_VF) {
-		pf->queues_left += vf->num_queue_pairs -
-				   I40E_DEFAULT_QUEUES_PER_VF;
+		pf->queues_left +=
+			vf->num_queue_pairs - I40E_DEFAULT_QUEUES_PER_VF;
 	}
 
 	/* free vsi & disconnect it from the parent uplink */
@@ -955,7 +1916,6 @@
 		i40e_vsi_release(pf->vsi[vf->lan_vsi_idx]);
 		vf->lan_vsi_idx = 0;
 		vf->lan_vsi_id = 0;
-		vf->num_mac = 0;
 	}
 
 	/* do the accounting and remove additional ADq VSI's */
@@ -1000,6 +1960,10 @@
 		wr32(hw, reg_idx, reg);
 		i40e_flush(hw);
 	}
+
+	i40e_free_vmvlan_list(NULL, vf);
+	i40e_free_vmmac_list(vf);
+
 	/* reset some of the state variables keeping track of the resources */
 	vf->num_queue_pairs = 0;
 	clear_bit(I40E_VF_STATE_MC_PROMISC, &vf->vf_states);
@@ -1068,6 +2032,8 @@
 	 */
 	vf->num_queue_pairs = total_queue_pairs;
 
+	/* set default queue type for the VF */
+	vf->queue_type = VFD_QUEUE_TYPE_RSS;
 	/* VF is now completely initialized */
 	set_bit(I40E_VF_STATE_INIT, &vf->vf_states);
 
@@ -1108,6 +2074,11 @@
 }
 
 static inline int i40e_getnum_vf_vsi_vlan_filters(struct i40e_vsi *vsi);
+static inline void i40e_get_vlan_list_sync(struct i40e_vsi *vsi, int *num_vlans,
+					   s16 **vlan_list);
+static inline i40e_status
+i40e_set_vsi_promisc(struct i40e_vf *vf, u16 seid, bool multi_enable,
+		     bool unicast_enable, s16 *vl, int num_vlans);
 
 /**
  * i40e_config_vf_promiscuous_mode
@@ -1124,109 +2095,61 @@
 						   bool allmulti,
 						   bool alluni)
 {
+	i40e_status aq_ret = I40E_SUCCESS;
 	struct i40e_pf *pf = vf->pf;
-	struct i40e_hw *hw = &pf->hw;
-	struct i40e_mac_filter *f;
-	i40e_status aq_ret = 0;
 	struct i40e_vsi *vsi;
-	int bkt;
+	int num_vlans;
+	s16 *vl;
 
 	vsi = i40e_find_vsi_from_id(pf, vsi_id);
 	if (!i40e_vc_isvalid_vsi_id(vf, vsi_id) || !vsi)
 		return I40E_ERR_PARAM;
 
 	if (vf->port_vlan_id) {
-		aq_ret = i40e_aq_set_vsi_mc_promisc_on_vlan(hw, vsi->seid,
-							    allmulti,
-							    vf->port_vlan_id,
-							    NULL);
-		if (aq_ret) {
-			int aq_err = pf->hw.aq.asq_last_status;
-
-			dev_err(&pf->pdev->dev,
-				"VF %d failed to set multicast promiscuous mode err %s aq_err %s\n",
-				vf->vf_id,
-				i40e_stat_str(&pf->hw, aq_ret),
-				i40e_aq_str(&pf->hw, aq_err));
-			return aq_ret;
-		}
-
-		aq_ret = i40e_aq_set_vsi_uc_promisc_on_vlan(hw, vsi->seid,
-							    alluni,
-							    vf->port_vlan_id,
-							    NULL);
-		if (aq_ret) {
-			int aq_err = pf->hw.aq.asq_last_status;
-
-			dev_err(&pf->pdev->dev,
-				"VF %d failed to set unicast promiscuous mode err %s aq_err %s\n",
-				vf->vf_id,
-				i40e_stat_str(&pf->hw, aq_ret),
-				i40e_aq_str(&pf->hw, aq_err));
-		}
+		aq_ret = i40e_set_vsi_promisc(vf, vsi->seid, allmulti,
+					      alluni, &vf->port_vlan_id, 1);
 		return aq_ret;
 	} else if (i40e_getnum_vf_vsi_vlan_filters(vsi)) {
-		hash_for_each(vsi->mac_filter_hash, bkt, f, hlist) {
-			if (f->vlan < 0 || f->vlan > I40E_MAX_VLANID)
-				continue;
-			aq_ret = i40e_aq_set_vsi_mc_promisc_on_vlan(hw,
-								    vsi->seid,
-								    allmulti,
-								    f->vlan,
-								    NULL);
-			if (aq_ret) {
-				int aq_err = pf->hw.aq.asq_last_status;
-
-				dev_err(&pf->pdev->dev,
-					"Could not add VLAN %d to multicast promiscuous domain err %s aq_err %s\n",
-					f->vlan,
-					i40e_stat_str(&pf->hw, aq_ret),
-					i40e_aq_str(&pf->hw, aq_err));
-			}
+		i40e_get_vlan_list_sync(vsi, &num_vlans, &vl);
 
-			aq_ret = i40e_aq_set_vsi_uc_promisc_on_vlan(hw,
-								    vsi->seid,
-								    alluni,
-								    f->vlan,
-								    NULL);
-			if (aq_ret) {
-				int aq_err = pf->hw.aq.asq_last_status;
+		if (!vl)
+			return I40E_ERR_NO_MEMORY;
 
-				dev_err(&pf->pdev->dev,
-					"Could not add VLAN %d to Unicast promiscuous domain err %s aq_err %s\n",
-					f->vlan,
-					i40e_stat_str(&pf->hw, aq_ret),
-					i40e_aq_str(&pf->hw, aq_err));
-			}
-		}
+		aq_ret = i40e_set_vsi_promisc(vf, vsi->seid, allmulti, alluni,
+					      vl, num_vlans);
+		kfree(vl);
 		return aq_ret;
 	}
-	aq_ret = i40e_aq_set_vsi_multicast_promiscuous(hw, vsi->seid, allmulti,
-						       NULL);
-	if (aq_ret) {
-		int aq_err = pf->hw.aq.asq_last_status;
+	/* no vlans to set on, set on vsi */
+	aq_ret = i40e_set_vsi_promisc(vf, vsi->seid, allmulti, alluni,
+				      NULL, 0);
+	return aq_ret;
+}
 
-		dev_err(&pf->pdev->dev,
-			"VF %d failed to set multicast promiscuous mode err %s aq_err %s\n",
-			vf->vf_id,
-			i40e_stat_str(&pf->hw, aq_ret),
-			i40e_aq_str(&pf->hw, aq_err));
-		return aq_ret;
-	}
+/**
+ * i40e_sync_vfr_reset
+ * @hw: pointer to hw struct
+ * @vf_id: VF identifier
+ *
+ * Before trigger hardware reset, we need to know if no other process has
+ * reserved the hardware for any reset operations. This check is done by
+ * examining the status of the ADMINQ bit in VF interrupt register.
+ **/
+static int i40e_sync_vfr_reset(struct i40e_hw *hw, int vf_id)
+{
+	u32 reg;
+	int i;
 
-	aq_ret = i40e_aq_set_vsi_unicast_promiscuous(hw, vsi->seid, alluni,
-						     NULL, true);
-	if (aq_ret) {
-		int aq_err = pf->hw.aq.asq_last_status;
+	for (i = 0; i < I40E_VFR_WAIT_COUNT; i++) {
+		reg = rd32(hw, I40E_VFINT_ICR0_ENA(vf_id)) &
+			   I40E_VFINT_ICR0_ADMINQ_MASK;
+		if (reg)
+			return 0;
 
-		dev_err(&pf->pdev->dev,
-			"VF %d failed to set unicast promiscuous mode err %s aq_err %s\n",
-			vf->vf_id,
-			i40e_stat_str(&pf->hw, aq_ret),
-			i40e_aq_str(&pf->hw, aq_err));
+		usleep_range(100, 200);
 	}
 
-	return aq_ret;
+	return -EAGAIN;
 }
 
 /**
@@ -1243,9 +2166,11 @@
 	struct i40e_pf *pf = vf->pf;
 	struct i40e_hw *hw = &pf->hw;
 	u32 reg, reg_idx, bit_idx;
+	bool vf_active;
+	u32 radq;
 
 	/* warn the VF */
-	clear_bit(I40E_VF_STATE_ACTIVE, &vf->vf_states);
+	vf_active = test_and_clear_bit(I40E_VF_STATE_ACTIVE, &vf->vf_states);
 
 	/* Disable VF's configuration API during reset. The flag is re-enabled
 	 * in i40e_alloc_vf_res(), when it's safe again to access VF's VSI.
@@ -1259,11 +2184,22 @@
 	 * just need to clean up, so don't hit the VFRTRIG register.
 	 */
 	if (!flr) {
-		/* reset VF using VPGEN_VFRTRIG reg */
+		/* Sync VFR reset before trigger next one */
+		radq = rd32(hw, I40E_VFINT_ICR0_ENA(vf->vf_id)) &
+			    I40E_VFINT_ICR0_ADMINQ_MASK;
+		if (vf_active && !radq)
+			/* waiting for finish reset by virtual driver */
+			if (i40e_sync_vfr_reset(hw, vf->vf_id))
+				dev_info(&pf->pdev->dev,
+					 "Reset VF %d never finished\n",
+					 vf->vf_id);
+
+		/* Reset VF using VPGEN_VFRTRIG reg. It is also setting
+		 * in progress state in rstat1 register.
+		 */
 		reg = rd32(hw, I40E_VPGEN_VFRTRIG(vf->vf_id));
 		reg |= I40E_VPGEN_VFRTRIG_VFSWR_MASK;
 		wr32(hw, I40E_VPGEN_VFRTRIG(vf->vf_id), reg);
-		i40e_flush(hw);
 	}
 	/* clear the VFLR bit in GLGEN_VFLRSTAT */
 	reg_idx = (hw->func_caps.vf_base_id + vf->vf_id) / 32;
@@ -1312,7 +2248,7 @@
 
 	/* reallocate VF resources to finish resetting the VSI state */
 	if (!i40e_alloc_vf_res(vf)) {
-		int abs_vf_id = vf->vf_id + hw->func_caps.vf_base_id;
+		int abs_vf_id = vf->vf_id + (int)hw->func_caps.vf_base_id;
 		i40e_enable_vf_mappings(vf);
 		set_bit(I40E_VF_STATE_ACTIVE, &vf->vf_states);
 		clear_bit(I40E_VF_STATE_DISABLED, &vf->vf_states);
@@ -1335,8 +2271,7 @@
  * @vf: pointer to the VF structure
  * @flr: VFLR was issued or not
  *
- * Returns true if the VF is in reset, resets successfully, or resets
- * are disabled and false otherwise.
+ * Returns true if the VF is reset, false otherwise.
  **/
 bool i40e_reset_vf(struct i40e_vf *vf, bool flr)
 {
@@ -1351,9 +2286,11 @@
 
 	/* If the VFs have been disabled, this means something else is
 	 * resetting the VF, so we shouldn't continue.
+	 * This is a global state of a PF, so it is possible that,
+	 * a different VF is in reset.
 	 */
 	if (test_and_set_bit(__I40E_VF_DISABLE, pf->state))
-		return true;
+		return false;
 
 	i40e_trigger_vf_reset(vf, flr);
 
@@ -1389,6 +2326,8 @@
 	i40e_cleanup_reset_vf(vf);
 
 	i40e_flush(hw);
+	usleep_range(20000, 40000);
+	vf->reset_timestamp = ktime_get_ns();
 	clear_bit(__I40E_VF_DISABLE, pf->state);
 
 	return true;
@@ -1493,11 +2432,17 @@
 		i40e_cleanup_reset_vf(&pf->vf[v]);
 
 	i40e_flush(hw);
+	usleep_range(20000, 40000);
 	clear_bit(__I40E_VF_DISABLE, pf->state);
 
 	return true;
 }
 
+#ifdef HAVE_NDO_SET_VF_LINK_STATE
+static int i40e_set_pf_egress_mirror(struct pci_dev *pdev, const int mirror);
+static int i40e_set_pf_ingress_mirror(struct pci_dev *pdev, const int mirror);
+#endif /* HAVE_NDO_SET_VF_LINK_STATE */
+
 /**
  * i40e_free_vfs
  * @pf: pointer to the PF structure
@@ -1509,14 +2454,60 @@
 	struct i40e_hw *hw = &pf->hw;
 	u32 reg_idx, bit_idx;
 	int i, tmp, vf_id;
+#ifdef HAVE_NDO_SET_VF_LINK_STATE
+	struct i40e_vsi *src_vsi;
+	u16 rule_type, rule_id;
+	int ret;
+#endif /* HAVE_NDO_SET_VF_LINK_STATE */
 
 	if (!pf->vf)
 		return;
+
+	set_bit(__I40E_VFS_RELEASING, pf->state);
+
 	while (test_and_set_bit(__I40E_VF_DISABLE, pf->state))
 		usleep_range(1000, 2000);
 
 	i40e_notify_client_of_vf_enable(pf, 0);
 
+#ifdef HAVE_NDO_SET_VF_LINK_STATE
+	if (pf->egress_vlan != I40E_NO_VF_MIRROR)
+		i40e_set_pf_egress_mirror(pf->pdev, I40E_NO_VF_MIRROR);
+	if (pf->ingress_vlan != I40E_NO_VF_MIRROR)
+		i40e_set_pf_ingress_mirror(pf->pdev, I40E_NO_VF_MIRROR);
+
+	/* At start we need to clear all ingress and egress mirroring setup.
+	 * We can contiune when we remove all mirroring.
+	 */
+	for (i = 0; i < pf->num_alloc_vfs; i++) {
+		src_vsi = pf->vsi[pf->vf[i].lan_vsi_idx];
+		if (I40E_IS_MIRROR_VLAN_ID_VALID(pf->vf[i].ingress_vlan)) {
+			rule_type = I40E_AQC_MIRROR_RULE_TYPE_VPORT_EGRESS;
+			rule_id = pf->vf[i].ingress_rule_id;
+			ret = i40e_del_ingress_egress_mirror(src_vsi, rule_type,
+							     rule_id);
+			if (ret)
+				dev_warn(&pf->pdev->dev,
+					 "Error %s when tried to remove ingress mirror on VF %d",
+					 i40e_aq_str
+					 (hw, hw->aq.asq_last_status),
+					 pf->vf[i].vf_id);
+		}
+		if (I40E_IS_MIRROR_VLAN_ID_VALID(pf->vf[i].egress_vlan)) {
+			rule_type = I40E_AQC_MIRROR_RULE_TYPE_VPORT_INGRESS;
+			rule_id = pf->vf[i].egress_rule_id;
+			ret = i40e_del_ingress_egress_mirror(src_vsi, rule_type,
+							     rule_id);
+			if (ret)
+				dev_warn(&pf->pdev->dev,
+					 "Error %s when tried to remove egress mirror on VF %d",
+					 i40e_aq_str
+					 (hw, hw->aq.asq_last_status),
+					 pf->vf[i].vf_id);
+		}
+	}
+#endif /* HAVE_NDO_SET_VF_LINK_STATE */
+
 	/* Disable IOV before freeing resources. This lets any VF drivers
 	 * running in the host get themselves cleaned up before we yank
 	 * the carpet out from underneath their feet.
@@ -1550,6 +2541,12 @@
 		/* disable qp mappings */
 		i40e_disable_vf_mappings(&pf->vf[i]);
 	}
+#ifdef HAVE_NDO_SET_VF_LINK_STATE
+	if (pf->vfd_obj) {
+		destroy_vfd_sysfs(pf->pdev, pf->vfd_obj);
+		pf->vfd_obj = NULL;
+	}
+#endif /* HAVE_NDO_SET_VF_LINK_STATE */
 
 	kfree(pf->vf);
 	pf->vf = NULL;
@@ -1569,6 +2566,7 @@
 		}
 	}
 	clear_bit(__I40E_VF_DISABLE, pf->state);
+	clear_bit(__I40E_VFS_RELEASING, pf->state);
 }
 
 #ifdef CONFIG_PCI_IOV
@@ -1604,18 +2602,39 @@
 	}
 	pf->vf = vfs;
 
+#ifdef HAVE_NDO_SET_VF_LINK_STATE
+	/* set vfd ops */
+	vfd_ops = &i40e_vfd_ops;
+	/* create the sriov kobjects */
+	pf->vfd_obj = create_vfd_sysfs(pf->pdev, num_alloc_vfs);
+#endif /* HAVE_NDO_SET_VF_LINK_STATE */
+
 	/* apply default profile */
 	for (i = 0; i < num_alloc_vfs; i++) {
 		vfs[i].pf = pf;
 		vfs[i].parent_type = I40E_SWITCH_ELEMENT_TYPE_VEB;
 		vfs[i].vf_id = i;
 
+#ifdef HAVE_NDO_SET_VF_LINK_STATE
+		/* setup default mirror values */
+		vfs[i].ingress_vlan = I40E_NO_VF_MIRROR;
+		vfs[i].egress_vlan = I40E_NO_VF_MIRROR;
+#endif /* HAVE_NDO_SET_VF_LINK_STATE */
+		/* assign default loopback value */
+		vfs[i].loopback = true;
+		/* assign default mac anti spoof value for untrusted VF */
+		vfs[i].mac_anti_spoof = true;
+		/* assign default allow_untagged value */
+		vfs[i].allow_untagged = true;
+		/* assign default allow_bcast value */
+		vfs[i].allow_bcast = true;
+		/* assign default vlan_stripping value */
+		vfs[i].vlan_stripping = true;
 		/* assign default capabilities */
 		set_bit(I40E_VIRTCHNL_VF_CAP_L2, &vfs[i].vf_caps);
-		vfs[i].spoofchk = true;
-
 		set_bit(I40E_VF_STATE_PRE_ENABLE, &vfs[i].vf_states);
-
+		INIT_LIST_HEAD(&vfs[i].vm_vlan_list);
+		INIT_LIST_HEAD(&vfs[i].vm_mac_list);
 	}
 	pf->num_alloc_vfs = num_alloc_vfs;
 
@@ -1634,6 +2653,7 @@
 }
 
 #endif
+#if defined(HAVE_SRIOV_CONFIGURE) || defined(HAVE_RHEL6_SRIOV_CONFIGURE)
 /**
  * i40e_pci_sriov_enable
  * @pdev: pointer to a pci_dev structure
@@ -1686,7 +2706,7 @@
 /**
  * i40e_pci_sriov_configure
  * @pdev: pointer to a pci_dev structure
- * @num_vfs: number of VFs to allocate
+ * @num_vfs: number of vfs to allocate
  *
  * Enable or change the number of VFs. Called when the user updates the number
  * of VFs in sysfs.
@@ -1704,16 +2724,15 @@
 	if (num_vfs) {
 		if (!(pf->flags & I40E_FLAG_VEB_MODE_ENABLED)) {
 			pf->flags |= I40E_FLAG_VEB_MODE_ENABLED;
-			i40e_do_reset_safe(pf, I40E_PF_RESET_FLAG);
+			i40e_do_reset_safe(pf, I40E_PF_RESET_AND_REBUILD_FLAG);
 		}
 		ret = i40e_pci_sriov_enable(pdev, num_vfs);
 		goto sriov_configure_out;
 	}
-
-	if (!pci_vfs_assigned(pf->pdev)) {
+	if (!pci_vfs_assigned(pdev)) {
 		i40e_free_vfs(pf);
 		pf->flags &= ~I40E_FLAG_VEB_MODE_ENABLED;
-		i40e_do_reset_safe(pf, I40E_PF_RESET_FLAG);
+		i40e_do_reset_safe(pf, I40E_PF_RESET_AND_REBUILD_FLAG);
 	} else {
 		dev_warn(&pdev->dev, "Unable to free VFs because some are assigned to VMs.\n");
 		ret = -EINVAL;
@@ -1723,21 +2742,24 @@
 	clear_bit(__I40E_VIRTCHNL_OP_PENDING, pf->state);
 	return ret;
 }
+#endif
 
 /***********************virtual channel routines******************/
 
 /**
- * i40e_vc_send_msg_to_vf
+ * i40e_vc_send_msg_to_vf_ex
  * @vf: pointer to the VF info
  * @v_opcode: virtual channel opcode
  * @v_retval: virtual channel return value
  * @msg: pointer to the msg buffer
  * @msglen: msg length
+ * @is_quiet: true for not printing unsuccessful return values, false otherwise
  *
  * send msg to VF
  **/
-static int i40e_vc_send_msg_to_vf(struct i40e_vf *vf, u32 v_opcode,
-				  u32 v_retval, u8 *msg, u16 msglen)
+static int i40e_vc_send_msg_to_vf_ex(struct i40e_vf *vf, u32 v_opcode,
+				     u32 v_retval, u8 *msg, u16 msglen,
+				     bool is_quiet)
 {
 	struct i40e_pf *pf;
 	struct i40e_hw *hw;
@@ -1753,7 +2775,7 @@
 	abs_vf_id = vf->vf_id + hw->func_caps.vf_base_id;
 
 	/* single place to detect unsuccessful return values */
-	if (v_retval) {
+	if (v_retval && !is_quiet) {
 		vf->num_invalid_msgs++;
 		dev_info(&pf->pdev->dev, "VF %d failed opcode %d, retval: %d\n",
 			 vf->vf_id, v_opcode, v_retval);
@@ -1771,7 +2793,7 @@
 		vf->num_invalid_msgs = 0;
 	}
 
-	aq_ret = i40e_aq_send_msg_to_vf(hw, abs_vf_id,	v_opcode, v_retval,
+	aq_ret = i40e_aq_send_msg_to_vf(hw, abs_vf_id, v_opcode, v_retval,
 					msg, msglen, NULL);
 	if (aq_ret) {
 		dev_info(&pf->pdev->dev,
@@ -1784,6 +2806,23 @@
 }
 
 /**
+ * i40e_vc_send_msg_to_vf
+ * @vf: pointer to the VF info
+ * @v_opcode: virtual channel opcode
+ * @v_retval: virtual channel return value
+ * @msg: pointer to the msg buffer
+ * @msglen: msg length
+ *
+ * send msg to VF
+ **/
+static int i40e_vc_send_msg_to_vf(struct i40e_vf *vf, u32 v_opcode,
+				  u32 v_retval, u8 *msg, u16 msglen)
+{
+	return i40e_vc_send_msg_to_vf_ex(vf, v_opcode, v_retval,
+					 msg, msglen, false);
+}
+
+/**
  * i40e_vc_send_resp_to_vf
  * @vf: pointer to the VF info
  * @opcode: operation code
@@ -1799,6 +2838,32 @@
 }
 
 /**
+ * i40e_sync_vf_state
+ * @vf: pointer to the VF info
+ * @state: VF state
+ *
+ * Called from a VF message to synchronize the service with a potential
+ * VF reset state
+ **/
+static bool i40e_sync_vf_state(struct i40e_vf *vf, enum i40e_vf_states state)
+{
+	int i;
+
+	/* When handling some messages, it needs vf state to be set.
+	 * It is possible that this flag is cleared during vf reset,
+	 * so there is a need to wait until the end of the reset to
+	 * handle the request message correctly.
+	 */
+	for (i = 0; i < I40E_VF_STATE_WAIT_COUNT; i++) {
+		if (test_bit(state, &vf->vf_states))
+			return true;
+		usleep_range(10000, 20000);
+	}
+
+	return test_bit(state, &vf->vf_states);
+}
+
+/**
  * i40e_vc_get_version_msg
  * @vf: pointer to the VF info
  * @msg: pointer to the msg buffer
@@ -1820,6 +2885,7 @@
 				      sizeof(struct virtchnl_version_info));
 }
 
+#ifdef __TC_MQPRIO_MODE_MAX
 /**
  * i40e_del_qch - delete all the additional VSIs created as a part of ADq
  * @vf: pointer to VF structure
@@ -1840,6 +2906,7 @@
 		}
 	}
 }
+#endif /* __TC_MQPRIO_MODE_MAX */
 
 /**
  * i40e_vc_get_vf_resources_msg
@@ -1851,19 +2918,21 @@
 static int i40e_vc_get_vf_resources_msg(struct i40e_vf *vf, u8 *msg)
 {
 	struct virtchnl_vf_resource *vfres = NULL;
+	i40e_status aq_ret = I40E_SUCCESS;
 	struct i40e_pf *pf = vf->pf;
-	i40e_status aq_ret = 0;
 	struct i40e_vsi *vsi;
 	int num_vsis = 1;
-	size_t len = 0;
+	int len = 0;
 	int ret;
 
-	if (!test_bit(I40E_VF_STATE_INIT, &vf->vf_states)) {
+	if (!i40e_sync_vf_state(vf, I40E_VF_STATE_INIT)) {
 		aq_ret = I40E_ERR_PARAM;
 		goto err;
 	}
 
-	len = struct_size(vfres, vsi_res, num_vsis);
+	len = (sizeof(struct virtchnl_vf_resource) +
+	       sizeof(struct virtchnl_vsi_resource) * num_vsis);
+
 	vfres = kzalloc(len, GFP_KERNEL);
 	if (!vfres) {
 		aq_ret = I40E_ERR_NO_MEMORY;
@@ -1877,18 +2946,12 @@
 				  VIRTCHNL_VF_OFFLOAD_RSS_REG |
 				  VIRTCHNL_VF_OFFLOAD_VLAN;
 
-	vfres->vf_cap_flags = VIRTCHNL_VF_OFFLOAD_L2;
-	vsi = pf->vsi[vf->lan_vsi_idx];
-	if (!vsi->info.pvid)
-		vfres->vf_cap_flags |= VIRTCHNL_VF_OFFLOAD_VLAN;
+	vfres->vf_cap_flags = VIRTCHNL_VF_OFFLOAD_L2 | VIRTCHNL_VF_OFFLOAD_VLAN;
+#ifdef VIRTCHNL_VF_CAP_ADV_LINK_SPEED
+	vfres->vf_cap_flags |= VIRTCHNL_VF_CAP_ADV_LINK_SPEED;
+#endif /* VIRTCHNL_VF_CAP_ADV_LINK_SPEED */
 
-	if (i40e_vf_client_capable(pf, vf->vf_id) &&
-	    (vf->driver_caps & VIRTCHNL_VF_OFFLOAD_IWARP)) {
-		vfres->vf_cap_flags |= VIRTCHNL_VF_OFFLOAD_IWARP;
-		set_bit(I40E_VF_STATE_IWARPENA, &vf->vf_states);
-	} else {
-		clear_bit(I40E_VF_STATE_IWARPENA, &vf->vf_states);
-	}
+	vsi = pf->vsi[vf->lan_vsi_idx];
 
 	if (vf->driver_caps & VIRTCHNL_VF_OFFLOAD_RSS_PF) {
 		vfres->vf_cap_flags |= VIRTCHNL_VF_OFFLOAD_RSS_PF;
@@ -1899,7 +2962,6 @@
 		else
 			vfres->vf_cap_flags |= VIRTCHNL_VF_OFFLOAD_RSS_REG;
 	}
-
 	if (pf->hw_features & I40E_HW_MULTIPLE_TCP_UDP_RSS_PCTYPE) {
 		if (vf->driver_caps & VIRTCHNL_VF_OFFLOAD_RSS_PCTYPE_V2)
 			vfres->vf_cap_flags |=
@@ -1933,8 +2995,10 @@
 	if (vf->driver_caps & VIRTCHNL_VF_OFFLOAD_REQ_QUEUES)
 		vfres->vf_cap_flags |= VIRTCHNL_VF_OFFLOAD_REQ_QUEUES;
 
+#ifdef __TC_MQPRIO_MODE_MAX
 	if (vf->driver_caps & VIRTCHNL_VF_OFFLOAD_ADQ)
 		vfres->vf_cap_flags |= VIRTCHNL_VF_OFFLOAD_ADQ;
+#endif /* __TC_MQPRIO_MODE_MAX */
 
 	vfres->num_vsis = num_vsis;
 	vfres->num_queue_pairs = vf->num_queue_pairs;
@@ -1948,12 +3012,17 @@
 		vfres->vsi_res[0].num_queue_pairs = vsi->alloc_queue_pairs;
 		/* VFs only use TC 0 */
 		vfres->vsi_res[0].qset_handle
-					  = le16_to_cpu(vsi->info.qs_handle[0]);
+					  = LE16_TO_CPU(vsi->info.qs_handle[0]);
 		ether_addr_copy(vfres->vsi_res[0].default_mac_addr,
 				vf->default_lan_addr.addr);
 	}
 	set_bit(I40E_VF_STATE_ACTIVE, &vf->vf_states);
-
+	set_bit(I40E_VF_STATE_RESOURCES_LOADED, &vf->vf_states);
+	/* if vf is in base mode, keep only the base capabilities that are
+	 * negotiated
+	 */
+	if (pf->vf_base_mode_only)
+		vfres->vf_cap_flags &= VF_BASE_MODE_OFFLOADS;
 err:
 	/* send the response back to the VF */
 	ret = i40e_vc_send_msg_to_vf(vf, VIRTCHNL_OP_GET_VF_RESOURCES,
@@ -1964,20 +3033,6 @@
 }
 
 /**
- * i40e_vc_reset_vf_msg
- * @vf: pointer to the VF info
- *
- * called from the VF to reset itself,
- * unlike other virtchnl messages, PF driver
- * doesn't send the response back to the VF
- **/
-static void i40e_vc_reset_vf_msg(struct i40e_vf *vf)
-{
-	if (test_bit(I40E_VF_STATE_ACTIVE, &vf->vf_states))
-		i40e_reset_vf(vf, false);
-}
-
-/**
  * i40e_getnum_vf_vsi_vlan_filters
  * @vsi: pointer to the vsi
  *
@@ -1997,6 +3052,124 @@
 }
 
 /**
+ * i40e_get_vlan_list_sync
+ * @vsi: pointer to the vsi
+ * @num_vlans: number of vlans present in mac_filter_hash, returned to caller
+ * @vlan_list: list of vlans present in mac_filter_hash, returned to caller.
+ *	       This array is allocated here, but has to be freed in caller.
+ *
+ * Called to get number of vlans and vlan list present in mac_filter_hash.
+ **/
+
+static inline void i40e_get_vlan_list_sync(struct i40e_vsi *vsi, int *num_vlans,
+					   s16 **vlan_list)
+{
+	struct i40e_mac_filter *f;
+	int bkt;
+	int i;
+
+	spin_lock_bh(&vsi->mac_filter_hash_lock);
+	*num_vlans = i40e_getnum_vf_vsi_vlan_filters(vsi);
+	*vlan_list = kcalloc(*num_vlans, sizeof(**vlan_list),
+			     GFP_ATOMIC);
+	if (!(*vlan_list))
+		goto err;
+
+	i = 0;
+	hash_for_each(vsi->mac_filter_hash, bkt, f, hlist) {
+		if (f->vlan < 0 || f->vlan > I40E_MAX_VLANID)
+			continue;
+		(*vlan_list)[i++] = f->vlan;
+	}
+err:
+	spin_unlock_bh(&vsi->mac_filter_hash_lock);
+}
+
+/**
+ * i40e_set_vsi_promisc
+ * @vf: pointer to the vf struct
+ * @seid: vsi number
+ * @multi_enable: set MAC L2 layer multicast promiscuous enable/disable
+ *		  for a given VLAN
+ * @unicast_enable: set MAC L2 layer unicast promiscuous enable/disable
+ *		    for a given VLAN
+ * @vl: List of vlans - apply filter for given vlans
+ * @num_vlans: Number of elements in vl
+ **/
+static inline i40e_status
+i40e_set_vsi_promisc(struct i40e_vf *vf, u16 seid, bool multi_enable,
+		     bool unicast_enable, s16 *vl, int num_vlans)
+{
+	i40e_status aq_ret = I40E_SUCCESS;
+	struct i40e_pf *pf = vf->pf;
+	struct i40e_hw *hw = &pf->hw;
+	int i;
+
+	/* No vlan to set promisc on, set on vsi */
+	if (!num_vlans || !vl) {
+		aq_ret = i40e_aq_set_vsi_multicast_promiscuous(hw, seid,
+							       multi_enable,
+							       NULL);
+		if (aq_ret) {
+			int aq_err = pf->hw.aq.asq_last_status;
+
+			dev_err(&pf->pdev->dev,
+				"VF %d failed to set multicast promiscuous mode err %s aq_err %s\n",
+				vf->vf_id,
+				i40e_stat_str(&pf->hw, aq_ret),
+				i40e_aq_str(&pf->hw, aq_err));
+
+			return aq_ret;
+		}
+
+		aq_ret = i40e_aq_set_vsi_unicast_promiscuous(hw, seid,
+							     unicast_enable,
+							     NULL, true);
+
+		if (aq_ret) {
+			int aq_err = pf->hw.aq.asq_last_status;
+
+			dev_err(&pf->pdev->dev,
+				"VF %d failed to set unicast promiscuous mode err %s aq_err %s\n",
+				vf->vf_id,
+				i40e_stat_str(&pf->hw, aq_ret),
+				i40e_aq_str(&pf->hw, aq_err));
+		}
+
+		return aq_ret;
+	}
+
+	for (i = 0; i < num_vlans; i++) {
+		aq_ret = i40e_aq_set_vsi_mc_promisc_on_vlan(hw, seid,
+							    multi_enable,
+							    vl[i], NULL);
+		if (aq_ret) {
+			int aq_err = pf->hw.aq.asq_last_status;
+
+			dev_err(&pf->pdev->dev,
+				"VF %d failed to set multicast promiscuous mode err %s aq_err %s\n",
+				vf->vf_id,
+				i40e_stat_str(&pf->hw, aq_ret),
+				i40e_aq_str(&pf->hw, aq_err));
+		}
+
+		aq_ret = i40e_aq_set_vsi_uc_promisc_on_vlan(hw, seid,
+							    unicast_enable,
+							    vl[i], NULL);
+		if (aq_ret) {
+			int aq_err = pf->hw.aq.asq_last_status;
+
+			dev_err(&pf->pdev->dev,
+				"VF %d failed to set unicast promiscuous mode err %s aq_err %s\n",
+				vf->vf_id,
+				i40e_stat_str(&pf->hw, aq_ret),
+				i40e_aq_str(&pf->hw, aq_err));
+		}
+	}
+	return aq_ret;
+}
+
+/**
  * i40e_vc_config_promiscuous_mode_msg
  * @vf: pointer to the VF info
  * @msg: pointer to the msg buffer
@@ -2008,12 +3181,12 @@
 {
 	struct virtchnl_promisc_info *info =
 	    (struct virtchnl_promisc_info *)msg;
+	i40e_status aq_ret = I40E_SUCCESS;
 	struct i40e_pf *pf = vf->pf;
-	i40e_status aq_ret = 0;
 	bool allmulti = false;
 	bool alluni = false;
 
-	if (!test_bit(I40E_VF_STATE_ACTIVE, &vf->vf_states)) {
+	if (!i40e_sync_vf_state(vf, I40E_VF_STATE_ACTIVE)) {
 		aq_ret = I40E_ERR_PARAM;
 		goto err_out;
 	}
@@ -2021,11 +3194,14 @@
 		dev_err(&pf->pdev->dev,
 			"Unprivileged VF %d is attempting to configure promiscuous mode\n",
 			vf->vf_id);
+		if (pf->vf_base_mode_only)
+			dev_err(&pf->pdev->dev, "VF %d is in base mode only, promiscuous mode is not be supported\n",
+				vf->vf_id);
 
 		/* Lie to the VF on purpose, because this is an error we can
 		 * ignore. Unprivileged VF is not a virtual channel error.
 		 */
-		aq_ret = 0;
+		aq_ret = I40E_SUCCESS;
 		goto err_out;
 	}
 
@@ -2045,6 +3221,7 @@
 
 	if (info->flags & FLAG_VF_UNICAST_PROMISC)
 		alluni = true;
+
 	aq_ret = i40e_config_vf_promiscuous_mode(vf, info->vsi_id, allmulti,
 						 alluni);
 	if (aq_ret)
@@ -2094,13 +3271,14 @@
 	struct virtchnl_vsi_queue_config_info *qci =
 	    (struct virtchnl_vsi_queue_config_info *)msg;
 	struct virtchnl_queue_pair_info *qpi;
-	struct i40e_pf *pf = vf->pf;
+	i40e_status aq_ret = I40E_SUCCESS;
 	u16 vsi_id, vsi_queue_id = 0;
-	u16 num_qps_all = 0;
-	i40e_status aq_ret = 0;
+	struct i40e_pf *pf = vf->pf;
+	struct i40e_vsi *vsi;
 	int i, j = 0, idx = 0;
+	u16 num_qps_all = 0;
 
-	if (!test_bit(I40E_VF_STATE_ACTIVE, &vf->vf_states)) {
+	if (!i40e_sync_vf_state(vf, I40E_VF_STATE_ACTIVE)) {
 		aq_ret = I40E_ERR_PARAM;
 		goto error_param;
 	}
@@ -2116,7 +3294,7 @@
 	}
 
 	if (vf->adq_enabled) {
-		for (i = 0; i < I40E_MAX_VF_VSI; i++)
+		for (i = 0; i < vf->num_tc; i++)
 			num_qps_all += vf->ch[i].num_qps;
 		if (num_qps_all != qci->num_queue_pairs) {
 			aq_ret = I40E_ERR_PARAM;
@@ -2182,14 +3360,21 @@
 			}
 		}
 	}
+
 	/* set vsi num_queue_pairs in use to num configured by VF */
 	if (!vf->adq_enabled) {
 		pf->vsi[vf->lan_vsi_idx]->num_queue_pairs =
 			qci->num_queue_pairs;
 	} else {
-		for (i = 0; i < vf->num_tc; i++)
-			pf->vsi[vf->ch[i].vsi_idx]->num_queue_pairs =
-			       vf->ch[i].num_qps;
+		for (i = 0; i < vf->num_tc; i++) {
+			vsi = pf->vsi[vf->ch[i].vsi_idx];
+			vsi->num_queue_pairs = vf->ch[i].num_qps;
+
+			if (i40e_update_adq_vsi_queues(vsi, i)) {
+				aq_ret = I40E_ERR_CONFIG;
+				goto error_param;
+			}
+		}
 	}
 
 error_param:
@@ -2200,6 +3385,7 @@
 
 /**
  * i40e_validate_queue_map
+ * @vf: pointer to the VF info
  * @vsi_id: vsi id
  * @queuemap: Tx or Rx queue map
  *
@@ -2237,12 +3423,12 @@
 {
 	struct virtchnl_irq_map_info *irqmap_info =
 	    (struct virtchnl_irq_map_info *)msg;
+	i40e_status aq_ret = I40E_SUCCESS;
 	struct virtchnl_vector_map *map;
 	u16 vsi_id;
-	i40e_status aq_ret = 0;
 	int i;
 
-	if (!test_bit(I40E_VF_STATE_ACTIVE, &vf->vf_states)) {
+	if (!i40e_sync_vf_state(vf, I40E_VF_STATE_ACTIVE)) {
 		aq_ret = I40E_ERR_PARAM;
 		goto error_param;
 	}
@@ -2327,12 +3513,12 @@
 }
 
 /**
- * i40e_vc_validate_vqs_bitmaps - validate Rx/Tx queue bitmaps from VIRTHCHNL
+ * i40e_vc_isvalid_vqs_bitmaps - validate Rx/Tx queue bitmaps from VIRTCHNL
  * @vqs: virtchnl_queue_select structure containing bitmaps to validate
  *
- * Returns true if validation was successful, else false.
+ * Returns true if bitmaps are valid, else false
  */
-static bool i40e_vc_validate_vqs_bitmaps(struct virtchnl_queue_select *vqs)
+static bool i40e_vc_isvalid_vqs_bitmaps(struct virtchnl_queue_select *vqs)
 {
 	if ((!vqs->rx_queues && !vqs->tx_queues) ||
 	    vqs->rx_queues >= BIT(I40E_MAX_VF_QUEUES) ||
@@ -2353,11 +3539,18 @@
 {
 	struct virtchnl_queue_select *vqs =
 	    (struct virtchnl_queue_select *)msg;
+	i40e_status aq_ret = I40E_SUCCESS;
 	struct i40e_pf *pf = vf->pf;
-	i40e_status aq_ret = 0;
 	int i;
 
-	if (!test_bit(I40E_VF_STATE_ACTIVE, &vf->vf_states)) {
+	if (vf->pf_ctrl_disable) {
+		aq_ret = I40E_ERR_PARAM;
+		dev_err(&pf->pdev->dev,
+			"Admin has disabled VF %d via sysfs, will not enable queues",
+			 vf->vf_id);
+		goto error_param;
+	}
+	if (!i40e_sync_vf_state(vf, I40E_VF_STATE_ACTIVE)) {
 		aq_ret = I40E_ERR_PARAM;
 		goto error_param;
 	}
@@ -2367,7 +3560,7 @@
 		goto error_param;
 	}
 
-	if (!i40e_vc_validate_vqs_bitmaps(vqs)) {
+	if (!i40e_vc_isvalid_vqs_bitmaps(vqs)) {
 		aq_ret = I40E_ERR_PARAM;
 		goto error_param;
 	}
@@ -2388,13 +3581,19 @@
 	if (vf->adq_enabled) {
 		/* zero belongs to LAN VSI */
 		for (i = 1; i < vf->num_tc; i++) {
-			if (i40e_vsi_start_rings(pf->vsi[vf->ch[i].vsi_idx]))
+			if (i40e_ctrl_vf_rx_rings(pf->vsi[vf->ch[i].vsi_idx],
+						  vqs->rx_queues, true)) {
+				aq_ret = I40E_ERR_TIMEOUT;
+				goto error_param;
+			}
+			if (i40e_ctrl_vf_tx_rings(pf->vsi[vf->ch[i].vsi_idx],
+						  vqs->tx_queues, true)) {
 				aq_ret = I40E_ERR_TIMEOUT;
+				goto error_param;
+			}
 		}
 	}
 
-	vf->queues_enabled = true;
-
 error_param:
 	/* send the response to the VF */
 	return i40e_vc_send_resp_to_vf(vf, VIRTCHNL_OP_ENABLE_QUEUES,
@@ -2413,13 +3612,10 @@
 {
 	struct virtchnl_queue_select *vqs =
 	    (struct virtchnl_queue_select *)msg;
+	i40e_status aq_ret = I40E_SUCCESS;
 	struct i40e_pf *pf = vf->pf;
-	i40e_status aq_ret = 0;
 
-	/* Immediately mark queues as disabled */
-	vf->queues_enabled = false;
-
-	if (!test_bit(I40E_VF_STATE_ACTIVE, &vf->vf_states)) {
+	if (!i40e_sync_vf_state(vf, I40E_VF_STATE_ACTIVE)) {
 		aq_ret = I40E_ERR_PARAM;
 		goto error_param;
 	}
@@ -2429,7 +3625,7 @@
 		goto error_param;
 	}
 
-	if (!i40e_vc_validate_vqs_bitmaps(vqs)) {
+	if (!i40e_vc_isvalid_vqs_bitmaps(vqs)) {
 		aq_ret = I40E_ERR_PARAM;
 		goto error_param;
 	}
@@ -2452,6 +3648,100 @@
 }
 
 /**
+ * i40e_check_enough_queue - find enough queue
+ * @vf: pointer to the VF info
+ * @needed: the number of items needed
+ *
+ * Returns the base item index of the queue, or negative for error
+ **/
+static int i40e_check_enough_queue(struct i40e_vf *vf, u16 needed)
+{
+	struct i40e_pf *pf = vf->pf;
+	struct i40e_vsi *vsi = pf->vsi[vf->lan_vsi_idx];
+	u16 i, cur_queues, more, pool_size;
+	struct i40e_lump_tracking *pile;
+
+	cur_queues = vsi->alloc_queue_pairs;
+
+	/* if current allocated queues is enough for need */
+	if (cur_queues >= needed)
+		return vsi->base_queue;
+
+	pile = pf->qp_pile;
+	if (cur_queues > 0) {
+		/*
+		 * if queues of allocated not zero, just check if
+		 * there is enough queues behind the allocated queues
+		 * for more.
+		 */
+		more = needed - cur_queues;
+		for (i = vsi->base_queue + cur_queues;
+			i < pile->num_entries; i++) {
+			if (pile->list[i] & I40E_PILE_VALID_BIT)
+				break;
+
+			if (more-- == 1)
+				/* there is enough */
+				return vsi->base_queue;
+		}
+	}
+
+	pool_size = 0;
+	for (i = 0; i < pile->num_entries; i++) {
+		if (pile->list[i] & I40E_PILE_VALID_BIT) {
+			pool_size = 0;
+			continue;
+		}
+		if (needed <= ++pool_size)
+			/* there is enough */
+			return i;
+	}
+
+	return -ENOMEM;
+}
+
+static int i40e_set_vf_num_queues(struct i40e_vf *vf, int num_queues)
+{
+	int cur_pairs = vf->num_queue_pairs;
+	struct i40e_pf *pf = vf->pf;
+	int max_size;
+
+	if (num_queues > I40E_MAX_VF_QUEUES) {
+		dev_err(&pf->pdev->dev, "Unable to configure %d VF queues, the maximum is %d\n",
+			num_queues,
+			I40E_MAX_VF_QUEUES);
+		return -EINVAL;
+	} else if (num_queues - cur_pairs > pf->queues_left) {
+		dev_warn(&pf->pdev->dev, "Unable to configure %d VF queues, only %d available\n",
+			 num_queues - cur_pairs,
+			 pf->queues_left);
+		return -EINVAL;
+	} else if (i40e_check_enough_queue(vf, num_queues) < 0) {
+		dev_warn(&pf->pdev->dev, "VF requested %d more queues, but there is not enough for it.\n",
+			 num_queues - cur_pairs);
+		return -EINVAL;
+	}
+
+	max_size = i40e_max_lump_qp(pf);
+	if (max_size < 0) {
+		dev_err(&pf->pdev->dev, "Unable to configure %d VF queues, pile=<null>\n",
+			num_queues);
+		return -EINVAL;
+	}
+
+	if (num_queues > max_size) {
+		dev_err(&pf->pdev->dev, "Unable to configure %d VF queues, only %d available\n",
+			num_queues, max_size);
+		return -EINVAL;
+	}
+
+	/* successful request */
+	vf->num_req_queues = num_queues;
+	i40e_vc_reset_vf(vf, true);
+	return 0;
+}
+
+/**
  * i40e_vc_request_queues_msg
  * @vf: pointer to the VF info
  * @msg: pointer to the msg buffer
@@ -2466,35 +3756,12 @@
 	struct virtchnl_vf_res_request *vfres =
 		(struct virtchnl_vf_res_request *)msg;
 	u16 req_pairs = vfres->num_queue_pairs;
-	u8 cur_pairs = vf->num_queue_pairs;
-	struct i40e_pf *pf = vf->pf;
 
-	if (!test_bit(I40E_VF_STATE_ACTIVE, &vf->vf_states))
+	if (!i40e_sync_vf_state(vf, I40E_VF_STATE_ACTIVE))
 		return -EINVAL;
 
-	if (req_pairs > I40E_MAX_VF_QUEUES) {
-		dev_err(&pf->pdev->dev,
-			"VF %d tried to request more than %d queues.\n",
-			vf->vf_id,
-			I40E_MAX_VF_QUEUES);
-		vfres->num_queue_pairs = I40E_MAX_VF_QUEUES;
-	} else if (req_pairs - cur_pairs > pf->queues_left) {
-		dev_warn(&pf->pdev->dev,
-			 "VF %d requested %d more queues, but only %d left.\n",
-			 vf->vf_id,
-			 req_pairs - cur_pairs,
-			 pf->queues_left);
-		vfres->num_queue_pairs = pf->queues_left + cur_pairs;
-	} else {
-		/* successful request */
-		vf->num_req_queues = req_pairs;
-		i40e_vc_notify_vf_reset(vf);
-		i40e_reset_vf(vf, false);
-		return 0;
-	}
+	return i40e_set_vf_num_queues(vf, req_pairs);
 
-	return i40e_vc_send_msg_to_vf(vf, VIRTCHNL_OP_REQUEST_QUEUES, 0,
-				      (u8 *)vfres, sizeof(*vfres));
 }
 
 /**
@@ -2508,14 +3775,14 @@
 {
 	struct virtchnl_queue_select *vqs =
 	    (struct virtchnl_queue_select *)msg;
+	i40e_status aq_ret = I40E_SUCCESS;
 	struct i40e_pf *pf = vf->pf;
 	struct i40e_eth_stats stats;
-	i40e_status aq_ret = 0;
 	struct i40e_vsi *vsi;
 
 	memset(&stats, 0, sizeof(struct i40e_eth_stats));
 
-	if (!test_bit(I40E_VF_STATE_ACTIVE, &vf->vf_states)) {
+	if (!i40e_sync_vf_state(vf, I40E_VF_STATE_ACTIVE)) {
 		aq_ret = I40E_ERR_PARAM;
 		goto error_param;
 	}
@@ -2539,16 +3806,26 @@
 				      (u8 *)&stats, sizeof(stats));
 }
 
+#define I40E_MAX_MACVLAN_PER_HW 3072
+#define I40E_MAX_MACVLAN_PER_PF(num_ports) (I40E_MAX_MACVLAN_PER_HW /	\
+	(num_ports))
 /* If the VF is not trusted restrict the number of MAC/VLAN it can program
  * MAC filters: 16 for multicast, 1 for MAC, 1 for broadcast
  */
 #define I40E_VC_MAX_MAC_ADDR_PER_VF (16 + 1 + 1)
 #define I40E_VC_MAX_VLAN_PER_VF 16
 
+#define I40E_VC_MAX_MACVLAN_PER_TRUSTED_VF(vf_num, num_ports)		\
+({	typeof(vf_num) vf_num_ = (vf_num);				\
+	typeof(num_ports) num_ports_ = (num_ports);			\
+	((I40E_MAX_MACVLAN_PER_PF(num_ports_) - vf_num_ *		\
+	I40E_VC_MAX_MAC_ADDR_PER_VF) / vf_num_) +			\
+	I40E_VC_MAX_MAC_ADDR_PER_VF; })
 /**
  * i40e_check_vf_permission
  * @vf: pointer to the VF info
  * @al: MAC address list from virtchnl
+ * @is_quiet: set true for printing msg without opcode info, false otherwise
  *
  * Check that the given list of MAC addresses is allowed. Will return -EPERM
  * if any address in the list is not valid. Checks the following conditions:
@@ -2563,29 +3840,26 @@
  * addresses might not be accurate.
  **/
 static inline int i40e_check_vf_permission(struct i40e_vf *vf,
-					   struct virtchnl_ether_addr_list *al)
+					   struct virtchnl_ether_addr_list *al,
+					   bool *is_quiet)
 {
 	struct i40e_pf *pf = vf->pf;
+	struct i40e_hw *hw = &pf->hw;
+	struct i40e_vsi *vsi = pf->vsi[vf->lan_vsi_idx];
+	int mac2add_cnt = 0;
 	int i;
 
-	/* If this VF is not privileged, then we can't add more than a limited
-	 * number of addresses. Check to make sure that the additions do not
-	 * push us over the limit.
-	 */
-	if (!test_bit(I40E_VIRTCHNL_VF_CAP_PRIVILEGE, &vf->vf_caps) &&
-	    (vf->num_mac + al->num_elements) > I40E_VC_MAX_MAC_ADDR_PER_VF) {
-		dev_err(&pf->pdev->dev,
-			"Cannot add more MAC addresses, VF is not trusted, switch the VF to trusted to add more functionality\n");
-		return -EPERM;
-	}
+	if (!is_quiet)
+		return -EINVAL;
 
+	*is_quiet = false;
 	for (i = 0; i < al->num_elements; i++) {
+		struct i40e_mac_filter *f;
 		u8 *addr = al->list[i].addr;
 
 		if (is_broadcast_ether_addr(addr) ||
 		    is_zero_ether_addr(addr)) {
-			dev_err(&pf->pdev->dev, "invalid VF MAC addr %pM\n",
-				addr);
+			dev_err(&pf->pdev->dev, "invalid VF MAC addr %pM\n", addr);
 			return I40E_ERR_INVALID_MAC_ADDR;
 		}
 
@@ -2600,36 +3874,180 @@
 		    !is_multicast_ether_addr(addr) && vf->pf_set_mac &&
 		    !ether_addr_equal(addr, vf->default_lan_addr.addr)) {
 			dev_err(&pf->pdev->dev,
-				"VF attempting to override administratively set MAC address, bring down and up the VF interface to resume normal operation\n");
+				"VF attempting to override administratively set MAC address\n");
+			*is_quiet = true;
 			return -EPERM;
 		}
+
+		/* count filters that really will be added */
+		f = i40e_find_mac(vsi, addr);
+		if (!f)
+			++mac2add_cnt;
 	}
 
+	/* If this VF is not privileged, then we can't add more than a limited
+	 * number of addresses. Check to make sure that the additions do not
+	 * push us over the limit.
+	 */
+	if (!test_bit(I40E_VIRTCHNL_VF_CAP_PRIVILEGE, &vf->vf_caps)) {
+		if ((i40e_count_filters(vsi) + mac2add_cnt) >
+		    I40E_VC_MAX_MAC_ADDR_PER_VF) {
+			dev_err(&pf->pdev->dev,
+				"Cannot add more MAC addresses, VF is not trusted, switch the VF to trusted to add more functionality\n");
+			if (pf->vf_base_mode_only)
+				dev_err(&pf->pdev->dev, "VF %d is in base mode only, cannot add more than %d filters\n",
+					vf->vf_id,
+					I40E_VC_MAX_MAC_ADDR_PER_VF);
+			return -EPERM;
+		}
+	/* If this VF is trusted, it can use more resources than untrusted.
+	 * However to ensure that every trusted VF has appropriate number of
+	 * resources, divide whole pool of resources per port and then across
+	 * all VFs.
+	 */
+	} else {
+		if ((i40e_count_filters(vsi) + mac2add_cnt) >
+		    I40E_VC_MAX_MACVLAN_PER_TRUSTED_VF(pf->num_alloc_vfs,
+						       hw->num_ports)) {
+			dev_err(&pf->pdev->dev,
+				"Cannot add more MAC addresses, trusted VF exhausted it's resources\n");
+			return -EPERM;
+		}
+	}
 	return 0;
 }
 
 /**
- * i40e_vc_add_mac_addr_msg
+ * i40e_check_vf_vlan_cap
  * @vf: pointer to the VF info
- * @msg: pointer to the msg buffer
+ *
+ * Check if VF can add another VLAN filter.
+ */
+static i40e_status
+i40e_check_vf_vlan_cap(struct i40e_vf *vf)
+{
+	struct i40e_pf *pf = vf->pf;
+
+	if ((vf->num_vlan + 1 > I40E_VC_MAX_VLAN_PER_VF) &&
+	    !test_bit(I40E_VIRTCHNL_VF_CAP_PRIVILEGE, &vf->vf_caps)) {
+		dev_err(&pf->pdev->dev,
+			"VF is not trusted, switch the VF to trusted to add more VLAN addresses\n");
+		if (pf->vf_base_mode_only)
+			dev_err(&pf->pdev->dev, "VF %d is in base mode only, cannot add more than %d vlans\n",
+				vf->vf_id, I40E_VC_MAX_VLAN_PER_VF);
+
+		return I40E_ERR_CONFIG;
+	}
+
+	return I40E_SUCCESS;
+}
+
+/**
+ * i40e_vc_ether_addr_type - get type of virtchnl_ether_addr
+ * @vc_ether_addr: used to extract the type
+ */
+static inline u8
+i40e_vc_ether_addr_type(struct virtchnl_ether_addr *vc_ether_addr)
+{
+	return vc_ether_addr->type & VIRTCHNL_ETHER_ADDR_TYPE_MASK;
+}
+
+/**
+ * i40e_is_vc_addr_legacy
+ * @vc_ether_addr: VIRTCHNL structure that contains MAC and type
+ *
+ * check if the MAC address is from an older VF
+ */
+static inline bool
+i40e_is_vc_addr_legacy(struct virtchnl_ether_addr __maybe_unused *vc_ether_addr)
+{
+	return i40e_vc_ether_addr_type(vc_ether_addr) ==
+		VIRTCHNL_ETHER_ADDR_LEGACY;
+}
+
+/**
+ * i40e_is_vc_addr_primary
+ * @vc_ether_addr: VIRTCHNL structure that contains MAC and type
+ *
+ * check if the MAC address is the VF's primary MAC
+ * This function should only be called when the MAC address in
+ * virtchnl_ether_addr is a valid unicast MAC
+ */
+static inline bool
+i40e_is_vc_addr_primary(struct virtchnl_ether_addr __maybe_unused *vc_ether_addr)
+{
+	return i40e_vc_ether_addr_type(vc_ether_addr) ==
+		VIRTCHNL_ETHER_ADDR_PRIMARY;
+}
+
+/**
+ * i40e_is_legacy_umac_expired
+ * @time_last_added_umac: time since the last delete of VFs default MAC
+ *
+ * check if last added legacy unicast MAC expired
+ */
+static inline bool
+i40e_is_legacy_umac_expired(unsigned long time_last_added_umac)
+{
+#define I40E_LEGACY_VF_MAC_CHANGE_EXPIRE_TIME	msecs_to_jiffies(3000)
+	return time_is_before_jiffies(time_last_added_umac +
+				      I40E_LEGACY_VF_MAC_CHANGE_EXPIRE_TIME);
+}
+
+/**
+ * i40e_update_vf_mac_addr
+ * @vf: VF to update
+ * @vc_ether_addr: structure from VIRTCHNL with MAC to add
+ *
+ * update the VF's cached hardware MAC if allowed
+ */
+static void
+i40e_update_vf_mac_addr(struct i40e_vf *vf,
+			struct virtchnl_ether_addr *vc_ether_addr)
+{
+	u8 *mac_addr = vc_ether_addr->addr;
+
+	if (!is_valid_ether_addr(mac_addr))
+		return;
+
+	/* if request to add MAC filter is a primary request
+	 * update its default MAC address with the requested one
+	 *
+	 * if it is a legacy request then check if current default is empty
+	 * if so update the default MAC
+	 * otherwise save it in case it is followed by a delete request
+	 * meaning VF wants to change its default MAC which will be updated
+	 * in the delete path
+	 */
+	if (i40e_is_vc_addr_primary(vc_ether_addr)) {
+		ether_addr_copy(vf->default_lan_addr.addr, mac_addr);
+	} else {
+		if (is_zero_ether_addr(vf->default_lan_addr.addr)) {
+			ether_addr_copy(vf->default_lan_addr.addr, mac_addr);
+		} else {
+			ether_addr_copy(vf->legacy_last_added_umac.addr,
+					mac_addr);
+			vf->legacy_last_added_umac.time_modified = jiffies;
+		}
+	}
+}
+
+/**
+ * i40e_add_vf_mac_filters
+ * @vf: pointer to the VF info
+ * @is_quiet: set true for printing msg without opcode info, false otherwise
+ * @al: pointer to the address list of MACs to add
  *
  * add guest mac address filter
  **/
-static int i40e_vc_add_mac_addr_msg(struct i40e_vf *vf, u8 *msg)
+static int i40e_add_vf_mac_filters(struct i40e_vf *vf, bool *is_quiet,
+				   struct virtchnl_ether_addr_list *al)
 {
-	struct virtchnl_ether_addr_list *al =
-	    (struct virtchnl_ether_addr_list *)msg;
 	struct i40e_pf *pf = vf->pf;
 	struct i40e_vsi *vsi = NULL;
-	i40e_status ret = 0;
+	i40e_status ret = I40E_SUCCESS;
 	int i;
 
-	if (!test_bit(I40E_VF_STATE_ACTIVE, &vf->vf_states) ||
-	    !i40e_vc_isvalid_vsi_id(vf, al->vsi_id)) {
-		ret = I40E_ERR_PARAM;
-		goto error_param;
-	}
-
 	vsi = pf->vsi[vf->lan_vsi_idx];
 
 	/* Lock once, because all function inside for loop accesses VSI's
@@ -2637,7 +4055,7 @@
 	 */
 	spin_lock_bh(&vsi->mac_filter_hash_lock);
 
-	ret = i40e_check_vf_permission(vf, al);
+	ret = i40e_check_vf_permission(vf, al, is_quiet);
 	if (ret) {
 		spin_unlock_bh(&vsi->mac_filter_hash_lock);
 		goto error_param;
@@ -2650,7 +4068,6 @@
 		f = i40e_find_mac(vsi, al->list[i].addr);
 		if (!f) {
 			f = i40e_add_mac_filter(vsi, al->list[i].addr);
-
 			if (!f) {
 				dev_err(&pf->pdev->dev,
 					"Unable to add MAC filter %pM for VF %d\n",
@@ -2658,10 +4075,16 @@
 				ret = I40E_ERR_PARAM;
 				spin_unlock_bh(&vsi->mac_filter_hash_lock);
 				goto error_param;
-			} else {
-				vf->num_mac++;
+			}
+
+			ret = i40e_add_vmmac_to_list(vf, al->list[i].addr);
+			if (ret) {
+				spin_unlock_bh(&vsi->mac_filter_hash_lock);
+				goto error_param;
 			}
 		}
+
+		i40e_update_vf_mac_addr(vf, &al->list[i]);
 	}
 	spin_unlock_bh(&vsi->mac_filter_hash_lock);
 
@@ -2670,79 +4093,141 @@
 	if (ret)
 		dev_err(&pf->pdev->dev, "Unable to program VF %d MAC filters, error %d\n",
 			vf->vf_id, ret);
-
 error_param:
-	/* send the response to the VF */
-	return i40e_vc_send_resp_to_vf(vf, VIRTCHNL_OP_ADD_ETH_ADDR,
-				       ret);
+	return ret;
 }
 
 /**
- * i40e_vc_del_mac_addr_msg
+ * i40e_vc_add_mac_addr_msg
  * @vf: pointer to the VF info
  * @msg: pointer to the msg buffer
  *
- * remove guest mac address filter
+ * add guest mac address filter
  **/
-static int i40e_vc_del_mac_addr_msg(struct i40e_vf *vf, u8 *msg)
+static int i40e_vc_add_mac_addr_msg(struct i40e_vf *vf, u8 *msg)
 {
 	struct virtchnl_ether_addr_list *al =
 	    (struct virtchnl_ether_addr_list *)msg;
-	struct i40e_pf *pf = vf->pf;
-	struct i40e_vsi *vsi = NULL;
-	i40e_status ret = 0;
-	int i;
+	bool is_quiet = false;
+	i40e_status ret = I40E_SUCCESS;
 
-	if (!test_bit(I40E_VF_STATE_ACTIVE, &vf->vf_states) ||
+	if (!i40e_sync_vf_state(vf, I40E_VF_STATE_ACTIVE) ||
 	    !i40e_vc_isvalid_vsi_id(vf, al->vsi_id)) {
 		ret = I40E_ERR_PARAM;
 		goto error_param;
 	}
 
-	for (i = 0; i < al->num_elements; i++) {
-		if (is_broadcast_ether_addr(al->list[i].addr) ||
-		    is_zero_ether_addr(al->list[i].addr)) {
-			dev_err(&pf->pdev->dev, "Invalid MAC addr %pM for VF %d\n",
-				al->list[i].addr, vf->vf_id);
-			ret = I40E_ERR_INVALID_MAC_ADDR;
-			goto error_param;
-		}
+	ret = i40e_add_vf_mac_filters(vf, &is_quiet, al);
 
-		if (vf->pf_set_mac &&
-		    ether_addr_equal(al->list[i].addr,
-				     vf->default_lan_addr.addr)) {
-			dev_err(&pf->pdev->dev,
-				"MAC addr %pM has been set by PF, cannot delete it for VF %d, reset VF to change MAC addr\n",
-				vf->default_lan_addr.addr, vf->vf_id);
-			ret = I40E_ERR_PARAM;
-			goto error_param;
+error_param:
+	/* send the response to the VF */
+	return i40e_vc_send_msg_to_vf_ex(vf, VIRTCHNL_OP_ADD_ETH_ADDR,
+					 ret, NULL, 0, is_quiet);
+}
+
+/**
+ * i40e_vf_clear_default_mac_addr - clear VF default MAC
+ * @vf: pointer to the VF info
+ * @is_legacy_unimac: is request to delete a legacy request
+ *
+ * clear VFs default MAC address
+ **/
+static void i40e_vf_clear_default_mac_addr(struct i40e_vf *vf,
+					   bool is_legacy_unimac)
+{
+	eth_zero_addr(vf->default_lan_addr.addr);
+
+	if (is_legacy_unimac) {
+		unsigned long time_added =
+			vf->legacy_last_added_umac.time_modified;
+
+		if (!i40e_is_legacy_umac_expired(time_added)) {
+			ether_addr_copy(vf->default_lan_addr.addr,
+					vf->legacy_last_added_umac.addr);
 		}
 	}
+}
+
+/**
+ * i40e_del_vf_mac_filters
+ * @vf: pointer to the VF info
+ * @al: pointer to the address list of MACs to delete
+ *
+ * remove guest mac address filters
+ **/
+static int i40e_del_vf_mac_filters(struct i40e_vf *vf,
+				   struct virtchnl_ether_addr_list *al)
+{
+	bool was_unimac_deleted = false;
+	bool is_legacy_unimac = false;
+	struct i40e_pf *pf = vf->pf;
+	struct i40e_vsi *vsi = NULL;
+	i40e_status ret = I40E_SUCCESS;
+	int i;
+
 	vsi = pf->vsi[vf->lan_vsi_idx];
 
 	spin_lock_bh(&vsi->mac_filter_hash_lock);
 	/* delete addresses from the list */
-	for (i = 0; i < al->num_elements; i++)
-		if (i40e_del_mac_filter(vsi, al->list[i].addr)) {
+	for (i = 0; i < al->num_elements; i++) {
+		if (ether_addr_equal(al->list[i].addr,
+				     vf->default_lan_addr.addr) &&
+		    (vf->trusted || !vf->pf_set_mac)) {
+			was_unimac_deleted = true;
+			is_legacy_unimac =
+				i40e_is_vc_addr_legacy(&al->list[i]);
+		}
+
+		if (is_broadcast_ether_addr(al->list[i].addr) ||
+		    is_zero_ether_addr(al->list[i].addr) ||
+		    i40e_del_mac_filter(vsi, al->list[i].addr)) {
+			dev_err(&pf->pdev->dev, "Invalid MAC addr %pM for VF %d\n",
+				al->list[i].addr, vf->vf_id);
 			ret = I40E_ERR_INVALID_MAC_ADDR;
 			spin_unlock_bh(&vsi->mac_filter_hash_lock);
 			goto error_param;
-		} else {
-			vf->num_mac--;
 		}
 
+		i40e_del_vmmac_from_list(vf, al->list[i].addr);
+	}
 	spin_unlock_bh(&vsi->mac_filter_hash_lock);
 
+	if (was_unimac_deleted)
+		i40e_vf_clear_default_mac_addr(vf, is_legacy_unimac);
+
 	/* program the updated filter list */
 	ret = i40e_sync_vsi_filters(vsi);
 	if (ret)
 		dev_err(&pf->pdev->dev, "Unable to program VF %d MAC filters, error %d\n",
 			vf->vf_id, ret);
+error_param:
+	return ret;
+}
+
+/**
+ * i40e_vc_del_mac_addr_msg
+ * @vf: pointer to the VF info
+ * @msg: pointer to the msg buffer
+ *
+ * remove guest mac address filter
+ **/
+static int i40e_vc_del_mac_addr_msg(struct i40e_vf *vf, u8 *msg)
+{
+	struct virtchnl_ether_addr_list *al =
+	    (struct virtchnl_ether_addr_list *)msg;
+	i40e_status ret = I40E_SUCCESS;
+
+	if (!i40e_sync_vf_state(vf, I40E_VF_STATE_ACTIVE) ||
+	    !i40e_vc_isvalid_vsi_id(vf, al->vsi_id)) {
+		ret = I40E_ERR_PARAM;
+		goto error_param;
+	}
+
+	ret = i40e_del_vf_mac_filters(vf, al);
 
 error_param:
 	/* send the response to the VF */
-	return i40e_vc_send_resp_to_vf(vf, VIRTCHNL_OP_DEL_ETH_ADDR,
-				       ret);
+	return i40e_vc_send_resp_to_vf(vf, VIRTCHNL_OP_DEL_ETH_ADDR, ret);
 }
 
 /**
@@ -2756,24 +4241,26 @@
 {
 	struct virtchnl_vlan_filter_list *vfl =
 	    (struct virtchnl_vlan_filter_list *)msg;
+	i40e_status aq_ret = I40E_SUCCESS;
 	struct i40e_pf *pf = vf->pf;
 	struct i40e_vsi *vsi = NULL;
-	i40e_status aq_ret = 0;
-	int i;
+	int ret;
+	u16 i;
 
-	if ((vf->num_vlan >= I40E_VC_MAX_VLAN_PER_VF) &&
-	    !test_bit(I40E_VIRTCHNL_VF_CAP_PRIVILEGE, &vf->vf_caps)) {
-		dev_err(&pf->pdev->dev,
-			"VF is not trusted, switch the VF to trusted to add more VLAN addresses\n");
-		goto error_param;
-	}
-	if (!test_bit(I40E_VF_STATE_ACTIVE, &vf->vf_states) ||
+	if (!i40e_sync_vf_state(vf, I40E_VF_STATE_ACTIVE) ||
 	    !i40e_vc_isvalid_vsi_id(vf, vfl->vsi_id)) {
 		aq_ret = I40E_ERR_PARAM;
 		goto error_param;
 	}
 
+	vsi = pf->vsi[vf->lan_vsi_idx];
 	for (i = 0; i < vfl->num_elements; i++) {
+		if (i40e_is_vid(&vsi->info) &&
+		    vfl->vlan_id[i]) {
+			aq_ret = I40E_ERR_PARAM;
+			goto error_param;
+		}
+
 		if (vfl->vlan_id[i] > I40E_MAX_VLANID) {
 			aq_ret = I40E_ERR_PARAM;
 			dev_err(&pf->pdev->dev,
@@ -2781,19 +4268,29 @@
 			goto error_param;
 		}
 	}
-	vsi = pf->vsi[vf->lan_vsi_idx];
-	if (vsi->info.pvid) {
-		aq_ret = I40E_ERR_PARAM;
-		goto error_param;
-	}
 
 	i40e_vlan_stripping_enable(vsi);
+
 	for (i = 0; i < vfl->num_elements; i++) {
-		/* add new VLAN filter */
-		int ret = i40e_vsi_add_vlan(vsi, vfl->vlan_id[i]);
-		if (!ret)
-			vf->num_vlan++;
+		aq_ret = i40e_check_vf_vlan_cap(vf);
+		if (aq_ret)
+			goto error_param;
+		/* VLANs are configured by PF, omit check VLAN 0
+		 * as it's already added by HW.
+		 */
+		if (vfl->vlan_id[i] && vf->trunk_set_by_pf) {
+			dev_err(&pf->pdev->dev, "Failed to add VLAN id %d for VF %d, as PF has already configured VF's trunk\n",
+				vfl->vlan_id[i], vf->vf_id);
+			aq_ret = I40E_ERR_CONFIG;
+			goto error_param;
+		}
+		ret = i40e_vsi_add_vlan(vsi, vfl->vlan_id[i]);
 
+		if (!ret && vfl->vlan_id[i]) {
+			aq_ret = i40e_add_vmvlan_to_list(vf, vfl, i);
+			if (aq_ret)
+				goto error_param;
+		}
 		if (test_bit(I40E_VF_STATE_UC_PROMISC, &vf->vf_states))
 			i40e_aq_set_vsi_uc_promisc_on_vlan(&pf->hw, vsi->seid,
 							   true,
@@ -2810,7 +4307,6 @@
 				"Unable to add VLAN filter %d for VF %d, error %d\n",
 				vfl->vlan_id[i], vf->vf_id, ret);
 	}
-
 error_param:
 	/* send the response to the VF */
 	return i40e_vc_send_resp_to_vf(vf, VIRTCHNL_OP_ADD_VLAN, aq_ret);
@@ -2827,17 +4323,24 @@
 {
 	struct virtchnl_vlan_filter_list *vfl =
 	    (struct virtchnl_vlan_filter_list *)msg;
+	i40e_status aq_ret = I40E_SUCCESS;
 	struct i40e_pf *pf = vf->pf;
 	struct i40e_vsi *vsi = NULL;
-	i40e_status aq_ret = 0;
-	int i;
+	u16 i;
 
-	if (!test_bit(I40E_VF_STATE_ACTIVE, &vf->vf_states) ||
+	if (!i40e_sync_vf_state(vf, I40E_VF_STATE_ACTIVE) ||
 	    !i40e_vc_isvalid_vsi_id(vf, vfl->vsi_id)) {
 		aq_ret = I40E_ERR_PARAM;
 		goto error_param;
 	}
 
+	if (!test_bit(I40E_VIRTCHNL_VF_CAP_PRIVILEGE, &vf->vf_caps) &&
+	    bitmap_weight(vf->trunk_vlans, VLAN_N_VID))
+		/* Silently fail the request if the VF is untrusted and trunk
+		 * VLANs are configured.
+		 */
+		goto error_param;
+
 	for (i = 0; i < vfl->num_elements; i++) {
 		if (vfl->vlan_id[i] > I40E_MAX_VLANID) {
 			aq_ret = I40E_ERR_PARAM;
@@ -2846,15 +4349,14 @@
 	}
 
 	vsi = pf->vsi[vf->lan_vsi_idx];
-	if (vsi->info.pvid) {
+	if (i40e_is_vid(&vsi->info)) {
 		if (vfl->num_elements > 1 || vfl->vlan_id[0])
 			aq_ret = I40E_ERR_PARAM;
 		goto error_param;
 	}
 
 	for (i = 0; i < vfl->num_elements; i++) {
-		i40e_vsi_kill_vlan(vsi, vfl->vlan_id[i]);
-		vf->num_vlan--;
+		i40e_del_vmvlan_from_list(vsi, vf, vfl->vlan_id[i]);
 
 		if (test_bit(I40E_VF_STATE_UC_PROMISC, &vf->vf_states))
 			i40e_aq_set_vsi_uc_promisc_on_vlan(&pf->hw, vsi->seid,
@@ -2874,70 +4376,6 @@
 }
 
 /**
- * i40e_vc_iwarp_msg
- * @vf: pointer to the VF info
- * @msg: pointer to the msg buffer
- * @msglen: msg length
- *
- * called from the VF for the iwarp msgs
- **/
-static int i40e_vc_iwarp_msg(struct i40e_vf *vf, u8 *msg, u16 msglen)
-{
-	struct i40e_pf *pf = vf->pf;
-	int abs_vf_id = vf->vf_id + pf->hw.func_caps.vf_base_id;
-	i40e_status aq_ret = 0;
-
-	if (!test_bit(I40E_VF_STATE_ACTIVE, &vf->vf_states) ||
-	    !test_bit(I40E_VF_STATE_IWARPENA, &vf->vf_states)) {
-		aq_ret = I40E_ERR_PARAM;
-		goto error_param;
-	}
-
-	i40e_notify_client_of_vf_msg(pf->vsi[pf->lan_vsi], abs_vf_id,
-				     msg, msglen);
-
-error_param:
-	/* send the response to the VF */
-	return i40e_vc_send_resp_to_vf(vf, VIRTCHNL_OP_IWARP,
-				       aq_ret);
-}
-
-/**
- * i40e_vc_iwarp_qvmap_msg
- * @vf: pointer to the VF info
- * @msg: pointer to the msg buffer
- * @config: config qvmap or release it
- *
- * called from the VF for the iwarp msgs
- **/
-static int i40e_vc_iwarp_qvmap_msg(struct i40e_vf *vf, u8 *msg, bool config)
-{
-	struct virtchnl_iwarp_qvlist_info *qvlist_info =
-				(struct virtchnl_iwarp_qvlist_info *)msg;
-	i40e_status aq_ret = 0;
-
-	if (!test_bit(I40E_VF_STATE_ACTIVE, &vf->vf_states) ||
-	    !test_bit(I40E_VF_STATE_IWARPENA, &vf->vf_states)) {
-		aq_ret = I40E_ERR_PARAM;
-		goto error_param;
-	}
-
-	if (config) {
-		if (i40e_config_iwarp_qvlist(vf, qvlist_info))
-			aq_ret = I40E_ERR_PARAM;
-	} else {
-		i40e_release_iwarp_qvlist(vf);
-	}
-
-error_param:
-	/* send the response to the VF */
-	return i40e_vc_send_resp_to_vf(vf,
-			       config ? VIRTCHNL_OP_CONFIG_IWARP_IRQ_MAP :
-			       VIRTCHNL_OP_RELEASE_IWARP_IRQ_MAP,
-			       aq_ret);
-}
-
-/**
  * i40e_vc_config_rss_key
  * @vf: pointer to the VF info
  * @msg: pointer to the msg buffer
@@ -2948,13 +4386,13 @@
 {
 	struct virtchnl_rss_key *vrk =
 		(struct virtchnl_rss_key *)msg;
+	i40e_status aq_ret = I40E_SUCCESS;
 	struct i40e_pf *pf = vf->pf;
 	struct i40e_vsi *vsi = NULL;
-	i40e_status aq_ret = 0;
 
-	if (!test_bit(I40E_VF_STATE_ACTIVE, &vf->vf_states) ||
+	if (!i40e_sync_vf_state(vf, I40E_VF_STATE_ACTIVE) ||
 	    !i40e_vc_isvalid_vsi_id(vf, vrk->vsi_id) ||
-	    (vrk->key_len != I40E_HKEY_ARRAY_SIZE)) {
+	    vrk->key_len != I40E_HKEY_ARRAY_SIZE) {
 		aq_ret = I40E_ERR_PARAM;
 		goto err;
 	}
@@ -2978,14 +4416,14 @@
 {
 	struct virtchnl_rss_lut *vrl =
 		(struct virtchnl_rss_lut *)msg;
+	i40e_status aq_ret = I40E_SUCCESS;
 	struct i40e_pf *pf = vf->pf;
 	struct i40e_vsi *vsi = NULL;
-	i40e_status aq_ret = 0;
 	u16 i;
 
-	if (!test_bit(I40E_VF_STATE_ACTIVE, &vf->vf_states) ||
+	if (!i40e_sync_vf_state(vf, I40E_VF_STATE_ACTIVE) ||
 	    !i40e_vc_isvalid_vsi_id(vf, vrl->vsi_id) ||
-	    (vrl->lut_entries != I40E_VF_HLUT_ARRAY_SIZE)) {
+	    vrl->lut_entries != I40E_VF_HLUT_ARRAY_SIZE) {
 		aq_ret = I40E_ERR_PARAM;
 		goto err;
 	}
@@ -3014,11 +4452,11 @@
 static int i40e_vc_get_rss_hena(struct i40e_vf *vf, u8 *msg)
 {
 	struct virtchnl_rss_hena *vrh = NULL;
+	i40e_status aq_ret = I40E_SUCCESS;
 	struct i40e_pf *pf = vf->pf;
-	i40e_status aq_ret = 0;
 	int len = 0;
 
-	if (!test_bit(I40E_VF_STATE_ACTIVE, &vf->vf_states)) {
+	if (!i40e_sync_vf_state(vf, I40E_VF_STATE_ACTIVE)) {
 		aq_ret = I40E_ERR_PARAM;
 		goto err;
 	}
@@ -3050,11 +4488,11 @@
 {
 	struct virtchnl_rss_hena *vrh =
 		(struct virtchnl_rss_hena *)msg;
+	i40e_status aq_ret = I40E_SUCCESS;
 	struct i40e_pf *pf = vf->pf;
 	struct i40e_hw *hw = &pf->hw;
-	i40e_status aq_ret = 0;
 
-	if (!test_bit(I40E_VF_STATE_ACTIVE, &vf->vf_states)) {
+	if (!i40e_sync_vf_state(vf, I40E_VF_STATE_ACTIVE)) {
 		aq_ret = I40E_ERR_PARAM;
 		goto err;
 	}
@@ -3076,17 +4514,18 @@
  **/
 static int i40e_vc_enable_vlan_stripping(struct i40e_vf *vf, u8 *msg)
 {
-	i40e_status aq_ret = 0;
+	i40e_status aq_ret = I40E_SUCCESS;
 	struct i40e_vsi *vsi;
 
-	if (!test_bit(I40E_VF_STATE_ACTIVE, &vf->vf_states)) {
+	if (!i40e_sync_vf_state(vf, I40E_VF_STATE_ACTIVE)) {
 		aq_ret = I40E_ERR_PARAM;
 		goto err;
 	}
 
 	vsi = vf->pf->vsi[vf->lan_vsi_idx];
-	i40e_vlan_stripping_enable(vsi);
-
+	aq_ret = i40e_vlan_stripping_enable(vsi);
+	if (!aq_ret)
+		vf->vlan_stripping = true;
 	/* send the response to the VF */
 err:
 	return i40e_vc_send_resp_to_vf(vf, VIRTCHNL_OP_ENABLE_VLAN_STRIPPING,
@@ -3102,27 +4541,29 @@
  **/
 static int i40e_vc_disable_vlan_stripping(struct i40e_vf *vf, u8 *msg)
 {
-	i40e_status aq_ret = 0;
+	i40e_status aq_ret = I40E_SUCCESS;
 	struct i40e_vsi *vsi;
 
-	if (!test_bit(I40E_VF_STATE_ACTIVE, &vf->vf_states)) {
+	if (!i40e_sync_vf_state(vf, I40E_VF_STATE_ACTIVE)) {
 		aq_ret = I40E_ERR_PARAM;
 		goto err;
 	}
 
 	vsi = vf->pf->vsi[vf->lan_vsi_idx];
-	i40e_vlan_stripping_disable(vsi);
-
+	aq_ret = i40e_vlan_stripping_disable(vsi);
+	if (!aq_ret)
+		vf->vlan_stripping = false;
 	/* send the response to the VF */
 err:
 	return i40e_vc_send_resp_to_vf(vf, VIRTCHNL_OP_DISABLE_VLAN_STRIPPING,
 				       aq_ret);
 }
 
+#ifdef __TC_MQPRIO_MODE_MAX
 /**
  * i40e_validate_cloud_filter
- * @mask: mask for TC filter
- * @data: data for TC filter
+ * @vf: pointer to the VF info
+ * @tc_filter: pointer to virtchnl_filter
  *
  * This function validates cloud filter programmed as TC filter for ADq
  **/
@@ -3146,8 +4587,7 @@
 	}
 
 	/* action_meta is TC number here to which the filter is applied */
-	if (!tc_filter->action_meta ||
-	    tc_filter->action_meta > I40E_MAX_VF_VSI) {
+	if (tc_filter->action_meta > I40E_MAX_VF_VSI) {
 		dev_info(&pf->pdev->dev, "VF %d: Invalid TC number %u\n",
 			 vf->vf_id, tc_filter->action_meta);
 		goto err;
@@ -3253,11 +4693,11 @@
 }
 
 /**
- * i40e_find_vsi_from_seid - searches for the vsi with the given seid
+ * i40e_find_vf_vsi_from_seid - searches for the vsi with the given seid
  * @vf: pointer to the VF info
- * @seid - seid of the vsi it is searching for
+ * @seid: seid of the vsi it is searching for
  **/
-static struct i40e_vsi *i40e_find_vsi_from_seid(struct i40e_vf *vf, u16 seid)
+static struct i40e_vsi *i40e_find_vf_vsi_from_seid(struct i40e_vf *vf, u16 seid)
 {
 	struct i40e_pf *pf = vf->pf;
 	struct i40e_vsi *vsi = NULL;
@@ -3287,7 +4727,7 @@
 
 	hlist_for_each_entry_safe(cfilter, node,
 				  &vf->cloud_filter_list, cloud_node) {
-		vsi = i40e_find_vsi_from_seid(vf, cfilter->seid);
+		vsi = i40e_find_vf_vsi_from_seid(vf, cfilter->seid);
 
 		if (!vsi) {
 			dev_err(&pf->pdev->dev, "VF %d: no VSI found for matching %u seid, can't delete cloud filter\n",
@@ -3326,13 +4766,13 @@
 	struct virtchnl_l4_spec mask = vcf->mask.tcp_spec;
 	struct virtchnl_l4_spec tcf = vcf->data.tcp_spec;
 	struct i40e_cloud_filter cfilter, *cf = NULL;
+	i40e_status aq_ret = I40E_SUCCESS;
 	struct i40e_pf *pf = vf->pf;
 	struct i40e_vsi *vsi = NULL;
 	struct hlist_node *node;
-	i40e_status aq_ret = 0;
 	int i, ret;
 
-	if (!test_bit(I40E_VF_STATE_ACTIVE, &vf->vf_states)) {
+	if (!i40e_sync_vf_state(vf, I40E_VF_STATE_ACTIVE)) {
 		aq_ret = I40E_ERR_PARAM;
 		goto err;
 	}
@@ -3458,12 +4898,16 @@
 	struct virtchnl_l4_spec mask = vcf->mask.tcp_spec;
 	struct virtchnl_l4_spec tcf = vcf->data.tcp_spec;
 	struct i40e_cloud_filter *cfilter = NULL;
+	i40e_status aq_ret = I40E_SUCCESS;
 	struct i40e_pf *pf = vf->pf;
 	struct i40e_vsi *vsi = NULL;
-	i40e_status aq_ret = 0;
+	char err_msg_buf[100];
+	bool is_quiet = false;
+	u16 err_msglen = 0;
+	u8 *err_msg = NULL;
 	int i, ret;
 
-	if (!test_bit(I40E_VF_STATE_ACTIVE, &vf->vf_states)) {
+	if (!i40e_sync_vf_state(vf, I40E_VF_STATE_ACTIVE)) {
 		aq_ret = I40E_ERR_PARAM;
 		goto err_out;
 	}
@@ -3476,12 +4920,23 @@
 		goto err_out;
 	}
 
+	if (pf->fdir_pf_active_filters ||
+	    (!hlist_empty(&pf->fdir_filter_list))) {
+		aq_ret = I40E_ERR_PARAM;
+		err_msglen = strlcpy(err_msg_buf,
+				     "Flow Director Sideband filters exists, turn ntuple off to configure cloud filters",
+				     sizeof(err_msg_buf));
+		err_msg = err_msg_buf;
+		is_quiet = true;
+		goto err_out;
+	}
+
 	if (i40e_validate_cloud_filter(vf, vcf)) {
 		dev_info(&pf->pdev->dev,
 			 "VF %d: Invalid input/s, can't apply cloud filter\n",
 			 vf->vf_id);
-		aq_ret = I40E_ERR_PARAM;
-		goto err_out;
+			aq_ret = I40E_ERR_PARAM;
+			goto err_out;
 	}
 
 	cfilter = kzalloc(sizeof(*cfilter), GFP_KERNEL);
@@ -3553,8 +5008,62 @@
 err_free:
 	kfree(cfilter);
 err_out:
-	return i40e_vc_send_resp_to_vf(vf, VIRTCHNL_OP_ADD_CLOUD_FILTER,
-				       aq_ret);
+	return i40e_vc_send_msg_to_vf_ex(vf, VIRTCHNL_OP_ADD_CLOUD_FILTER,
+					 aq_ret, err_msg, err_msglen,
+					 is_quiet);
+}
+
+/**
+ * i40e_is_ok_to_alloc_vsi - check resources to create new vsi for tc
+ * @pf: board private structure
+ * @pile: the pile of resource to search
+ * @qp_needed: the number of queue pairs needed
+ * @num_vsi: count of new vsi
+ *
+ * Returns true if there is enough resources, otherwise false
+ **/
+static bool i40e_is_ok_to_alloc_vsi(struct i40e_pf *pf,
+				    struct i40e_lump_tracking *pile,
+				    u16 qp_needed, u8 num_vsi)
+{
+	u16 i = 0, qp_free = 0;
+
+	if (!pile || qp_needed == 0)
+		return false;
+
+	/* Start from beginning because earlier areas may have been freed */
+	while (i < pile->num_entries) {
+		/* Skip already allocated entries */
+		if (pile->list[i] & I40E_PILE_VALID_BIT) {
+			i++;
+			continue;
+		}
+
+		/* Do we have enough in this lump? */
+		for (qp_free = 0; (qp_free < qp_needed) &&
+		     ((i + qp_free) < pile->num_entries); qp_free++) {
+			if (pile->list[i + qp_free] & I40E_PILE_VALID_BIT)
+				break;
+		}
+		if (qp_free >= qp_needed)
+			break;
+
+		/* Not enough, so skip over it and continue looking */
+		i += qp_free;
+	}
+
+	if (qp_free < qp_needed)
+		return false;
+
+	/* Quick scan to look for free VSIs */
+	if (pf->next_vsi + num_vsi >= pf->num_alloc_vsi) {
+		i = 0;
+		while (i < pf->next_vsi && pf->vsi[i])
+			i++;
+		if (i + num_vsi >= pf->num_alloc_vsi)
+			return false;
+	}
+	return true;
 }
 
 /**
@@ -3564,23 +5073,23 @@
  **/
 static int i40e_vc_add_qch_msg(struct i40e_vf *vf, u8 *msg)
 {
-	struct virtchnl_tc_info *tci =
-		(struct virtchnl_tc_info *)msg;
+	struct virtchnl_tc_info *tci = (struct virtchnl_tc_info *)msg;
+	i40e_status aq_ret = I40E_SUCCESS;
 	struct i40e_pf *pf = vf->pf;
-	struct i40e_link_status *ls = &pf->hw.phy.link_info;
+	struct i40e_link_status *ls;
 	int i, adq_request_qps = 0;
-	i40e_status aq_ret = 0;
-	u64 speed = 0;
+	u32 speed;
 
-	if (!test_bit(I40E_VF_STATE_ACTIVE, &vf->vf_states)) {
+	ls = &pf->hw.phy.link_info;
+	if (!i40e_sync_vf_state(vf, I40E_VF_STATE_ACTIVE)) {
 		aq_ret = I40E_ERR_PARAM;
 		goto err;
 	}
 
 	/* ADq cannot be applied if spoof check is ON */
-	if (vf->spoofchk) {
+	if (vf->mac_anti_spoof) {
 		dev_err(&pf->pdev->dev,
-			"Spoof check is ON, turn it OFF to enable ADq\n");
+			"Spoof check is ON, turn OFF both MAC and VLAN anti spoof to enable ADq\n");
 		aq_ret = I40E_ERR_PARAM;
 		goto err;
 	}
@@ -3617,6 +5126,16 @@
 	/* need Max VF queues but already have default number of queues */
 	adq_request_qps = I40E_MAX_VF_QUEUES - I40E_DEFAULT_QUEUES_PER_VF;
 
+	if (tci->num_tc > 1 &&
+	    !(i40e_is_ok_to_alloc_vsi(pf, pf->qp_pile,
+				      (tci->num_tc - 1) * vf->num_queue_pairs,
+				      tci->num_tc - 1))) {
+		dev_err(&pf->pdev->dev, "Lack of resources to allocate %d TCs for VF %d\n",
+			tci->num_tc, vf->vf_id);
+		aq_ret = I40E_ERR_CONFIG;
+		goto err;
+	}
+
 	if (pf->queues_left < adq_request_qps) {
 		dev_err(&pf->pdev->dev,
 			"No queues left to allocate to VF %d\n",
@@ -3632,28 +5151,9 @@
 	}
 
 	/* get link speed in MB to validate rate limit */
-	switch (ls->link_speed) {
-	case VIRTCHNL_LINK_SPEED_100MB:
-		speed = SPEED_100;
-		break;
-	case VIRTCHNL_LINK_SPEED_1GB:
-		speed = SPEED_1000;
-		break;
-	case VIRTCHNL_LINK_SPEED_10GB:
-		speed = SPEED_10000;
-		break;
-	case VIRTCHNL_LINK_SPEED_20GB:
-		speed = SPEED_20000;
-		break;
-	case VIRTCHNL_LINK_SPEED_25GB:
-		speed = SPEED_25000;
-		break;
-	case VIRTCHNL_LINK_SPEED_40GB:
-		speed = SPEED_40000;
-		break;
-	default:
-		dev_err(&pf->pdev->dev,
-			"Cannot detect link speed\n");
+	speed = i40e_vc_link_speed2mbps(ls->link_speed);
+	if (speed == SPEED_UNKNOWN) {
+		dev_err(&pf->pdev->dev, "Cannot detect link speed\n");
 		aq_ret = I40E_ERR_PARAM;
 		goto err;
 	}
@@ -3679,15 +5179,9 @@
 
 	/* set this flag only after making sure all inputs are sane */
 	vf->adq_enabled = true;
-	/* num_req_queues is set when user changes number of queues via ethtool
-	 * and this causes issue for default VSI(which depends on this variable)
-	 * when ADq is enabled, hence reset it.
-	 */
-	vf->num_req_queues = 0;
 
 	/* reset the VF in order to allocate resources */
-	i40e_vc_notify_vf_reset(vf);
-	i40e_reset_vf(vf, false);
+	i40e_vc_reset_vf(vf, true);
 
 	return I40E_SUCCESS;
 
@@ -3704,10 +5198,10 @@
  **/
 static int i40e_vc_del_qch_msg(struct i40e_vf *vf, u8 *msg)
 {
+	i40e_status aq_ret = I40E_SUCCESS;
 	struct i40e_pf *pf = vf->pf;
-	i40e_status aq_ret = 0;
 
-	if (!test_bit(I40E_VF_STATE_ACTIVE, &vf->vf_states)) {
+	if (!i40e_sync_vf_state(vf, I40E_VF_STATE_ACTIVE)) {
 		aq_ret = I40E_ERR_PARAM;
 		goto err;
 	}
@@ -3727,8 +5221,7 @@
 	}
 
 	/* reset the VF in order to allocate resources */
-	i40e_vc_notify_vf_reset(vf);
-	i40e_reset_vf(vf, false);
+	i40e_vc_reset_vf(vf, true);
 
 	return I40E_SUCCESS;
 
@@ -3736,6 +5229,7 @@
 	return i40e_vc_send_resp_to_vf(vf, VIRTCHNL_OP_DISABLE_CHANNELS,
 				       aq_ret);
 }
+#endif /* __TC_MQPRIO_MODE_MAX */
 
 /**
  * i40e_vc_process_vf_msg
@@ -3790,7 +5284,8 @@
 		i40e_vc_notify_vf_link_state(vf);
 		break;
 	case VIRTCHNL_OP_RESET_VF:
-		i40e_vc_reset_vf_msg(vf);
+		clear_bit(I40E_VF_STATE_RESOURCES_LOADED, &vf->vf_states);
+		i40e_vc_reset_vf(vf, false);
 		ret = 0;
 		break;
 	case VIRTCHNL_OP_CONFIG_PROMISCUOUS_MODE:
@@ -3824,15 +5319,6 @@
 	case VIRTCHNL_OP_GET_STATS:
 		ret = i40e_vc_get_stats_msg(vf, msg);
 		break;
-	case VIRTCHNL_OP_IWARP:
-		ret = i40e_vc_iwarp_msg(vf, msg, msglen);
-		break;
-	case VIRTCHNL_OP_CONFIG_IWARP_IRQ_MAP:
-		ret = i40e_vc_iwarp_qvmap_msg(vf, msg, true);
-		break;
-	case VIRTCHNL_OP_RELEASE_IWARP_IRQ_MAP:
-		ret = i40e_vc_iwarp_qvmap_msg(vf, msg, false);
-		break;
 	case VIRTCHNL_OP_CONFIG_RSS_KEY:
 		ret = i40e_vc_config_rss_key(vf, msg);
 		break;
@@ -3854,6 +5340,7 @@
 	case VIRTCHNL_OP_REQUEST_QUEUES:
 		ret = i40e_vc_request_queues_msg(vf, msg);
 		break;
+#ifdef __TC_MQPRIO_MODE_MAX
 	case VIRTCHNL_OP_ENABLE_CHANNELS:
 		ret = i40e_vc_add_qch_msg(vf, msg);
 		break;
@@ -3866,6 +5353,7 @@
 	case VIRTCHNL_OP_DEL_CLOUD_FILTER:
 		ret = i40e_vc_del_cloud_filter(vf, msg);
 		break;
+#endif /* __TC_MQPRIO_MODE_MAX */
 	case VIRTCHNL_OP_UNKNOWN:
 	default:
 		dev_err(&pf->pdev->dev, "Unsupported opcode %d from VF %d\n",
@@ -3920,52 +5408,26 @@
 	return 0;
 }
 
+#ifdef IFLA_VF_MAX
+
 /**
- * i40e_validate_vf
- * @pf: the physical function
- * @vf_id: VF identifier
+ * i40e_set_vf_mac
+ * @vf: the VF
+ * @vsi: VF VSI to configure
+ * @mac: the mac address
  *
- * Check that the VF is enabled and the VSI exists.
+ * This function allows the administrator to set the mac address for the VF
  *
  * Returns 0 on success, negative on failure
- **/
-static int i40e_validate_vf(struct i40e_pf *pf, int vf_id)
-{
-	struct i40e_vsi *vsi;
-	struct i40e_vf *vf;
-	int ret = 0;
-
-	if (vf_id >= pf->num_alloc_vfs) {
-		dev_err(&pf->pdev->dev,
-			"Invalid VF Identifier %d\n", vf_id);
-		ret = -EINVAL;
-		goto err_out;
-	}
-	vf = &pf->vf[vf_id];
-	vsi = i40e_find_vsi_from_id(pf, vf->lan_vsi_id);
-	if (!vsi)
-		ret = -EINVAL;
-err_out:
-	return ret;
-}
-
-/**
- * i40e_ndo_set_vf_mac
- * @netdev: network interface device structure
- * @vf_id: VF identifier
- * @mac: mac address
  *
- * program VF mac address
  **/
-int i40e_ndo_set_vf_mac(struct net_device *netdev, int vf_id, u8 *mac)
+static int i40e_set_vf_mac(struct i40e_vf *vf, struct i40e_vsi *vsi,
+			   const u8 *mac)
 {
-	struct i40e_netdev_priv *np = netdev_priv(netdev);
-	struct i40e_vsi *vsi = np->vsi;
 	struct i40e_pf *pf = vsi->back;
 	struct i40e_mac_filter *f;
-	struct i40e_vf *vf;
-	int ret = 0;
 	struct hlist_node *h;
+	int ret = 0;
 	int bkt;
 	u8 i;
 
@@ -3974,41 +5436,32 @@
 		return -EAGAIN;
 	}
 
-	/* validate the request */
-	ret = i40e_validate_vf(pf, vf_id);
-	if (ret)
+	if (is_multicast_ether_addr(mac)) {
+		dev_err(&pf->pdev->dev,
+			"Invalid Ethernet address %pM for VF %d\n",
+			mac, vf->vf_id);
+		ret = -EINVAL;
 		goto error_param;
-
-	vf = &pf->vf[vf_id];
-	vsi = pf->vsi[vf->lan_vsi_idx];
+	}
 
 	/* When the VF is resetting wait until it is done.
 	 * It can take up to 200 milliseconds,
 	 * but wait for up to 300 milliseconds to be safe.
-	 * If the VF is indeed in reset, the vsi pointer has
-	 * to show on the newly loaded vsi under pf->vsi[id].
+	 * Acquire the vsi pointer only after the VF has been
+	 * properly initialized.
 	 */
 	for (i = 0; i < 15; i++) {
-		if (test_bit(I40E_VF_STATE_INIT, &vf->vf_states)) {
-			if (i > 0)
-				vsi = pf->vsi[vf->lan_vsi_idx];
+		if (test_bit(I40E_VF_STATE_INIT, &vf->vf_states))
 			break;
-		}
 		msleep(20);
 	}
 	if (!test_bit(I40E_VF_STATE_INIT, &vf->vf_states)) {
 		dev_err(&pf->pdev->dev, "VF %d still in reset. Try again.\n",
-			vf_id);
+			vf->vf_id);
 		ret = -EAGAIN;
 		goto error_param;
 	}
-
-	if (is_multicast_ether_addr(mac)) {
-		dev_err(&pf->pdev->dev,
-			"Invalid Ethernet address %pM for VF %d\n", mac, vf_id);
-		ret = -EINVAL;
-		goto error_param;
-	}
+	vsi = pf->vsi[vf->lan_vsi_idx];
 
 	/* Lock once because below invoked function add/del_filter requires
 	 * mac_filter_hash_lock to be held
@@ -4028,59 +5481,63 @@
 	spin_unlock_bh(&vsi->mac_filter_hash_lock);
 
 	/* program mac filter */
+	vsi->flags |= I40E_VSI_FLAG_FILTER_CHANGED;
+	set_bit(__I40E_MACVLAN_SYNC_PENDING, vsi->back->state);
 	if (i40e_sync_vsi_filters(vsi)) {
 		dev_err(&pf->pdev->dev, "Unable to program ucast filters\n");
 		ret = -EIO;
 		goto error_param;
 	}
+
 	ether_addr_copy(vf->default_lan_addr.addr, mac);
 
+	i40e_free_vmvlan_list(NULL, vf);
+
 	if (is_zero_ether_addr(mac)) {
 		vf->pf_set_mac = false;
-		dev_info(&pf->pdev->dev, "Removing MAC on VF %d\n", vf_id);
+		dev_info(&pf->pdev->dev, "Removing MAC on VF %d\n", vf->vf_id);
 	} else {
 		vf->pf_set_mac = true;
 		dev_info(&pf->pdev->dev, "Setting MAC %pM on VF %d\n",
-			 mac, vf_id);
+			 mac, vf->vf_id);
 	}
 
 	/* Force the VF interface down so it has to bring up with new MAC
 	 * address
 	 */
-	i40e_vc_disable_vf(vf);
+	i40e_vc_reset_vf(vf, true);
 	dev_info(&pf->pdev->dev, "Bring down and up the VF interface to make this change effective.\n");
-
 error_param:
 	clear_bit(__I40E_VIRTCHNL_OP_PENDING, pf->state);
 	return ret;
 }
 
 /**
- * i40e_vsi_has_vlans - True if VSI has configured VLANs
- * @vsi: pointer to the vsi
+ * i40e_ndo_set_vf_mac
+ * @netdev: network interface device structure
+ * @vf_id: VF identifier
+ * @mac: mac address
  *
- * Check if a VSI has configured any VLANs. False if we have a port VLAN or if
- * we have no configured VLANs. Do not call while holding the
- * mac_filter_hash_lock.
- */
-static bool i40e_vsi_has_vlans(struct i40e_vsi *vsi)
+ * program VF mac address
+ **/
+int i40e_ndo_set_vf_mac(struct net_device *netdev, int vf_id, u8 *mac)
 {
-	bool have_vlans;
-
-	/* If we have a port VLAN, then the VSI cannot have any VLANs
-	 * configured, as all MAC/VLAN filters will be assigned to the PVID.
-	 */
-	if (vsi->info.pvid)
-		return false;
+	struct i40e_netdev_priv *np = netdev_priv(netdev);
+	struct i40e_vsi *vsi = np->vsi;
+	struct i40e_pf *pf = vsi->back;
+	struct i40e_vf *vf;
+	int ret = 0;
 
-	/* Since we don't have a PVID, we know that if the device is in VLAN
-	 * mode it must be because of a VLAN filter configured on this VSI.
-	 */
-	spin_lock_bh(&vsi->mac_filter_hash_lock);
-	have_vlans = i40e_is_vsi_in_vlan(vsi);
-	spin_unlock_bh(&vsi->mac_filter_hash_lock);
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto error_param;
 
-	return have_vlans;
+	vf = &pf->vf[vf_id];
+	vsi = pf->vsi[vf->lan_vsi_idx];
+	ret = i40e_set_vf_mac(vf, vsi, mac);
+error_param:
+	return ret;
 }
 
 /**
@@ -4093,8 +5550,13 @@
  *
  * program VF vlan id and/or qos
  **/
+#ifdef IFLA_VF_VLAN_INFO_MAX
 int i40e_ndo_set_vf_port_vlan(struct net_device *netdev, int vf_id,
 			      u16 vlan_id, u8 qos, __be16 vlan_proto)
+#else
+int i40e_ndo_set_vf_port_vlan(struct net_device *netdev,
+			      int vf_id, u16 vlan_id, u8 qos)
+#endif /* IFLA_VF_VLAN_INFO_MAX */
 {
 	u16 vlanprio = vlan_id | (qos << I40E_VLAN_PRIORITY_SHIFT);
 	struct i40e_netdev_priv *np = netdev_priv(netdev);
@@ -4102,7 +5564,8 @@
 	struct i40e_pf *pf = np->vsi->back;
 	struct i40e_vsi *vsi;
 	struct i40e_vf *vf;
-	int ret = 0;
+	__le16 *pvid;
+	int ret;
 
 	if (test_and_set_bit(__I40E_VIRTCHNL_OP_PENDING, pf->state)) {
 		dev_warn(&pf->pdev->dev, "Unable to configure VFs, other operation is pending.\n");
@@ -4119,12 +5582,14 @@
 		ret = -EINVAL;
 		goto error_pvid;
 	}
+#ifdef IFLA_VF_VLAN_INFO_MAX
 
 	if (vlan_proto != htons(ETH_P_8021Q)) {
 		dev_err(&pf->pdev->dev, "VF VLAN protocol is not supported\n");
 		ret = -EPROTONOSUPPORT;
 		goto error_pvid;
 	}
+#endif
 
 	vf = &pf->vf[vf_id];
 	vsi = pf->vsi[vf->lan_vsi_idx];
@@ -4135,23 +5600,25 @@
 		goto error_pvid;
 	}
 
-	if (le16_to_cpu(vsi->info.pvid) == vlanprio)
-		/* duplicate request, so just return success */
-		goto error_pvid;
+	pvid = i40e_get_current_vid(vsi);
 
-	if (i40e_vsi_has_vlans(vsi)) {
-		dev_err(&pf->pdev->dev,
-			"VF %d has already configured VLAN filters and the administrator is requesting a port VLAN override.\nPlease unload and reload the VF driver for this change to take effect.\n",
-			vf_id);
-		/* Administrator Error - knock the VF offline until he does
-		 * the right thing by reconfiguring his network correctly
-		 * and then reloading the VF driver.
-		 */
-		i40e_vc_disable_vf(vf);
-		/* During reset the VF got a new VSI, so refresh the pointer. */
-		vsi = pf->vsi[vf->lan_vsi_idx];
+	if (le16_to_cpu(*pvid) == vlanprio) {
+#ifdef HAVE_NDO_SET_VF_LINK_STATE
+		/* if vlan is being removed then clear trunk_vlan */
+		if (!(*pvid))
+			memset(vf->trunk_vlans, 0,
+			       BITS_TO_LONGS(VLAN_N_VID) * sizeof(long));
+#endif /* HAVE_NDO_SET_VF_LINK_STATE */
+		goto error_pvid;
 	}
 
+	i40e_vlan_stripping_enable(vsi);
+	/* do VF reset to renegotiate its capabilities and reinitialize */
+	i40e_vc_reset_vf(vf, true);
+	/* During reset the VF got a new VSI, so refresh the pointer. */
+	vsi = pf->vsi[vf->lan_vsi_idx];
+	pvid = i40e_get_current_vid(vsi);
+
 	/* Locked once because multiple functions below iterate list */
 	spin_lock_bh(&vsi->mac_filter_hash_lock);
 
@@ -4164,9 +5631,9 @@
 	 * MAC addresses deleted.
 	 */
 	if ((!(vlan_id || qos) ||
-	    vlanprio != le16_to_cpu(vsi->info.pvid)) &&
-	    vsi->info.pvid) {
-		ret = i40e_add_vlan_all_mac(vsi, I40E_VLAN_ANY);
+	     vlanprio != le16_to_cpu(*pvid)) &&
+	    *pvid) {
+		ret = i40e_add_vlan_all_mac(vsi, 0);
 		if (ret) {
 			dev_info(&vsi->back->pdev->dev,
 				 "add VF VLAN failed, ret=%d aq_err=%d\n", ret,
@@ -4176,10 +5643,13 @@
 		}
 	}
 
-	if (vsi->info.pvid) {
+	if (*pvid) {
+		s16 mask = VLAN_VID_MASK;
+
+		mask &= ~(0x1);
 		/* remove all filters on the old VLAN */
-		i40e_rm_vlan_all_mac(vsi, (le16_to_cpu(vsi->info.pvid) &
-					   VLAN_VID_MASK));
+		i40e_rm_vlan_all_mac(vsi, (le16_to_cpu(*pvid) &
+					   mask));
 	}
 
 	spin_unlock_bh(&vsi->mac_filter_hash_lock);
@@ -4192,13 +5662,35 @@
 		goto error_pvid;
 	}
 
-	if (vlan_id || qos)
+	if (vlan_id || qos) {
 		ret = i40e_vsi_add_pvid(vsi, vlanprio);
-	else
+		if (ret) {
+			dev_info(&vsi->back->pdev->dev,
+				 "add VF VLAN failed, ret=%d aq_err=%d\n", ret,
+				 vsi->back->hw.aq.asq_last_status);
+			goto error_pvid;
+		}
+		/* as there is no MacVlan pair left, set
+		 * allow_untagged to off
+		 */
+		vf->allow_untagged = false;
+	} else {
 		i40e_vsi_remove_pvid(vsi);
+#ifdef HAVE_NDO_SET_VF_LINK_STATE
+		/* if vlan is being removed then clear also trunk_vlan */
+		if (!(*pvid))
+			memset(vf->trunk_vlans, 0,
+			       BITS_TO_LONGS(VLAN_N_VID) * sizeof(long));
+#endif /* HAVE_NDO_SET_VF_LINK_STATE */
+		vf->allow_untagged = true;
+	}
 	spin_lock_bh(&vsi->mac_filter_hash_lock);
 
 	if (vlan_id) {
+#ifdef HAVE_NDO_SET_VF_LINK_STATE
+		int tmp;
+
+#endif /* HAVE_NDO_SET_VF_LINK_STATE */
 		dev_info(&pf->pdev->dev, "Setting VLAN %d, QOS 0x%x on VF %d\n",
 			 vlan_id, qos, vf_id);
 
@@ -4211,9 +5703,22 @@
 			spin_unlock_bh(&vsi->mac_filter_hash_lock);
 			goto error_pvid;
 		}
-
-		/* remove the previously added non-VLAN MAC filters */
-		i40e_rm_vlan_all_mac(vsi, I40E_VLAN_ANY);
+#ifdef HAVE_NDO_SET_VF_LINK_STATE
+		/* only pvid should be present in trunk */
+		clear_bit(le16_to_cpu(*pvid), vf->trunk_vlans);
+		for_each_set_bit(tmp, vf->trunk_vlans,
+				 BITS_TO_LONGS(VLAN_N_VID) * sizeof(long)) {
+			if (tmp != 0)
+				i40e_rm_vlan_all_mac(vsi, tmp);
+		}
+		memset(vf->trunk_vlans, 0,
+		       BITS_TO_LONGS(VLAN_N_VID) * sizeof(long));
+		set_bit(le16_to_cpu(*pvid), vf->trunk_vlans);
+
+		vf->allow_untagged = false;
+		vsi->flags |= I40E_VSI_FLAG_FILTER_CHANGED;
+		set_bit(__I40E_MACVLAN_SYNC_PENDING, vsi->back->state);
+#endif /* HAVE_NDO_SET_VF_LINK_STATE */
 	}
 
 	spin_unlock_bh(&vsi->mac_filter_hash_lock);
@@ -4224,26 +5729,23 @@
 	if (test_bit(I40E_VF_STATE_MC_PROMISC, &vf->vf_states))
 		allmulti = true;
 
-	/* Schedule the worker thread to take care of applying changes */
-	i40e_service_event_schedule(vsi->back);
-
-	if (ret) {
-		dev_err(&pf->pdev->dev, "Unable to update VF vsi context\n");
-		goto error_pvid;
-	}
-
 	/* The Port VLAN needs to be saved across resets the same as the
 	 * default LAN MAC address.
 	 */
-	vf->port_vlan_id = le16_to_cpu(vsi->info.pvid);
-
-	ret = i40e_config_vf_promiscuous_mode(vf, vsi->id, allmulti, alluni);
-	if (ret) {
-		dev_err(&pf->pdev->dev, "Unable to config vf promiscuous mode\n");
-		goto error_pvid;
+	vf->port_vlan_id = le16_to_cpu(*pvid);
+	if (*pvid) {
+		ret = i40e_config_vf_promiscuous_mode(vf,
+						      vsi->id,
+						      allmulti,
+						      alluni);
+		if (ret) {
+			dev_err(&pf->pdev->dev, "Unable to config vf promiscuous mode\n");
+			goto error_pvid;
+		}
 	}
 
-	ret = 0;
+	/* Schedule the worker thread to take care of applying changes */
+	i40e_service_event_schedule(vsi->back);
 
 error_pvid:
 	clear_bit(__I40E_VIRTCHNL_OP_PENDING, pf->state);
@@ -4259,8 +5761,12 @@
  *
  * configure VF Tx rate
  **/
+#ifdef HAVE_NDO_SET_VF_MIN_MAX_TX_RATE
 int i40e_ndo_set_vf_bw(struct net_device *netdev, int vf_id, int min_tx_rate,
 		       int max_tx_rate)
+#else
+int i40e_ndo_set_vf_bw(struct net_device *netdev, int vf_id, int max_tx_rate)
+#endif
 {
 	struct i40e_netdev_priv *np = netdev_priv(netdev);
 	struct i40e_pf *pf = np->vsi->back;
@@ -4278,13 +5784,6 @@
 	if (ret)
 		goto error;
 
-	if (min_tx_rate) {
-		dev_err(&pf->pdev->dev, "Invalid min tx rate (%d) (greater than 0) specified for VF %d.\n",
-			min_tx_rate, vf_id);
-		ret = -EINVAL;
-		goto error;
-	}
-
 	vf = &pf->vf[vf_id];
 	vsi = pf->vsi[vf->lan_vsi_idx];
 	if (!test_bit(I40E_VF_STATE_INIT, &vf->vf_states)) {
@@ -4305,6 +5804,19 @@
 }
 
 /**
+ * i40e_ndo_enable_vf
+ * @netdev: network interface device structure
+ * @vf_id: VF identifier
+ * @enable: true to enable & false to disable
+ *
+ * enable/disable VF
+ **/
+int i40e_ndo_enable_vf(struct net_device *netdev, int vf_id, bool enable)
+{
+	return -EOPNOTSUPP;
+}
+
+/**
  * i40e_ndo_get_vf_config
  * @netdev: network interface device structure
  * @vf_id: VF identifier
@@ -4343,19 +5855,36 @@
 
 	ether_addr_copy(ivi->mac, vf->default_lan_addr.addr);
 
+#ifdef HAVE_NDO_SET_VF_MIN_MAX_TX_RATE
 	ivi->max_tx_rate = vf->tx_rate;
 	ivi->min_tx_rate = 0;
-	ivi->vlan = le16_to_cpu(vsi->info.pvid) & I40E_VLAN_MASK;
-	ivi->qos = (le16_to_cpu(vsi->info.pvid) & I40E_PRIORITY_MASK) >>
-		   I40E_VLAN_PRIORITY_SHIFT;
+#else
+	ivi->tx_rate = vf->tx_rate;
+#endif
+	if (vsi->info.pvid) {
+		ivi->vlan = le16_to_cpu(vsi->info.pvid) & I40E_VLAN_MASK;
+		ivi->qos = (le16_to_cpu(vsi->info.pvid) &
+			    I40E_PRIORITY_MASK) >> I40E_VLAN_PRIORITY_SHIFT;
+	} else {
+		ivi->vlan = le16_to_cpu(vsi->info.outer_vlan) & I40E_VLAN_MASK;
+		ivi->qos = (le16_to_cpu(vsi->info.outer_vlan) &
+			    I40E_PRIORITY_MASK) >> I40E_VLAN_PRIORITY_SHIFT;
+	}
+
+#ifdef HAVE_NDO_SET_VF_LINK_STATE
 	if (vf->link_forced == false)
 		ivi->linkstate = IFLA_VF_LINK_STATE_AUTO;
 	else if (vf->link_up == true)
 		ivi->linkstate = IFLA_VF_LINK_STATE_ENABLE;
 	else
 		ivi->linkstate = IFLA_VF_LINK_STATE_DISABLE;
-	ivi->spoofchk = vf->spoofchk;
+#endif
+#ifdef HAVE_VF_SPOOFCHK_CONFIGURE
+	ivi->spoofchk = vf->mac_anti_spoof;
+#endif
+#ifdef HAVE_NDO_SET_VF_TRUST
 	ivi->trusted = vf->trusted;
+#endif /* HAVE_NDO_SET_VF_TRUST */
 	ret = 0;
 
 error_param:
@@ -4363,6 +5892,7 @@
 	return ret;
 }
 
+#ifdef HAVE_NDO_SET_VF_LINK_STATE
 /**
  * i40e_ndo_set_vf_link_state
  * @netdev: network interface device structure
@@ -4375,6 +5905,7 @@
 {
 	struct i40e_netdev_priv *np = netdev_priv(netdev);
 	struct i40e_pf *pf = np->vsi->back;
+	struct i40e_link_status *ls = &pf->hw.phy.link_info;
 	struct virtchnl_pf_event pfe;
 	struct i40e_hw *hw = &pf->hw;
 	struct i40e_vf *vf;
@@ -4402,37 +5933,34 @@
 	switch (link) {
 	case IFLA_VF_LINK_STATE_AUTO:
 		vf->link_forced = false;
-		pfe.event_data.link_event.link_status =
-			pf->hw.phy.link_info.link_info & I40E_AQ_LINK_UP;
-		pfe.event_data.link_event.link_speed =
-			(enum virtchnl_link_speed)
-			pf->hw.phy.link_info.link_speed;
+		i40e_set_vf_link_state(vf, &pfe, ls);
 		break;
 	case IFLA_VF_LINK_STATE_ENABLE:
 		vf->link_forced = true;
 		vf->link_up = true;
-		pfe.event_data.link_event.link_status = true;
-		pfe.event_data.link_event.link_speed = VIRTCHNL_LINK_SPEED_40GB;
+		i40e_set_vf_link_state(vf, &pfe, ls);
 		break;
 	case IFLA_VF_LINK_STATE_DISABLE:
 		vf->link_forced = true;
 		vf->link_up = false;
-		pfe.event_data.link_event.link_status = false;
-		pfe.event_data.link_event.link_speed = 0;
+		i40e_set_vf_link_state(vf, &pfe, ls);
 		break;
 	default:
 		ret = -EINVAL;
 		goto error_out;
 	}
+
 	/* Notify the VF of its new link state */
 	i40e_aq_send_msg_to_vf(hw, abs_vf_id, VIRTCHNL_OP_EVENT,
-			       0, (u8 *)&pfe, sizeof(pfe), NULL);
+			       I40E_SUCCESS, (u8 *)&pfe, sizeof(pfe), NULL);
 
 error_out:
 	clear_bit(__I40E_VIRTCHNL_OP_PENDING, pf->state);
 	return ret;
 }
 
+#endif /* HAVE_NDO_SET_VF_LINK_STATE */
+#ifdef HAVE_VF_SPOOFCHK_CONFIGURE
 /**
  * i40e_ndo_set_vf_spoofchk
  * @netdev: network interface device structure
@@ -4471,17 +5999,16 @@
 		goto out;
 	}
 
-	if (enable == vf->spoofchk)
+	if (enable == vf->mac_anti_spoof)
 		goto out;
 
-	vf->spoofchk = enable;
+	vf->mac_anti_spoof = enable;
 	memset(&ctxt, 0, sizeof(ctxt));
 	ctxt.seid = pf->vsi[vf->lan_vsi_idx]->seid;
 	ctxt.pf_num = pf->hw.pf_id;
 	ctxt.info.valid_sections = cpu_to_le16(I40E_AQ_VSI_PROP_SECURITY_VALID);
 	if (enable)
-		ctxt.info.sec_flags |= (I40E_AQ_VSI_SEC_FLAG_ENABLE_VLAN_CHK |
-					I40E_AQ_VSI_SEC_FLAG_ENABLE_MAC_CHK);
+		ctxt.info.sec_flags |= I40E_AQ_VSI_SEC_FLAG_ENABLE_MAC_CHK;
 	ret = i40e_aq_update_vsi_params(hw, &ctxt, NULL);
 	if (ret) {
 		dev_err(&pf->pdev->dev, "Error %d updating VSI parameters\n",
@@ -4493,6 +6020,8 @@
 	return ret;
 }
 
+#endif /* HAVE_VF_SPOOFCHK_CONFIGURE */
+#ifdef HAVE_NDO_SET_VF_TRUST
 /**
  * i40e_ndo_set_vf_trust
  * @netdev: network interface device structure of the pf
@@ -4528,14 +6057,2316 @@
 
 	vf = &pf->vf[vf_id];
 
+	/* if vf is in base mode, make it untrusted */
+	if (pf->vf_base_mode_only)
+		setting = false;
 	if (setting == vf->trusted)
 		goto out;
 
 	vf->trusted = setting;
-	i40e_vc_disable_vf(vf);
+	i40e_vc_reset_vf(vf, true);
 	dev_info(&pf->pdev->dev, "VF %u is now %strusted\n",
 		 vf_id, setting ? "" : "un");
 
+#ifdef __TC_MQPRIO_MODE_MAX
+	if (vf->adq_enabled) {
+		if (!vf->trusted) {
+			dev_info(&pf->pdev->dev,
+				 "VF %u no longer Trusted, deleting all cloud filters\n",
+				 vf_id);
+			i40e_del_all_cloud_filters(vf);
+		}
+	}
+#endif /* __TC_MQPRIO_MODE_MAX */
+
+out:
+	clear_bit(__I40E_VIRTCHNL_OP_PENDING, pf->state);
+	return ret;
+}
+#endif /* HAVE_NDO_SET_VF_TRUST */
+#ifdef HAVE_VF_STATS
+
+/**
+ * i40e_get_vf_stats - populate some stats for the VF
+ * @netdev: the netdev of the PF
+ * @vf_id: the host OS identifier (0-127)
+ * @vf_stats: pointer to the OS memory to be initialized
+ */
+int i40e_get_vf_stats(struct net_device *netdev, int vf_id,
+		      struct ifla_vf_stats *vf_stats)
+{
+	struct i40e_netdev_priv *np = netdev_priv(netdev);
+	struct i40e_pf *pf = np->vsi->back;
+	struct i40e_eth_stats *stats;
+	struct i40e_vsi *vsi;
+	struct i40e_vf *vf;
+
+	/* validate the request */
+	if (i40e_validate_vf(pf, vf_id))
+		return -EINVAL;
+
+	vf = &pf->vf[vf_id];
+	if (!test_bit(I40E_VF_STATE_INIT, &vf->vf_states)) {
+		dev_err(&pf->pdev->dev, "VF %d in reset. Try again.\n", vf_id);
+		return -EBUSY;
+	}
+
+	vsi = pf->vsi[vf->lan_vsi_idx];
+	if (!vsi)
+		return -EINVAL;
+
+	i40e_update_eth_stats(vsi);
+	stats = &vsi->eth_stats;
+
+	memset(vf_stats, 0, sizeof(*vf_stats));
+
+	vf_stats->rx_packets = stats->rx_unicast + stats->rx_broadcast +
+		stats->rx_multicast;
+	vf_stats->tx_packets = stats->tx_unicast + stats->tx_broadcast +
+		stats->tx_multicast;
+	vf_stats->rx_bytes   = stats->rx_bytes;
+	vf_stats->tx_bytes   = stats->tx_bytes;
+	vf_stats->broadcast  = stats->rx_broadcast;
+	vf_stats->multicast  = stats->rx_multicast;
+#ifdef HAVE_VF_STATS_DROPPED
+	vf_stats->rx_dropped = stats->rx_discards;
+	vf_stats->tx_dropped = stats->tx_discards;
+#endif
+
+	return 0;
+}
+#endif /* HAVE_VF_STATS */
+#endif /* IFLA_VF_MAX */
+#ifdef HAVE_NDO_SET_VF_LINK_STATE
+
+/**
+ * i40e_get_vlan_anti_spoof
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @enable: on success, true if enabled, false if not
+ *
+ * This function queries if VLAN anti spoof is enabled or not
+ *
+ * Returns 0 on success, negative on failure.
+ **/
+static int i40e_get_vlan_anti_spoof(struct pci_dev *pdev, int vf_id,
+				    bool *enable)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vsi *vsi;
+	struct i40e_vf *vf;
+	int ret;
+
+	if (test_and_set_bit(__I40E_VIRTCHNL_OP_PENDING, pf->state)) {
+		dev_warn(&pdev->dev, "Unable to configure VFs, other operation is pending.\n");
+		return -EAGAIN;
+	}
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto out;
+	vf = &pf->vf[vf_id];
+	vsi = pf->vsi[vf->lan_vsi_idx];
+	if ((vsi->info.valid_sections &
+	    CPU_TO_LE16(I40E_AQ_VSI_PROP_SECURITY_VALID)) &&
+	    (vsi->info.sec_flags & I40E_AQ_VSI_SEC_FLAG_ENABLE_VLAN_CHK))
+		*enable = true;
+	else
+		*enable = false;
+out:
+	clear_bit(__I40E_VIRTCHNL_OP_PENDING, pf->state);
+	return ret;
+}
+
+/**
+ * i40e_set_vlan_anti_spoof
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @enable: enable/disable
+ *
+ * This function enables or disables VLAN anti-spoof
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_set_vlan_anti_spoof(struct pci_dev *pdev, int vf_id,
+				    const bool enable)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vsi *vsi;
+	struct i40e_vf *vf;
+	u8 sec_flag;
+	int ret;
+
+	if (test_and_set_bit(__I40E_VIRTCHNL_OP_PENDING, pf->state)) {
+		dev_warn(&pdev->dev, "Unable to configure VFs, other operation is pending.\n");
+		return -EAGAIN;
+	}
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto out;
+	vf = &pf->vf[vf_id];
+	vsi = pf->vsi[vf->lan_vsi_idx];
+	sec_flag = I40E_AQ_VSI_SEC_FLAG_ENABLE_VLAN_CHK;
+	ret = i40e_set_spoof_settings(vsi, sec_flag, enable);
+	if (!ret)
+		vf->vlan_anti_spoof = enable;
+out:
+	clear_bit(__I40E_VIRTCHNL_OP_PENDING, pf->state);
+	return ret;
+}
+
+/**
+ * i40e_get_mac_anti_spoof
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @enable: on success, true if enabled, false if not
+ *
+ * This function queries if MAC anti spoof is enabled or not.
+ *
+ * Returns 0 on success, negative error on failure.
+ **/
+static int i40e_get_mac_anti_spoof(struct pci_dev *pdev, int vf_id,
+				   bool *enable)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vf *vf;
+	int ret;
+
+	if (test_and_set_bit(__I40E_VIRTCHNL_OP_PENDING, pf->state)) {
+		dev_warn(&pdev->dev, "Unable to configure VFs, other operation is pending.\n");
+		return -EAGAIN;
+	}
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto out;
+	vf = &pf->vf[vf_id];
+	*enable = vf->mac_anti_spoof;
+out:
+	clear_bit(__I40E_VIRTCHNL_OP_PENDING, pf->state);
+	return ret;
+}
+
+/**
+ * i40e_set_mac_anti_spoof
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @enable: enable/disable
+ *
+ * This function enables or disables MAC anti-spoof
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_set_mac_anti_spoof(struct pci_dev *pdev, int vf_id,
+				   const bool enable)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vsi *vsi;
+	struct i40e_vf *vf;
+	u8 sec_flag;
+	int ret;
+
+	if (test_and_set_bit(__I40E_VIRTCHNL_OP_PENDING, pf->state)) {
+		dev_warn(&pdev->dev, "Unable to configure VFs, other operation is pending.\n");
+		return -EAGAIN;
+	}
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto out;
+	vf = &pf->vf[vf_id];
+	vsi = pf->vsi[vf->lan_vsi_idx];
+	sec_flag = I40E_AQ_VSI_SEC_FLAG_ENABLE_MAC_CHK;
+	ret = i40e_set_spoof_settings(vsi, sec_flag, enable);
+	if (!ret)
+		vf->mac_anti_spoof = enable;
+out:
+	clear_bit(__I40E_VIRTCHNL_OP_PENDING, pf->state);
+	return ret;
+}
+
+/**
+ * i40e_get_trunk - Gets the configured VLAN filters
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @trunk_vlans: trunk vlans
+ *
+ * Gets the active trunk vlans
+ *
+ * Returns the number of active vlans filters on success,
+ * negative on failure
+ **/
+static int i40e_get_trunk(struct pci_dev *pdev, int vf_id,
+			  unsigned long *trunk_vlans)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vsi *vsi;
+	struct i40e_vf *vf;
+	int ret;
+
+	if (test_and_set_bit(__I40E_VIRTCHNL_OP_PENDING, pf->state)) {
+		dev_warn(&pdev->dev, "Unable to configure VFs, other operation is pending.\n");
+		return -EAGAIN;
+	}
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto out;
+	vf = &pf->vf[vf_id];
+	/* checking if pvid has been set through netdev */
+	vsi = pf->vsi[vf->lan_vsi_idx];
+	if (i40e_is_vid(&vsi->info)) {
+		memset(trunk_vlans, 0,
+		       BITS_TO_LONGS(VLAN_N_VID) * sizeof(long));
+		if (vsi->info.pvid)
+			set_bit(le16_to_cpu(vsi->info.pvid), trunk_vlans);
+		else
+			set_bit(le16_to_cpu(vsi->info.outer_vlan), trunk_vlans);
+	} else {
+		bitmap_copy(trunk_vlans, vf->trunk_vlans, VLAN_N_VID);
+	}
+
+	bitmap_copy(trunk_vlans, vf->trunk_vlans, VLAN_N_VID);
+	ret = bitmap_weight(trunk_vlans, VLAN_N_VID);
+out:
+	clear_bit(__I40E_VIRTCHNL_OP_PENDING, pf->state);
+	return ret;
+}
+
+/**
+ * i40e_set_trunk - Configure VLAN filters
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @vlan_bitmap: vlans to filter on
+ *
+ * Applies the VLAN filters
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_set_trunk(struct pci_dev *pdev, int vf_id,
+			  const unsigned long *vlan_bitmap)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vsi *vsi;
+	struct i40e_vf *vf;
+	int ret;
+	u16 vid;
+
+	if (test_and_set_bit(__I40E_VIRTCHNL_OP_PENDING, pf->state)) {
+		dev_warn(&pdev->dev, "Unable to configure VFs, other operation is pending.\n");
+		return -EAGAIN;
+	}
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto out;
+	vf = &pf->vf[vf_id];
+	vsi = pf->vsi[vf->lan_vsi_idx];
+	i40e_vlan_stripping_enable(vsi);
+
+	/* checking if pvid has been set through netdev */
+	vid = __le16_to_cpu(*i40e_get_current_vid(vsi));
+
+	if (vid) {
+		i40e_vsi_remove_pvid(vsi);
+		/* Remove pvid and vlan 0 from trunk */
+		clear_bit(vid, vf->trunk_vlans);
+		clear_bit(0, vf->trunk_vlans);
+	}
+
+	if (bitmap_weight(vlan_bitmap, VLAN_N_VID) && !vf->trunk_set_by_pf)
+		i40e_free_vmvlan_list(vsi, vf);
+
+	/* Add vlans */
+	for_each_set_bit(vid, vlan_bitmap, VLAN_N_VID) {
+		if (!test_bit(vid, vf->trunk_vlans)) {
+			ret = i40e_vsi_add_vlan(vsi, vid);
+			if (ret)
+				goto out;
+		}
+	}
+
+	/* If to empty trunk filter is added, remove I40E_VLAN_ANY.
+	 * Removal of this filter sets allow_untagged to false.
+	 */
+	if (bitmap_weight(vlan_bitmap, VLAN_N_VID) &&
+	    !bitmap_weight(vf->trunk_vlans, VLAN_N_VID)) {
+		vf->allow_untagged = false;
+		vf->trunk_set_by_pf = true;
+	}
+
+	/* If deleting all vlan filters, check if we have VLAN 0 filters
+	 * existing. If we don't, add filters to allow all traffic i.e
+	 * VLAN tag = -1 before deleting all filters because the in the
+	 * delete all filters flow, we check if there are VLAN 0 filters
+	 * and then replace them with filters of VLAN id = -1
+	 */
+	if (!bitmap_weight(vlan_bitmap, VLAN_N_VID)) {
+		vf->allow_untagged = true;
+		vf->trunk_set_by_pf = false;
+	}
+
+	/* Del vlans */
+	for_each_set_bit(vid, vf->trunk_vlans, VLAN_N_VID) {
+		if (!test_bit(vid, vlan_bitmap))
+			i40e_vsi_kill_vlan(vsi, vid);
+	}
+	/* Copy over the updated bitmap */
+	bitmap_copy(vf->trunk_vlans, vlan_bitmap, VLAN_N_VID);
+out:
+	clear_bit(__I40E_VIRTCHNL_OP_PENDING, pf->state);
+	return ret;
+}
+
+/**
+ * i40e_get_mirror - Gets the configured  VLAN mirrors
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @mirror_vlans: mirror vlans
+ *
+ * Gets the active mirror vlans
+ *
+ * Returns the number of active mirror vlans on success,
+ * negative on failure
+ **/
+static int i40e_get_mirror(struct pci_dev *pdev, int vf_id,
+			   unsigned long *mirror_vlans)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vf *vf;
+	int ret;
+
+	if (test_and_set_bit(__I40E_VIRTCHNL_OP_PENDING, pf->state)) {
+		dev_warn(&pdev->dev, "Unable to configure VFs, other operation is pending.\n");
+		return -EAGAIN;
+	}
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto out;
+	vf = &pf->vf[vf_id];
+	bitmap_copy(mirror_vlans, vf->mirror_vlans, VLAN_N_VID);
+	ret = bitmap_weight(mirror_vlans, VLAN_N_VID);
+out:
+	clear_bit(__I40E_VIRTCHNL_OP_PENDING, pf->state);
+	return ret;
+}
+
+/**
+ * i40e_set_mirror - Configure VLAN mirrors
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @vlan_bitmap: vlans to configure as mirrors
+ *
+ * Configures the mirror vlans
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_set_mirror(struct pci_dev *pdev, int vf_id,
+			   const unsigned long *vlan_bitmap)
+{
+	u16 vid, sw_seid, dst_seid, rule_id, rule_type;
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	int ret, num = 0, cnt, add, status;
+	u16 rules_used, rules_free;
+	struct i40e_vsi *vsi;
+	struct i40e_vf *vf;
+	__le16 *mr_list;
+
+	DECLARE_BITMAP(num_vlans, VLAN_N_VID);
+
+	if (test_and_set_bit(__I40E_VIRTCHNL_OP_PENDING, pf->state)) {
+		dev_warn(&pdev->dev, "Unable to configure VFs, other operation is pending.\n");
+		return -EAGAIN;
+	}
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto out;
+	vf = &pf->vf[vf_id];
+	vsi = pf->vsi[vf->lan_vsi_idx];
+	sw_seid = vsi->uplink_seid;
+	dst_seid = vsi->seid;
+	rule_type = I40E_AQC_MIRROR_RULE_TYPE_VLAN;
+	bitmap_xor(num_vlans, vf->mirror_vlans, vlan_bitmap, VLAN_N_VID);
+	cnt = bitmap_weight(num_vlans, VLAN_N_VID);
+	if (!cnt)
+		goto out;
+	mr_list = kcalloc(cnt, sizeof(__le16), GFP_KERNEL);
+	if (!mr_list) {
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	/* figure out if adding or deleting */
+	bitmap_and(num_vlans, vlan_bitmap, num_vlans, VLAN_N_VID);
+	add = bitmap_weight(num_vlans, VLAN_N_VID);
+	if (add) {
+		/* Add mirrors */
+		for_each_set_bit(vid, vlan_bitmap, VLAN_N_VID) {
+			if (!test_bit(vid, vf->mirror_vlans)) {
+				mr_list[num] = CPU_TO_LE16(vid);
+				num++;
+			}
+		}
+		status = i40e_aq_add_mirrorrule(&pf->hw, sw_seid,
+						rule_type, dst_seid,
+						cnt, mr_list, NULL,
+						&rule_id, &rules_used,
+						&rules_free);
+
+		if (pf->hw.aq.asq_last_status == I40E_AQ_RC_ENOSPC)
+			dev_warn(&pdev->dev, "Not enough resources to assign a mirror rule. Maximum limit of mirrored VLANs is 192.\n");
+
+		if (status == I40E_ERR_ADMIN_QUEUE_ERROR && cnt == 1) {
+			dev_warn(&pdev->dev, "Unable to add vlan mirror rule to VF %d.\n", vf_id);
+			ret = -EPERM;
+			goto err_free;
+		}
+
+		if (status) {
+			ret = -EINVAL;
+			goto err_free;
+		}
+		vf->vlan_rule_id = rule_id;
+	} else {
+		/* Del mirrors */
+		for_each_set_bit(vid, vf->mirror_vlans, VLAN_N_VID) {
+			if (!test_bit(vid, vlan_bitmap)) {
+				mr_list[num] = CPU_TO_LE16(vid);
+				num++;
+			}
+		}
+		status = i40e_aq_delete_mirrorrule(&pf->hw, sw_seid, rule_type,
+						   vf->vlan_rule_id, cnt, mr_list,
+						   NULL, &rules_used,
+						   &rules_free);
+		if (status) {
+			ret = -EINVAL;
+			goto err_free;
+		}
+	}
+
+	/* Copy over the updated bitmap */
+	bitmap_copy(vf->mirror_vlans, vlan_bitmap, VLAN_N_VID);
+err_free:
+	kfree(mr_list);
+out:
+	clear_bit(__I40E_VIRTCHNL_OP_PENDING, pf->state);
+	return ret;
+}
+
+/**
+ * i40e_get_allow_untagged
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @on: on or off
+ *
+ * This functions checks if the untagged packets
+ * are allowed or not.
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_get_allow_untagged(struct pci_dev *pdev, int vf_id,
+				   bool *on)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vf *vf;
+	int ret;
+
+	if (test_and_set_bit(__I40E_VIRTCHNL_OP_PENDING, pf->state)) {
+		dev_warn(&pdev->dev, "Unable to configure VFs, other operation is pending.\n");
+		return -EAGAIN;
+	}
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto out;
+	vf = &pf->vf[vf_id];
+	*on = vf->allow_untagged;
+out:
+	clear_bit(__I40E_VIRTCHNL_OP_PENDING, pf->state);
+	return ret;
+}
+
+/**
+ * i40e_set_allow_untagged
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @on: on or off
+ *
+ * This functions allows or stops untagged packets
+ * on the VF.
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_set_allow_untagged(struct pci_dev *pdev, int vf_id,
+				   const bool on)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vsi *vsi;
+	struct i40e_vf *vf;
+	int ret;
+
+	if (test_and_set_bit(__I40E_VIRTCHNL_OP_PENDING, pf->state)) {
+		dev_warn(&pdev->dev, "Unable to configure VFs, other operation is pending.\n");
+		return -EAGAIN;
+	}
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto out;
+	vf = &pf->vf[vf_id];
+	vsi = pf->vsi[vf->lan_vsi_idx];
+
+	if (i40e_is_vid(&vsi->info) && on)
+		dev_info(&pf->pdev->dev,
+			 "VF has port VLAN configured, setting allow_untagged to on\n");
+
+	i40e_service_event_schedule(vsi->back);
+	vf->allow_untagged = on;
+
+	vsi->flags |= I40E_VSI_FLAG_FILTER_CHANGED;
+	set_bit(__I40E_MACVLAN_SYNC_PENDING, vsi->back->state);
+out:
+	clear_bit(__I40E_VIRTCHNL_OP_PENDING, pf->state);
+	return ret;
+}
+
+/**
+ * i40e_get_loopback
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @enable: enable or disable
+ *
+ * This function checks loopback is enabled
+ *
+ * Returns 1 if enabled, 0 if disabled
+ **/
+static int i40e_get_loopback(struct pci_dev *pdev, int vf_id, bool *enable)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vf *vf;
+	int ret = 0;
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto err_out;
+	vf = &pf->vf[vf_id];
+	*enable = vf->loopback;
+err_out:
+	return ret;
+}
+
+/**
+ * i40e_set_loopback
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @enable: enable or disable
+ *
+ * This function enables or disables loopback
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_set_loopback(struct pci_dev *pdev, int vf_id,
+			     const bool enable)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vsi *vsi;
+	struct i40e_vf *vf;
+	int ret;
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto err_out;
+	vf = &pf->vf[vf_id];
+	vsi = pf->vsi[vf->lan_vsi_idx];
+	ret = i40e_configure_vf_loopback(vsi, vf_id, enable);
+	if (!ret)
+		vf->loopback = enable;
+err_out:
+	return ret;
+}
+
+/**
+ * i40e_get_vlan_strip
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @enable: enable or disable
+ *
+ * This function checks if vlan stripping is enabled or not
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_get_vlan_strip(struct pci_dev *pdev, int vf_id, bool *enable)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vf *vf;
+	int ret;
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto err_out;
+	vf = &pf->vf[vf_id];
+	*enable = vf->vlan_stripping;
+err_out:
+	return ret;
+}
+
+/**
+ * i40e_set_vlan_strip
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @enable: enable/disable
+ *
+ * This function enables or disables VLAN stripping on a VF
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_set_vlan_strip(struct pci_dev *pdev, int vf_id,
+			       const bool enable)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vsi *vsi;
+	struct i40e_vf *vf;
+	int ret;
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto err_out;
+	vf = &pf->vf[vf_id];
+	vsi = pf->vsi[vf->lan_vsi_idx];
+	ret = i40e_configure_vf_vlan_stripping(vsi, vf_id, enable);
+	if (ret)
+		goto err_out;
+	vf->vlan_stripping = enable;
+
+	if (enable)
+		ret = i40e_vc_send_resp_to_vf(vf, VIRTCHNL_OP_ENABLE_VLAN_STRIPPING,
+					      I40E_SUCCESS);
+	else
+		ret = i40e_vc_send_resp_to_vf(vf, VIRTCHNL_OP_DISABLE_VLAN_STRIPPING,
+					      I40E_SUCCESS);
+err_out:
+	return ret;
+}
+
+/**
+ * i40e_reset_vf_stats
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ *
+ * This function resets all the stats for the VF
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_reset_vf_stats(struct pci_dev *pdev, int vf_id)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vsi *vsi;
+	struct i40e_vf *vf;
+	int ret = 0;
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto err_out;
+	vf = &pf->vf[vf_id];
+	vsi = pf->vsi[vf->lan_vsi_idx];
+	i40e_vsi_reset_stats(vsi);
+
+err_out:
+	return ret;
+}
+
+/**
+ * i40e_get_vf_bw_share
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @bw_share: bw share of the VF
+ *
+ * This function retrieves the bw share configured for the VF
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_get_vf_bw_share(struct pci_dev *pdev, int vf_id, u8 *bw_share)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vf *vf;
+	int ret = 0;
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto err_out;
+
+	vf = &pf->vf[vf_id];
+
+	if (vf->tc_bw_share_req) {
+		ret = -EPERM;
+		goto err_out;
+	}
+
+	if (vf->bw_share_applied)
+		*bw_share = vf->bw_share;
+	else
+		ret = -EINVAL;
+
+err_out:
+	return ret;
+}
+
+/**
+ * i40e_store_vf_bw_share
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @bw_share: bw share of the VF
+ *
+ * This function stores bw share configured for the VF
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_store_vf_bw_share(struct pci_dev *pdev, int vf_id, u8 bw_share)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vf *vf;
+	int ret = 0;
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto err_out;
+	vf = &pf->vf[vf_id];
+
+	if (vf->tc_bw_share_req)
+		return -EPERM;
+
+	vf->bw_share = bw_share;
+
+	/* this tracking bool is set to true when 'apply' attribute is used */
+	vf->bw_share_applied = false;
+	pf->vf_bw_applied = false;
+err_out:
+	return ret;
+}
+
+/**
+ * i40e_get_link_state
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @enabled: link state
+ * @link_speed: link speed of the VF
+ *
+ * Gets the status of link and the link speed
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_get_link_state(struct pci_dev *pdev, int vf_id, bool *enabled,
+			       enum vfd_link_speed *link_speed)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_link_status *ls;
+	struct i40e_vf *vf;
+	int ret;
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto err_out;
+	vf = &pf->vf[vf_id];
+	ls = &pf->hw.phy.link_info;
+	if (vf->link_forced)
+		*enabled = vf->link_up;
+	else
+		*enabled = ls->link_info & I40E_AQ_LINK_UP;
+	switch (ls->link_speed) {
+	case I40E_LINK_SPEED_UNKNOWN:
+		*link_speed = VFD_LINK_SPEED_UNKNOWN;
+		break;
+	case I40E_LINK_SPEED_100MB:
+		*link_speed = VFD_LINK_SPEED_100MB;
+		break;
+	case I40E_LINK_SPEED_1GB:
+		*link_speed = VFD_LINK_SPEED_1GB;
+		break;
+	case I40E_LINK_SPEED_2_5GB:
+		*link_speed = VFD_LINK_SPEED_2_5GB;
+		break;
+	case I40E_LINK_SPEED_5GB:
+		*link_speed = VFD_LINK_SPEED_5GB;
+		break;
+	case I40E_LINK_SPEED_10GB:
+		*link_speed = VFD_LINK_SPEED_10GB;
+		break;
+	case I40E_LINK_SPEED_20GB:
+		*link_speed = VFD_LINK_SPEED_20GB;
+		break;
+	case I40E_LINK_SPEED_25GB:
+		*link_speed = VFD_LINK_SPEED_25GB;
+		break;
+	case I40E_LINK_SPEED_40GB:
+		*link_speed = VFD_LINK_SPEED_40GB;
+		break;
+	default:
+		*link_speed = VFD_LINK_SPEED_UNKNOWN;
+	}
+err_out:
+	return ret;
+}
+
+/**
+ * i40e_set_link_state
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @link: the link state to configure
+ *
+ * Configures link for a vf
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_set_link_state(struct pci_dev *pdev, int vf_id, const u8 link)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vf *vf;
+	int ret;
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto error_out;
+	vf = &pf->vf[vf_id];
+	ret = i40e_configure_vf_link(vf, link);
+error_out:
+	return ret;
+}
+
+#ifdef CONFIG_DCB
+/**
+ * i40e_enable_vf_queues
+ * @vsi: PCI device information struct
+ * @enable: VF identifier
+ *
+ * Disable/enable VF queues.
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_enable_vf_queues(struct i40e_vsi *vsi, bool enable)
+{
+	struct i40e_pf *pf = vsi->back;
+	int vf_id = -1, v, ret;
+	unsigned long q_map;
+	struct i40e_vf *vf;
+
+	if (!pf->vf)
+		return 0;
+
+	for (v = 0; v < pf->num_alloc_vfs; v++) {
+		if (pf->vsi[pf->vf[v].lan_vsi_idx] == vsi) {
+			vf_id = v;
+			break;
+		}
+	}
+
+	if (vf_id == -1) {
+		ret = -ENOENT;
+		goto err_out;
+	}
+
+	vf = &pf->vf[vf_id];
+	q_map = BIT(vsi->num_queue_pairs) - 1;
+	if (!enable) {
+		ret = i40e_set_link_state(pf->pdev, vf_id, VFD_LINKSTATE_OFF);
+		if (ret)
+			goto err_out;
+	}
+	ret = i40e_ctrl_vf_tx_rings(vsi, q_map, enable);
+	if (ret)
+		goto err_out;
+	ret = i40e_ctrl_vf_rx_rings(vsi, q_map, enable);
+	if (ret)
+		goto err_out;
+
+	if (enable) {
+		i40e_vc_notify_vf_reset(vf);
+		i40e_reset_vf(vf, false);
+		ret = i40e_set_link_state(pf->pdev, vf_id, VFD_LINKSTATE_AUTO);
+	}
+err_out:
+	return ret;
+}
+#endif /* CONFIG_DCB */
+
+/**
+ * i40e_set_vf_enable
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @enable: enable/disable
+ *
+ * This function enables or disables a VF
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_set_vf_enable(struct pci_dev *pdev, int vf_id,
+			      const bool enable)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vsi *vsi;
+	unsigned long q_map;
+	struct i40e_vf *vf;
+	int ret, tmp;
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto err_out;
+	vf = &pf->vf[vf_id];
+
+	/* allow the VF to get enabled */
+	if (enable) {
+		vf->pf_ctrl_disable = false;
+		/* reset needed to reinit VF resources */
+		i40e_vc_reset_vf(vf, true);
+		ret = i40e_set_link_state(pdev, vf_id, VFD_LINKSTATE_AUTO);
+	} else {
+		vsi = pf->vsi[vf->lan_vsi_idx];
+		q_map = BIT(vsi->num_queue_pairs) - 1;
+
+		/* force link down to prevent tx hangs */
+		ret = i40e_set_link_state(pdev, vf_id, VFD_LINKSTATE_OFF);
+		if (ret)
+			goto err_out;
+		vf->pf_ctrl_disable = true;
+
+		/* Try to stop both Tx&Rx rings even if one of the calls fails
+		 * to ensure we stop the rings even in case of errors.
+		 * If any of them returns with an error then the first
+		 * error that occurred will be returned.
+		 */
+		tmp = i40e_ctrl_vf_tx_rings(vsi, q_map, enable);
+		ret = i40e_ctrl_vf_rx_rings(vsi, q_map, enable);
+
+		ret = tmp ? tmp : ret;
+	}
+
+err_out:
+	return ret;
+}
+
+static int i40e_get_vf_enable(struct pci_dev *pdev, int vf_id, bool *enable)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vf *vf;
+	int ret;
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		return ret;
+	vf = &pf->vf[vf_id];
+	*enable = !vf->pf_ctrl_disable;
+	return 0;
+}
+
+/**
+ * i40e_get_rx_bytes
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @rx_bytes: pointer to the caller's rx_bytes variable
+ *
+ * This function gets the received bytes on the VF
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_get_rx_bytes(struct pci_dev *pdev, int vf_id,
+			     u64 *rx_bytes)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vsi *vsi;
+	struct i40e_vf *vf;
+	int ret;
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto err_out;
+	vf = &pf->vf[vf_id];
+	vsi = pf->vsi[vf->lan_vsi_idx];
+	i40e_update_eth_stats(vsi);
+	*rx_bytes = vsi->eth_stats.rx_bytes;
+err_out:
+	return ret;
+}
+
+/**
+ * i40e_get_rx_dropped
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @rx_dropped: pointer to the caller's rx_dropped variable
+ *
+ * This function gets the dropped received bytes on the VF
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_get_rx_dropped(struct pci_dev *pdev, int vf_id,
+			       u64 *rx_dropped)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vsi *vsi;
+	struct i40e_vf *vf;
+	int ret;
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto err_out;
+	vf = &pf->vf[vf_id];
+	vsi = pf->vsi[vf->lan_vsi_idx];
+	i40e_update_eth_stats(vsi);
+	*rx_dropped = vsi->eth_stats.rx_discards;
+err_out:
+	return ret;
+}
+
+/**
+ * i40e_get_rx_packets
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @rx_packets: pointer to the caller's rx_packets variable
+ *
+ * This function gets the number of packets received on the VF
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_get_rx_packets(struct pci_dev *pdev, int vf_id,
+			       u64 *rx_packets)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vsi *vsi;
+	struct i40e_vf *vf;
+	int ret;
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto err_out;
+	vf = &pf->vf[vf_id];
+	vsi = pf->vsi[vf->lan_vsi_idx];
+	i40e_update_eth_stats(vsi);
+	*rx_packets = vsi->eth_stats.rx_unicast + vsi->eth_stats.rx_multicast +
+		      vsi->eth_stats.rx_broadcast;
+err_out:
+	return ret;
+}
+
+/**
+ * i40e_get_tx_bytes
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @tx_bytes: pointer to the caller's tx_bytes variable
+ *
+ * This function gets the transmitted bytes by the VF
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_get_tx_bytes(struct pci_dev *pdev, int vf_id,
+			     u64 *tx_bytes)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vsi *vsi;
+	struct i40e_vf *vf;
+	int ret;
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto err_out;
+	vf = &pf->vf[vf_id];
+	vsi = pf->vsi[vf->lan_vsi_idx];
+	i40e_update_eth_stats(vsi);
+	*tx_bytes = vsi->eth_stats.tx_bytes;
+err_out:
+	return ret;
+}
+
+/**
+ * i40e_get_tx_dropped
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @tx_dropped: pointer to the caller's tx_dropped variable
+ *
+ * This function gets the dropped tx bytes by the VF
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_get_tx_dropped(struct pci_dev *pdev, int vf_id,
+			       u64 *tx_dropped)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vsi *vsi;
+	struct i40e_vf *vf;
+	int ret;
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto err_out;
+	vf = &pf->vf[vf_id];
+	vsi = pf->vsi[vf->lan_vsi_idx];
+	i40e_update_eth_stats(vsi);
+	*tx_dropped = vsi->eth_stats.tx_discards;
+err_out:
+	return ret;
+}
+
+/**
+ * i40e_get_tx_packets
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @tx_packets: pointer to the caller's tx_packets variable
+ *
+ * This function gets the number of packets transmitted by the VF
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_get_tx_packets(struct pci_dev *pdev, int vf_id,
+			       u64 *tx_packets)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vsi *vsi;
+	struct i40e_vf *vf;
+	int ret;
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto err_out;
+	vf = &pf->vf[vf_id];
+	vsi = pf->vsi[vf->lan_vsi_idx];
+	i40e_update_eth_stats(vsi);
+	*tx_packets = vsi->eth_stats.tx_unicast + vsi->eth_stats.tx_multicast +
+		      vsi->eth_stats.tx_broadcast;
+err_out:
+	return ret;
+}
+
+/**
+ * i40e_get_tx_errors
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @tx_errors: pointer to the caller's tx_errors variable
+ *
+ * This function gets the number of packets transmitted by the VF
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_get_tx_errors(struct pci_dev *pdev, int vf_id,
+			      u64 *tx_errors)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vsi *vsi;
+	struct i40e_vf *vf;
+	int ret;
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto err_out;
+	vf = &pf->vf[vf_id];
+	vsi = pf->vsi[vf->lan_vsi_idx];
+	i40e_update_eth_stats(vsi);
+	*tx_errors = vsi->eth_stats.tx_errors;
+err_out:
+	return ret;
+}
+
+/**
+ * i40e_get_mac
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @mac: the default mac address
+ *
+ * This function gets the default mac address
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_get_mac(struct pci_dev *pdev, int vf_id, u8 *mac)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vf *vf;
+	int ret;
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto err_out;
+	vf = &pf->vf[vf_id];
+	ether_addr_copy(mac, vf->default_lan_addr.addr);
+err_out:
+	return ret;
+}
+
+/**
+ * i40e_set_mac
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @mac: the default mac address to set
+ *
+ * This function sets the default mac address for the VF
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_set_mac(struct pci_dev *pdev, int vf_id, const u8 *mac)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vsi *vsi;
+	struct i40e_vf *vf;
+	int ret;
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto err_out;
+	vf = &pf->vf[vf_id];
+	vsi = pf->vsi[vf->lan_vsi_idx];
+	ret = i40e_set_vf_mac(vf, vsi, mac);
+err_out:
+	return ret;
+}
+
+/**
+ * i40e_get_promisc
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @promisc_mode: current promiscuous mode
+ *
+ * This function gets the current promiscuous mode configuration.
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_get_promisc(struct pci_dev *pdev, int vf_id, u8 *promisc_mode)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vf *vf;
+	int ret;
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto err_out;
+	vf = &pf->vf[vf_id];
+	*promisc_mode = vf->promisc_mode;
+err_out:
+	return ret;
+}
+
+/**
+ * i40e_set_promisc
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @promisc_mode: promiscuous mode to be set
+ *
+ * This function sets the promiscuous mode configuration.
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_set_promisc(struct pci_dev *pdev, int vf_id,
+			    const u8 promisc_mode)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vsi *vsi;
+	struct i40e_vf *vf;
+	int ret;
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto err;
+	vf = &pf->vf[vf_id];
+	vsi = pf->vsi[vf->lan_vsi_idx];
+	ret = i40e_configure_vf_promisc_mode(vf, vsi, promisc_mode);
+err:
+	return ret;
+}
+
+/**
+ * i40e_get_ingress_mirror - Gets the configured ingress mirror
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @mirror: pointer to return the ingress mirror
+ *
+ * Gets the ingress mirror configured
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_get_ingress_mirror(struct pci_dev *pdev, int vf_id, int *mirror)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vf *vf;
+	int ret;
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto err_out;
+	vf = &pf->vf[vf_id];
+	*mirror = vf->ingress_vlan;
+err_out:
+	return ret;
+}
+
+/**
+ * i40e_set_ingress_mirror - Configure ingress mirror
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @mirror: mirror vf
+ *
+ * Configures the ingress mirror
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_set_ingress_mirror(struct pci_dev *pdev, int vf_id,
+				   const int mirror)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vsi *src_vsi, *mirror_vsi;
+	struct i40e_vf *vf, *mirror_vf;
+	u16 rule_type, rule_id;
+	int ret;
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto err_out;
+	vf = &pf->vf[vf_id];
+
+	/* The Admin Queue mirroring rules refer to the traffic
+	 * directions from the perspective of the switch, not the VSI
+	 * we apply the mirroring rule on - so the behaviour of a VSI
+	 * ingress mirror is classified as an egress rule
+	 */
+	rule_type = I40E_AQC_MIRROR_RULE_TYPE_VPORT_EGRESS;
+	src_vsi = pf->vsi[vf->lan_vsi_idx];
+	if (mirror == I40E_NO_VF_MIRROR) {
+		/* Del mirrors */
+		rule_id = vf->ingress_rule_id;
+		ret = i40e_del_ingress_egress_mirror(src_vsi, rule_type,
+						     rule_id);
+		if (ret)
+			goto err_out;
+		vf->ingress_vlan = I40E_NO_VF_MIRROR;
+	} else {
+		/* validate the mirror */
+		ret = i40e_validate_vf(pf, mirror);
+		if (ret)
+			goto err_out;
+		mirror_vf = &pf->vf[mirror];
+		mirror_vsi = pf->vsi[mirror_vf->lan_vsi_idx];
+
+		/* Add mirrors */
+		ret = i40e_add_ingress_egress_mirror(src_vsi, mirror_vsi,
+						     rule_type, &rule_id);
+		if (ret)
+			goto err_out;
+		vf->ingress_vlan = mirror;
+		vf->ingress_rule_id = rule_id;
+	}
+err_out:
+	return ret;
+}
+
+/**
+ * i40e_get_egress_mirror - Gets the configured egress mirror
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @mirror: pointer to return the egress mirror
+ *
+ * Gets the egress mirror configured
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_get_egress_mirror(struct pci_dev *pdev, int vf_id, int *mirror)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vf *vf;
+	int ret;
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto err_out;
+	vf = &pf->vf[vf_id];
+	*mirror = vf->egress_vlan;
+err_out:
+	return ret;
+}
+
+/**
+ * i40e_set_egress_mirror - Configure egress mirror
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @mirror: mirror vf
+ *
+ * Configures the egress mirror
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_set_egress_mirror(struct pci_dev *pdev, int vf_id,
+				  const int mirror)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vsi *src_vsi, *mirror_vsi;
+	struct i40e_vf *vf, *mirror_vf;
+	u16 rule_type, rule_id;
+	int ret;
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto err_out;
+	vf = &pf->vf[vf_id];
+
+	/* The Admin Queue mirroring rules refer to the traffic
+	 * directions from the perspective of the switch, not the VSI
+	 * we apply the mirroring rule on - so the behaviour of a VSI
+	 * egress mirror is classified as an ingress rule
+	 */
+	rule_type = I40E_AQC_MIRROR_RULE_TYPE_VPORT_INGRESS;
+	src_vsi = pf->vsi[vf->lan_vsi_idx];
+	if (mirror == I40E_NO_VF_MIRROR) {
+		/* Del mirrors */
+		rule_id = vf->egress_rule_id;
+		ret = i40e_del_ingress_egress_mirror(src_vsi, rule_type,
+						     rule_id);
+		if (ret)
+			goto err_out;
+		vf->egress_vlan = I40E_NO_VF_MIRROR;
+	} else {
+		/* validate the mirror */
+		ret = i40e_validate_vf(pf, mirror);
+		if (ret)
+			goto err_out;
+		mirror_vf = &pf->vf[mirror];
+		mirror_vsi = pf->vsi[mirror_vf->lan_vsi_idx];
+
+		/* Add mirrors */
+		ret = i40e_add_ingress_egress_mirror(src_vsi, mirror_vsi,
+						     rule_type, &rule_id);
+		if (ret)
+			goto err_out;
+		vf->egress_vlan = mirror;
+		vf->egress_rule_id = rule_id;
+	}
+err_out:
+	return ret;
+}
+
+/*
+ * i40e_get_mac_list
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @list_head: list of mac addresses
+ *
+ * This function returns the list of mac address configured on the VF. It is
+ * the responsibility of the caller to free the allocated list when finished.
+ *
+ * Returns 0 on success, negative on failure
+ */
+static int i40e_get_mac_list(struct pci_dev *pdev, int vf_id,
+			     struct list_head *mac_list)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vsi *vsi;
+	struct i40e_vf *vf;
+	int ret;
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto error_out;
+	vf = &pf->vf[vf_id];
+	vsi = pf->vsi[vf->lan_vsi_idx];
+	ret = i40e_copy_mac_list_sync(vsi, mac_list);
+error_out:
+	return ret;
+}
+
+#define I40E_MAC_FILTERS_LIMIT (PAGE_SIZE / (3 * ETH_ALEN))
+/* determined by kernel: ((1024 - header) / (3 * ETH_ALEN)) = 51 */
+#define I40E_MAC_LISTING_LIMIT 51
+
+/*
+ * i40e_add_macs_to_list
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @list_head: list of mac addresses
+ *
+ * This function adds a list of mac addresses for a VF
+ *
+ * Returns 0 on success, negative on failure
+ */
+static int i40e_add_macs_to_list(struct pci_dev *pdev, int vf_id,
+				 struct list_head *mac_list)
+{
+	unsigned int mac_num_allowed, mac_num_list = 0;
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	int off_limits_count = 0, idx = 0;
+	struct i40e_mac_filter *f;
+	struct vfd_macaddr *tmp;
+	char *off_limits = NULL;
+	struct i40e_vsi *vsi;
+	struct i40e_vf *vf;
+	int ret, bkt;
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto error_out;
+	vf = &pf->vf[vf_id];
+	vsi = pf->vsi[vf->lan_vsi_idx];
+
+	spin_lock_bh(&vsi->mac_filter_hash_lock);
+	hash_for_each(vsi->mac_filter_hash, bkt, f, hlist)
+		mac_num_list++;
+
+	mac_num_allowed = I40E_MAC_FILTERS_LIMIT - mac_num_list;
+	off_limits = kzalloc(I40E_MAC_LISTING_LIMIT * 3 * ETH_ALEN,
+			     GFP_ATOMIC);
+	if (!off_limits) {
+		spin_unlock_bh(&vsi->mac_filter_hash_lock);
+		return -ENOMEM;
+	}
+
+	list_for_each_entry(tmp, mac_list, list) {
+		f = i40e_find_mac(vsi, tmp->mac);
+		if (!f && mac_num_allowed) {
+			f = i40e_add_mac_filter(vsi, tmp->mac);
+			if (!f) {
+				dev_err(&pf->pdev->dev,
+					"Unable to add MAC filter %pM for VF %d\n",
+					tmp->mac, vf->vf_id);
+				ret = I40E_ERR_PARAM;
+				spin_unlock_bh(&vsi->mac_filter_hash_lock);
+				goto error_out;
+			}
+			mac_num_allowed--;
+		} else if (!f && !mac_num_allowed) {
+			if (!off_limits_count) {
+				idx = scnprintf(off_limits,
+						3 * ETH_ALEN,
+						"%pM", tmp->mac);
+				off_limits_count++;
+			} else if (off_limits_count + 1 >=
+				   I40E_MAC_LISTING_LIMIT) {
+				scnprintf(&off_limits[idx],
+					  3 * ETH_ALEN + 1,
+					  ",%pM", tmp->mac);
+				dev_warn(&pf->pdev->dev,
+					 "No more MAC addresses can be added. <%s> not added\n",
+					 off_limits);
+				off_limits_count = 0;
+				idx = 0;
+			} else {
+				idx += scnprintf(&off_limits[idx],
+						 3 * ETH_ALEN + 1,
+						 ",%pM", tmp->mac);
+				off_limits_count++;
+			}
+		}
+	}
+	spin_unlock_bh(&vsi->mac_filter_hash_lock);
+
+	if (off_limits_count)
+		dev_warn(&pf->pdev->dev,
+			 "No more MAC addresses can be added. <%s> not added\n",
+			 off_limits);
+
+	/* program the updated filter list */
+	ret = i40e_sync_vsi_filters(vsi);
+	if (ret)
+		dev_err(&pf->pdev->dev, "Unable to program VF %d MAC filters, error %d\n",
+			vf->vf_id, ret);
+error_out:
+	kfree(off_limits);
+
+	return ret;
+}
+
+/*
+ * i40e_rem_macs_from_list
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @list_head: list of mac addresses
+ *
+ * This function removes a list of mac addresses from a VF
+ *
+ * Returns 0 on success, negative on failure
+ */
+static int i40e_rem_macs_from_list(struct pci_dev *pdev, int vf_id,
+				   struct list_head *mac_list)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct vfd_macaddr *tmp;
+	struct i40e_vsi *vsi;
+	struct i40e_vf *vf;
+	int ret;
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto error_out;
+	vf = &pf->vf[vf_id];
+	vsi = pf->vsi[vf->lan_vsi_idx];
+	spin_lock_bh(&vsi->mac_filter_hash_lock);
+	list_for_each_entry(tmp, mac_list, list) {
+		if (i40e_del_mac_filter(vsi, tmp->mac)) {
+			ret = I40E_ERR_INVALID_MAC_ADDR;
+			spin_unlock_bh(&vsi->mac_filter_hash_lock);
+			goto error_out;
+		}
+	}
+	spin_unlock_bh(&vsi->mac_filter_hash_lock);
+
+	/* program the updated filter list */
+	ret = i40e_sync_vsi_filters(vsi);
+	if (ret)
+		dev_err(&pf->pdev->dev, "Unable to program VF %d MAC filters, error %d\n",
+			vf->vf_id, ret);
+error_out:
+	return ret;
+}
+
+/*
+ * i40e_set_pf_qos_apply
+ * @pdev: PCI device information struct
+ *
+ * This function applies the bw shares stored across all VFs.
+ * If there are VFs with share configured per traffic class, it configures
+ * VEB's TC bandwidth.
+ *
+ * Returns 0 on success, negative on failure
+ */
+static int i40e_set_pf_qos_apply(struct pci_dev *pdev)
+{
+	struct i40e_aqc_configure_vsi_tc_bw_data bw_data;
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	int i, j, ret = 0, total_share = 0;
+	struct i40e_vf *vf = pf->vf;
+	struct i40e_vsi *vsi;
+#ifdef CONFIG_DCB
+	u16 total_mib_bw[I40E_MAX_TRAFFIC_CLASS] = {0};
+	bool reconfig_vf_vsi = false;
+	u8 enabled_tc = 0;
+	s16 total_bw = 0;
+#endif
+
+	for (i = 0; i < pf->num_alloc_vfs; i++, vf++)
+		total_share += vf->bw_share;
+
+	/* verify BW share distribution */
+	if (total_share > 100) {
+		dev_err(&pdev->dev, "Total share is greater than 100 percent");
+		return I40E_ERR_PARAM;
+	}
+
+	memset(&bw_data, 0,
+	       sizeof(struct i40e_aqc_configure_vsi_tc_bw_data));
+	for (i = 0; i < pf->num_alloc_vfs; i++) {
+		ret = i40e_validate_vf(pf, vf->vf_id);
+		if (ret)
+			continue;
+		vf = &pf->vf[i];
+		if (vf->tc_bw_share_req)
+			continue;
+		if (!vf->bw_share)
+			continue;
+		if (!test_bit(I40E_VF_STATE_INIT, &vf->vf_states)) {
+			dev_err(&pf->pdev->dev, "VF %d still in reset. Try again.\n",
+				vf->vf_id);
+			ret = I40E_ERR_PARAM;
+			goto error_param;
+		}
+		vsi = pf->vsi[vf->lan_vsi_idx];
+		bw_data.tc_valid_bits = 1;
+		bw_data.tc_bw_credits[0] = vf->bw_share;
+
+		ret = i40e_aq_config_vsi_tc_bw(&pf->hw, vsi->seid,
+					       &bw_data, NULL);
+		if (ret) {
+			dev_info(&pf->pdev->dev,
+				 "AQ command Config VSI BW allocation per TC failed = %d\n",
+				 pf->hw.aq.asq_last_status);
+			vf->bw_share_applied = false;
+			return -EINVAL;
+		}
+
+		for (j = 0; j < I40E_MAX_TRAFFIC_CLASS; j++)
+			vsi->info.qs_handle[j] = bw_data.qs_handles[j];
+
+		/* set the tracking bool to true */
+		vf->bw_share_applied = true;
+	}
+	pf->vf_bw_applied = true;
+
+#ifdef CONFIG_DCB
+	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++)
+		if (pf->dcb_user_up_map[i] !=
+		    I40E_MULTIPLE_TRAFFIC_CLASS_NO_ENTRY)
+			enabled_tc |= BIT(pf->dcb_user_up_map[i]);
+
+	if (pf->dcb_user_reconfig) {
+		/* first gather what is set by user */
+		for (i = 0, vf = pf->vf; i < pf->num_alloc_vfs; i++, vf++)
+			for (j = 0; j < I40E_MAX_TRAFFIC_CLASS; j++) {
+				total_bw += vf->tc_info.requested_tc_share[j];
+				total_mib_bw[j] +=
+					vf->tc_info.requested_tc_share[j];
+			}
+
+		/* set missing mib_bw to 100 if it's missing */
+		for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++)
+			if (total_mib_bw[i] == 0 && (enabled_tc & BIT(i))) {
+				total_mib_bw[i] = 100;
+				total_bw += 100;
+			}
+
+		for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
+			if (total_mib_bw[i] > 100) {
+				dev_err(&pdev->dev, "Cannot apply ETS settings, sum of VF share settings for TC %d is different than 100",
+					i);
+				return I40E_ERR_PARAM;
+			}
+			if (unlikely(total_bw == 0)) {
+				dev_err(&pdev->dev, "Cannot apply ETS settings, total bandwidth used is 0");
+				return I40E_ERR_PARAM;
+			}
+			/* accommodate for total_bw */
+			total_mib_bw[i] = total_mib_bw[i] * 100 / total_bw;
+		}
+
+		/* assign remaining bw to TC0 */
+		total_bw = 0;
+		for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++)
+			total_bw += total_mib_bw[i];
+		for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
+			if (((s16)(total_mib_bw[i]) + 100 - total_bw) > 0) {
+				total_mib_bw[i] += 100 - total_bw;
+				break;
+			}
+		}
+
+		for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
+			pf->dcb_mib_bw_map[i] = total_mib_bw[i];
+			total_mib_bw[i] = 0;
+		}
+
+		/* quiesce VFs */
+		vf = pf->vf;
+		for (i = 0; i < pf->num_alloc_vfs; i++, vf++)
+			i40e_enable_vf_queues(pf->vsi[vf->lan_vsi_idx], false);
+		/* Configure port to ETS */
+		i40e_update_ets(pf);
+		pf->dcb_user_reconfig = false;
+		vf = pf->vf;
+		/* unquiesce VFs */
+		for (i = 0; i < pf->num_alloc_vfs; i++, vf++)
+			if (!vf->pf_ctrl_disable)
+				i40e_enable_vf_queues(pf->vsi[vf->lan_vsi_idx],
+						      true);
+	}
+
+	/* Reconfig VF VSI for TC */
+	for (i = 0, vf = pf->vf; i < pf->num_alloc_vfs; i++, vf++) {
+		total_bw = 0;
+
+		for (j = 0; j < I40E_MAX_TRAFFIC_CLASS; j++) {
+			/* TC must be continuous */
+			if (!(enabled_tc & BIT(j)) &&
+			    vf->tc_info.requested_tc_share[j]) {
+				dev_info(&pdev->dev, "User tried to set non continuous TC, Not setting TC on VF %d",
+					 vf->vf_id);
+				for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++)
+					vf->tc_info.requested_tc_share[j] = 0;
+				continue;
+			}
+			total_bw += vf->tc_info.requested_tc_share[j];
+		}
+
+		ret = i40e_vsi_config_tc(pf->vsi[vf->lan_vsi_idx],
+					 enabled_tc);
+		if (ret) {
+			dev_info(&pdev->dev,
+				 "Failed configuring TC for VSI seid=%d\n",
+				 pf->vsi[vf->lan_vsi_idx]->seid);
+			/* Will try to configure as many components */
+		} else {
+			reconfig_vf_vsi = true;
+			vf->tc_info.applied = true;
+		}
+	}
+
+	/* exhaust whole TC BW, redistribute remaining TC BW to every VF,
+	 * which does not have share assigned to it.
+	 */
+	vf = pf->vf;
+	for (i = 0; i < pf->num_alloc_vfs; i++, vf++) {
+		if (!vf->tc_info.applied)
+			continue;
+		for (j = 0; j < I40E_MAX_TRAFFIC_CLASS; j++)
+			total_mib_bw[j] += vf->tc_info.requested_tc_share[j];
+	}
+
+	for (i = 0; i < I40E_MAX_TRAFFIC_CLASS; i++) {
+		total_bw = 0;
+
+		if (!(pf->vsi[pf->lan_vsi]->tc_config.enabled_tc & BIT(i)))
+			break;
+
+		vf = pf->vf;
+		for (j = 0; j < pf->num_alloc_vfs; j++, vf++) {
+			if (!vf->tc_info.applied)
+				continue;
+
+			if (!vf->tc_info.requested_tc_share[i])
+				total_bw++;
+		}
+		total_mib_bw[i] = 100 - total_mib_bw[i];
+		if (total_mib_bw[i] && total_bw) {
+			total_mib_bw[i] /= total_bw;
+			vf = pf->vf;
+			for (j = 0; j < pf->num_alloc_vfs; j++, vf++) {
+				if (!vf->tc_info.applied)
+					continue;
+				if (!vf->tc_info.requested_tc_share[i])
+					vf->tc_info.requested_tc_share[i] =
+						total_mib_bw[i];
+			}
+		}
+	}
+
+	if (reconfig_vf_vsi) {
+		for (i = 0, vf = pf->vf; i < pf->num_alloc_vfs; i++, vf++) {
+			if (vf->tc_info.applied)
+				ret = i40e_apply_vsi_tc_bw
+					(vf, vf->tc_info.requested_tc_share);
+			if (ret)
+				continue;
+			for (j = 0; j < I40E_MAX_TRAFFIC_CLASS; j++)
+				vf->tc_info.applied_tc_share[j] =
+					vf->tc_info.requested_tc_share[j];
+		}
+	}
+#endif /* CONFIG_DCB */
+error_param:
+	return ret;
+}
+
+/**
+ * i40e_get_pf_ingress_mirror - Gets the configured ingress mirror for PF
+ * @pdev: PCI device information struct
+ * @mirror: pointer to return the ingress mirror
+ *
+ * Gets the ingress mirror configured
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_get_pf_ingress_mirror(struct pci_dev *pdev, int *mirror)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	*mirror = pf->ingress_vlan;
+	return 0;
+}
+
+/**
+ * i40e_set_pf_ingress_mirror - Sets the configured ingress mirror for PF
+ * @pdev: PCI device information struct
+ * @mirror: pointer to return the ingress mirror
+ *
+ * Gets the ingress mirror configured
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_set_pf_ingress_mirror(struct pci_dev *pdev, const int mirror)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vsi *src_vsi, *mirror_vsi;
+	struct i40e_vf *mirror_vf;
+	u16 rule_type, rule_id;
+	int ret;
+
+	/* The Admin Queue mirroring rules refer to the traffic
+	 * directions from the perspective of the switch, not the VSI
+	 * we apply the mirroring rule on - so the behaviour of a VSI
+	 * ingress mirror is classified as an egress rule
+	 */
+	rule_type = I40E_AQC_MIRROR_RULE_TYPE_VPORT_EGRESS;
+	src_vsi = pf->vsi[pf->lan_vsi];
+	if (mirror == I40E_NO_VF_MIRROR) {
+		/* Del mirrors */
+		rule_id = pf->ingress_rule_id;
+		ret = i40e_del_ingress_egress_mirror(src_vsi, rule_type,
+						     rule_id);
+		if (ret)
+			goto err_out;
+		pf->ingress_vlan = I40E_NO_VF_MIRROR;
+	} else {
+		/* validate the mirror */
+		ret = i40e_validate_vf(pf, mirror);
+		if (ret)
+			goto err_out;
+		mirror_vf = &pf->vf[mirror];
+		mirror_vsi = pf->vsi[mirror_vf->lan_vsi_idx];
+
+		/* Add mirrors */
+		ret = i40e_add_ingress_egress_mirror(src_vsi, mirror_vsi,
+						     rule_type, &rule_id);
+		if (ret)
+			goto err_out;
+		pf->ingress_vlan = mirror;
+		pf->ingress_rule_id = rule_id;
+	}
+err_out:
+	return ret;
+}
+
+/**
+ * i40e_get_pf_egress_mirror - Gets the configured egress mirror for PF
+ * @pdev: PCI device information struct
+ * @mirror: pointer to return the ingress mirror
+ *
+ * Gets the ingress mirror configured
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_get_pf_egress_mirror(struct pci_dev *pdev, int *mirror)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	*mirror = pf->egress_vlan;
+	return 0;
+}
+
+/**
+ * i40e_set_pf_egress_mirror - Sets the configured egress mirror for PF
+ * @pdev: PCI device information struct
+ * @mirror: pointer to return the ingress mirror
+ *
+ * Gets the ingress mirror configured
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_set_pf_egress_mirror(struct pci_dev *pdev, const int mirror)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vsi *src_vsi, *mirror_vsi;
+	struct i40e_vf *mirror_vf;
+	u16 rule_type, rule_id;
+	int ret;
+
+	/* The Admin Queue mirroring rules refer to the traffic
+	 * directions from the perspective of the switch, not the VSI
+	 * we apply the mirroring rule on - so the behaviour of a VSI
+	 * egress mirror is classified as an ingress rule
+	 */
+	rule_type = I40E_AQC_MIRROR_RULE_TYPE_VPORT_INGRESS;
+	src_vsi = pf->vsi[pf->lan_vsi];
+	if (mirror == I40E_NO_VF_MIRROR) {
+		/* Del mirrors */
+		rule_id = pf->egress_rule_id;
+		ret = i40e_del_ingress_egress_mirror(src_vsi, rule_type,
+						     rule_id);
+		if (ret)
+			goto err_out;
+		pf->egress_vlan = I40E_NO_VF_MIRROR;
+	} else {
+		/* validate the mirror */
+		ret = i40e_validate_vf(pf, mirror);
+		if (ret)
+			goto err_out;
+		mirror_vf = &pf->vf[mirror];
+		mirror_vsi = pf->vsi[mirror_vf->lan_vsi_idx];
+
+		/* Add mirrors */
+		ret = i40e_add_ingress_egress_mirror(src_vsi, mirror_vsi,
+						     rule_type, &rule_id);
+		if (ret)
+			goto err_out;
+		pf->egress_vlan = mirror;
+		pf->egress_rule_id = rule_id;
+	}
+err_out:
+	return ret;
+}
+
+#define I40E_GL_SWT_L2TAGCTRL(_i) (0x001C0A70 + ((_i) * 4))
+#define I40E_GL_SWT_L2TAGCTRL_ETHERTYPE_SHIFT 16
+#define OUTER_TAG_IDX 2
+static int i40e_get_pf_tpid(struct pci_dev *pdev, u16 *tp_id)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+
+	if (!(pf->hw.flags & I40E_HW_FLAG_802_1AD_CAPABLE))
+		return -EOPNOTSUPP;
+
+	*tp_id = (u16)(rd32(&pf->hw, I40E_GL_SWT_L2TAGCTRL(OUTER_TAG_IDX)) >>
+		      I40E_GL_SWT_L2TAGCTRL_ETHERTYPE_SHIFT);
+
+	return 0;
+}
+
+static int i40e_set_pf_tpid(struct pci_dev *pdev, u16 tp_id)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	u16 sw_flags = 0, valid_flags = 0;
+	int ret = 0;
+
+	if (!(pf->hw.flags & I40E_HW_FLAG_802_1AD_CAPABLE))
+		return -EOPNOTSUPP;
+
+	if (tp_id != ETH_P_8021Q && tp_id != ETH_P_8021AD) {
+		dev_err(&pdev->dev,
+			"Only TPIDs 0x88a8 and 0x8100 are allowed.\n");
+		return -EINVAL;
+	}
+
+	pf->hw.first_tag = tp_id;
+	dev_info(&pdev->dev,
+		 "TPID configuration only supported for PF 0. Please ensure to manually set same TPID on all PFs.\n");
+	if (pf->hw.pf_id == 0) {
+		ret = i40e_aq_set_switch_config(&pf->hw, sw_flags, valid_flags,
+						0, NULL);
+		if (ret)
+			/* not a fatal problem, just keep going */
+			dev_info(&pf->pdev->dev,
+				 "couldn't set switch config bits, err %s\n",
+				 i40e_stat_str(&pf->hw, ret));
+	}
+
+	return ret;
+}
+
+static int i40e_get_num_queues(struct pci_dev *pdev, int vf_id, int *num_queues)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vf *vf;
+	int ret;
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		return ret;
+	vf = &pf->vf[vf_id];
+
+	*num_queues = vf->num_queue_pairs;
+
+	return ret;
+}
+
+static int i40e_set_num_queues(struct pci_dev *pdev, int vf_id, int num_queues)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vf *vf;
+	int ret;
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		return ret;
+	vf = &pf->vf[vf_id];
+
+	if (test_bit(I40E_VF_STATE_RESOURCES_LOADED, &vf->vf_states)) {
+		dev_err(&pdev->dev,
+			"Unable to configure %d queues, please unbind the driver for VF %d\n",
+			num_queues, vf_id);
+		return -EAGAIN;
+	}
+
+	return i40e_set_vf_num_queues(vf, num_queues);
+}
+
+/**
+ * i40e_get_max_tx_rate
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @max_tx_rate: max transmit bandwidth rate
+ *
+ * This function returns the value of transmit bandwidth, in Mbps,
+ * for the specified VF,
+ * value 0 means rate limiting is disabled.
+ *
+ * Returns 0 on success, negative on failure
+ */
+static int i40e_get_max_tx_rate(struct pci_dev *pdev, int vf_id,
+				unsigned int *max_tx_rate)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vsi *vsi;
+	struct i40e_vf *vf;
+	int ret;
+
+	if (test_and_set_bit(__I40E_VIRTCHNL_OP_PENDING, pf->state)) {
+		dev_warn(&pf->pdev->dev,
+			 "Unable to configure VFs, other operation is pending.\n");
+		return -EAGAIN;
+	}
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto error_param;
+
+	vf = &pf->vf[vf_id];
+	vsi = pf->vsi[vf->lan_vsi_idx];
+	if (!vsi) {
+		ret = -ENOENT;
+		goto error_param;
+	}
+
+	*max_tx_rate = vf->tx_rate;
+
+error_param:
+	clear_bit(__I40E_VIRTCHNL_OP_PENDING, pf->state);
+	return ret;
+}
+
+/**
+ * i40e_set_max_tx_rate
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @max_tx_rate: max transmit bandwidth rate to set
+ *
+ * This function sets the value of max transmit bandwidth, in Mbps,
+ * for the specified VF,
+ * value 0 means rate limiting is disabled.
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_set_max_tx_rate(struct pci_dev *pdev, int vf_id,
+				unsigned int *max_tx_rate)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vsi *vsi;
+	struct i40e_vf *vf;
+	int ret;
+
+	if (test_and_set_bit(__I40E_VIRTCHNL_OP_PENDING, pf->state)) {
+		dev_warn(&pf->pdev->dev,
+			 "Unable to configure VFs, other operation is pending.\n");
+		return -EAGAIN;
+	}
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto error;
+
+	vf = &pf->vf[vf_id];
+	vsi = pf->vsi[vf->lan_vsi_idx];
+	if (!test_bit(I40E_VF_STATE_INIT, &vf->vf_states)) {
+		dev_err(&pf->pdev->dev, "VF %d still in reset. Try again.\n",
+			vf_id);
+		ret = -EAGAIN;
+		goto error;
+	}
+
+	ret = i40e_set_bw_limit(vsi, vsi->seid, *max_tx_rate);
+	if (ret)
+		goto error;
+
+	vf->tx_rate = *max_tx_rate;
+error:
+	clear_bit(__I40E_VIRTCHNL_OP_PENDING, pf->state);
+	return ret;
+}
+
+/**
+ * i40e_get_trust_state
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @enable: on success, true if enabled, false if not
+ *
+ * Gets VF trust configure.
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_get_trust_state(struct pci_dev *pdev, int vf_id, bool *enable)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vf *vf;
+	int ret;
+
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		return ret;
+	vf = &pf->vf[vf_id];
+
+	*enable = vf->trusted;
+
+	return ret;
+}
+
+/**
+ * i40e_set_trust_state
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @enable: enable or disable trust
+ *
+ * Sets the VF trust configure
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_set_trust_state(struct pci_dev *pdev, int vf_id, bool enable)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vf *vf;
+	int ret;
+
+	if (test_and_set_bit(__I40E_VIRTCHNL_OP_PENDING, pf->state)) {
+		dev_warn(&pf->pdev->dev, "Unable to configure VFs, other operation is pending.\n");
+		return -EAGAIN;
+	}
+
+	if (pf->flags & I40E_FLAG_MFP_ENABLED) {
+		dev_err(&pf->pdev->dev, "Trusted VF not supported in MFP mode.\n");
+		ret = -EINVAL;
+		goto out;
+	}
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto out;
+
+	vf = &pf->vf[vf_id];
+	/* if vf is in base mode, make it untrusted */
+	if (pf->vf_base_mode_only)
+		enable = false;
+	if (enable == vf->trusted)
+		goto out;
+
+	vf->trusted = enable;
+
+	/* request PF to sync mac/vlan filters for the VF */
+	set_bit(__I40E_MACVLAN_SYNC_PENDING, pf->state);
+	pf->vsi[vf->lan_vsi_idx]->flags |= I40E_VSI_FLAG_FILTER_CHANGED;
+
+	i40e_vc_reset_vf(vf, true);
+	dev_info(&pf->pdev->dev, "VF %u is now %strusted\n",
+		 vf_id, enable ? "" : "un");
+
+#ifdef __TC_MQPRIO_MODE_MAX
 	if (vf->adq_enabled) {
 		if (!vf->trusted) {
 			dev_info(&pf->pdev->dev,
@@ -4544,8 +8375,554 @@
 			i40e_del_all_cloud_filters(vf);
 		}
 	}
+#endif /* __TC_MQPRIO_MODE_MAX */
 
 out:
 	clear_bit(__I40E_VIRTCHNL_OP_PENDING, pf->state);
 	return ret;
 }
+
+static int i40e_get_queue_type(struct pci_dev *pdev, int vf_id, u8 *queue_type)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vf *vf;
+	int ret;
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		return ret;
+	vf = &pf->vf[vf_id];
+
+	*queue_type = vf->queue_type;
+
+	return ret;
+}
+
+static int i40e_set_queue_type(struct pci_dev *pdev, int vf_id, u8 queue_type)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vf *vf;
+	int ret = 0;
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		return ret;
+	vf = &pf->vf[vf_id];
+
+	if (queue_type != VFD_QUEUE_TYPE_RSS &&
+	    queue_type != VFD_QUEUE_TYPE_QOS) {
+		dev_err(&pdev->dev,
+			"Unable to configure queue_type for VF %d, invalid argument\n",
+			vf_id);
+		return -EINVAL;
+	}
+	vf->queue_type = queue_type;
+
+	return ret;
+}
+
+/**
+ * i40e_get_allow_bcast
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @allow: on success, true if allowed, false if not
+ *
+ * Gets VF allow_bcast configure.
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_get_allow_bcast(struct pci_dev *pdev, int vf_id, bool *allow)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vf *vf;
+	int ret;
+
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		return ret;
+	vf = &pf->vf[vf_id];
+
+	*allow = vf->allow_bcast;
+
+	return ret;
+}
+
+/**
+ * i40e_set_allow_bcast
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @allow: allow or disallow VF broadcast
+ *
+ * Sets the VF allow_bcast configure
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_set_allow_bcast(struct pci_dev *pdev, int vf_id, bool allow)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	u8 broadcast[ETH_ALEN];
+	struct i40e_vsi *vsi;
+	struct i40e_vf *vf;
+	int ret = 0;
+
+	/* validate the request */
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto out;
+
+	vf = &pf->vf[vf_id];
+	if (allow == vf->allow_bcast)
+		goto out;
+
+	vf->allow_bcast = allow;
+	eth_broadcast_addr(broadcast);
+	vsi = pf->vsi[vf->lan_vsi_idx];
+
+	spin_lock_bh(&vsi->mac_filter_hash_lock);
+	if (!allow)
+		i40e_del_mac_filter(vsi, broadcast);
+	else if (!i40e_add_mac_filter(vsi, broadcast))
+		dev_info(&pf->pdev->dev,
+			 "Could not allocate VF broadcast filter\n");
+	spin_unlock_bh(&vsi->mac_filter_hash_lock);
+
+out:
+	return ret;
+}
+
+/**
+ * i40e_set_pf_qos_tc_max_bw
+ * @pdev: PCI device information struct
+ * @tc: Traffic class number
+ * @req_bw: Requested bandwidth
+ *
+ * Set bandwidth assigned for given TC
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_set_pf_qos_tc_max_bw(struct pci_dev *pdev, int tc, u16 req_bw)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vsi *vsi;
+	int ret;
+
+	vsi = pf->vsi[pf->lan_vsi];
+	ret = i40e_get_link_speed(vsi);
+	if (req_bw > ret) {
+		dev_err(&pdev->dev, "Failed to set PF max bandwidth. Value must be between 0 and %d",
+			ret);
+		return -EINVAL;
+	}
+
+	if (req_bw % I40E_BW_CREDIT_DIVISOR) {
+		dev_err(&pdev->dev, "Failed to set PF max bandwidth. Value must be multiple of %d",
+			I40E_BW_CREDIT_DIVISOR);
+		return -EINVAL;
+	}
+
+	pf->dcb_veb_bw_map[tc] = req_bw / I40E_BW_CREDIT_DIVISOR;
+	pf->dcb_user_reconfig = true;
+
+	return 0;
+}
+
+/**
+ * i40e_get_pf_qos_tc_max_bw
+ * @pdev: PCI device information struct
+ * @tc: Traffic class number
+ * @req_bw: Requested bandwidth
+ *
+ * Get bandwidth assigned for given TC
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_get_pf_qos_tc_max_bw(struct pci_dev *pdev, int tc, u16 *req_bw)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vsi *vsi;
+
+	vsi = pf->vsi[pf->lan_vsi];
+
+	if (tc > I40E_MAX_TRAFFIC_CLASS ||
+	    (!(vsi->tc_config.enabled_tc & BIT(tc)))) {
+		dev_err(&pdev->dev, "Invalid TC value. Value must be between 0-7 and TC must be configured");
+		return -EINVAL;
+	}
+
+	*req_bw = pf->dcb_veb_bw_map[tc] * I40E_BW_CREDIT_DIVISOR;
+
+	return 0;
+}
+
+/**
+ * i40e_set_pf_qos_tc_lsp
+ * @pdev: PCI device information struct
+ * @tc: Traffic class number
+ * @on: true if link strict priority is on, false otherwise
+ *
+ * Set link strict priority for given TC
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_set_pf_qos_tc_lsp(struct pci_dev *pdev, int tc, bool on)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+
+	pf->dcb_user_lsp_map[tc] = on;
+	pf->dcb_user_reconfig = true;
+
+	return 0;
+}
+
+/**
+ * i40e_get_pf_qos_tc_lsp
+ * @pdev: PCI device information struct
+ * @tc: Traffic class number
+ * @on: true if link strict priority is on, false otherwise
+ *
+ * Get link strict priority for given TC
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_get_pf_qos_tc_lsp(struct pci_dev *pdev, int tc, bool *on)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vsi *vsi =  pf->vsi[pf->lan_vsi];
+
+	if (!(pf->flags & I40E_FLAG_DCB_ENABLED)) {
+		dev_err(&pdev->dev, "Port is not configured to DCB");
+		return -EPERM;
+	}
+
+	if (tc > I40E_MAX_TRAFFIC_CLASS ||
+	    (!(vsi->tc_config.enabled_tc & BIT(tc)))) {
+		dev_err(&pdev->dev, "Invalid TC value. Value must be between 0-7 and TC must be configured");
+		return -EINVAL;
+	}
+
+	*on = !!pf->dcb_user_lsp_map[tc];
+
+	return 0;
+}
+
+/**
+ * i40e_set_pf_qos_tc_priority
+ * @pdev: PCI device information struct
+ * @tc: Traffic class number
+ * @tc_bitmap: Priority bitmap
+ *
+ * Set priority bitmap for given TC
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_set_pf_qos_tc_priority(struct pci_dev *pdev, int tc,
+				       char tc_bitmap)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	u8 new_up[I40E_MAX_USER_PRIORITY];
+	u8 old_up[I40E_MAX_USER_PRIORITY];
+	u8 tmp;
+	int i;
+
+	/* Check if up is already set by another TC */
+	for (i = 0; i < I40E_MAX_USER_PRIORITY; i++) {
+		if (BIT(i) & tc_bitmap) {
+			tmp = pf->dcb_user_up_map[i];
+			if (!(tmp == I40E_MULTIPLE_TRAFFIC_CLASS_NO_ENTRY) ==
+			    !(tmp == tc)) {
+				dev_err(&pdev->dev, "Failed to set user priority for TC %d. Priority %d already taken by <TC num>",
+					tc, i);
+				return -EPERM;
+			}
+			new_up[i] = tc;
+			continue;
+		}
+		new_up[i] = I40E_MULTIPLE_TRAFFIC_CLASS_NO_ENTRY;
+	}
+
+	for (i = 0; i < I40E_MAX_USER_PRIORITY; i++) {
+		if (pf->dcb_user_up_map[i] == tc)
+			old_up[i] = tc;
+		else
+			old_up[i] = I40E_MULTIPLE_TRAFFIC_CLASS_NO_ENTRY;
+	}
+
+	/* enable for change again */
+	for (i = 0; i < I40E_MAX_USER_PRIORITY; i++)
+		if (new_up[i] == I40E_MULTIPLE_TRAFFIC_CLASS_NO_ENTRY &&
+		    old_up[i] != I40E_MULTIPLE_TRAFFIC_CLASS_NO_ENTRY)
+			pf->dcb_user_up_map[i] =
+				I40E_MULTIPLE_TRAFFIC_CLASS_NO_ENTRY;
+
+	for (i = 0; i < I40E_MAX_USER_PRIORITY; i++)
+		if (BIT(i) & tc_bitmap)
+			pf->dcb_user_up_map[i] = tc;
+
+	pf->dcb_user_reconfig = true;
+
+	return 0;
+}
+
+/**
+ * i40e_get_pf_qos_tc_priority
+ * @pdev: PCI device information struct
+ * @tc: Traffic class number
+ * @tc_bitmap: Priority bitmap
+ *
+ * Get priority bitmap for given TC
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_get_pf_qos_tc_priority(struct pci_dev *pdev, int tc,
+				       char *tc_bitmap)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	int i;
+
+	*tc_bitmap = 0;
+	if (!(pf->flags & I40E_FLAG_DCB_ENABLED)) {
+		dev_err(&pdev->dev, "Port is not configured to DCB");
+		return -EPERM;
+	}
+
+	for (i = 0; i < I40E_MAX_USER_PRIORITY; i++) {
+		if (pf->dcb_user_up_map[i] == tc)
+			*tc_bitmap |= BIT(i);
+	}
+
+	return 0;
+}
+
+/**
+ * i40e_set_vf_max_tc_tx_rate
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @tc: Traffic class number
+ * @rate: Max TC tx rate in Mbps for VF
+ *
+ * Get max transfer speed for VF for given TC
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_set_vf_max_tc_tx_rate(struct pci_dev *pdev, int vf_id, int tc,
+				      int rate)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vsi *vsi;
+	struct i40e_vf *vf;
+	int ret;
+
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto err;
+
+	vf = &pf->vf[vf_id];
+	vsi = pf->vsi[vf->lan_vsi_idx];
+
+	ret = i40e_get_link_speed(vsi);
+	if (rate > ret || rate < 0) {
+		dev_err(&pdev->dev, "Failed to set VF max TC tx rate. Value must be between 0 and %d",
+			ret);
+		return -EINVAL;
+	}
+
+	if (rate % I40E_BW_CREDIT_DIVISOR) {
+		dev_err(&pdev->dev, "Failed to set VF max TC tx rate. Value must be multiple of %d",
+			I40E_BW_CREDIT_DIVISOR);
+		return -EINVAL;
+	}
+
+	if (tc > I40E_MAX_TRAFFIC_CLASS || (!(vsi->tc_config.enabled_tc & BIT(tc)))) {
+		dev_err(&pdev->dev, "Invalid TC value. Value must be between 0-7 and TC must be configured");
+		return -EINVAL;
+	}
+
+	vsi->tc_config.tc_info[tc].tc_bw_credits = rate /
+		I40E_BW_CREDIT_DIVISOR;
+	ret = i40e_vsi_configure_tc_max_bw(vsi);
+	if (!ret)
+		vf->tc_info.max_tc_tx_rate[tc] =
+			vsi->tc_config.tc_info[tc].tc_bw_credits;
+err:
+	return ret;
+}
+
+/**
+ * i40e_get_vf_max_tc_tx_rate
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @tc: Traffic class number
+ * @rate: Max TC tx rate in Mbps for VF
+ *
+ * Get max transfer speed for VF for given TC
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_get_vf_max_tc_tx_rate(struct pci_dev *pdev, int vf_id, int tc,
+				      int *rate)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vsi *vsi;
+	struct i40e_vf *vf;
+	int ret;
+
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto err;
+
+	vf = &pf->vf[vf_id];
+	vsi = pf->vsi[vf->lan_vsi_idx];
+
+	if (tc > I40E_MAX_TRAFFIC_CLASS ||
+	    (!(vsi->tc_config.enabled_tc & BIT(tc)))) {
+		dev_err(&pdev->dev, "Invalid TC value. Value must be between 0-7 and TC must be configured");
+		return -EINVAL;
+	}
+
+	*rate = vf->tc_info.max_tc_tx_rate[tc] * I40E_BW_CREDIT_DIVISOR;
+err:
+	return ret;
+}
+
+/**
+ * i40e_set_vf_qos_tc_share
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @tc: Traffic class number
+ * @share: percentage share of TC
+ *
+ * Set percentage share of TC resources for VF
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_set_vf_qos_tc_share(struct pci_dev *pdev, int vf_id, int tc,
+				    u8 share)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vf *vf;
+	int ret;
+
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto err;
+
+	vf = &pf->vf[vf_id];
+
+	if (vf->tc_info.requested_tc_share[tc] && !share) {
+		dev_err(&pdev->dev, "Invalid share value. Can't set share back to 0");
+		return -EINVAL;
+	}
+
+	if (vf->bw_share_applied)
+		return -EPERM;
+
+	vf->tc_info.requested_tc_share[tc] = share;
+	vf->tc_bw_share_req = true;
+	pf->dcb_user_reconfig = true;
+err:
+	return ret;
+}
+
+/**
+ * i40e_get_vf_qos_tc_share
+ * @pdev: PCI device information struct
+ * @vf_id: VF identifier
+ * @tc: Traffic class number
+ * @share: percentage share of TC
+ *
+ * Get percentage share of TC resources for VF
+ * Returns 0 on success, negative on failure
+ **/
+static int i40e_get_vf_qos_tc_share(struct pci_dev *pdev, int vf_id, int tc,
+				    u8 *share)
+{
+	struct i40e_pf *pf = pci_get_drvdata(pdev);
+	struct i40e_vsi *vsi;
+	struct i40e_vf *vf;
+	int ret;
+
+	ret = i40e_validate_vf(pf, vf_id);
+	if (ret)
+		goto err;
+
+	vf = &pf->vf[vf_id];
+	vsi = pf->vsi[vf->lan_vsi_idx];
+
+	if (tc > I40E_MAX_TRAFFIC_CLASS ||
+	    (!(vsi->tc_config.enabled_tc & BIT(tc)))) {
+		dev_err(&pdev->dev, "Invalid TC value. Value must be between 0-7 and TC must be configured");
+		return -EINVAL;
+	}
+
+	if (vf->bw_share_applied)
+		return -EPERM;
+
+	*share = vf->tc_info.applied_tc_share[tc];
+err:
+	return ret;
+}
+
+const struct vfd_ops i40e_vfd_ops = {
+	.get_trunk		= i40e_get_trunk,
+	.set_trunk		= i40e_set_trunk,
+	.get_vlan_mirror	= i40e_get_mirror,
+	.set_vlan_mirror	= i40e_set_mirror,
+	.get_mac_anti_spoof	= i40e_get_mac_anti_spoof,
+	.set_mac_anti_spoof	= i40e_set_mac_anti_spoof,
+	.get_vlan_anti_spoof	= i40e_get_vlan_anti_spoof,
+	.set_vlan_anti_spoof	= i40e_set_vlan_anti_spoof,
+	.set_allow_untagged	= i40e_set_allow_untagged,
+	.get_allow_untagged	= i40e_get_allow_untagged,
+	.get_loopback		= i40e_get_loopback,
+	.set_loopback		= i40e_set_loopback,
+	.get_vlan_strip		= i40e_get_vlan_strip,
+	.set_vlan_strip		= i40e_set_vlan_strip,
+	.get_rx_bytes		= i40e_get_rx_bytes,
+	.get_rx_dropped		= i40e_get_rx_dropped,
+	.get_rx_packets		= i40e_get_rx_packets,
+	.get_tx_bytes		= i40e_get_tx_bytes,
+	.get_tx_dropped		= i40e_get_tx_dropped,
+	.get_tx_packets		= i40e_get_tx_packets,
+	.get_tx_errors		= i40e_get_tx_errors,
+	.get_mac		= i40e_get_mac,
+	.set_mac		= i40e_set_mac,
+	.get_promisc		= i40e_get_promisc,
+	.set_promisc		= i40e_set_promisc,
+	.get_ingress_mirror	= i40e_get_ingress_mirror,
+	.set_ingress_mirror	= i40e_set_ingress_mirror,
+	.get_egress_mirror	= i40e_get_egress_mirror,
+	.set_egress_mirror	= i40e_set_egress_mirror,
+	.get_link_state		= i40e_get_link_state,
+	.set_link_state		= i40e_set_link_state,
+	.get_mac_list		= i40e_get_mac_list,
+	.add_macs_to_list	= i40e_add_macs_to_list,
+	.rem_macs_from_list	= i40e_rem_macs_from_list,
+	.get_vf_enable		= i40e_get_vf_enable,
+	.set_vf_enable		= i40e_set_vf_enable,
+	.reset_stats		= i40e_reset_vf_stats,
+	.set_vf_bw_share	= i40e_store_vf_bw_share,
+	.get_vf_bw_share	= i40e_get_vf_bw_share,
+	.set_pf_qos_apply	= i40e_set_pf_qos_apply,
+	.get_pf_ingress_mirror	= i40e_get_pf_ingress_mirror,
+	.set_pf_ingress_mirror	= i40e_set_pf_ingress_mirror,
+	.get_pf_egress_mirror	= i40e_get_pf_egress_mirror,
+	.set_pf_egress_mirror	= i40e_set_pf_egress_mirror,
+	.get_pf_tpid		= i40e_get_pf_tpid,
+	.set_pf_tpid		= i40e_set_pf_tpid,
+	.get_num_queues		= i40e_get_num_queues,
+	.set_num_queues		= i40e_set_num_queues,
+	.get_max_tx_rate	= i40e_get_max_tx_rate,
+	.set_max_tx_rate	= i40e_set_max_tx_rate,
+	.get_trust_state	= i40e_get_trust_state,
+	.set_trust_state	= i40e_set_trust_state,
+	.get_queue_type		= i40e_get_queue_type,
+	.set_queue_type		= i40e_set_queue_type,
+	.get_allow_bcast	= i40e_get_allow_bcast,
+	.set_allow_bcast	= i40e_set_allow_bcast,
+	.set_pf_qos_tc_max_bw	= i40e_set_pf_qos_tc_max_bw,
+	.get_pf_qos_tc_max_bw	= i40e_get_pf_qos_tc_max_bw,
+	.set_pf_qos_tc_lsp	= i40e_set_pf_qos_tc_lsp,
+	.get_pf_qos_tc_lsp	= i40e_get_pf_qos_tc_lsp,
+	.set_pf_qos_tc_priority	= i40e_set_pf_qos_tc_priority,
+	.get_pf_qos_tc_priority	= i40e_get_pf_qos_tc_priority,
+	.set_vf_max_tc_tx_rate	= i40e_set_vf_max_tc_tx_rate,
+	.get_vf_max_tc_tx_rate	= i40e_get_vf_max_tc_tx_rate,
+	.set_vf_qos_tc_share	= i40e_set_vf_qos_tc_share,
+	.get_vf_qos_tc_share	= i40e_get_vf_qos_tc_share,
+
+};
+#endif /* HAVE_NDO_SET_VF_LINK_STATE */
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_virtchnl_pf.h linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_virtchnl_pf.h
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_virtchnl_pf.h	2024-05-10 01:26:45.405079876 -0400
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_virtchnl_pf.h	2024-05-13 03:58:25.372491940 -0400
@@ -1,5 +1,5 @@
 /* SPDX-License-Identifier: GPL-2.0 */
-/* Copyright(c) 2013 - 2018 Intel Corporation. */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
 
 #ifndef _I40E_VIRTCHNL_PF_H_
 #define _I40E_VIRTCHNL_PF_H_
@@ -19,6 +19,10 @@
 
 #define I40E_MAX_VF_PROMISC_FLAGS	3
 
+#define I40E_VF_STATE_WAIT_COUNT	20
+#define I40E_VFR_WAIT_COUNT		100
+#define I40E_VF_RESET_TIME_MIN		30000000	// time in nsec
+
 /* Various queue ctrls */
 enum i40e_queue_ctrl {
 	I40E_QUEUE_CTRL_UNKNOWN = 0,
@@ -34,18 +38,17 @@
 enum i40e_vf_states {
 	I40E_VF_STATE_INIT = 0,
 	I40E_VF_STATE_ACTIVE,
-	I40E_VF_STATE_IWARPENA,
 	I40E_VF_STATE_DISABLED,
 	I40E_VF_STATE_MC_PROMISC,
 	I40E_VF_STATE_UC_PROMISC,
 	I40E_VF_STATE_PRE_ENABLE,
+	I40E_VF_STATE_RESOURCES_LOADED,
 };
 
 /* VF capabilities */
 enum i40e_vf_capabilities {
 	I40E_VIRTCHNL_VF_CAP_PRIVILEGE = 0,
 	I40E_VIRTCHNL_VF_CAP_L2,
-	I40E_VIRTCHNL_VF_CAP_IWARP,
 };
 
 /* In ADq, max 4 VSI's can be allocated per VF including primary VF VSI.
@@ -61,6 +64,32 @@
 	u64 max_tx_rate; /* bandwidth rate allocation for VSIs */
 };
 
+/* used for VLAN list 'vm_vlan_list' by VM for trusted and untrusted VF */
+struct i40e_vm_vlan {
+	struct list_head list;
+	s16 vlan;
+	u16 vsi_id;
+};
+
+/* used for MAC list 'vm_mac_list' to recognize MACs added by VM */
+struct i40e_vm_mac {
+	struct list_head list;
+	u8 macaddr[ETH_ALEN];
+};
+
+/* used for following share for given traffic class by VF*/
+struct i40e_vf_tc_info {
+	bool applied;
+	u8 applied_tc_share[I40E_MAX_TRAFFIC_CLASS];
+	u8 requested_tc_share[I40E_MAX_TRAFFIC_CLASS];
+	u16 max_tc_tx_rate[I40E_MAX_TRAFFIC_CLASS];
+};
+
+struct i40e_time_mac {
+	unsigned long time_modified;
+	u8 addr[ETH_ALEN];
+};
+
 /* VF information structure */
 struct i40e_vf {
 	struct i40e_pf *pf;
@@ -76,9 +105,11 @@
 	u16 stag;
 
 	struct virtchnl_ether_addr default_lan_addr;
-	u16 port_vlan_id;
+	struct i40e_time_mac legacy_last_added_umac; /* keeps last added MAC address */
+	s16 port_vlan_id;
 	bool pf_set_mac;	/* The VMM admin set the VF MAC address */
 	bool trusted;
+	u64 reset_timestamp;
 
 	/* VSI indices - actual VSI pointers are maintained in the PF structure
 	 * When assigned, these will be non-zero, because VSI 0 is always
@@ -97,26 +128,51 @@
 	unsigned long vf_caps;	/* vf's adv. capabilities */
 	unsigned long vf_states;	/* vf's runtime states */
 	unsigned int tx_rate;	/* Tx bandwidth limit in Mbps */
+#ifdef HAVE_NDO_SET_VF_LINK_STATE
 	bool link_forced;
 	bool link_up;		/* only valid if VF link is forced */
-	bool queues_enabled;	/* true if the VF queues are enabled */
-	bool spoofchk;
-	u16 num_mac;
+#endif
+	bool mac_anti_spoof;
+	bool vlan_anti_spoof;
 	u16 num_vlan;
-
+	DECLARE_BITMAP(mirror_vlans, VLAN_N_VID);
+	u16 vlan_rule_id;
+#define I40E_NO_VF_MIRROR	-1
+/* assuming vf ids' range is <0..max_supported> */
+#define I40E_IS_MIRROR_VLAN_ID_VALID(id) ((id) >= 0)
+	u16 ingress_rule_id;
+	int ingress_vlan;
+	u16 egress_rule_id;
+	int egress_vlan;
+	DECLARE_BITMAP(trunk_vlans, VLAN_N_VID);
+	bool trunk_set_by_pf;
+	bool allow_untagged; /* update filters, when changing value */
+	bool loopback;
+	bool vlan_stripping;
+	u8 promisc_mode;
+	u8 bw_share;
+	bool bw_share_applied; /* true if config is applied to the device */
+	bool tc_bw_share_req;
+	bool pf_ctrl_disable; /* bool for PF ctrl of VF enable/disable */
+	u8 queue_type;
+	bool allow_bcast;
+	/* VLAN list created by VM for trusted and untrusted VF */
+	struct list_head vm_vlan_list;
+	/* MAC list created by VM */
+	struct list_head vm_mac_list;
 	/* ADq related variables */
 	bool adq_enabled; /* flag to enable adq */
 	u8 num_tc;
 	struct i40evf_channel ch[I40E_MAX_VF_VSI];
 	struct hlist_head cloud_filter_list;
 	u16 num_cloud_filters;
-
-	/* RDMA Client */
-	struct virtchnl_iwarp_qvlist_info *qvlist_info;
+	struct i40e_vf_tc_info tc_info;
 };
 
 void i40e_free_vfs(struct i40e_pf *pf);
+#if defined(HAVE_SRIOV_CONFIGURE) || defined(HAVE_RHEL6_SRIOV_CONFIGURE)
 int i40e_pci_sriov_configure(struct pci_dev *dev, int num_vfs);
+#endif
 int i40e_alloc_vfs(struct i40e_pf *pf, u16 num_alloc_vfs);
 int i40e_vc_process_vf_msg(struct i40e_pf *pf, s16 vf_id, u32 v_opcode,
 			   u32 v_retval, u8 *msg, u16 msglen);
@@ -127,17 +183,41 @@
 
 /* VF configuration related iplink handlers */
 int i40e_ndo_set_vf_mac(struct net_device *netdev, int vf_id, u8 *mac);
+#ifdef IFLA_VF_VLAN_INFO_MAX
 int i40e_ndo_set_vf_port_vlan(struct net_device *netdev, int vf_id,
 			      u16 vlan_id, u8 qos, __be16 vlan_proto);
+#else
+int i40e_ndo_set_vf_port_vlan(struct net_device *netdev,
+			      int vf_id, u16 vlan_id, u8 qos);
+#endif
+#ifdef HAVE_NDO_SET_VF_MIN_MAX_TX_RATE
 int i40e_ndo_set_vf_bw(struct net_device *netdev, int vf_id, int min_tx_rate,
 		       int max_tx_rate);
+#else
+int i40e_ndo_set_vf_bw(struct net_device *netdev, int vf_id, int tx_rate);
+#endif
+#ifdef HAVE_NDO_SET_VF_TRUST
 int i40e_ndo_set_vf_trust(struct net_device *netdev, int vf_id, bool setting);
+#endif
+int i40e_ndo_enable_vf(struct net_device *netdev, int vf_id, bool enable);
+#ifdef IFLA_VF_MAX
 int i40e_ndo_get_vf_config(struct net_device *netdev,
 			   int vf_id, struct ifla_vf_info *ivi);
+#ifdef HAVE_NDO_SET_VF_LINK_STATE
 int i40e_ndo_set_vf_link_state(struct net_device *netdev, int vf_id, int link);
+#endif
+#ifdef HAVE_VF_SPOOFCHK_CONFIGURE
 int i40e_ndo_set_vf_spoofchk(struct net_device *netdev, int vf_id, bool enable);
+#endif
+#endif
 
 void i40e_vc_notify_link_state(struct i40e_pf *pf);
 void i40e_vc_notify_reset(struct i40e_pf *pf);
+void i40e_restore_all_vfs_msi_state(struct pci_dev *pdev);
+#ifdef HAVE_VF_STATS
+int i40e_get_vf_stats(struct net_device *netdev, int vf_id,
+		      struct ifla_vf_stats *vf_stats);
+#endif
+extern const struct vfd_ops i40e_vfd_ops;
 
 #endif /* _I40E_VIRTCHNL_PF_H_ */
Binary files linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_virtchnl_pf.o and linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_virtchnl_pf.o differ
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_xsk.c linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_xsk.c
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_xsk.c	2024-05-10 01:26:45.405079876 -0400
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_xsk.c	1969-12-31 19:00:00.000000000 -0500
@@ -1,890 +0,0 @@
-// SPDX-License-Identifier: GPL-2.0
-/* Copyright(c) 2018 Intel Corporation. */
-
-#include <linux/bpf_trace.h>
-#include <net/xdp_sock.h>
-#include <net/xdp.h>
-
-#include "i40e.h"
-#include "i40e_txrx_common.h"
-#include "i40e_xsk.h"
-
-static struct i40e_rx_buffer *i40e_rx_bi(struct i40e_ring *rx_ring, u32 idx)
-{
-	return &rx_ring->rx_bi[idx];
-}
-
-/**
- * i40e_xsk_umem_dma_map - DMA maps all UMEM memory for the netdev
- * @vsi: Current VSI
- * @umem: UMEM to DMA map
- *
- * Returns 0 on success, <0 on failure
- **/
-static int i40e_xsk_umem_dma_map(struct i40e_vsi *vsi, struct xdp_umem *umem)
-{
-	struct i40e_pf *pf = vsi->back;
-	struct device *dev;
-	unsigned int i, j;
-	dma_addr_t dma;
-
-	dev = &pf->pdev->dev;
-	for (i = 0; i < umem->npgs; i++) {
-		dma = dma_map_page_attrs(dev, umem->pgs[i], 0, PAGE_SIZE,
-					 DMA_BIDIRECTIONAL, I40E_RX_DMA_ATTR);
-		if (dma_mapping_error(dev, dma))
-			goto out_unmap;
-
-		umem->pages[i].dma = dma;
-	}
-
-	return 0;
-
-out_unmap:
-	for (j = 0; j < i; j++) {
-		dma_unmap_page_attrs(dev, umem->pages[i].dma, PAGE_SIZE,
-				     DMA_BIDIRECTIONAL, I40E_RX_DMA_ATTR);
-		umem->pages[i].dma = 0;
-	}
-
-	return -1;
-}
-
-/**
- * i40e_xsk_umem_dma_unmap - DMA unmaps all UMEM memory for the netdev
- * @vsi: Current VSI
- * @umem: UMEM to DMA map
- **/
-static void i40e_xsk_umem_dma_unmap(struct i40e_vsi *vsi, struct xdp_umem *umem)
-{
-	struct i40e_pf *pf = vsi->back;
-	struct device *dev;
-	unsigned int i;
-
-	dev = &pf->pdev->dev;
-
-	for (i = 0; i < umem->npgs; i++) {
-		dma_unmap_page_attrs(dev, umem->pages[i].dma, PAGE_SIZE,
-				     DMA_BIDIRECTIONAL, I40E_RX_DMA_ATTR);
-
-		umem->pages[i].dma = 0;
-	}
-}
-
-/**
- * i40e_xsk_umem_enable - Enable/associate a UMEM to a certain ring/qid
- * @vsi: Current VSI
- * @umem: UMEM
- * @qid: Rx ring to associate UMEM to
- *
- * Returns 0 on success, <0 on failure
- **/
-static int i40e_xsk_umem_enable(struct i40e_vsi *vsi, struct xdp_umem *umem,
-				u16 qid)
-{
-	struct net_device *netdev = vsi->netdev;
-	struct xdp_umem_fq_reuse *reuseq;
-	bool if_running;
-	int err;
-
-	if (vsi->type != I40E_VSI_MAIN)
-		return -EINVAL;
-
-	if (qid >= vsi->num_queue_pairs)
-		return -EINVAL;
-
-	if (qid >= netdev->real_num_rx_queues ||
-	    qid >= netdev->real_num_tx_queues)
-		return -EINVAL;
-
-	reuseq = xsk_reuseq_prepare(vsi->rx_rings[0]->count);
-	if (!reuseq)
-		return -ENOMEM;
-
-	xsk_reuseq_free(xsk_reuseq_swap(umem, reuseq));
-
-	err = i40e_xsk_umem_dma_map(vsi, umem);
-	if (err)
-		return err;
-
-	set_bit(qid, vsi->af_xdp_zc_qps);
-
-	if_running = netif_running(vsi->netdev) && i40e_enabled_xdp_vsi(vsi);
-
-	if (if_running) {
-		err = i40e_queue_pair_disable(vsi, qid);
-		if (err)
-			return err;
-
-		err = i40e_queue_pair_enable(vsi, qid);
-		if (err)
-			return err;
-
-		/* Kick start the NAPI context so that receiving will start */
-		err = i40e_xsk_wakeup(vsi->netdev, qid, XDP_WAKEUP_RX);
-		if (err)
-			return err;
-	}
-
-	return 0;
-}
-
-/**
- * i40e_xsk_umem_disable - Disassociate a UMEM from a certain ring/qid
- * @vsi: Current VSI
- * @qid: Rx ring to associate UMEM to
- *
- * Returns 0 on success, <0 on failure
- **/
-static int i40e_xsk_umem_disable(struct i40e_vsi *vsi, u16 qid)
-{
-	struct net_device *netdev = vsi->netdev;
-	struct xdp_umem *umem;
-	bool if_running;
-	int err;
-
-	umem = xdp_get_umem_from_qid(netdev, qid);
-	if (!umem)
-		return -EINVAL;
-
-	if_running = netif_running(vsi->netdev) && i40e_enabled_xdp_vsi(vsi);
-
-	if (if_running) {
-		err = i40e_queue_pair_disable(vsi, qid);
-		if (err)
-			return err;
-	}
-
-	clear_bit(qid, vsi->af_xdp_zc_qps);
-	i40e_xsk_umem_dma_unmap(vsi, umem);
-
-	if (if_running) {
-		err = i40e_queue_pair_enable(vsi, qid);
-		if (err)
-			return err;
-	}
-
-	return 0;
-}
-
-/**
- * i40e_xsk_umem_setup - Enable/disassociate a UMEM to/from a ring/qid
- * @vsi: Current VSI
- * @umem: UMEM to enable/associate to a ring, or NULL to disable
- * @qid: Rx ring to (dis)associate UMEM (from)to
- *
- * This function enables or disables a UMEM to a certain ring.
- *
- * Returns 0 on success, <0 on failure
- **/
-int i40e_xsk_umem_setup(struct i40e_vsi *vsi, struct xdp_umem *umem,
-			u16 qid)
-{
-	return umem ? i40e_xsk_umem_enable(vsi, umem, qid) :
-		i40e_xsk_umem_disable(vsi, qid);
-}
-
-/**
- * i40e_run_xdp_zc - Executes an XDP program on an xdp_buff
- * @rx_ring: Rx ring
- * @xdp: xdp_buff used as input to the XDP program
- *
- * This function enables or disables a UMEM to a certain ring.
- *
- * Returns any of I40E_XDP_{PASS, CONSUMED, TX, REDIR}
- **/
-static int i40e_run_xdp_zc(struct i40e_ring *rx_ring, struct xdp_buff *xdp)
-{
-	struct xdp_umem *umem = rx_ring->xsk_umem;
-	int err, result = I40E_XDP_PASS;
-	struct i40e_ring *xdp_ring;
-	struct bpf_prog *xdp_prog;
-	u64 offset;
-	u32 act;
-
-	rcu_read_lock();
-	/* NB! xdp_prog will always be !NULL, due to the fact that
-	 * this path is enabled by setting an XDP program.
-	 */
-	xdp_prog = READ_ONCE(rx_ring->xdp_prog);
-	act = bpf_prog_run_xdp(xdp_prog, xdp);
-	offset = xdp->data - xdp->data_hard_start;
-
-	xdp->handle = xsk_umem_adjust_offset(umem, xdp->handle, offset);
-
-	switch (act) {
-	case XDP_PASS:
-		break;
-	case XDP_TX:
-		xdp_ring = rx_ring->vsi->xdp_rings[rx_ring->queue_index];
-		result = i40e_xmit_xdp_tx_ring(xdp, xdp_ring);
-		break;
-	case XDP_REDIRECT:
-		err = xdp_do_redirect(rx_ring->netdev, xdp, xdp_prog);
-		result = !err ? I40E_XDP_REDIR : I40E_XDP_CONSUMED;
-		break;
-	default:
-		bpf_warn_invalid_xdp_action(act);
-		/* fall through */
-	case XDP_ABORTED:
-		trace_xdp_exception(rx_ring->netdev, xdp_prog, act);
-		/* fallthrough -- handle aborts by dropping packet */
-	case XDP_DROP:
-		result = I40E_XDP_CONSUMED;
-		break;
-	}
-	rcu_read_unlock();
-	return result;
-}
-
-/**
- * i40e_alloc_buffer_zc - Allocates an i40e_rx_buffer
- * @rx_ring: Rx ring
- * @bi: Rx buffer to populate
- *
- * This function allocates an Rx buffer. The buffer can come from fill
- * queue, or via the recycle queue (next_to_alloc).
- *
- * Returns true for a successful allocation, false otherwise
- **/
-static bool i40e_alloc_buffer_zc(struct i40e_ring *rx_ring,
-				 struct i40e_rx_buffer *bi)
-{
-	struct xdp_umem *umem = rx_ring->xsk_umem;
-	void *addr = bi->addr;
-	u64 handle, hr;
-
-	if (addr) {
-		rx_ring->rx_stats.page_reuse_count++;
-		return true;
-	}
-
-	if (!xsk_umem_peek_addr(umem, &handle)) {
-		rx_ring->rx_stats.alloc_page_failed++;
-		return false;
-	}
-
-	hr = umem->headroom + XDP_PACKET_HEADROOM;
-
-	bi->dma = xdp_umem_get_dma(umem, handle);
-	bi->dma += hr;
-
-	bi->addr = xdp_umem_get_data(umem, handle);
-	bi->addr += hr;
-
-	bi->handle = xsk_umem_adjust_offset(umem, handle, umem->headroom);
-
-	xsk_umem_discard_addr(umem);
-	return true;
-}
-
-/**
- * i40e_alloc_buffer_slow_zc - Allocates an i40e_rx_buffer
- * @rx_ring: Rx ring
- * @bi: Rx buffer to populate
- *
- * This function allocates an Rx buffer. The buffer can come from fill
- * queue, or via the reuse queue.
- *
- * Returns true for a successful allocation, false otherwise
- **/
-static bool i40e_alloc_buffer_slow_zc(struct i40e_ring *rx_ring,
-				      struct i40e_rx_buffer *bi)
-{
-	struct xdp_umem *umem = rx_ring->xsk_umem;
-	u64 handle, hr;
-
-	if (!xsk_umem_peek_addr_rq(umem, &handle)) {
-		rx_ring->rx_stats.alloc_page_failed++;
-		return false;
-	}
-
-	handle &= rx_ring->xsk_umem->chunk_mask;
-
-	hr = umem->headroom + XDP_PACKET_HEADROOM;
-
-	bi->dma = xdp_umem_get_dma(umem, handle);
-	bi->dma += hr;
-
-	bi->addr = xdp_umem_get_data(umem, handle);
-	bi->addr += hr;
-
-	bi->handle = xsk_umem_adjust_offset(umem, handle, umem->headroom);
-
-	xsk_umem_discard_addr_rq(umem);
-	return true;
-}
-
-static __always_inline bool
-__i40e_alloc_rx_buffers_zc(struct i40e_ring *rx_ring, u16 count,
-			   bool alloc(struct i40e_ring *rx_ring,
-				      struct i40e_rx_buffer *bi))
-{
-	u16 ntu = rx_ring->next_to_use;
-	union i40e_rx_desc *rx_desc;
-	struct i40e_rx_buffer *bi;
-	bool ok = true;
-
-	rx_desc = I40E_RX_DESC(rx_ring, ntu);
-	bi = i40e_rx_bi(rx_ring, ntu);
-	do {
-		if (!alloc(rx_ring, bi)) {
-			ok = false;
-			goto no_buffers;
-		}
-
-		dma_sync_single_range_for_device(rx_ring->dev, bi->dma, 0,
-						 rx_ring->rx_buf_len,
-						 DMA_BIDIRECTIONAL);
-
-		rx_desc->read.pkt_addr = cpu_to_le64(bi->dma);
-
-		rx_desc++;
-		bi++;
-		ntu++;
-
-		if (unlikely(ntu == rx_ring->count)) {
-			rx_desc = I40E_RX_DESC(rx_ring, 0);
-			bi = i40e_rx_bi(rx_ring, 0);
-			ntu = 0;
-		}
-
-		rx_desc->wb.qword1.status_error_len = 0;
-		count--;
-	} while (count);
-
-no_buffers:
-	if (rx_ring->next_to_use != ntu)
-		i40e_release_rx_desc(rx_ring, ntu);
-
-	return ok;
-}
-
-/**
- * i40e_alloc_rx_buffers_zc - Allocates a number of Rx buffers
- * @rx_ring: Rx ring
- * @count: The number of buffers to allocate
- *
- * This function allocates a number of Rx buffers from the reuse queue
- * or fill ring and places them on the Rx ring.
- *
- * Returns true for a successful allocation, false otherwise
- **/
-bool i40e_alloc_rx_buffers_zc(struct i40e_ring *rx_ring, u16 count)
-{
-	return __i40e_alloc_rx_buffers_zc(rx_ring, count,
-					  i40e_alloc_buffer_slow_zc);
-}
-
-/**
- * i40e_alloc_rx_buffers_fast_zc - Allocates a number of Rx buffers
- * @rx_ring: Rx ring
- * @count: The number of buffers to allocate
- *
- * This function allocates a number of Rx buffers from the fill ring
- * or the internal recycle mechanism and places them on the Rx ring.
- *
- * Returns true for a successful allocation, false otherwise
- **/
-static bool i40e_alloc_rx_buffers_fast_zc(struct i40e_ring *rx_ring, u16 count)
-{
-	return __i40e_alloc_rx_buffers_zc(rx_ring, count,
-					  i40e_alloc_buffer_zc);
-}
-
-/**
- * i40e_get_rx_buffer_zc - Return the current Rx buffer
- * @rx_ring: Rx ring
- * @size: The size of the rx buffer (read from descriptor)
- *
- * This function returns the current, received Rx buffer, and also
- * does DMA synchronization.  the Rx ring.
- *
- * Returns the received Rx buffer
- **/
-static struct i40e_rx_buffer *i40e_get_rx_buffer_zc(struct i40e_ring *rx_ring,
-						    const unsigned int size)
-{
-	struct i40e_rx_buffer *bi;
-
-	bi = i40e_rx_bi(rx_ring, rx_ring->next_to_clean);
-
-	/* we are reusing so sync this buffer for CPU use */
-	dma_sync_single_range_for_cpu(rx_ring->dev,
-				      bi->dma, 0,
-				      size,
-				      DMA_BIDIRECTIONAL);
-
-	return bi;
-}
-
-/**
- * i40e_reuse_rx_buffer_zc - Recycle an Rx buffer
- * @rx_ring: Rx ring
- * @old_bi: The Rx buffer to recycle
- *
- * This function recycles a finished Rx buffer, and places it on the
- * recycle queue (next_to_alloc).
- **/
-static void i40e_reuse_rx_buffer_zc(struct i40e_ring *rx_ring,
-				    struct i40e_rx_buffer *old_bi)
-{
-	struct i40e_rx_buffer *new_bi = i40e_rx_bi(rx_ring,
-						   rx_ring->next_to_alloc);
-	u16 nta = rx_ring->next_to_alloc;
-
-	/* update, and store next to alloc */
-	nta++;
-	rx_ring->next_to_alloc = (nta < rx_ring->count) ? nta : 0;
-
-	/* transfer page from old buffer to new buffer */
-	new_bi->dma = old_bi->dma;
-	new_bi->addr = old_bi->addr;
-	new_bi->handle = old_bi->handle;
-
-	old_bi->addr = NULL;
-}
-
-/**
- * i40e_zca_free - Free callback for MEM_TYPE_ZERO_COPY allocations
- * @alloc: Zero-copy allocator
- * @handle: Buffer handle
- **/
-void i40e_zca_free(struct zero_copy_allocator *alloc, unsigned long handle)
-{
-	struct i40e_rx_buffer *bi;
-	struct i40e_ring *rx_ring;
-	u64 hr, mask;
-	u16 nta;
-
-	rx_ring = container_of(alloc, struct i40e_ring, zca);
-	hr = rx_ring->xsk_umem->headroom + XDP_PACKET_HEADROOM;
-	mask = rx_ring->xsk_umem->chunk_mask;
-
-	nta = rx_ring->next_to_alloc;
-	bi = i40e_rx_bi(rx_ring, nta);
-
-	nta++;
-	rx_ring->next_to_alloc = (nta < rx_ring->count) ? nta : 0;
-
-	handle &= mask;
-
-	bi->dma = xdp_umem_get_dma(rx_ring->xsk_umem, handle);
-	bi->dma += hr;
-
-	bi->addr = xdp_umem_get_data(rx_ring->xsk_umem, handle);
-	bi->addr += hr;
-
-	bi->handle = xsk_umem_adjust_offset(rx_ring->xsk_umem, (u64)handle,
-					    rx_ring->xsk_umem->headroom);
-}
-
-/**
- * i40e_construct_skb_zc - Create skbufff from zero-copy Rx buffer
- * @rx_ring: Rx ring
- * @bi: Rx buffer
- * @xdp: xdp_buff
- *
- * This functions allocates a new skb from a zero-copy Rx buffer.
- *
- * Returns the skb, or NULL on failure.
- **/
-static struct sk_buff *i40e_construct_skb_zc(struct i40e_ring *rx_ring,
-					     struct i40e_rx_buffer *bi,
-					     struct xdp_buff *xdp)
-{
-	unsigned int metasize = xdp->data - xdp->data_meta;
-	unsigned int datasize = xdp->data_end - xdp->data;
-	struct sk_buff *skb;
-
-	/* allocate a skb to store the frags */
-	skb = __napi_alloc_skb(&rx_ring->q_vector->napi,
-			       xdp->data_end - xdp->data_hard_start,
-			       GFP_ATOMIC | __GFP_NOWARN);
-	if (unlikely(!skb))
-		return NULL;
-
-	skb_reserve(skb, xdp->data - xdp->data_hard_start);
-	memcpy(__skb_put(skb, datasize), xdp->data, datasize);
-	if (metasize)
-		skb_metadata_set(skb, metasize);
-
-	i40e_reuse_rx_buffer_zc(rx_ring, bi);
-	return skb;
-}
-
-/**
- * i40e_inc_ntc: Advance the next_to_clean index
- * @rx_ring: Rx ring
- **/
-static void i40e_inc_ntc(struct i40e_ring *rx_ring)
-{
-	u32 ntc = rx_ring->next_to_clean + 1;
-
-	ntc = (ntc < rx_ring->count) ? ntc : 0;
-	rx_ring->next_to_clean = ntc;
-	prefetch(I40E_RX_DESC(rx_ring, ntc));
-}
-
-/**
- * i40e_clean_rx_irq_zc - Consumes Rx packets from the hardware ring
- * @rx_ring: Rx ring
- * @budget: NAPI budget
- *
- * Returns amount of work completed
- **/
-int i40e_clean_rx_irq_zc(struct i40e_ring *rx_ring, int budget)
-{
-	unsigned int total_rx_bytes = 0, total_rx_packets = 0;
-	u16 cleaned_count = I40E_DESC_UNUSED(rx_ring);
-	unsigned int xdp_res, xdp_xmit = 0;
-	bool failure = false;
-	struct sk_buff *skb;
-	struct xdp_buff xdp;
-
-	xdp.rxq = &rx_ring->xdp_rxq;
-
-	while (likely(total_rx_packets < (unsigned int)budget)) {
-		struct i40e_rx_buffer *bi;
-		union i40e_rx_desc *rx_desc;
-		unsigned int size;
-		u64 qword;
-
-		if (cleaned_count >= I40E_RX_BUFFER_WRITE) {
-			failure = failure ||
-				  !i40e_alloc_rx_buffers_fast_zc(rx_ring,
-								 cleaned_count);
-			cleaned_count = 0;
-		}
-
-		rx_desc = I40E_RX_DESC(rx_ring, rx_ring->next_to_clean);
-		qword = le64_to_cpu(rx_desc->wb.qword1.status_error_len);
-
-		/* This memory barrier is needed to keep us from reading
-		 * any other fields out of the rx_desc until we have
-		 * verified the descriptor has been written back.
-		 */
-		dma_rmb();
-
-		bi = i40e_clean_programming_status(rx_ring, rx_desc,
-						   qword);
-		if (unlikely(bi)) {
-			i40e_reuse_rx_buffer_zc(rx_ring, bi);
-			cleaned_count++;
-			continue;
-		}
-
-		size = (qword & I40E_RXD_QW1_LENGTH_PBUF_MASK) >>
-		       I40E_RXD_QW1_LENGTH_PBUF_SHIFT;
-		if (!size)
-			break;
-
-		bi = i40e_get_rx_buffer_zc(rx_ring, size);
-		xdp.data = bi->addr;
-		xdp.data_meta = xdp.data;
-		xdp.data_hard_start = xdp.data - XDP_PACKET_HEADROOM;
-		xdp.data_end = xdp.data + size;
-		xdp.handle = bi->handle;
-
-		xdp_res = i40e_run_xdp_zc(rx_ring, &xdp);
-		if (xdp_res) {
-			if (xdp_res & (I40E_XDP_TX | I40E_XDP_REDIR)) {
-				xdp_xmit |= xdp_res;
-				bi->addr = NULL;
-			} else {
-				i40e_reuse_rx_buffer_zc(rx_ring, bi);
-			}
-
-			total_rx_bytes += size;
-			total_rx_packets++;
-
-			cleaned_count++;
-			i40e_inc_ntc(rx_ring);
-			continue;
-		}
-
-		/* XDP_PASS path */
-
-		/* NB! We are not checking for errors using
-		 * i40e_test_staterr with
-		 * BIT(I40E_RXD_QW1_ERROR_SHIFT). This is due to that
-		 * SBP is *not* set in PRT_SBPVSI (default not set).
-		 */
-		skb = i40e_construct_skb_zc(rx_ring, bi, &xdp);
-		if (!skb) {
-			rx_ring->rx_stats.alloc_buff_failed++;
-			break;
-		}
-
-		cleaned_count++;
-		i40e_inc_ntc(rx_ring);
-
-		if (eth_skb_pad(skb))
-			continue;
-
-		total_rx_bytes += skb->len;
-		total_rx_packets++;
-
-		i40e_process_skb_fields(rx_ring, rx_desc, skb);
-		napi_gro_receive(&rx_ring->q_vector->napi, skb);
-	}
-
-	i40e_finalize_xdp_rx(rx_ring, xdp_xmit);
-	i40e_update_rx_stats(rx_ring, total_rx_bytes, total_rx_packets);
-
-	if (xsk_umem_uses_need_wakeup(rx_ring->xsk_umem)) {
-		if (failure || rx_ring->next_to_clean == rx_ring->next_to_use)
-			xsk_set_rx_need_wakeup(rx_ring->xsk_umem);
-		else
-			xsk_clear_rx_need_wakeup(rx_ring->xsk_umem);
-
-		return (int)total_rx_packets;
-	}
-	return failure ? budget : (int)total_rx_packets;
-}
-
-/**
- * i40e_xmit_zc - Performs zero-copy Tx AF_XDP
- * @xdp_ring: XDP Tx ring
- * @budget: NAPI budget
- *
- * Returns true if the work is finished.
- **/
-static bool i40e_xmit_zc(struct i40e_ring *xdp_ring, unsigned int budget)
-{
-	struct i40e_tx_desc *tx_desc = NULL;
-	struct i40e_tx_buffer *tx_bi;
-	bool work_done = true;
-	struct xdp_desc desc;
-	dma_addr_t dma;
-
-	while (budget-- > 0) {
-		if (!unlikely(I40E_DESC_UNUSED(xdp_ring))) {
-			xdp_ring->tx_stats.tx_busy++;
-			work_done = false;
-			break;
-		}
-
-		if (!xsk_umem_consume_tx(xdp_ring->xsk_umem, &desc))
-			break;
-
-		dma = xdp_umem_get_dma(xdp_ring->xsk_umem, desc.addr);
-
-		dma_sync_single_for_device(xdp_ring->dev, dma, desc.len,
-					   DMA_BIDIRECTIONAL);
-
-		tx_bi = &xdp_ring->tx_bi[xdp_ring->next_to_use];
-		tx_bi->bytecount = desc.len;
-
-		tx_desc = I40E_TX_DESC(xdp_ring, xdp_ring->next_to_use);
-		tx_desc->buffer_addr = cpu_to_le64(dma);
-		tx_desc->cmd_type_offset_bsz =
-			build_ctob(I40E_TX_DESC_CMD_ICRC
-				   | I40E_TX_DESC_CMD_EOP,
-				   0, desc.len, 0);
-
-		xdp_ring->next_to_use++;
-		if (xdp_ring->next_to_use == xdp_ring->count)
-			xdp_ring->next_to_use = 0;
-	}
-
-	if (tx_desc) {
-		/* Request an interrupt for the last frame and bump tail ptr. */
-		tx_desc->cmd_type_offset_bsz |= (I40E_TX_DESC_CMD_RS <<
-						 I40E_TXD_QW1_CMD_SHIFT);
-		i40e_xdp_ring_update_tail(xdp_ring);
-
-		xsk_umem_consume_tx_done(xdp_ring->xsk_umem);
-	}
-
-	return !!budget && work_done;
-}
-
-/**
- * i40e_clean_xdp_tx_buffer - Frees and unmaps an XDP Tx entry
- * @tx_ring: XDP Tx ring
- * @tx_bi: Tx buffer info to clean
- **/
-static void i40e_clean_xdp_tx_buffer(struct i40e_ring *tx_ring,
-				     struct i40e_tx_buffer *tx_bi)
-{
-	xdp_return_frame(tx_bi->xdpf);
-	dma_unmap_single(tx_ring->dev,
-			 dma_unmap_addr(tx_bi, dma),
-			 dma_unmap_len(tx_bi, len), DMA_TO_DEVICE);
-	dma_unmap_len_set(tx_bi, len, 0);
-}
-
-/**
- * i40e_clean_xdp_tx_irq - Completes AF_XDP entries, and cleans XDP entries
- * @tx_ring: XDP Tx ring
- * @tx_bi: Tx buffer info to clean
- *
- * Returns true if cleanup/tranmission is done.
- **/
-bool i40e_clean_xdp_tx_irq(struct i40e_vsi *vsi,
-			   struct i40e_ring *tx_ring, int napi_budget)
-{
-	unsigned int ntc, total_bytes = 0, budget = vsi->work_limit;
-	u32 i, completed_frames, frames_ready, xsk_frames = 0;
-	struct xdp_umem *umem = tx_ring->xsk_umem;
-	u32 head_idx = i40e_get_head(tx_ring);
-	bool work_done = true, xmit_done;
-	struct i40e_tx_buffer *tx_bi;
-
-	if (head_idx < tx_ring->next_to_clean)
-		head_idx += tx_ring->count;
-	frames_ready = head_idx - tx_ring->next_to_clean;
-
-	if (frames_ready == 0) {
-		goto out_xmit;
-	} else if (frames_ready > budget) {
-		completed_frames = budget;
-		work_done = false;
-	} else {
-		completed_frames = frames_ready;
-	}
-
-	ntc = tx_ring->next_to_clean;
-
-	for (i = 0; i < completed_frames; i++) {
-		tx_bi = &tx_ring->tx_bi[ntc];
-
-		if (tx_bi->xdpf)
-			i40e_clean_xdp_tx_buffer(tx_ring, tx_bi);
-		else
-			xsk_frames++;
-
-		tx_bi->xdpf = NULL;
-		total_bytes += tx_bi->bytecount;
-
-		if (++ntc >= tx_ring->count)
-			ntc = 0;
-	}
-
-	tx_ring->next_to_clean += completed_frames;
-	if (unlikely(tx_ring->next_to_clean >= tx_ring->count))
-		tx_ring->next_to_clean -= tx_ring->count;
-
-	if (xsk_frames)
-		xsk_umem_complete_tx(umem, xsk_frames);
-
-	i40e_arm_wb(tx_ring, vsi, budget);
-	i40e_update_tx_stats(tx_ring, completed_frames, total_bytes);
-
-out_xmit:
-	if (xsk_umem_uses_need_wakeup(tx_ring->xsk_umem))
-		xsk_set_tx_need_wakeup(tx_ring->xsk_umem);
-
-	xmit_done = i40e_xmit_zc(tx_ring, budget);
-
-	return work_done && xmit_done;
-}
-
-/**
- * i40e_xsk_wakeup - Implements the ndo_xsk_wakeup
- * @dev: the netdevice
- * @queue_id: queue id to wake up
- * @flags: ignored in our case since we have Rx and Tx in the same NAPI.
- *
- * Returns <0 for errors, 0 otherwise.
- **/
-int i40e_xsk_wakeup(struct net_device *dev, u32 queue_id, u32 flags)
-{
-	struct i40e_netdev_priv *np = netdev_priv(dev);
-	struct i40e_vsi *vsi = np->vsi;
-	struct i40e_pf *pf = vsi->back;
-	struct i40e_ring *ring;
-
-	if (test_bit(__I40E_CONFIG_BUSY, pf->state))
-		return -EAGAIN;
-
-	if (test_bit(__I40E_VSI_DOWN, vsi->state))
-		return -ENETDOWN;
-
-	if (!i40e_enabled_xdp_vsi(vsi))
-		return -ENXIO;
-
-	if (queue_id >= vsi->num_queue_pairs)
-		return -ENXIO;
-
-	if (!vsi->xdp_rings[queue_id]->xsk_umem)
-		return -ENXIO;
-
-	ring = vsi->xdp_rings[queue_id];
-
-	/* The idea here is that if NAPI is running, mark a miss, so
-	 * it will run again. If not, trigger an interrupt and
-	 * schedule the NAPI from interrupt context. If NAPI would be
-	 * scheduled here, the interrupt affinity would not be
-	 * honored.
-	 */
-	if (!napi_if_scheduled_mark_missed(&ring->q_vector->napi))
-		i40e_force_wb(vsi, ring->q_vector);
-
-	return 0;
-}
-
-void i40e_xsk_clean_rx_ring(struct i40e_ring *rx_ring)
-{
-	u16 i;
-
-	for (i = 0; i < rx_ring->count; i++) {
-		struct i40e_rx_buffer *rx_bi = i40e_rx_bi(rx_ring, i);
-
-		if (!rx_bi->addr)
-			continue;
-
-		xsk_umem_fq_reuse(rx_ring->xsk_umem, rx_bi->handle);
-		rx_bi->addr = NULL;
-	}
-}
-
-/**
- * i40e_xsk_clean_xdp_ring - Clean the XDP Tx ring on shutdown
- * @xdp_ring: XDP Tx ring
- **/
-void i40e_xsk_clean_tx_ring(struct i40e_ring *tx_ring)
-{
-	u16 ntc = tx_ring->next_to_clean, ntu = tx_ring->next_to_use;
-	struct xdp_umem *umem = tx_ring->xsk_umem;
-	struct i40e_tx_buffer *tx_bi;
-	u32 xsk_frames = 0;
-
-	while (ntc != ntu) {
-		tx_bi = &tx_ring->tx_bi[ntc];
-
-		if (tx_bi->xdpf)
-			i40e_clean_xdp_tx_buffer(tx_ring, tx_bi);
-		else
-			xsk_frames++;
-
-		tx_bi->xdpf = NULL;
-
-		ntc++;
-		if (ntc >= tx_ring->count)
-			ntc = 0;
-	}
-
-	if (xsk_frames)
-		xsk_umem_complete_tx(umem, xsk_frames);
-}
-
-/**
- * i40e_xsk_any_rx_ring_enabled - Checks if Rx rings have AF_XDP UMEM attached
- * @vsi: vsi
- *
- * Returns true if any of the Rx rings has an AF_XDP UMEM attached
- **/
-bool i40e_xsk_any_rx_ring_enabled(struct i40e_vsi *vsi)
-{
-	struct net_device *netdev = vsi->netdev;
-	int i;
-
-	for (i = 0; i < vsi->num_queue_pairs; i++) {
-		if (xdp_get_umem_from_qid(netdev, i))
-			return true;
-	}
-
-	return false;
-}
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_xsk.h linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_xsk.h
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_xsk.h	2024-05-10 01:26:45.405079876 -0400
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_xsk.h	1969-12-31 19:00:00.000000000 -0500
@@ -1,23 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 */
-/* Copyright(c) 2018 Intel Corporation. */
-
-#ifndef _I40E_XSK_H_
-#define _I40E_XSK_H_
-
-struct i40e_vsi;
-struct xdp_umem;
-struct zero_copy_allocator;
-
-int i40e_queue_pair_disable(struct i40e_vsi *vsi, int queue_pair);
-int i40e_queue_pair_enable(struct i40e_vsi *vsi, int queue_pair);
-int i40e_xsk_umem_setup(struct i40e_vsi *vsi, struct xdp_umem *umem,
-			u16 qid);
-void i40e_zca_free(struct zero_copy_allocator *alloc, unsigned long handle);
-bool i40e_alloc_rx_buffers_zc(struct i40e_ring *rx_ring, u16 cleaned_count);
-int i40e_clean_rx_irq_zc(struct i40e_ring *rx_ring, int budget);
-
-bool i40e_clean_xdp_tx_irq(struct i40e_vsi *vsi,
-			   struct i40e_ring *tx_ring, int napi_budget);
-int i40e_xsk_wakeup(struct net_device *dev, u32 queue_id, u32 flags);
-
-#endif /* _I40E_XSK_H_ */
Binary files linux-5.4.86/drivers/net/ethernet/intel/i40e/i40e_xsk.o and linux-5.4.86.new/drivers/net/ethernet/intel/i40e/i40e_xsk.o differ
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/kcompat.c linux-5.4.86.new/drivers/net/ethernet/intel/i40e/kcompat.c
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/kcompat.c	1969-12-31 19:00:00.000000000 -0500
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/kcompat.c	2024-05-13 03:58:25.372491940 -0400
@@ -0,0 +1,2954 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
+
+#include "i40e.h"
+#include "kcompat.h"
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,4,8) ) || defined __VMKLNX__
+/* From lib/vsprintf.c */
+#include <asm/div64.h>
+
+static int skip_atoi(const char **s)
+{
+	int i=0;
+
+	while (isdigit(**s))
+		i = i*10 + *((*s)++) - '0';
+	return i;
+}
+
+#define _kc_ZEROPAD	1		/* pad with zero */
+#define _kc_SIGN	2		/* unsigned/signed long */
+#define _kc_PLUS	4		/* show plus */
+#define _kc_SPACE	8		/* space if plus */
+#define _kc_LEFT	16		/* left justified */
+#define _kc_SPECIAL	32		/* 0x */
+#define _kc_LARGE	64		/* use 'ABCDEF' instead of 'abcdef' */
+
+static char * number(char * buf, char * end, long long num, int base, int size, int precision, int type)
+{
+	char c,sign,tmp[66];
+	const char *digits;
+	const char small_digits[] = "0123456789abcdefghijklmnopqrstuvwxyz";
+	const char large_digits[] = "0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ";
+	int i;
+
+	digits = (type & _kc_LARGE) ? large_digits : small_digits;
+	if (type & _kc_LEFT)
+		type &= ~_kc_ZEROPAD;
+	if (base < 2 || base > 36)
+		return 0;
+	c = (type & _kc_ZEROPAD) ? '0' : ' ';
+	sign = 0;
+	if (type & _kc_SIGN) {
+		if (num < 0) {
+			sign = '-';
+			num = -num;
+			size--;
+		} else if (type & _kc_PLUS) {
+			sign = '+';
+			size--;
+		} else if (type & _kc_SPACE) {
+			sign = ' ';
+			size--;
+		}
+	}
+	if (type & _kc_SPECIAL) {
+		if (base == 16)
+			size -= 2;
+		else if (base == 8)
+			size--;
+	}
+	i = 0;
+	if (num == 0)
+		tmp[i++]='0';
+	else while (num != 0)
+		tmp[i++] = digits[do_div(num,base)];
+	if (i > precision)
+		precision = i;
+	size -= precision;
+	if (!(type&(_kc_ZEROPAD+_kc_LEFT))) {
+		while(size-->0) {
+			if (buf <= end)
+				*buf = ' ';
+			++buf;
+		}
+	}
+	if (sign) {
+		if (buf <= end)
+			*buf = sign;
+		++buf;
+	}
+	if (type & _kc_SPECIAL) {
+		if (base==8) {
+			if (buf <= end)
+				*buf = '0';
+			++buf;
+		} else if (base==16) {
+			if (buf <= end)
+				*buf = '0';
+			++buf;
+			if (buf <= end)
+				*buf = digits[33];
+			++buf;
+		}
+	}
+	if (!(type & _kc_LEFT)) {
+		while (size-- > 0) {
+			if (buf <= end)
+				*buf = c;
+			++buf;
+		}
+	}
+	while (i < precision--) {
+		if (buf <= end)
+			*buf = '0';
+		++buf;
+	}
+	while (i-- > 0) {
+		if (buf <= end)
+			*buf = tmp[i];
+		++buf;
+	}
+	while (size-- > 0) {
+		if (buf <= end)
+			*buf = ' ';
+		++buf;
+	}
+	return buf;
+}
+
+int _kc_vsnprintf(char *buf, size_t size, const char *fmt, va_list args)
+{
+	int len;
+	unsigned long long num;
+	int i, base;
+	char *str, *end, c;
+	const char *s;
+
+	int flags;		/* flags to number() */
+
+	int field_width;	/* width of output field */
+	int precision;		/* min. # of digits for integers; max
+				   number of chars for from string */
+	int qualifier;		/* 'h', 'l', or 'L' for integer fields */
+				/* 'z' support added 23/7/1999 S.H.    */
+				/* 'z' changed to 'Z' --davidm 1/25/99 */
+
+	str = buf;
+	end = buf + size - 1;
+
+	if (end < buf - 1) {
+		end = ((void *) -1);
+		size = end - buf + 1;
+	}
+
+	for (; *fmt ; ++fmt) {
+		if (*fmt != '%') {
+			if (str <= end)
+				*str = *fmt;
+			++str;
+			continue;
+		}
+
+		/* process flags */
+		flags = 0;
+		repeat:
+			++fmt;		/* this also skips first '%' */
+			switch (*fmt) {
+				case '-': flags |= _kc_LEFT; goto repeat;
+				case '+': flags |= _kc_PLUS; goto repeat;
+				case ' ': flags |= _kc_SPACE; goto repeat;
+				case '#': flags |= _kc_SPECIAL; goto repeat;
+				case '0': flags |= _kc_ZEROPAD; goto repeat;
+			}
+
+		/* get field width */
+		field_width = -1;
+		if (isdigit(*fmt))
+			field_width = skip_atoi(&fmt);
+		else if (*fmt == '*') {
+			++fmt;
+			/* it's the next argument */
+			field_width = va_arg(args, int);
+			if (field_width < 0) {
+				field_width = -field_width;
+				flags |= _kc_LEFT;
+			}
+		}
+
+		/* get the precision */
+		precision = -1;
+		if (*fmt == '.') {
+			++fmt;
+			if (isdigit(*fmt))
+				precision = skip_atoi(&fmt);
+			else if (*fmt == '*') {
+				++fmt;
+				/* it's the next argument */
+				precision = va_arg(args, int);
+			}
+			if (precision < 0)
+				precision = 0;
+		}
+
+		/* get the conversion qualifier */
+		qualifier = -1;
+		if (*fmt == 'h' || *fmt == 'l' || *fmt == 'L' || *fmt =='Z') {
+			qualifier = *fmt;
+			++fmt;
+		}
+
+		/* default base */
+		base = 10;
+
+		switch (*fmt) {
+			case 'c':
+				if (!(flags & _kc_LEFT)) {
+					while (--field_width > 0) {
+						if (str <= end)
+							*str = ' ';
+						++str;
+					}
+				}
+				c = (unsigned char) va_arg(args, int);
+				if (str <= end)
+					*str = c;
+				++str;
+				while (--field_width > 0) {
+					if (str <= end)
+						*str = ' ';
+					++str;
+				}
+				continue;
+
+			case 's':
+				s = va_arg(args, char *);
+				if (!s)
+					s = "<NULL>";
+
+				len = strnlen(s, precision);
+
+				if (!(flags & _kc_LEFT)) {
+					while (len < field_width--) {
+						if (str <= end)
+							*str = ' ';
+						++str;
+					}
+				}
+				for (i = 0; i < len; ++i) {
+					if (str <= end)
+						*str = *s;
+					++str; ++s;
+				}
+				while (len < field_width--) {
+					if (str <= end)
+						*str = ' ';
+					++str;
+				}
+				continue;
+
+			case 'p':
+				if ('M' == *(fmt+1)) {
+					str = get_mac(str, end, va_arg(args, unsigned char *));
+					fmt++;
+				} else	{
+					if (field_width == -1) {
+						field_width = 2*sizeof(void *);
+						flags |= _kc_ZEROPAD;
+					}
+					str = number(str, end,
+							(unsigned long) va_arg(args, void *),
+							16, field_width, precision, flags);
+				}
+				continue;
+
+			case 'n':
+				/* FIXME:
+				* What does C99 say about the overflow case here? */
+				if (qualifier == 'l') {
+					long * ip = va_arg(args, long *);
+					*ip = (str - buf);
+				} else if (qualifier == 'Z') {
+					size_t * ip = va_arg(args, size_t *);
+					*ip = (str - buf);
+				} else {
+					int * ip = va_arg(args, int *);
+					*ip = (str - buf);
+				}
+				continue;
+
+			case '%':
+				if (str <= end)
+					*str = '%';
+				++str;
+				continue;
+
+				/* integer number formats - set up the flags and "break" */
+			case 'o':
+				base = 8;
+				break;
+
+			case 'X':
+				flags |= _kc_LARGE;
+			case 'x':
+				base = 16;
+				break;
+
+			case 'd':
+			case 'i':
+				flags |= _kc_SIGN;
+			case 'u':
+				break;
+
+			default:
+				if (str <= end)
+					*str = '%';
+				++str;
+				if (*fmt) {
+					if (str <= end)
+						*str = *fmt;
+					++str;
+				} else {
+					--fmt;
+				}
+				continue;
+		}
+		if (qualifier == 'L')
+			num = va_arg(args, long long);
+		else if (qualifier == 'l') {
+			num = va_arg(args, unsigned long);
+			if (flags & _kc_SIGN)
+				num = (signed long) num;
+		} else if (qualifier == 'Z') {
+			num = va_arg(args, size_t);
+		} else if (qualifier == 'h') {
+			num = (unsigned short) va_arg(args, int);
+			if (flags & _kc_SIGN)
+				num = (signed short) num;
+		} else {
+			num = va_arg(args, unsigned int);
+			if (flags & _kc_SIGN)
+				num = (signed int) num;
+		}
+		str = number(str, end, num, base,
+				field_width, precision, flags);
+	}
+	if (str <= end)
+		*str = '\0';
+	else if (size > 0)
+		/* don't write out a null byte if the buf size is zero */
+		*end = '\0';
+	/* the trailing null byte doesn't count towards the total
+	* ++str;
+	*/
+	return str-buf;
+}
+
+int _kc_snprintf(char * buf, size_t size, const char *fmt, ...)
+{
+	va_list args;
+	int i;
+
+	va_start(args, fmt);
+	i = _kc_vsnprintf(buf,size,fmt,args);
+	va_end(args);
+	return i;
+}
+#endif /* < 2.4.8 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,4,13) )
+
+/**************************************/
+/* PCI DMA MAPPING */
+
+#if defined(CONFIG_HIGHMEM)
+
+#ifndef PCI_DRAM_OFFSET
+#define PCI_DRAM_OFFSET 0
+#endif
+
+u64
+_kc_pci_map_page(struct pci_dev *dev, struct page *page, unsigned long offset,
+                 size_t size, int direction)
+{
+	return (((u64) (page - mem_map) << PAGE_SHIFT) + offset +
+		PCI_DRAM_OFFSET);
+}
+
+#else /* CONFIG_HIGHMEM */
+
+u64
+_kc_pci_map_page(struct pci_dev *dev, struct page *page, unsigned long offset,
+                 size_t size, int direction)
+{
+	return pci_map_single(dev, (void *)page_address(page) + offset, size,
+			      direction);
+}
+
+#endif /* CONFIG_HIGHMEM */
+
+void
+_kc_pci_unmap_page(struct pci_dev *dev, u64 dma_addr, size_t size,
+                   int direction)
+{
+	return pci_unmap_single(dev, dma_addr, size, direction);
+}
+
+#endif /* 2.4.13 => 2.4.3 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,4,3) )
+
+/**************************************/
+/* PCI DRIVER API */
+
+int
+_kc_pci_set_dma_mask(struct pci_dev *dev, dma_addr_t mask)
+{
+	if (!pci_dma_supported(dev, mask))
+		return -EIO;
+	dev->dma_mask = mask;
+	return 0;
+}
+
+int
+_kc_pci_request_regions(struct pci_dev *dev, char *res_name)
+{
+	int i;
+
+	for (i = 0; i < 6; i++) {
+		if (pci_resource_len(dev, i) == 0)
+			continue;
+
+		if (pci_resource_flags(dev, i) & IORESOURCE_IO) {
+			if (!request_region(pci_resource_start(dev, i), pci_resource_len(dev, i), res_name)) {
+				pci_release_regions(dev);
+				return -EBUSY;
+			}
+		} else if (pci_resource_flags(dev, i) & IORESOURCE_MEM) {
+			if (!request_mem_region(pci_resource_start(dev, i), pci_resource_len(dev, i), res_name)) {
+				pci_release_regions(dev);
+				return -EBUSY;
+			}
+		}
+	}
+	return 0;
+}
+
+void
+_kc_pci_release_regions(struct pci_dev *dev)
+{
+	int i;
+
+	for (i = 0; i < 6; i++) {
+		if (pci_resource_len(dev, i) == 0)
+			continue;
+
+		if (pci_resource_flags(dev, i) & IORESOURCE_IO)
+			release_region(pci_resource_start(dev, i), pci_resource_len(dev, i));
+
+		else if (pci_resource_flags(dev, i) & IORESOURCE_MEM)
+			release_mem_region(pci_resource_start(dev, i), pci_resource_len(dev, i));
+	}
+}
+
+/**************************************/
+/* NETWORK DRIVER API */
+
+struct net_device *
+_kc_alloc_etherdev(int sizeof_priv)
+{
+	struct net_device *dev;
+	int alloc_size;
+
+	alloc_size = sizeof(*dev) + sizeof_priv + IFNAMSIZ + 31;
+	dev = kzalloc(alloc_size, GFP_KERNEL);
+	if (!dev)
+		return NULL;
+
+	if (sizeof_priv)
+		dev->priv = (void *) (((unsigned long)(dev + 1) + 31) & ~31);
+	dev->name[0] = '\0';
+	ether_setup(dev);
+
+	return dev;
+}
+
+int
+_kc_is_valid_ether_addr(u8 *addr)
+{
+	const char zaddr[6] = { 0, };
+
+	return !(addr[0] & 1) && memcmp(addr, zaddr, 6);
+}
+
+#endif /* 2.4.3 => 2.4.0 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,4,6) )
+
+int
+_kc_pci_set_power_state(struct pci_dev *dev, int state)
+{
+	return 0;
+}
+
+int
+_kc_pci_enable_wake(struct pci_dev *pdev, u32 state, int enable)
+{
+	return 0;
+}
+
+#endif /* 2.4.6 => 2.4.3 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,0) )
+void _kc_skb_fill_page_desc(struct sk_buff *skb, int i, struct page *page,
+                            int off, int size)
+{
+	skb_frag_t *frag = &skb_shinfo(skb)->frags[i];
+	frag->page = page;
+	frag->page_offset = off;
+	frag->size = size;
+	skb_shinfo(skb)->nr_frags = i + 1;
+}
+
+/*
+ * Original Copyright:
+ * find_next_bit.c: fallback find next bit implementation
+ *
+ * Copyright (C) 2004 Red Hat, Inc. All Rights Reserved.
+ * Written by David Howells (dhowells@redhat.com)
+ */
+
+/**
+ * find_next_bit - find the next set bit in a memory region
+ * @addr: The address to base the search on
+ * @offset: The bitnumber to start searching at
+ * @size: The maximum size to search
+ */
+unsigned long find_next_bit(const unsigned long *addr, unsigned long size,
+                            unsigned long offset)
+{
+	const unsigned long *p = addr + BITOP_WORD(offset);
+	unsigned long result = offset & ~(BITS_PER_LONG-1);
+	unsigned long tmp;
+
+	if (offset >= size)
+		return size;
+	size -= result;
+	offset %= BITS_PER_LONG;
+	if (offset) {
+		tmp = *(p++);
+		tmp &= (~0UL << offset);
+		if (size < BITS_PER_LONG)
+			goto found_first;
+		if (tmp)
+			goto found_middle;
+		size -= BITS_PER_LONG;
+		result += BITS_PER_LONG;
+	}
+	while (size & ~(BITS_PER_LONG-1)) {
+		if ((tmp = *(p++)))
+			goto found_middle;
+		result += BITS_PER_LONG;
+		size -= BITS_PER_LONG;
+	}
+	if (!size)
+		return result;
+	tmp = *p;
+
+found_first:
+	tmp &= (~0UL >> (BITS_PER_LONG - size));
+	if (tmp == 0UL)		/* Are any bits set? */
+		return result + size;	/* Nope. */
+found_middle:
+	return result + ffs(tmp);
+}
+
+size_t _kc_strlcpy(char *dest, const char *src, size_t size)
+{
+	size_t ret = strlen(src);
+
+	if (size) {
+		size_t len = (ret >= size) ? size - 1 : ret;
+		memcpy(dest, src, len);
+		dest[len] = '\0';
+	}
+	return ret;
+}
+
+#ifndef do_div
+#if BITS_PER_LONG == 32
+uint32_t __attribute__((weak)) _kc__div64_32(uint64_t *n, uint32_t base)
+{
+	uint64_t rem = *n;
+	uint64_t b = base;
+	uint64_t res, d = 1;
+	uint32_t high = rem >> 32;
+
+	/* Reduce the thing a bit first */
+	res = 0;
+	if (high >= base) {
+		high /= base;
+		res = (uint64_t) high << 32;
+		rem -= (uint64_t) (high*base) << 32;
+	}
+
+	while ((int64_t)b > 0 && b < rem) {
+		b = b+b;
+		d = d+d;
+	}
+
+	do {
+		if (rem >= b) {
+			rem -= b;
+			res += d;
+		}
+		b >>= 1;
+		d >>= 1;
+	} while (d);
+
+	*n = res;
+	return rem;
+}
+#endif /* BITS_PER_LONG == 32 */
+#endif /* do_div */
+#endif /* 2.6.0 => 2.4.6 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,4) )
+int _kc_scnprintf(char * buf, size_t size, const char *fmt, ...)
+{
+	va_list args;
+	int i;
+
+	va_start(args, fmt);
+	i = vsnprintf(buf, size, fmt, args);
+	va_end(args);
+	return (i >= size) ? (size - 1) : i;
+}
+#endif /* < 2.6.4 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,10) )
+DECLARE_BITMAP(_kcompat_node_online_map, MAX_NUMNODES) = {1};
+#endif /* < 2.6.10 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,13) )
+char *_kc_kstrdup(const char *s, unsigned int gfp)
+{
+	size_t len;
+	char *buf;
+
+	if (!s)
+		return NULL;
+
+	len = strlen(s) + 1;
+	buf = kmalloc(len, gfp);
+	if (buf)
+		memcpy(buf, s, len);
+	return buf;
+}
+#endif /* < 2.6.13 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,14) )
+void *_kc_kzalloc(size_t size, int flags)
+{
+	void *ret = kmalloc(size, flags);
+	if (ret)
+		memset(ret, 0, size);
+	return ret;
+}
+#endif /* <= 2.6.13 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,19) )
+int _kc_skb_pad(struct sk_buff *skb, int pad)
+{
+	int ntail;
+
+        /* If the skbuff is non linear tailroom is always zero.. */
+        if(!skb_cloned(skb) && skb_tailroom(skb) >= pad) {
+		memset(skb->data+skb->len, 0, pad);
+		return 0;
+        }
+
+	ntail = skb->data_len + pad - (skb->end - skb->tail);
+	if (likely(skb_cloned(skb) || ntail > 0)) {
+		if (pskb_expand_head(skb, 0, ntail, GFP_ATOMIC))
+			goto free_skb;
+	}
+
+#ifdef MAX_SKB_FRAGS
+	if (skb_is_nonlinear(skb) &&
+	    !__pskb_pull_tail(skb, skb->data_len))
+		goto free_skb;
+
+#endif
+	memset(skb->data + skb->len, 0, pad);
+        return 0;
+
+free_skb:
+	kfree_skb(skb);
+	return -ENOMEM;
+}
+
+#if (!(RHEL_RELEASE_CODE && RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(5,4)))
+int _kc_pci_save_state(struct pci_dev *pdev)
+{
+	struct adapter_struct *adapter = pci_get_drvdata(pdev);
+	int size = PCI_CONFIG_SPACE_LEN, i;
+	u16 pcie_cap_offset, pcie_link_status;
+
+#if ( LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,0) )
+	/* no ->dev for 2.4 kernels */
+	WARN_ON(pdev->dev.driver_data == NULL);
+#endif
+	pcie_cap_offset = pci_find_capability(pdev, PCI_CAP_ID_EXP);
+	if (pcie_cap_offset) {
+		if (!pci_read_config_word(pdev,
+		                          pcie_cap_offset + PCIE_LINK_STATUS,
+		                          &pcie_link_status))
+		size = PCIE_CONFIG_SPACE_LEN;
+	}
+	pci_config_space_ich8lan();
+#ifdef HAVE_PCI_ERS
+	if (adapter->config_space == NULL)
+#else
+	WARN_ON(adapter->config_space != NULL);
+#endif
+		adapter->config_space = kmalloc(size, GFP_KERNEL);
+	if (!adapter->config_space) {
+		printk(KERN_ERR "Out of memory in pci_save_state\n");
+		return -ENOMEM;
+	}
+	for (i = 0; i < (size / 4); i++)
+		pci_read_config_dword(pdev, i * 4, &adapter->config_space[i]);
+	return 0;
+}
+
+void _kc_pci_restore_state(struct pci_dev *pdev)
+{
+	struct adapter_struct *adapter = pci_get_drvdata(pdev);
+	int size = PCI_CONFIG_SPACE_LEN, i;
+	u16 pcie_cap_offset;
+	u16 pcie_link_status;
+
+	if (adapter->config_space != NULL) {
+		pcie_cap_offset = pci_find_capability(pdev, PCI_CAP_ID_EXP);
+		if (pcie_cap_offset &&
+		    !pci_read_config_word(pdev,
+		                          pcie_cap_offset + PCIE_LINK_STATUS,
+		                          &pcie_link_status))
+			size = PCIE_CONFIG_SPACE_LEN;
+
+		pci_config_space_ich8lan();
+		for (i = 0; i < (size / 4); i++)
+		pci_write_config_dword(pdev, i * 4, adapter->config_space[i]);
+#ifndef HAVE_PCI_ERS
+		kfree(adapter->config_space);
+		adapter->config_space = NULL;
+#endif
+	}
+}
+#endif /* !(RHEL_RELEASE_CODE >= RHEL 5.4) */
+
+#ifdef HAVE_PCI_ERS
+void _kc_free_netdev(struct net_device *netdev)
+{
+	struct adapter_struct *adapter = netdev_priv(netdev);
+
+	kfree(adapter->config_space);
+#ifdef CONFIG_SYSFS
+	if (netdev->reg_state == NETREG_UNINITIALIZED) {
+		kfree((char *)netdev - netdev->padded);
+	} else {
+		BUG_ON(netdev->reg_state != NETREG_UNREGISTERED);
+		netdev->reg_state = NETREG_RELEASED;
+		class_device_put(&netdev->class_dev);
+	}
+#else
+	kfree((char *)netdev - netdev->padded);
+#endif
+}
+#endif
+
+void *_kc_kmemdup(const void *src, size_t len, unsigned gfp)
+{
+	void *p;
+
+	p = kzalloc(len, gfp);
+	if (p)
+		memcpy(p, src, len);
+	return p;
+}
+#endif /* <= 2.6.19 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,21) )
+struct pci_dev *_kc_netdev_to_pdev(struct net_device *netdev)
+{
+	return ((struct adapter_struct *)netdev_priv(netdev))->pdev;
+}
+#endif /* < 2.6.21 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,22) )
+/* hexdump code taken from lib/hexdump.c */
+static void _kc_hex_dump_to_buffer(const void *buf, size_t len, int rowsize,
+			int groupsize, unsigned char *linebuf,
+			size_t linebuflen, bool ascii)
+{
+	const u8 *ptr = buf;
+	u8 ch;
+	int j, lx = 0;
+	int ascii_column;
+
+	if (rowsize != 16 && rowsize != 32)
+		rowsize = 16;
+
+	if (!len)
+		goto nil;
+	if (len > rowsize)		/* limit to one line at a time */
+		len = rowsize;
+	if ((len % groupsize) != 0)	/* no mixed size output */
+		groupsize = 1;
+
+	switch (groupsize) {
+	case 8: {
+		const u64 *ptr8 = buf;
+		int ngroups = len / groupsize;
+
+		for (j = 0; j < ngroups; j++)
+			lx += scnprintf((char *)(linebuf + lx), linebuflen - lx,
+				"%s%16.16llx", j ? " " : "",
+				(unsigned long long)*(ptr8 + j));
+		ascii_column = 17 * ngroups + 2;
+		break;
+	}
+
+	case 4: {
+		const u32 *ptr4 = buf;
+		int ngroups = len / groupsize;
+
+		for (j = 0; j < ngroups; j++)
+			lx += scnprintf((char *)(linebuf + lx), linebuflen - lx,
+				"%s%8.8x", j ? " " : "", *(ptr4 + j));
+		ascii_column = 9 * ngroups + 2;
+		break;
+	}
+
+	case 2: {
+		const u16 *ptr2 = buf;
+		int ngroups = len / groupsize;
+
+		for (j = 0; j < ngroups; j++)
+			lx += scnprintf((char *)(linebuf + lx), linebuflen - lx,
+				"%s%4.4x", j ? " " : "", *(ptr2 + j));
+		ascii_column = 5 * ngroups + 2;
+		break;
+	}
+
+	default:
+		for (j = 0; (j < len) && (lx + 3) <= linebuflen; j++) {
+			ch = ptr[j];
+			linebuf[lx++] = hex_asc(ch >> 4);
+			linebuf[lx++] = hex_asc(ch & 0x0f);
+			linebuf[lx++] = ' ';
+		}
+		if (j)
+			lx--;
+
+		ascii_column = 3 * rowsize + 2;
+		break;
+	}
+	if (!ascii)
+		goto nil;
+
+	while (lx < (linebuflen - 1) && lx < (ascii_column - 1))
+		linebuf[lx++] = ' ';
+	for (j = 0; (j < len) && (lx + 2) < linebuflen; j++)
+		linebuf[lx++] = (isascii(ptr[j]) && isprint(ptr[j])) ? ptr[j]
+				: '.';
+nil:
+	linebuf[lx++] = '\0';
+}
+
+void _kc_print_hex_dump(const char *level,
+			const char *prefix_str, int prefix_type,
+			int rowsize, int groupsize,
+			const void *buf, size_t len, bool ascii)
+{
+	const u8 *ptr = buf;
+	int i, linelen, remaining = len;
+	unsigned char linebuf[200];
+
+	if (rowsize != 16 && rowsize != 32)
+		rowsize = 16;
+
+	for (i = 0; i < len; i += rowsize) {
+		linelen = min(remaining, rowsize);
+		remaining -= rowsize;
+		_kc_hex_dump_to_buffer(ptr + i, linelen, rowsize, groupsize,
+				linebuf, sizeof(linebuf), ascii);
+
+		switch (prefix_type) {
+		case DUMP_PREFIX_ADDRESS:
+			printk("%s%s%*p: %s\n", level, prefix_str,
+				(int)(2 * sizeof(void *)), ptr + i, linebuf);
+			break;
+		case DUMP_PREFIX_OFFSET:
+			printk("%s%s%.8x: %s\n", level, prefix_str, i, linebuf);
+			break;
+		default:
+			printk("%s%s%s\n", level, prefix_str, linebuf);
+			break;
+		}
+	}
+}
+
+#endif /* < 2.6.22 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,24) )
+#ifdef NAPI
+struct net_device *napi_to_poll_dev(const struct napi_struct *napi)
+{
+	struct adapter_q_vector *q_vector = container_of(napi,
+	                                                struct adapter_q_vector,
+	                                                napi);
+	return &q_vector->poll_dev;
+}
+
+int __kc_adapter_clean(struct net_device *netdev, int *budget)
+{
+	int work_done;
+	int work_to_do = min(*budget, netdev->quota);
+	/* kcompat.h netif_napi_add puts napi struct in "fake netdev->priv" */
+	struct napi_struct *napi = netdev->priv;
+	work_done = napi->poll(napi, work_to_do);
+	*budget -= work_done;
+	netdev->quota -= work_done;
+	return (work_done >= work_to_do) ? 1 : 0;
+}
+#endif /* NAPI */
+#endif /* <= 2.6.24 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26) )
+void _kc_pci_disable_link_state(struct pci_dev *pdev, int state)
+{
+	struct pci_dev *parent = pdev->bus->self;
+	u16 link_state;
+	int pos;
+
+	if (!parent)
+		return;
+
+	pos = pci_find_capability(parent, PCI_CAP_ID_EXP);
+	if (pos) {
+		pci_read_config_word(parent, pos + PCI_EXP_LNKCTL, &link_state);
+		link_state &= ~state;
+		pci_write_config_word(parent, pos + PCI_EXP_LNKCTL, link_state);
+	}
+}
+#endif /* < 2.6.26 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,27) )
+#ifdef HAVE_TX_MQ
+void _kc_netif_tx_stop_all_queues(struct net_device *netdev)
+{
+	struct adapter_struct *adapter = netdev_priv(netdev);
+	int i;
+
+	netif_stop_queue(netdev);
+	if (netif_is_multiqueue(netdev))
+		for (i = 0; i < adapter->num_tx_queues; i++)
+			netif_stop_subqueue(netdev, i);
+}
+void _kc_netif_tx_wake_all_queues(struct net_device *netdev)
+{
+	struct adapter_struct *adapter = netdev_priv(netdev);
+	int i;
+
+	netif_wake_queue(netdev);
+	if (netif_is_multiqueue(netdev))
+		for (i = 0; i < adapter->num_tx_queues; i++)
+			netif_wake_subqueue(netdev, i);
+}
+void _kc_netif_tx_start_all_queues(struct net_device *netdev)
+{
+	struct adapter_struct *adapter = netdev_priv(netdev);
+	int i;
+
+	netif_start_queue(netdev);
+	if (netif_is_multiqueue(netdev))
+		for (i = 0; i < adapter->num_tx_queues; i++)
+			netif_start_subqueue(netdev, i);
+}
+#endif /* HAVE_TX_MQ */
+
+void __kc_warn_slowpath(const char *file, int line, const char *fmt, ...)
+{
+	va_list args;
+
+	printk(KERN_WARNING "------------[ cut here ]------------\n");
+	printk(KERN_WARNING "WARNING: at %s:%d \n", file, line);
+	va_start(args, fmt);
+	vprintk(fmt, args);
+	va_end(args);
+
+	dump_stack();
+}
+#endif /* __VMKLNX__ */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,28) )
+
+int
+_kc_pci_prepare_to_sleep(struct pci_dev *dev)
+{
+	pci_power_t target_state;
+	int error;
+
+	target_state = pci_choose_state(dev, PMSG_SUSPEND);
+
+	pci_enable_wake(dev, target_state, true);
+
+	error = pci_set_power_state(dev, target_state);
+
+	if (error)
+		pci_enable_wake(dev, target_state, false);
+
+	return error;
+}
+
+int
+_kc_pci_wake_from_d3(struct pci_dev *dev, bool enable)
+{
+	int err;
+
+	err = pci_enable_wake(dev, PCI_D3cold, enable);
+	if (err)
+		goto out;
+
+	err = pci_enable_wake(dev, PCI_D3hot, enable);
+
+out:
+	return err;
+}
+#endif /* < 2.6.28 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,29) )
+static void __kc_pci_set_master(struct pci_dev *pdev, bool enable)
+{
+	u16 old_cmd, cmd;
+
+	pci_read_config_word(pdev, PCI_COMMAND, &old_cmd);
+	if (enable)
+		cmd = old_cmd | PCI_COMMAND_MASTER;
+	else
+		cmd = old_cmd & ~PCI_COMMAND_MASTER;
+	if (cmd != old_cmd) {
+		dev_dbg(pci_dev_to_dev(pdev), "%s bus mastering\n",
+			enable ? "enabling" : "disabling");
+		pci_write_config_word(pdev, PCI_COMMAND, cmd);
+	}
+#if ( LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,7) )
+	pdev->is_busmaster = enable;
+#endif
+}
+
+void _kc_pci_clear_master(struct pci_dev *dev)
+{
+	__kc_pci_set_master(dev, false);
+}
+#endif /* < 2.6.29 */
+
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,34) )
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(6,0))
+int _kc_pci_num_vf(struct pci_dev __maybe_unused *dev)
+{
+	int num_vf = 0;
+#ifdef CONFIG_PCI_IOV
+	struct pci_dev *vfdev;
+
+	/* loop through all ethernet devices starting at PF dev */
+	vfdev = pci_get_class(PCI_CLASS_NETWORK_ETHERNET << 8, NULL);
+	while (vfdev) {
+		if (vfdev->is_virtfn && vfdev->physfn == dev)
+			num_vf++;
+
+		vfdev = pci_get_class(PCI_CLASS_NETWORK_ETHERNET << 8, vfdev);
+	}
+
+#endif
+	return num_vf;
+}
+#endif /* RHEL_RELEASE_CODE */
+#endif /* < 2.6.34 */
+
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,35) )
+#ifdef HAVE_TX_MQ
+#if (!(RHEL_RELEASE_CODE && RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(6,0)))
+#ifndef CONFIG_NETDEVICES_MULTIQUEUE
+int _kc_netif_set_real_num_tx_queues(struct net_device *dev, unsigned int txq)
+{
+	unsigned int real_num = dev->real_num_tx_queues;
+	struct Qdisc *qdisc;
+	int i;
+
+	if (txq < 1 || txq > dev->num_tx_queues)
+		return -EINVAL;
+
+	else if (txq > real_num)
+		dev->real_num_tx_queues = txq;
+	else if (txq < real_num) {
+		dev->real_num_tx_queues = txq;
+		for (i = txq; i < dev->num_tx_queues; i++) {
+			qdisc = netdev_get_tx_queue(dev, i)->qdisc;
+			if (qdisc) {
+				spin_lock_bh(qdisc_lock(qdisc));
+				qdisc_reset(qdisc);
+				spin_unlock_bh(qdisc_lock(qdisc));
+			}
+		}
+	}
+
+	return 0;
+}
+#endif /* CONFIG_NETDEVICES_MULTIQUEUE */
+#endif /* !(RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(6,0)) */
+#endif /* HAVE_TX_MQ */
+
+ssize_t _kc_simple_write_to_buffer(void *to, size_t available, loff_t *ppos,
+				   const void __user *from, size_t count)
+{
+        loff_t pos = *ppos;
+        size_t res;
+
+        if (pos < 0)
+                return -EINVAL;
+        if (pos >= available || !count)
+                return 0;
+        if (count > available - pos)
+                count = available - pos;
+        res = copy_from_user(to + pos, from, count);
+        if (res == count)
+                return -EFAULT;
+        count -= res;
+        *ppos = pos + count;
+        return count;
+}
+
+#endif /* < 2.6.35 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,36) )
+static const u32 _kc_flags_dup_features =
+	(ETH_FLAG_LRO | ETH_FLAG_NTUPLE | ETH_FLAG_RXHASH);
+
+u32 _kc_ethtool_op_get_flags(struct net_device *dev)
+{
+	return dev->features & _kc_flags_dup_features;
+}
+
+int _kc_ethtool_op_set_flags(struct net_device *dev, u32 data, u32 supported)
+{
+	if (data & ~supported)
+		return -EINVAL;
+
+	dev->features = ((dev->features & ~_kc_flags_dup_features) |
+			 (data & _kc_flags_dup_features));
+	return 0;
+}
+#endif /* < 2.6.36 */
+
+/******************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,39) )
+#if (!(RHEL_RELEASE_CODE && RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(6,0)))
+#ifdef HAVE_NETDEV_SELECT_QUEUE
+#include <net/ip.h>
+#include <linux/pkt_sched.h>
+
+u16 ___kc_skb_tx_hash(struct net_device *dev, const struct sk_buff *skb,
+		      u16 num_tx_queues)
+{
+	u32 hash;
+	u16 qoffset = 0;
+	u16 qcount = num_tx_queues;
+
+	if (skb_rx_queue_recorded(skb)) {
+		hash = skb_get_rx_queue(skb);
+		while (unlikely(hash >= num_tx_queues))
+			hash -= num_tx_queues;
+		return hash;
+	}
+
+	if (skb->sk && skb->sk->sk_hash)
+		hash = skb->sk->sk_hash;
+	else
+#ifdef NETIF_F_RXHASH
+		hash = (__force u16) skb->protocol ^ skb->rxhash;
+#else
+		hash = skb->protocol;
+#endif
+
+	hash = jhash_1word(hash, _kc_hashrnd);
+
+	return (u16) (((u64) hash * qcount) >> 32) + qoffset;
+}
+#endif /* HAVE_NETDEV_SELECT_QUEUE */
+
+u8 _kc_netdev_get_num_tc(struct net_device *dev)
+{
+	struct i40e_netdev_priv *np = netdev_priv(dev);
+	struct i40e_vsi *vsi = np->vsi;
+	struct i40e_pf *pf = vsi->back;
+	if (pf->flags & I40E_FLAG_DCB_ENABLED)
+		return vsi->tc_config.numtc;
+
+	return 0;
+}
+
+int _kc_netdev_set_num_tc(struct net_device *dev, u8 num_tc)
+{
+	struct i40e_netdev_priv *np = netdev_priv(dev);
+	struct i40e_vsi *vsi = np->vsi;
+
+	if (num_tc > I40E_MAX_TRAFFIC_CLASS)
+		return -EINVAL;
+
+	vsi->tc_config.numtc = num_tc;
+
+	return 0;
+}
+
+u8 _kc_netdev_get_prio_tc_map(struct net_device *dev, u8 up)
+{
+	struct i40e_netdev_priv *np = netdev_priv(dev);
+	struct i40e_vsi *vsi = np->vsi;
+	struct i40e_pf *pf = vsi->back;
+	struct i40e_hw *hw = &pf->hw;
+	struct i40e_dcbx_config *dcbcfg = &hw->local_dcbx_config;
+
+	return dcbcfg->etscfg.prioritytable[up];
+}
+
+#endif /* !(RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(6,0)) */
+#endif /* < 2.6.39 */
+
+/******************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,4,0) )
+void _kc_skb_add_rx_frag(struct sk_buff *skb, int i, struct page *page,
+			 int off, int size, unsigned int truesize)
+{
+	skb_fill_page_desc(skb, i, page, off, size);
+	skb->len += size;
+	skb->data_len += size;
+	skb->truesize += truesize;
+}
+
+#if !(SLE_VERSION_CODE && SLE_VERSION_CODE >= SLE_VERSION(11,3,0))
+int _kc_simple_open(struct inode *inode, struct file *file)
+{
+        if (inode->i_private)
+                file->private_data = inode->i_private;
+
+        return 0;
+}
+#endif /* SLE_VERSION < 11,3,0 */
+
+#endif /* < 3.4.0 */
+
+/******************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,7,0) )
+static inline int __kc_pcie_cap_version(struct pci_dev *dev)
+{
+	int pos;
+	u16 reg16;
+
+	pos = pci_find_capability(dev, PCI_CAP_ID_EXP);
+	if (!pos)
+		return 0;
+	pci_read_config_word(dev, pos + PCI_EXP_FLAGS, &reg16);
+	return reg16 & PCI_EXP_FLAGS_VERS;
+}
+
+static inline bool __kc_pcie_cap_has_devctl(const struct pci_dev __always_unused *dev)
+{
+	return true;
+}
+
+static inline bool __kc_pcie_cap_has_lnkctl(struct pci_dev *dev)
+{
+	int type = pci_pcie_type(dev);
+
+	return __kc_pcie_cap_version(dev) > 1 ||
+	       type == PCI_EXP_TYPE_ROOT_PORT ||
+	       type == PCI_EXP_TYPE_ENDPOINT ||
+	       type == PCI_EXP_TYPE_LEG_END;
+}
+
+static inline bool __kc_pcie_cap_has_sltctl(struct pci_dev *dev)
+{
+	int type = pci_pcie_type(dev);
+	int pos;
+	u16 pcie_flags_reg;
+
+	pos = pci_find_capability(dev, PCI_CAP_ID_EXP);
+	if (!pos)
+		return false;
+	pci_read_config_word(dev, pos + PCI_EXP_FLAGS, &pcie_flags_reg);
+
+	return __kc_pcie_cap_version(dev) > 1 ||
+	       type == PCI_EXP_TYPE_ROOT_PORT ||
+	       (type == PCI_EXP_TYPE_DOWNSTREAM &&
+		pcie_flags_reg & PCI_EXP_FLAGS_SLOT);
+}
+
+static inline bool __kc_pcie_cap_has_rtctl(struct pci_dev *dev)
+{
+	int type = pci_pcie_type(dev);
+
+	return __kc_pcie_cap_version(dev) > 1 ||
+	       type == PCI_EXP_TYPE_ROOT_PORT ||
+	       type == PCI_EXP_TYPE_RC_EC;
+}
+
+static bool __kc_pcie_capability_reg_implemented(struct pci_dev *dev, int pos)
+{
+	if (!pci_is_pcie(dev))
+		return false;
+
+	switch (pos) {
+	case PCI_EXP_FLAGS_TYPE:
+		return true;
+	case PCI_EXP_DEVCAP:
+	case PCI_EXP_DEVCTL:
+	case PCI_EXP_DEVSTA:
+		return __kc_pcie_cap_has_devctl(dev);
+	case PCI_EXP_LNKCAP:
+	case PCI_EXP_LNKCTL:
+	case PCI_EXP_LNKSTA:
+		return __kc_pcie_cap_has_lnkctl(dev);
+	case PCI_EXP_SLTCAP:
+	case PCI_EXP_SLTCTL:
+	case PCI_EXP_SLTSTA:
+		return __kc_pcie_cap_has_sltctl(dev);
+	case PCI_EXP_RTCTL:
+	case PCI_EXP_RTCAP:
+	case PCI_EXP_RTSTA:
+		return __kc_pcie_cap_has_rtctl(dev);
+	case PCI_EXP_DEVCAP2:
+	case PCI_EXP_DEVCTL2:
+	case PCI_EXP_LNKCAP2:
+	case PCI_EXP_LNKCTL2:
+	case PCI_EXP_LNKSTA2:
+		return __kc_pcie_cap_version(dev) > 1;
+	default:
+		return false;
+	}
+}
+
+/*
+ * Note that these accessor functions are only for the "PCI Express
+ * Capability" (see PCIe spec r3.0, sec 7.8).  They do not apply to the
+ * other "PCI Express Extended Capabilities" (AER, VC, ACS, MFVC, etc.)
+ */
+int __kc_pcie_capability_read_word(struct pci_dev *dev, int pos, u16 *val)
+{
+	int ret;
+
+	*val = 0;
+	if (pos & 1)
+		return -EINVAL;
+
+	if (__kc_pcie_capability_reg_implemented(dev, pos)) {
+		ret = pci_read_config_word(dev, pci_pcie_cap(dev) + pos, val);
+		/*
+		 * Reset *val to 0 if pci_read_config_word() fails, it may
+		 * have been written as 0xFFFF if hardware error happens
+		 * during pci_read_config_word().
+		 */
+		if (ret)
+			*val = 0;
+		return ret;
+	}
+
+	/*
+	 * For Functions that do not implement the Slot Capabilities,
+	 * Slot Status, and Slot Control registers, these spaces must
+	 * be hardwired to 0b, with the exception of the Presence Detect
+	 * State bit in the Slot Status register of Downstream Ports,
+	 * which must be hardwired to 1b.  (PCIe Base Spec 3.0, sec 7.8)
+	 */
+	if (pci_is_pcie(dev) && pos == PCI_EXP_SLTSTA &&
+	    pci_pcie_type(dev) == PCI_EXP_TYPE_DOWNSTREAM) {
+		*val = PCI_EXP_SLTSTA_PDS;
+	}
+
+	return 0;
+}
+
+int __kc_pcie_capability_read_dword(struct pci_dev *dev, int pos, u32 *val)
+{
+	int ret;
+
+	*val = 0;
+	if (pos & 3)
+		return -EINVAL;
+
+	if (__kc_pcie_capability_reg_implemented(dev, pos)) {
+		ret = pci_read_config_dword(dev, pci_pcie_cap(dev) + pos, val);
+		/*
+		 * Reset *val to 0 if pci_read_config_dword() fails, it may
+		 * have been written as 0xFFFFFFFF if hardware error happens
+		 * during pci_read_config_dword().
+		 */
+		if (ret)
+			*val = 0;
+		return ret;
+	}
+
+	if (pci_is_pcie(dev) && pos == PCI_EXP_SLTSTA &&
+	    pci_pcie_type(dev) == PCI_EXP_TYPE_DOWNSTREAM) {
+		*val = PCI_EXP_SLTSTA_PDS;
+	}
+
+	return 0;
+}
+
+int __kc_pcie_capability_write_word(struct pci_dev *dev, int pos, u16 val)
+{
+	if (pos & 1)
+		return -EINVAL;
+
+	if (!__kc_pcie_capability_reg_implemented(dev, pos))
+		return 0;
+
+	return pci_write_config_word(dev, pci_pcie_cap(dev) + pos, val);
+}
+
+int __kc_pcie_capability_clear_and_set_word(struct pci_dev *dev, int pos,
+					    u16 clear, u16 set)
+{
+	int ret;
+	u16 val;
+
+	ret = __kc_pcie_capability_read_word(dev, pos, &val);
+	if (!ret) {
+		val &= ~clear;
+		val |= set;
+		ret = __kc_pcie_capability_write_word(dev, pos, val);
+	}
+
+	return ret;
+}
+
+int __kc_pcie_capability_clear_word(struct pci_dev *dev, int pos,
+					     u16 clear)
+{
+	return __kc_pcie_capability_clear_and_set_word(dev, pos, clear, 0);
+}
+#endif /* < 3.7.0 */
+
+/******************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,9,0) )
+#ifdef CONFIG_XPS
+#if NR_CPUS < 64
+#define _KC_MAX_XPS_CPUS	NR_CPUS
+#else
+#define _KC_MAX_XPS_CPUS	64
+#endif
+
+/*
+ * netdev_queue sysfs structures and functions.
+ */
+struct _kc_netdev_queue_attribute {
+	struct attribute attr;
+	ssize_t (*show)(struct netdev_queue *queue,
+	    struct _kc_netdev_queue_attribute *attr, char *buf);
+	ssize_t (*store)(struct netdev_queue *queue,
+	    struct _kc_netdev_queue_attribute *attr, const char *buf, size_t len);
+};
+
+#define to_kc_netdev_queue_attr(_attr) container_of(_attr,		\
+    struct _kc_netdev_queue_attribute, attr)
+
+int __kc_netif_set_xps_queue(struct net_device *dev, const struct cpumask *mask,
+			     u16 index)
+{
+	struct netdev_queue *txq = netdev_get_tx_queue(dev, index);
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,38) )
+	/* Redhat requires some odd extended netdev structures */
+	struct netdev_tx_queue_extended *txq_ext =
+					netdev_extended(dev)->_tx_ext + index;
+	struct kobj_type *ktype = txq_ext->kobj.ktype;
+#else
+	struct kobj_type *ktype = txq->kobj.ktype;
+#endif
+	struct _kc_netdev_queue_attribute *xps_attr;
+	struct attribute *attr = NULL;
+	int i, len, err;
+#define _KC_XPS_BUFLEN	(DIV_ROUND_UP(_KC_MAX_XPS_CPUS, 32) * 9)
+	char buf[_KC_XPS_BUFLEN];
+
+	if (!ktype)
+		return -ENOMEM;
+
+	/* attempt to locate the XPS attribute in the Tx queue */
+	for (i = 0; (attr = ktype->default_attrs[i]); i++) {
+		if (!strcmp("xps_cpus", attr->name))
+			break;
+	}
+
+	/* if we did not find it return an error */
+	if (!attr)
+		return -EINVAL;
+
+	/* copy the mask into a string */
+	len = bitmap_scnprintf(buf, _KC_XPS_BUFLEN,
+			       cpumask_bits(mask), _KC_MAX_XPS_CPUS);
+	if (!len)
+		return -ENOMEM;
+
+	xps_attr = to_kc_netdev_queue_attr(attr);
+
+	/* Store the XPS value using the SYSFS store call */
+	err = xps_attr->store(txq, xps_attr, buf, len);
+
+	/* we only had an error on err < 0 */
+	return (err < 0) ? err : 0;
+}
+#endif /* CONFIG_XPS */
+#ifdef HAVE_NETDEV_SELECT_QUEUE
+static inline int kc_get_xps_queue(struct net_device *dev, struct sk_buff *skb)
+{
+#ifdef CONFIG_XPS
+	struct xps_dev_maps *dev_maps;
+	struct xps_map *map;
+	int queue_index = -1;
+
+	rcu_read_lock();
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,38) )
+	/* Redhat requires some odd extended netdev structures */
+	dev_maps = rcu_dereference(netdev_extended(dev)->xps_maps);
+#else
+	dev_maps = rcu_dereference(dev->xps_maps);
+#endif
+	if (dev_maps) {
+		map = rcu_dereference(
+		    dev_maps->cpu_map[raw_smp_processor_id()]);
+		if (map) {
+			if (map->len == 1)
+				queue_index = map->queues[0];
+			else {
+				u32 hash;
+				if (skb->sk && skb->sk->sk_hash)
+					hash = skb->sk->sk_hash;
+				else
+					hash = (__force u16) skb->protocol ^
+					    skb->rxhash;
+				hash = jhash_1word(hash, _kc_hashrnd);
+				queue_index = map->queues[
+				    ((u64)hash * map->len) >> 32];
+			}
+			if (unlikely(queue_index >= dev->real_num_tx_queues))
+				queue_index = -1;
+		}
+	}
+	rcu_read_unlock();
+
+	return queue_index;
+#else
+	return -1;
+#endif
+}
+
+u16 __kc_netdev_pick_tx(struct net_device *dev, struct sk_buff *skb)
+{
+	struct sock *sk = skb->sk;
+	int queue_index = sk_tx_queue_get(sk);
+	int new_index;
+
+	if (queue_index >= 0 && queue_index < dev->real_num_tx_queues) {
+#ifdef CONFIG_XPS
+		if (!skb->ooo_okay)
+#endif
+			return queue_index;
+	}
+
+	new_index = kc_get_xps_queue(dev, skb);
+	if (new_index < 0)
+		new_index = skb_tx_hash(dev, skb);
+
+	if (queue_index != new_index && sk) {
+		struct dst_entry *dst =
+			    rcu_dereference(sk->sk_dst_cache);
+
+		if (dst && skb_dst(skb) == dst)
+			sk_tx_queue_set(sk, new_index);
+
+	}
+
+	return new_index;
+}
+
+#endif /* HAVE_NETDEV_SELECT_QUEUE */
+#endif /* 3.9.0 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,10,0) )
+#ifdef HAVE_FDB_OPS
+#ifdef USE_CONST_DEV_UC_CHAR
+int __kc_ndo_dflt_fdb_add(struct ndmsg *ndm, struct nlattr *tb[],
+			  struct net_device *dev, const unsigned char *addr,
+			  u16 flags)
+#else
+int __kc_ndo_dflt_fdb_add(struct ndmsg *ndm, struct net_device *dev,
+			  unsigned char *addr, u16 flags)
+#endif
+{
+	int err = -EINVAL;
+
+	/* If aging addresses are supported device will need to
+	 * implement its own handler for this.
+	 */
+	if (ndm->ndm_state && !(ndm->ndm_state & NUD_PERMANENT)) {
+		pr_info("%s: FDB only supports static addresses\n", dev->name);
+		return err;
+	}
+
+	if (is_unicast_ether_addr(addr) || is_link_local_ether_addr(addr))
+		err = dev_uc_add_excl(dev, addr);
+	else if (is_multicast_ether_addr(addr))
+		err = dev_mc_add_excl(dev, addr);
+
+	/* Only return duplicate errors if NLM_F_EXCL is set */
+	if (err == -EEXIST && !(flags & NLM_F_EXCL))
+		err = 0;
+
+	return err;
+}
+
+#ifdef USE_CONST_DEV_UC_CHAR
+#ifdef HAVE_FDB_DEL_NLATTR
+int __kc_ndo_dflt_fdb_del(struct ndmsg *ndm, struct nlattr *tb[],
+			  struct net_device *dev, const unsigned char *addr)
+#else
+int __kc_ndo_dflt_fdb_del(struct ndmsg *ndm, struct net_device *dev,
+			  const unsigned char *addr)
+#endif
+#else
+int __kc_ndo_dflt_fdb_del(struct ndmsg *ndm, struct net_device *dev,
+			  unsigned char *addr)
+#endif
+{
+	int err = -EINVAL;
+
+	/* If aging addresses are supported device will need to
+	 * implement its own handler for this.
+	 */
+	if (!(ndm->ndm_state & NUD_PERMANENT)) {
+		pr_info("%s: FDB only supports static addresses\n", dev->name);
+		return err;
+	}
+
+	if (is_unicast_ether_addr(addr) || is_link_local_ether_addr(addr))
+		err = dev_uc_del(dev, addr);
+	else if (is_multicast_ether_addr(addr))
+		err = dev_mc_del(dev, addr);
+
+	return err;
+}
+
+#endif /* HAVE_FDB_OPS */
+#ifdef CONFIG_PCI_IOV
+int __kc_pci_vfs_assigned(struct pci_dev __maybe_unused *dev)
+{
+	unsigned int vfs_assigned = 0;
+#ifdef HAVE_PCI_DEV_FLAGS_ASSIGNED
+	int pos;
+	struct pci_dev *vfdev;
+	unsigned short dev_id;
+
+	/* only search if we are a PF */
+	if (!dev->is_physfn)
+		return 0;
+
+	/* find SR-IOV capability */
+	pos = pci_find_ext_capability(dev, PCI_EXT_CAP_ID_SRIOV);
+	if (!pos)
+		return 0;
+
+	/*
+	 * determine the device ID for the VFs, the vendor ID will be the
+	 * same as the PF so there is no need to check for that one
+	 */
+	pci_read_config_word(dev, pos + PCI_SRIOV_VF_DID, &dev_id);
+
+	/* loop through all the VFs to see if we own any that are assigned */
+	vfdev = pci_get_device(dev->vendor, dev_id, NULL);
+	while (vfdev) {
+		/*
+		 * It is considered assigned if it is a virtual function with
+		 * our dev as the physical function and the assigned bit is set
+		 */
+		if (vfdev->is_virtfn && (vfdev->physfn == dev) &&
+		    (vfdev->dev_flags & PCI_DEV_FLAGS_ASSIGNED))
+			vfs_assigned++;
+
+		vfdev = pci_get_device(dev->vendor, dev_id, vfdev);
+	}
+
+#endif /* HAVE_PCI_DEV_FLAGS_ASSIGNED */
+	return vfs_assigned;
+}
+
+#endif /* CONFIG_PCI_IOV */
+#endif /* 3.10.0 */
+
+static const unsigned char __maybe_unused pcie_link_speed[] = {
+	PCI_SPEED_UNKNOWN,      /* 0 */
+	PCIE_SPEED_2_5GT,       /* 1 */
+	PCIE_SPEED_5_0GT,       /* 2 */
+	PCIE_SPEED_8_0GT,       /* 3 */
+	PCIE_SPEED_16_0GT,      /* 4 */
+	PCI_SPEED_UNKNOWN,      /* 5 */
+	PCI_SPEED_UNKNOWN,      /* 6 */
+	PCI_SPEED_UNKNOWN,      /* 7 */
+	PCI_SPEED_UNKNOWN,      /* 8 */
+	PCI_SPEED_UNKNOWN,      /* 9 */
+	PCI_SPEED_UNKNOWN,      /* A */
+	PCI_SPEED_UNKNOWN,      /* B */
+	PCI_SPEED_UNKNOWN,      /* C */
+	PCI_SPEED_UNKNOWN,      /* D */
+	PCI_SPEED_UNKNOWN,      /* E */
+	PCI_SPEED_UNKNOWN       /* F */
+};
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,12,0) )
+int __kc_pcie_get_minimum_link(struct pci_dev *dev, enum pci_bus_speed *speed,
+			       enum pcie_link_width *width)
+{
+
+	*speed = PCI_SPEED_UNKNOWN;
+	*width = PCIE_LNK_WIDTH_UNKNOWN;
+
+	while (dev) {
+		u16 lnksta;
+		enum pci_bus_speed next_speed;
+		enum pcie_link_width next_width;
+		int ret = pcie_capability_read_word(dev, PCI_EXP_LNKSTA, &lnksta);
+
+		if (ret)
+			return ret;
+
+		next_speed = pcie_link_speed[lnksta & PCI_EXP_LNKSTA_CLS];
+		next_width = (lnksta & PCI_EXP_LNKSTA_NLW) >>
+			PCI_EXP_LNKSTA_NLW_SHIFT;
+
+		if (next_speed < *speed)
+			*speed = next_speed;
+
+		if (next_width < *width)
+			*width = next_width;
+
+		dev = dev->bus->self;
+	}
+
+	return 0;
+}
+
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(6,7))
+int _kc_pci_wait_for_pending_transaction(struct pci_dev *dev)
+{
+	int i;
+	u16 status;
+
+	/* Wait for Transaction Pending bit clean */
+	for (i = 0; i < 4; i++) {
+		if (i)
+			msleep((1 << (i - 1)) * 100);
+
+		pcie_capability_read_word(dev, PCI_EXP_DEVSTA, &status);
+		if (!(status & PCI_EXP_DEVSTA_TRPND))
+			return 1;
+	}
+
+	return 0;
+}
+#endif /* <RHEL6.7 */
+
+#endif /* <3.12 */
+
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,13,0) )
+int __kc_dma_set_mask_and_coherent(struct device *dev, u64 mask)
+{
+	int err = dma_set_mask(dev, mask);
+
+	if (!err)
+		/* coherent mask for the same size will always succeed if
+		 * dma_set_mask does. However we store the error anyways, due
+		 * to some kernels which use gcc's warn_unused_result on their
+		 * definition of dma_set_coherent_mask.
+		 */
+		err = dma_set_coherent_mask(dev, mask);
+	return err;
+}
+
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,0))
+static bool _kc_pci_bus_read_dev_vendor_id(struct pci_bus *bus, int devfn,
+					   u32 *l, int crs_timeout)
+{
+	int delay = 1;
+
+	if (pci_bus_read_config_dword(bus, devfn, PCI_VENDOR_ID, l))
+		return false;
+
+	/* some broken boards return 0 or ~0 if a slot is empty: */
+	if (*l == 0xffffffff || *l == 0x00000000 ||
+	    *l == 0x0000ffff || *l == 0xffff0000)
+		return false;
+
+	/* Configuration request Retry Status */
+	while (*l == 0xffff0001) {
+		if (!crs_timeout)
+			return false;
+
+		msleep(delay);
+		delay *= 2;
+		if (pci_bus_read_config_dword(bus, devfn, PCI_VENDOR_ID, l))
+			return false;
+		/* Card hasn't responded in 60 seconds?  Must be stuck. */
+		if (delay > crs_timeout) {
+			printk(KERN_WARNING "pci %04x:%02x:%02x.%d: not "
+			       "responding\n", pci_domain_nr(bus),
+			       bus->number, PCI_SLOT(devfn),
+			       PCI_FUNC(devfn));
+			return false;
+		}
+	}
+
+	return true;
+}
+
+bool _kc_pci_device_is_present(struct pci_dev *pdev)
+{
+	u32 v;
+
+	return _kc_pci_bus_read_dev_vendor_id(pdev->bus, pdev->devfn, &v, 0);
+}
+#endif /* <RHEL7.0 */
+#endif /* 3.13.0 */
+
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,14,0) )
+/******************************************************************************
+ * ripped from linux/net/ipv6/exthdrs_core.c, GPL2, no direct copyright,
+ * inferred copyright from kernel
+ */
+int __kc_ipv6_find_hdr(const struct sk_buff *skb, unsigned int *offset,
+		       int target, unsigned short *fragoff, int *flags)
+{
+	unsigned int start = skb_network_offset(skb) + sizeof(struct ipv6hdr);
+	u8 nexthdr = ipv6_hdr(skb)->nexthdr;
+	bool found;
+
+#define __KC_IP6_FH_F_FRAG	BIT(0)
+#define __KC_IP6_FH_F_AUTH	BIT(1)
+#define __KC_IP6_FH_F_SKIP_RH	BIT(2)
+
+	if (fragoff)
+		*fragoff = 0;
+
+	if (*offset) {
+		struct ipv6hdr _ip6, *ip6;
+
+		ip6 = skb_header_pointer(skb, *offset, sizeof(_ip6), &_ip6);
+		if (!ip6 || (ip6->version != 6)) {
+			printk(KERN_ERR "IPv6 header not found\n");
+			return -EBADMSG;
+		}
+		start = *offset + sizeof(struct ipv6hdr);
+		nexthdr = ip6->nexthdr;
+	}
+
+	do {
+		struct ipv6_opt_hdr _hdr, *hp;
+		unsigned int hdrlen;
+		found = (nexthdr == target);
+
+		if ((!ipv6_ext_hdr(nexthdr)) || nexthdr == NEXTHDR_NONE) {
+			if (target < 0 || found)
+				break;
+			return -ENOENT;
+		}
+
+		hp = skb_header_pointer(skb, start, sizeof(_hdr), &_hdr);
+		if (!hp)
+			return -EBADMSG;
+
+		if (nexthdr == NEXTHDR_ROUTING) {
+			struct ipv6_rt_hdr _rh, *rh;
+
+			rh = skb_header_pointer(skb, start, sizeof(_rh),
+						&_rh);
+			if (!rh)
+				return -EBADMSG;
+
+			if (flags && (*flags & __KC_IP6_FH_F_SKIP_RH) &&
+			    rh->segments_left == 0)
+				found = false;
+		}
+
+		if (nexthdr == NEXTHDR_FRAGMENT) {
+			unsigned short _frag_off;
+			__be16 *fp;
+
+			if (flags)	/* Indicate that this is a fragment */
+				*flags |= __KC_IP6_FH_F_FRAG;
+			fp = skb_header_pointer(skb,
+						start+offsetof(struct frag_hdr,
+							       frag_off),
+						sizeof(_frag_off),
+						&_frag_off);
+			if (!fp)
+				return -EBADMSG;
+
+			_frag_off = ntohs(*fp) & ~0x7;
+			if (_frag_off) {
+				if (target < 0 &&
+				    ((!ipv6_ext_hdr(hp->nexthdr)) ||
+				     hp->nexthdr == NEXTHDR_NONE)) {
+					if (fragoff)
+						*fragoff = _frag_off;
+					return hp->nexthdr;
+				}
+				return -ENOENT;
+			}
+			hdrlen = 8;
+		} else if (nexthdr == NEXTHDR_AUTH) {
+			if (flags && (*flags & __KC_IP6_FH_F_AUTH) && (target < 0))
+				break;
+			hdrlen = (hp->hdrlen + 2) << 2;
+		} else
+			hdrlen = ipv6_optlen(hp);
+
+		if (!found) {
+			nexthdr = hp->nexthdr;
+			start += hdrlen;
+		}
+	} while (!found);
+
+	*offset = start;
+	return nexthdr;
+}
+
+int __kc_pci_enable_msix_range(struct pci_dev *dev, struct msix_entry *entries,
+			       int minvec, int maxvec)
+{
+        int nvec = maxvec;
+        int rc;
+
+        if (maxvec < minvec)
+                return -ERANGE;
+
+        do {
+                rc = pci_enable_msix(dev, entries, nvec);
+                if (rc < 0) {
+                        return rc;
+                } else if (rc > 0) {
+                        if (rc < minvec)
+                                return -ENOSPC;
+                        nvec = rc;
+                }
+        } while (rc);
+
+        return nvec;
+}
+#endif /* 3.14.0 */
+
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,15,0))
+char *_kc_devm_kstrdup(struct device *dev, const char *s, gfp_t gfp)
+{
+	size_t size;
+	char *buf;
+
+	if (!s)
+		return NULL;
+
+	size = strlen(s) + 1;
+	buf = devm_kzalloc(dev, size, gfp);
+	if (buf)
+		memcpy(buf, s, size);
+	return buf;
+}
+
+void __kc_netdev_rss_key_fill(void *buffer, size_t len)
+{
+	/* Set of random keys generated using kernel random number generator */
+	static const u8 seed[NETDEV_RSS_KEY_LEN] = {0xE6, 0xFA, 0x35, 0x62,
+				0x95, 0x12, 0x3E, 0xA3, 0xFB, 0x46, 0xC1, 0x5F,
+				0xB1, 0x43, 0x82, 0x5B, 0x6A, 0x49, 0x50, 0x95,
+				0xCD, 0xAB, 0xD8, 0x11, 0x8F, 0xC5, 0xBD, 0xBC,
+				0x6A, 0x4A, 0xB2, 0xD4, 0x1F, 0xFE, 0xBC, 0x41,
+				0xBF, 0xAC, 0xB2, 0x9A, 0x8F, 0x70, 0xE9, 0x2A,
+				0xD7, 0xB2, 0x80, 0xB6, 0x5B, 0xAA, 0x9D, 0x20};
+
+	BUG_ON(len > NETDEV_RSS_KEY_LEN);
+	memcpy(buffer, seed, len);
+}
+#endif /* 3.15.0 */
+
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,16,0) )
+#ifdef HAVE_SET_RX_MODE
+#ifdef NETDEV_HW_ADDR_T_UNICAST
+int __kc_hw_addr_sync_dev(struct netdev_hw_addr_list *list,
+		struct net_device *dev,
+		int (*sync)(struct net_device *, const unsigned char *),
+		int (*unsync)(struct net_device *, const unsigned char *))
+{
+	struct netdev_hw_addr *ha, *tmp;
+	int err;
+
+	/* first go through and flush out any stale entries */
+	list_for_each_entry_safe(ha, tmp, &list->list, list) {
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,10,0) )
+		if (!ha->synced || ha->refcount != 1)
+#else
+		if (!ha->sync_cnt || ha->refcount != 1)
+#endif
+			continue;
+
+		if (unsync && unsync(dev, ha->addr))
+			continue;
+
+		list_del_rcu(&ha->list);
+		kfree_rcu(ha, rcu_head);
+		list->count--;
+	}
+
+	/* go through and sync new entries to the list */
+	list_for_each_entry_safe(ha, tmp, &list->list, list) {
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,10,0) )
+		if (ha->synced)
+#else
+		if (ha->sync_cnt)
+#endif
+			continue;
+
+		err = sync(dev, ha->addr);
+		if (err)
+			return err;
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,10,0) )
+		ha->synced = true;
+#else
+		ha->sync_cnt++;
+#endif
+		ha->refcount++;
+	}
+
+	return 0;
+}
+
+void __kc_hw_addr_unsync_dev(struct netdev_hw_addr_list *list,
+		struct net_device *dev,
+		int (*unsync)(struct net_device *, const unsigned char *))
+{
+	struct netdev_hw_addr *ha, *tmp;
+
+	list_for_each_entry_safe(ha, tmp, &list->list, list) {
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,10,0) )
+		if (!ha->synced)
+#else
+		if (!ha->sync_cnt)
+#endif
+			continue;
+
+		if (unsync && unsync(dev, ha->addr))
+			continue;
+
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,10,0) )
+		ha->synced = false;
+#else
+		ha->sync_cnt--;
+#endif
+		if (--ha->refcount)
+			continue;
+
+		list_del_rcu(&ha->list);
+		kfree_rcu(ha, rcu_head);
+		list->count--;
+	}
+}
+
+#endif /* NETDEV_HW_ADDR_T_UNICAST  */
+#ifndef NETDEV_HW_ADDR_T_MULTICAST
+int __kc_dev_addr_sync_dev(struct dev_addr_list **list, int *count,
+		struct net_device *dev,
+		int (*sync)(struct net_device *, const unsigned char *),
+		int (*unsync)(struct net_device *, const unsigned char *))
+{
+	struct dev_addr_list *da, **next = list;
+	int err;
+
+	/* first go through and flush out any stale entries */
+	while ((da = *next) != NULL) {
+		if (da->da_synced && da->da_users == 1) {
+			if (!unsync || !unsync(dev, da->da_addr)) {
+				*next = da->next;
+				kfree(da);
+				(*count)--;
+				continue;
+			}
+		}
+		next = &da->next;
+	}
+
+	/* go through and sync new entries to the list */
+	for (da = *list; da != NULL; da = da->next) {
+		if (da->da_synced)
+			continue;
+
+		err = sync(dev, da->da_addr);
+		if (err)
+			return err;
+
+		da->da_synced++;
+		da->da_users++;
+	}
+
+	return 0;
+}
+
+void __kc_dev_addr_unsync_dev(struct dev_addr_list **list, int *count,
+		struct net_device *dev,
+		int (*unsync)(struct net_device *, const unsigned char *))
+{
+	struct dev_addr_list *da;
+
+	while ((da = *list) != NULL) {
+		if (da->da_synced) {
+			if (!unsync || !unsync(dev, da->da_addr)) {
+				da->da_synced--;
+				if (--da->da_users == 0) {
+					*list = da->next;
+					kfree(da);
+					(*count)--;
+					continue;
+				}
+			}
+		}
+		list = &da->next;
+	}
+}
+#endif /* NETDEV_HW_ADDR_T_MULTICAST  */
+#endif /* HAVE_SET_RX_MODE */
+void *__kc_devm_kmemdup(struct device *dev, const void *src, size_t len,
+			gfp_t gfp)
+{
+	void *p;
+
+	p = devm_kzalloc(dev, len, gfp);
+	if (p)
+		memcpy(p, src, len);
+
+	return p;
+}
+#endif /* 3.16.0 */
+
+/******************************************************************************/
+#if ((LINUX_VERSION_CODE < KERNEL_VERSION(3,17,0)) && \
+     (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,5)))
+#endif /* <3.17.0 && RHEL_RELEASE_CODE < RHEL7.5 */
+
+/******************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,18,0) )
+#ifndef NO_PTP_SUPPORT
+static void __kc_sock_efree(struct sk_buff *skb)
+{
+	sock_put(skb->sk);
+}
+
+struct sk_buff *__kc_skb_clone_sk(struct sk_buff *skb)
+{
+	struct sock *sk = skb->sk;
+	struct sk_buff *clone;
+
+	if (!sk || !atomic_inc_not_zero(&sk->sk_refcnt))
+		return NULL;
+
+	clone = skb_clone(skb, GFP_ATOMIC);
+	if (!clone) {
+		sock_put(sk);
+		return NULL;
+	}
+
+	clone->sk = sk;
+	clone->destructor = __kc_sock_efree;
+
+	return clone;
+}
+
+void __kc_skb_complete_tx_timestamp(struct sk_buff *skb,
+				    struct skb_shared_hwtstamps *hwtstamps)
+{
+	struct sock_exterr_skb *serr;
+	struct sock *sk = skb->sk;
+	int err;
+
+	sock_hold(sk);
+
+	*skb_hwtstamps(skb) = *hwtstamps;
+
+	serr = SKB_EXT_ERR(skb);
+	memset(serr, 0, sizeof(*serr));
+	serr->ee.ee_errno = ENOMSG;
+	serr->ee.ee_origin = SO_EE_ORIGIN_TIMESTAMPING;
+
+	err = sock_queue_err_skb(sk, skb);
+	if (err)
+		kfree_skb(skb);
+
+	sock_put(sk);
+}
+#endif
+
+/* include headers needed for get_headlen function */
+#if defined(CONFIG_FCOE) || defined(CONFIG_FCOE_MODULE)
+#include <scsi/fc/fc_fcoe.h>
+#endif
+#ifdef HAVE_SCTP
+#include <linux/sctp.h>
+#endif
+
+u32 __kc_eth_get_headlen(const struct net_device __always_unused *dev,
+			 unsigned char *data, unsigned int max_len)
+{
+	union {
+		unsigned char *network;
+		/* l2 headers */
+		struct ethhdr *eth;
+		struct vlan_hdr *vlan;
+		/* l3 headers */
+		struct iphdr *ipv4;
+		struct ipv6hdr *ipv6;
+	} hdr;
+	__be16 proto;
+	u8 nexthdr = 0;	/* default to not TCP */
+	u8 hlen;
+
+	/* this should never happen, but better safe than sorry */
+	if (max_len < ETH_HLEN)
+		return max_len;
+
+	/* initialize network frame pointer */
+	hdr.network = data;
+
+	/* set first protocol and move network header forward */
+	proto = hdr.eth->h_proto;
+	hdr.network += ETH_HLEN;
+
+again:
+	switch (proto) {
+	/* handle any vlan tag if present */
+	case __constant_htons(ETH_P_8021AD):
+	case __constant_htons(ETH_P_8021Q):
+		if ((hdr.network - data) > (max_len - VLAN_HLEN))
+			return max_len;
+
+		proto = hdr.vlan->h_vlan_encapsulated_proto;
+		hdr.network += VLAN_HLEN;
+		goto again;
+	/* handle L3 protocols */
+	case __constant_htons(ETH_P_IP):
+		if ((hdr.network - data) > (max_len - sizeof(struct iphdr)))
+			return max_len;
+
+		/* access ihl as a u8 to avoid unaligned access on ia64 */
+		hlen = (hdr.network[0] & 0x0F) << 2;
+
+		/* verify hlen meets minimum size requirements */
+		if (hlen < sizeof(struct iphdr))
+			return hdr.network - data;
+
+		/* record next protocol if header is present */
+		if (!(hdr.ipv4->frag_off & htons(IP_OFFSET)))
+			nexthdr = hdr.ipv4->protocol;
+
+		hdr.network += hlen;
+		break;
+#ifdef NETIF_F_TSO6
+	case __constant_htons(ETH_P_IPV6):
+		if ((hdr.network - data) > (max_len - sizeof(struct ipv6hdr)))
+			return max_len;
+
+		/* record next protocol */
+		nexthdr = hdr.ipv6->nexthdr;
+		hdr.network += sizeof(struct ipv6hdr);
+		break;
+#endif /* NETIF_F_TSO6 */
+#if defined(CONFIG_FCOE) || defined(CONFIG_FCOE_MODULE)
+	case __constant_htons(ETH_P_FCOE):
+		hdr.network += FCOE_HEADER_LEN;
+		break;
+#endif
+	default:
+		return hdr.network - data;
+	}
+
+	/* finally sort out L4 */
+	switch (nexthdr) {
+	case IPPROTO_TCP:
+		if ((hdr.network - data) > (max_len - sizeof(struct tcphdr)))
+			return max_len;
+
+		/* access doff as a u8 to avoid unaligned access on ia64 */
+		hdr.network += max_t(u8, sizeof(struct tcphdr),
+				     (hdr.network[12] & 0xF0) >> 2);
+
+		break;
+	case IPPROTO_UDP:
+	case IPPROTO_UDPLITE:
+		hdr.network += sizeof(struct udphdr);
+		break;
+#ifdef HAVE_SCTP
+	case IPPROTO_SCTP:
+		hdr.network += sizeof(struct sctphdr);
+		break;
+#endif
+	}
+
+	/*
+	 * If everything has gone correctly hdr.network should be the
+	 * data section of the packet and will be the end of the header.
+	 * If not then it probably represents the end of the last recognized
+	 * header.
+	 */
+	return min_t(unsigned int, hdr.network - data, max_len);
+}
+
+#endif /* < 3.18.0 */
+
+/******************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,19,0) )
+#ifdef HAVE_NET_GET_RANDOM_ONCE
+static u8 __kc_netdev_rss_key[NETDEV_RSS_KEY_LEN];
+
+void __kc_netdev_rss_key_fill(void *buffer, size_t len)
+{
+	BUG_ON(len > sizeof(__kc_netdev_rss_key));
+	net_get_random_once(__kc_netdev_rss_key, sizeof(__kc_netdev_rss_key));
+	memcpy(buffer, __kc_netdev_rss_key, len);
+}
+#endif
+
+int _kc_bitmap_print_to_pagebuf(bool list, char *buf,
+				const unsigned long *maskp,
+				int nmaskbits)
+{
+	ptrdiff_t len = PTR_ALIGN(buf + PAGE_SIZE - 1, PAGE_SIZE) - buf - 2;
+	int n = 0;
+
+	if (len > 1) {
+		n = list ? bitmap_scnlistprintf(buf, len, maskp, nmaskbits) :
+			   bitmap_scnprintf(buf, len, maskp, nmaskbits);
+		buf[n++] = '\n';
+		buf[n] = '\0';
+	}
+	return n;
+}
+#endif
+
+/******************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(4,1,0) )
+#if !((RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(6,8) && RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,0)) && \
+      (RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(7,2)) && \
+      (SLE_VERSION_CODE > SLE_VERSION(12,1,0)))
+unsigned int _kc_cpumask_local_spread(unsigned int i, int node)
+{
+	int cpu;
+
+	/* Wrap: we always want a cpu. */
+	i %= num_online_cpus();
+
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,28) )
+	/* Kernels prior to 2.6.28 do not have for_each_cpu or
+	 * cpumask_of_node, so just use for_each_online_cpu()
+	 */
+	for_each_online_cpu(cpu)
+		if (i-- == 0)
+			return cpu;
+
+	return 0;
+#else
+	if (node == -1) {
+		for_each_cpu(cpu, cpu_online_mask)
+			if (i-- == 0)
+				return cpu;
+	} else {
+		/* NUMA first. */
+		for_each_cpu_and(cpu, cpumask_of_node(node), cpu_online_mask)
+			if (i-- == 0)
+				return cpu;
+
+		for_each_cpu(cpu, cpu_online_mask) {
+			/* Skip NUMA nodes, done above. */
+			if (cpumask_test_cpu(cpu, cpumask_of_node(node)))
+				continue;
+
+			if (i-- == 0)
+				return cpu;
+		}
+	}
+#endif /* KERNEL_VERSION >= 2.6.28 */
+	BUG();
+}
+#endif
+#endif
+
+/******************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4,3,0))
+#if (!(RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,4)) && \
+     !(SLE_VERSION_CODE >= SLE_VERSION(12,2,0)))
+/**
+ * _kc_skb_flow_dissect_flow_keys - parse SKB to fill _kc_flow_keys
+ * @skb: SKB used to fille _kc_flow_keys
+ * @flow: _kc_flow_keys to set with SKB fields
+ * @flags: currently unused flags
+ *
+ * The purpose of using kcompat for this function is so the caller doesn't have
+ * to care about which kernel version they are on, which prevents a larger than
+ * normal #ifdef mess created by using a HAVE_* flag for this case. This is also
+ * done for 4.2 kernels to simplify calling skb_flow_dissect_flow_keys()
+ * because in 4.2 kernels skb_flow_dissect_flow_keys() exists, but only has 2
+ * arguments. Recent kernels have skb_flow_dissect_flow_keys() that has 3
+ * arguments.
+ *
+ * The caller needs to understand that this function was only implemented as a
+ * bare-minimum replacement for recent versions of skb_flow_dissect_flow_keys()
+ * and this function is in no way similar to skb_flow_dissect_flow_keys(). An
+ * example use can be found in the ice driver, specifically ice_arfs.c.
+ *
+ * This function is treated as a allowlist of supported fields the SKB can
+ * parse. If new functionality is added make sure to keep this format (i.e. only
+ * check for fields that are explicity wanted).
+ *
+ * Current allowlist:
+ *
+ * TCPv4, TCPv6, UDPv4, UDPv6
+ *
+ * If any unexpected protocol or other field is found this function memsets the
+ * flow passed in back to 0 and returns false. Otherwise the flow is populated
+ * and returns true.
+ */
+bool
+_kc_skb_flow_dissect_flow_keys(const struct sk_buff *skb,
+			       struct _kc_flow_keys *flow,
+			       unsigned int __always_unused flags)
+{
+	memset(flow, 0, sizeof(*flow));
+
+	flow->basic.n_proto = skb->protocol;
+	switch (flow->basic.n_proto) {
+	case htons(ETH_P_IP):
+		flow->basic.ip_proto = ip_hdr(skb)->protocol;
+		flow->addrs.v4addrs.src = ip_hdr(skb)->saddr;
+		flow->addrs.v4addrs.dst = ip_hdr(skb)->daddr;
+		break;
+	case htons(ETH_P_IPV6):
+		flow->basic.ip_proto = ipv6_hdr(skb)->nexthdr;
+		memcpy(&flow->addrs.v6addrs.src, &ipv6_hdr(skb)->saddr,
+		       sizeof(struct in6_addr));
+		memcpy(&flow->addrs.v6addrs.dst, &ipv6_hdr(skb)->daddr,
+		       sizeof(struct in6_addr));
+		break;
+	default:
+		netdev_dbg(skb->dev, "%s: Unsupported/unimplemented layer 3 protocol %04x\n", __func__, htons(flow->basic.n_proto));
+		goto unsupported;
+	}
+
+	switch (flow->basic.ip_proto) {
+	case IPPROTO_TCP:
+	{
+		struct tcphdr *tcph;
+
+		tcph = tcp_hdr(skb);
+		flow->ports.src = tcph->source;
+		flow->ports.dst = tcph->dest;
+		break;
+	}
+	case IPPROTO_UDP:
+	{
+		struct udphdr *udph;
+
+		udph = udp_hdr(skb);
+		flow->ports.src = udph->source;
+		flow->ports.dst = udph->dest;
+		break;
+	}
+	default:
+		netdev_dbg(skb->dev, "%s: Unsupported/unimplemented layer 4 protocol %02x\n", __func__, flow->basic.ip_proto);
+		return false;
+	}
+
+	return true;
+
+unsupported:
+	memset(flow, 0, sizeof(*flow));
+	return false;
+}
+#endif /* ! >= RHEL7.4 && ! >= SLES12.2 */
+#endif /* 4.3.0 */
+
+/******************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(4,5,0) )
+#if (!(RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,3)))
+#ifdef CONFIG_SPARC
+#include <asm/idprom.h>
+#include <asm/prom.h>
+#endif
+int _kc_eth_platform_get_mac_address(struct device *dev __maybe_unused,
+				     u8 *mac_addr __maybe_unused)
+{
+#if (((LINUX_VERSION_CODE < KERNEL_VERSION(3,1,0)) && defined(CONFIG_OF) && \
+      !defined(HAVE_STRUCT_DEVICE_OF_NODE) || !defined(CONFIG_OF)) && \
+     !defined(CONFIG_SPARC))
+	return -ENODEV;
+#else
+	const unsigned char *addr;
+	struct device_node *dp;
+
+	if (dev_is_pci(dev))
+		dp = pci_device_to_OF_node(to_pci_dev(dev));
+	else
+#if defined(HAVE_STRUCT_DEVICE_OF_NODE) && defined(CONFIG_OF)
+		dp = dev->of_node;
+#else
+		dp = NULL;
+#endif
+
+	addr = NULL;
+	if (dp)
+		addr = of_get_mac_address(dp);
+#ifdef CONFIG_SPARC
+	/* Kernel hasn't implemented arch_get_platform_mac_address, but we
+	 * should handle the SPARC case here since it was supported
+	 * originally. This is replaced by arch_get_platform_mac_address()
+	 * upstream.
+	 */
+	if (!addr)
+		addr = idprom->id_ethaddr;
+#endif
+	if (!addr)
+		return -ENODEV;
+
+	ether_addr_copy(mac_addr, addr);
+	return 0;
+#endif
+}
+#endif /* !(RHEL_RELEASE >= 7.3) */
+#endif /* < 4.5.0 */
+
+/*****************************************************************************/
+#if ((LINUX_VERSION_CODE < KERNEL_VERSION(4,14,0)) || \
+     (SLE_VERSION_CODE && (SLE_VERSION_CODE <= SLE_VERSION(12,3,0))) || \
+     (RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE <= RHEL_RELEASE_VERSION(7,5))))
+const char *_kc_phy_speed_to_str(int speed)
+{
+	switch (speed) {
+	case SPEED_10:
+		return "10Mbps";
+	case SPEED_100:
+		return "100Mbps";
+	case SPEED_1000:
+		return "1Gbps";
+	case SPEED_2500:
+		return "2.5Gbps";
+	case SPEED_5000:
+		return "5Gbps";
+	case SPEED_10000:
+		return "10Gbps";
+	case SPEED_14000:
+		return "14Gbps";
+	case SPEED_20000:
+		return "20Gbps";
+	case SPEED_25000:
+		return "25Gbps";
+	case SPEED_40000:
+		return "40Gbps";
+	case SPEED_50000:
+		return "50Gbps";
+	case SPEED_56000:
+		return "56Gbps";
+#ifdef SPEED_100000
+	case SPEED_100000:
+		return "100Gbps";
+#endif
+#ifdef SPEED_200000
+	case SPEED_200000:
+		return "200Gbps";
+#endif
+	case SPEED_UNKNOWN:
+		return "Unknown";
+	default:
+		return "Unsupported (update phy-core.c)";
+	}
+}
+#endif /* (LINUX < 4.14.0) || (SLES <= 12.3.0) || (RHEL <= 7.5) */
+
+/******************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(4,15,0) )
+void _kc_ethtool_intersect_link_masks(struct ethtool_link_ksettings *dst,
+				      struct ethtool_link_ksettings *src)
+{
+	unsigned int size = BITS_TO_LONGS(__ETHTOOL_LINK_MODE_MASK_NBITS);
+	unsigned int idx = 0;
+
+	for (; idx < size; idx++) {
+		dst->link_modes.supported[idx] &=
+			src->link_modes.supported[idx];
+		dst->link_modes.advertising[idx] &=
+			src->link_modes.advertising[idx];
+	}
+}
+#endif /* 4.15.0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4,16,0))
+#if !(RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(8,0)) && \
+     !(SLE_VERSION_CODE >= SLE_VERSION(12,5,0) && \
+       SLE_VERSION_CODE < SLE_VERSION(15,0,0) || \
+       SLE_VERSION_CODE >= SLE_VERSION(15,1,0))
+#if BITS_PER_LONG == 64
+/**
+ * bitmap_from_arr32 - copy the contents of u32 array of bits to bitmap
+ * @bitmap: array of unsigned longs, the destination bitmap
+ * @buf: array of u32 (in host byte order), the source bitmap
+ * @nbits: number of bits in @bitmap
+ */
+void bitmap_from_arr32(unsigned long *bitmap, const u32 *buf, unsigned int nbits)
+{
+	unsigned int i, halfwords;
+
+	halfwords = DIV_ROUND_UP(nbits, 32);
+	for (i = 0; i < halfwords; i++) {
+		bitmap[i/2] = (unsigned long) buf[i];
+		if (++i < halfwords)
+			bitmap[i/2] |= ((unsigned long) buf[i]) << 32;
+	}
+
+	/* Clear tail bits in last word beyond nbits. */
+	if (nbits % BITS_PER_LONG)
+		bitmap[(halfwords - 1) / 2] &= BITMAP_LAST_WORD_MASK(nbits);
+}
+#endif /* BITS_PER_LONG == 64 */
+#endif /* !(RHEL >= 8.0) && !(SLES >= 12.5 && SLES < 15.0 || SLES >= 15.1) */
+#endif /* 4.16.0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4,17,0))
+/* PCIe link information */
+#define PCIE_SPEED2STR(speed) \
+	((speed) == PCIE_SPEED_16_0GT ? "16 GT/s" : \
+	 (speed) == PCIE_SPEED_8_0GT ? "8 GT/s" : \
+	 (speed) == PCIE_SPEED_5_0GT ? "5 GT/s" : \
+	 (speed) == PCIE_SPEED_2_5GT ? "2.5 GT/s" : \
+	 "Unknown speed")
+
+/* PCIe speed to Mb/s reduced by encoding overhead */
+#define PCIE_SPEED2MBS_ENC(speed) \
+	((speed) == PCIE_SPEED_16_0GT ? 16000*128/130 : \
+	 (speed) == PCIE_SPEED_8_0GT  ?  8000*128/130 : \
+	 (speed) == PCIE_SPEED_5_0GT  ?  5000*8/10 : \
+	 (speed) == PCIE_SPEED_2_5GT  ?  2500*8/10 : \
+	 0)
+
+static u32
+_kc_pcie_bandwidth_available(struct pci_dev *dev,
+			     struct pci_dev **limiting_dev,
+			     enum pci_bus_speed *speed,
+			     enum pcie_link_width *width)
+{
+	u16 lnksta;
+	enum pci_bus_speed next_speed;
+	enum pcie_link_width next_width;
+	u32 bw, next_bw;
+
+	if (speed)
+		*speed = PCI_SPEED_UNKNOWN;
+	if (width)
+		*width = PCIE_LNK_WIDTH_UNKNOWN;
+
+	bw = 0;
+
+	while (dev) {
+		pcie_capability_read_word(dev, PCI_EXP_LNKSTA, &lnksta);
+
+		next_speed = pcie_link_speed[lnksta & PCI_EXP_LNKSTA_CLS];
+		next_width = (lnksta & PCI_EXP_LNKSTA_NLW) >>
+			PCI_EXP_LNKSTA_NLW_SHIFT;
+
+		next_bw = next_width * PCIE_SPEED2MBS_ENC(next_speed);
+
+		/* Check if current device limits the total bandwidth */
+		if (!bw || next_bw <= bw) {
+			bw = next_bw;
+
+			if (limiting_dev)
+				*limiting_dev = dev;
+			if (speed)
+				*speed = next_speed;
+			if (width)
+				*width = next_width;
+		}
+
+		dev = pci_upstream_bridge(dev);
+	}
+
+	return bw;
+}
+
+static enum pci_bus_speed _kc_pcie_get_speed_cap(struct pci_dev *dev)
+{
+	u32 lnkcap2, lnkcap;
+
+	/*
+	 * PCIe r4.0 sec 7.5.3.18 recommends using the Supported Link
+	 * Speeds Vector in Link Capabilities 2 when supported, falling
+	 * back to Max Link Speed in Link Capabilities otherwise.
+	 */
+	pcie_capability_read_dword(dev, PCI_EXP_LNKCAP2, &lnkcap2);
+	if (lnkcap2) { /* PCIe r3.0-compliant */
+		if (lnkcap2 & PCI_EXP_LNKCAP2_SLS_16_0GB)
+			return PCIE_SPEED_16_0GT;
+		else if (lnkcap2 & PCI_EXP_LNKCAP2_SLS_8_0GB)
+			return PCIE_SPEED_8_0GT;
+		else if (lnkcap2 & PCI_EXP_LNKCAP2_SLS_5_0GB)
+			return PCIE_SPEED_5_0GT;
+		else if (lnkcap2 & PCI_EXP_LNKCAP2_SLS_2_5GB)
+			return PCIE_SPEED_2_5GT;
+		return PCI_SPEED_UNKNOWN;
+	}
+
+	pcie_capability_read_dword(dev, PCI_EXP_LNKCAP, &lnkcap);
+	if (lnkcap) {
+		if (lnkcap & PCI_EXP_LNKCAP_SLS_16_0GB)
+			return PCIE_SPEED_16_0GT;
+		else if (lnkcap & PCI_EXP_LNKCAP_SLS_8_0GB)
+			return PCIE_SPEED_8_0GT;
+		else if (lnkcap & PCI_EXP_LNKCAP_SLS_5_0GB)
+			return PCIE_SPEED_5_0GT;
+		else if (lnkcap & PCI_EXP_LNKCAP_SLS_2_5GB)
+			return PCIE_SPEED_2_5GT;
+	}
+
+	return PCI_SPEED_UNKNOWN;
+}
+
+static enum pcie_link_width _kc_pcie_get_width_cap(struct pci_dev *dev)
+{
+	u32 lnkcap;
+
+	pcie_capability_read_dword(dev, PCI_EXP_LNKCAP, &lnkcap);
+	if (lnkcap)
+		return (lnkcap & PCI_EXP_LNKCAP_MLW) >> 4;
+
+	return PCIE_LNK_WIDTH_UNKNOWN;
+}
+
+static u32
+_kc_pcie_bandwidth_capable(struct pci_dev *dev, enum pci_bus_speed *speed,
+			   enum pcie_link_width *width)
+{
+	*speed = _kc_pcie_get_speed_cap(dev);
+	*width = _kc_pcie_get_width_cap(dev);
+
+	if (*speed == PCI_SPEED_UNKNOWN || *width == PCIE_LNK_WIDTH_UNKNOWN)
+		return 0;
+
+	return *width * PCIE_SPEED2MBS_ENC(*speed);
+}
+
+void _kc_pcie_print_link_status(struct pci_dev *dev) {
+	enum pcie_link_width width, width_cap;
+	enum pci_bus_speed speed, speed_cap;
+	struct pci_dev *limiting_dev = NULL;
+	u32 bw_avail, bw_cap;
+
+	bw_cap = _kc_pcie_bandwidth_capable(dev, &speed_cap, &width_cap);
+	bw_avail = _kc_pcie_bandwidth_available(dev, &limiting_dev, &speed,
+						&width);
+
+	if (bw_avail >= bw_cap)
+		pci_info(dev, "%u.%03u Gb/s available PCIe bandwidth (%s x%d link)\n",
+			 bw_cap / 1000, bw_cap % 1000,
+			 PCIE_SPEED2STR(speed_cap), width_cap);
+	else
+		pci_info(dev, "%u.%03u Gb/s available PCIe bandwidth, limited by %s x%d link at %s (capable of %u.%03u Gb/s with %s x%d link)\n",
+			 bw_avail / 1000, bw_avail % 1000,
+			 PCIE_SPEED2STR(speed), width,
+			 limiting_dev ? pci_name(limiting_dev) : "<unknown>",
+			 bw_cap / 1000, bw_cap % 1000,
+			 PCIE_SPEED2STR(speed_cap), width_cap);
+}
+#endif /* 4.17.0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(5,1,0)) || (RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(8,1)))
+#ifdef HAVE_TC_SETUP_CLSFLOWER
+#define FLOW_DISSECTOR_MATCH(__rule, __type, __out)				\
+	const struct flow_match *__m = &(__rule)->match;			\
+	struct flow_dissector *__d = (__m)->dissector;				\
+										\
+	(__out)->key = skb_flow_dissector_target(__d, __type, (__m)->key);	\
+	(__out)->mask = skb_flow_dissector_target(__d, __type, (__m)->mask);	\
+
+void flow_rule_match_basic(const struct flow_rule *rule,
+			   struct flow_match_basic *out)
+{
+	FLOW_DISSECTOR_MATCH(rule, FLOW_DISSECTOR_KEY_BASIC, out);
+}
+
+void flow_rule_match_control(const struct flow_rule *rule,
+			     struct flow_match_control *out)
+{
+	FLOW_DISSECTOR_MATCH(rule, FLOW_DISSECTOR_KEY_CONTROL, out);
+}
+
+void flow_rule_match_eth_addrs(const struct flow_rule *rule,
+			       struct flow_match_eth_addrs *out)
+{
+	FLOW_DISSECTOR_MATCH(rule, FLOW_DISSECTOR_KEY_ETH_ADDRS, out);
+}
+
+#ifdef HAVE_TC_FLOWER_ENC
+void flow_rule_match_enc_keyid(const struct flow_rule *rule,
+			       struct flow_match_enc_keyid *out)
+{
+	FLOW_DISSECTOR_MATCH(rule, FLOW_DISSECTOR_KEY_ENC_KEYID, out);
+}
+
+void flow_rule_match_enc_ports(const struct flow_rule *rule,
+			       struct flow_match_ports *out)
+{
+	FLOW_DISSECTOR_MATCH(rule, FLOW_DISSECTOR_KEY_ENC_PORTS, out);
+}
+
+void flow_rule_match_enc_control(const struct flow_rule *rule,
+				 struct flow_match_control *out)
+{
+	FLOW_DISSECTOR_MATCH(rule, FLOW_DISSECTOR_KEY_ENC_CONTROL, out);
+}
+
+void flow_rule_match_enc_ipv4_addrs(const struct flow_rule *rule,
+				    struct flow_match_ipv4_addrs *out)
+{
+	FLOW_DISSECTOR_MATCH(rule, FLOW_DISSECTOR_KEY_ENC_IPV4_ADDRS, out);
+}
+
+void flow_rule_match_enc_ipv6_addrs(const struct flow_rule *rule,
+				    struct flow_match_ipv6_addrs *out)
+{
+	FLOW_DISSECTOR_MATCH(rule, FLOW_DISSECTOR_KEY_ENC_IPV6_ADDRS, out);
+}
+#endif
+
+#ifndef HAVE_TC_FLOWER_VLAN_IN_TAGS
+void flow_rule_match_vlan(const struct flow_rule *rule,
+			  struct flow_match_vlan *out)
+{
+	FLOW_DISSECTOR_MATCH(rule, FLOW_DISSECTOR_KEY_VLAN, out);
+}
+#endif
+
+void flow_rule_match_ipv4_addrs(const struct flow_rule *rule,
+				struct flow_match_ipv4_addrs *out)
+{
+	FLOW_DISSECTOR_MATCH(rule, FLOW_DISSECTOR_KEY_IPV4_ADDRS, out);
+}
+
+void flow_rule_match_ipv6_addrs(const struct flow_rule *rule,
+				struct flow_match_ipv6_addrs *out)
+{
+	FLOW_DISSECTOR_MATCH(rule, FLOW_DISSECTOR_KEY_IPV6_ADDRS, out);
+}
+
+void flow_rule_match_ports(const struct flow_rule *rule,
+			   struct flow_match_ports *out)
+{
+	FLOW_DISSECTOR_MATCH(rule, FLOW_DISSECTOR_KEY_PORTS, out);
+}
+#endif /* HAVE_TC_SETUP_CLSFLOWER */
+#endif /* 5.1.0 || (RHEL && RHEL < 8.1) */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(5,3,0))
+#if (!(RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(8,2))))
+#ifdef HAVE_TC_CB_AND_SETUP_QDISC_MQPRIO
+int _kc_flow_block_cb_setup_simple(struct flow_block_offload *f,
+				   struct list_head __always_unused *driver_list,
+				   tc_setup_cb_t *cb,
+				   void *cb_ident, void *cb_priv,
+				   bool ingress_only)
+{
+	if (ingress_only &&
+	    f->binder_type != TCF_BLOCK_BINDER_TYPE_CLSACT_INGRESS)
+		return -EOPNOTSUPP;
+
+	/* Note: Upstream has driver_block_list, but older kernels do not */
+	switch (f->command) {
+	case TC_BLOCK_BIND:
+#ifdef HAVE_TCF_BLOCK_CB_REGISTER_EXTACK
+		return tcf_block_cb_register(f->block, cb, cb_ident, cb_priv,
+					     f->extack);
+#else
+		return tcf_block_cb_register(f->block, cb, cb_ident, cb_priv);
+#endif
+	case TC_BLOCK_UNBIND:
+		tcf_block_cb_unregister(f->block, cb, cb_ident);
+		return 0;
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+#endif /* HAVE_TC_CB_AND_SETUP_QDISC_MQPRIO */
+#endif /* !RHEL >= 8.2 */
+#endif /* 5.3.0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(5,7,0))
+u64 _kc_pci_get_dsn(struct pci_dev *dev)
+{
+	u32 dword;
+	u64 dsn;
+	int pos;
+
+	pos = pci_find_ext_capability(dev, PCI_EXT_CAP_ID_DSN);
+	if (!pos)
+		return 0;
+
+	/*
+	 * The Device Serial Number is two dwords offset 4 bytes from the
+	 * capability position. The specification says that the first dword is
+	 * the lower half, and the second dword is the upper half.
+	 */
+	pos += 4;
+	pci_read_config_dword(dev, pos, &dword);
+	dsn = (u64)dword;
+	pci_read_config_dword(dev, pos + 4, &dword);
+	dsn |= ((u64)dword) << 32;
+
+	return dsn;
+}
+#endif /* 5.7.0 */
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/kcompat.h linux-5.4.86.new/drivers/net/ethernet/intel/i40e/kcompat.h
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/kcompat.h	1969-12-31 19:00:00.000000000 -0500
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/kcompat.h	2024-05-13 03:58:25.372491940 -0400
@@ -0,0 +1,7346 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
+
+#ifndef _KCOMPAT_H_
+#define _KCOMPAT_H_
+
+#ifndef LINUX_VERSION_CODE
+#include <linux/version.h>
+#else
+#define KERNEL_VERSION(a,b,c) (((a) << 16) + ((b) << 8) + (c))
+#endif
+#include <linux/io.h>
+#include <linux/delay.h>
+#include <linux/errno.h>
+#include <linux/etherdevice.h>
+#include <linux/ethtool.h>
+#include <linux/if_vlan.h>
+#include <linux/in.h>
+#include <linux/if_link.h>
+#include <linux/init.h>
+#include <linux/ioport.h>
+#include <linux/ip.h>
+#include <linux/ipv6.h>
+#include <linux/list.h>
+#include <linux/mii.h>
+#include <linux/module.h>
+#include <linux/netdevice.h>
+#include <linux/pci.h>
+#include <linux/sched.h>
+#include <linux/skbuff.h>
+#include <linux/slab.h>
+#include <linux/string.h>
+#include <linux/tcp.h>
+#include <linux/types.h>
+#include <linux/udp.h>
+#include <linux/vmalloc.h>
+
+#ifndef GCC_VERSION
+#define GCC_VERSION (__GNUC__ * 10000		\
+		     + __GNUC_MINOR__ * 100	\
+		     + __GNUC_PATCHLEVEL__)
+#endif /* GCC_VERSION */
+
+#ifndef IEEE_8021QAZ_APP_SEL_DSCP
+#define IEEE_8021QAZ_APP_SEL_DSCP	5
+#endif
+
+/* Backport macros for controlling GCC diagnostics */
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(4,18,0) )
+
+/* Compilers before gcc-4.6 do not understand "#pragma GCC diagnostic push" */
+#if GCC_VERSION >= 40600
+#define __diag_str1(s)		#s
+#define __diag_str(s)		__diag_str1(s)
+#define __diag(s)		_Pragma(__diag_str(GCC diagnostic s))
+#else
+#define __diag(s)
+#endif /* GCC_VERSION >= 4.6 */
+#define __diag_push()	__diag(push)
+#define __diag_pop()	__diag(pop)
+#endif /* LINUX_VERSION < 4.18.0 */
+
+#ifndef NSEC_PER_MSEC
+#define NSEC_PER_MSEC 1000000L
+#endif
+#include <net/ipv6.h>
+/* UTS_RELEASE is in a different header starting in kernel 2.6.18 */
+#ifndef UTS_RELEASE
+/* utsrelease.h changed locations in 2.6.33 */
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33) )
+#include <linux/utsrelease.h>
+#else
+#include <generated/utsrelease.h>
+#endif
+#endif
+
+/* NAPI enable/disable flags here */
+#define NAPI
+
+#define adapter_struct i40e_pf
+#define adapter_q_vector i40e_q_vector
+
+/* and finally set defines so that the code sees the changes */
+#ifdef NAPI
+#ifndef CONFIG_I40E_NAPI
+#define CONFIG_I40E_NAPI
+#endif
+#else
+#undef CONFIG_I40E_NAPI
+#endif /* NAPI */
+
+/* Dynamic LTR and deeper C-State support disable/enable */
+
+/* packet split disable/enable */
+#ifdef DISABLE_PACKET_SPLIT
+#ifndef CONFIG_I40E_DISABLE_PACKET_SPLIT
+#define CONFIG_I40E_DISABLE_PACKET_SPLIT
+#endif
+#endif /* DISABLE_PACKET_SPLIT */
+
+/* MSI compatibility code for all kernels and drivers */
+#ifdef DISABLE_PCI_MSI
+#undef CONFIG_PCI_MSI
+#endif
+#ifndef CONFIG_PCI_MSI
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,8) )
+struct msix_entry {
+	u16 vector; /* kernel uses to write allocated vector */
+	u16 entry;  /* driver uses to specify entry, OS writes */
+};
+#endif
+#undef pci_enable_msi
+#define pci_enable_msi(a) -ENOTSUPP
+#undef pci_disable_msi
+#define pci_disable_msi(a) do {} while (0)
+#undef pci_enable_msix
+#define pci_enable_msix(a, b, c) -ENOTSUPP
+#undef pci_disable_msix
+#define pci_disable_msix(a) do {} while (0)
+#define msi_remove_pci_irq_vectors(a) do {} while (0)
+#endif /* CONFIG_PCI_MSI */
+#ifdef DISABLE_PM
+#undef CONFIG_PM
+#endif
+
+#ifdef DISABLE_NET_POLL_CONTROLLER
+#undef CONFIG_NET_POLL_CONTROLLER
+#endif
+
+#ifndef PMSG_SUSPEND
+#define PMSG_SUSPEND 3
+#endif
+
+/* generic boolean compatibility */
+#undef TRUE
+#undef FALSE
+#define TRUE true
+#define FALSE false
+#ifdef GCC_VERSION
+#if ( GCC_VERSION < 3000 )
+#define _Bool char
+#endif
+#else
+#define _Bool char
+#endif
+
+#ifndef BIT
+#define BIT(nr)         (1UL << (nr))
+#endif
+
+#undef __always_unused
+#define __always_unused __attribute__((__unused__))
+
+#undef __maybe_unused
+#define __maybe_unused __attribute__((__unused__))
+
+/* kernels less than 2.4.14 don't have this */
+#ifndef ETH_P_8021Q
+#define ETH_P_8021Q 0x8100
+#endif
+
+#ifndef module_param
+#define module_param(v,t,p) MODULE_PARM(v, "i");
+#endif
+
+#ifndef DMA_64BIT_MASK
+#define DMA_64BIT_MASK  0xffffffffffffffffULL
+#endif
+
+#ifndef DMA_32BIT_MASK
+#define DMA_32BIT_MASK  0x00000000ffffffffULL
+#endif
+
+#ifndef PCI_CAP_ID_EXP
+#define PCI_CAP_ID_EXP 0x10
+#endif
+
+#ifndef uninitialized_var
+#define uninitialized_var(x) x = x
+#endif
+
+#ifndef PCIE_LINK_STATE_L0S
+#define PCIE_LINK_STATE_L0S 1
+#endif
+#ifndef PCIE_LINK_STATE_L1
+#define PCIE_LINK_STATE_L1 2
+#endif
+
+#ifndef SET_NETDEV_DEV
+#define SET_NETDEV_DEV(net, pdev)
+#endif
+
+#if !defined(HAVE_FREE_NETDEV) && ( LINUX_VERSION_CODE < KERNEL_VERSION(3,1,0) )
+#define free_netdev(x)	kfree(x)
+#endif
+
+#ifdef HAVE_POLL_CONTROLLER
+#define CONFIG_NET_POLL_CONTROLLER
+#endif
+
+#ifndef SKB_DATAREF_SHIFT
+/* if we do not have the infrastructure to detect if skb_header is cloned
+   just return false in all cases */
+#define skb_header_cloned(x) 0
+#endif
+
+#ifndef NETIF_F_GSO
+#define gso_size tso_size
+#define gso_segs tso_segs
+#endif
+
+#ifndef NETIF_F_GRO
+#define vlan_gro_receive(_napi, _vlgrp, _vlan, _skb) \
+		vlan_hwaccel_receive_skb(_skb, _vlgrp, _vlan)
+#define napi_gro_receive(_napi, _skb) netif_receive_skb(_skb)
+#endif
+
+#ifndef NETIF_F_SCTP_CSUM
+#define NETIF_F_SCTP_CSUM 0
+#endif
+
+#ifndef NETIF_F_LRO
+#define NETIF_F_LRO BIT(15)
+#endif
+
+#ifndef NETIF_F_NTUPLE
+#define NETIF_F_NTUPLE BIT(27)
+#endif
+
+#ifndef NETIF_F_ALL_FCOE
+#define NETIF_F_ALL_FCOE	(NETIF_F_FCOE_CRC | NETIF_F_FCOE_MTU | \
+				 NETIF_F_FSO)
+#endif
+
+#ifndef IPPROTO_SCTP
+#define IPPROTO_SCTP 132
+#endif
+
+#ifndef IPPROTO_UDPLITE
+#define IPPROTO_UDPLITE 136
+#endif
+
+#ifndef CHECKSUM_PARTIAL
+#define CHECKSUM_PARTIAL CHECKSUM_HW
+#define CHECKSUM_COMPLETE CHECKSUM_HW
+#endif
+
+#ifndef __read_mostly
+#define __read_mostly
+#endif
+
+#ifndef MII_RESV1
+#define MII_RESV1		0x17		/* Reserved...		*/
+#endif
+
+#ifndef unlikely
+#define unlikely(_x) _x
+#define likely(_x) _x
+#endif
+
+#ifndef WARN_ON
+#define WARN_ON(x) ({0;})
+#endif
+
+#ifndef PCI_DEVICE
+#define PCI_DEVICE(vend,dev) \
+	.vendor = (vend), .device = (dev), \
+	.subvendor = PCI_ANY_ID, .subdevice = PCI_ANY_ID
+#endif
+
+#ifndef node_online
+#define node_online(node) ((node) == 0)
+#endif
+
+#ifndef _LINUX_RANDOM_H
+#include <linux/random.h>
+#endif
+
+#ifndef BITS_PER_TYPE
+#define BITS_PER_TYPE(type) (sizeof(type) * BITS_PER_BYTE)
+#endif
+
+#ifndef BITS_TO_LONGS
+#define BITS_TO_LONGS(bits) (((bits)+BITS_PER_LONG-1)/BITS_PER_LONG)
+#endif
+
+#ifndef DECLARE_BITMAP
+#define DECLARE_BITMAP(name,bits) long name[BITS_TO_LONGS(bits)]
+#endif
+
+#ifndef VLAN_HLEN
+#define VLAN_HLEN 4
+#endif
+
+#ifndef VLAN_ETH_HLEN
+#define VLAN_ETH_HLEN 18
+#endif
+
+#ifndef VLAN_ETH_FRAME_LEN
+#define VLAN_ETH_FRAME_LEN 1518
+#endif
+
+#ifndef DCA_GET_TAG_TWO_ARGS
+#define dca3_get_tag(a,b) dca_get_tag(b)
+#endif
+
+#ifndef CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS
+#if defined(__i386__) || defined(__x86_64__)
+#define CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS
+#endif
+#endif
+
+/* taken from 2.6.24 definition in linux/kernel.h */
+#ifndef IS_ALIGNED
+#define IS_ALIGNED(x,a)         (((x) % ((typeof(x))(a))) == 0)
+#endif
+
+#ifdef IS_ENABLED
+#undef IS_ENABLED
+#undef __ARG_PLACEHOLDER_1
+#undef config_enabled
+#undef _config_enabled
+#undef __config_enabled
+#undef ___config_enabled
+#endif
+
+#define __ARG_PLACEHOLDER_1 0,
+#define config_enabled(cfg) _config_enabled(cfg)
+#ifdef __CHECKER__
+/* cppcheck-suppress preprocessorErrorDirective */
+#endif /* __CHECKER__ */
+#define _config_enabled(value) __config_enabled(__ARG_PLACEHOLDER_##value)
+#define __config_enabled(arg1_or_junk) ___config_enabled(arg1_or_junk 1, 0)
+#define ___config_enabled(__ignored, val, ...) val
+
+#define IS_ENABLED(option) \
+	(config_enabled(option) || config_enabled(option##_MODULE))
+
+#if !defined(NETIF_F_HW_VLAN_TX) && !defined(NETIF_F_HW_VLAN_CTAG_TX)
+struct _kc_vlan_ethhdr {
+	unsigned char	h_dest[ETH_ALEN];
+	unsigned char	h_source[ETH_ALEN];
+	__be16		h_vlan_proto;
+	__be16		h_vlan_TCI;
+	__be16		h_vlan_encapsulated_proto;
+};
+#define vlan_ethhdr _kc_vlan_ethhdr
+struct _kc_vlan_hdr {
+	__be16		h_vlan_TCI;
+	__be16		h_vlan_encapsulated_proto;
+};
+#define vlan_hdr _kc_vlan_hdr
+#define vlan_tx_tag_present(_skb) 0
+#define vlan_tx_tag_get(_skb) 0
+#endif /* NETIF_F_HW_VLAN_TX && NETIF_F_HW_VLAN_CTAG_TX */
+
+#ifndef VLAN_PRIO_SHIFT
+#define VLAN_PRIO_SHIFT 13
+#endif
+
+#ifndef PCI_EXP_LNKSTA_CLS_2_5GB
+#define PCI_EXP_LNKSTA_CLS_2_5GB 0x0001
+#endif
+
+#ifndef PCI_EXP_LNKSTA_CLS_5_0GB
+#define PCI_EXP_LNKSTA_CLS_5_0GB 0x0002
+#endif
+
+#ifndef PCI_EXP_LNKSTA_CLS_8_0GB
+#define PCI_EXP_LNKSTA_CLS_8_0GB 0x0003
+#endif
+
+#ifndef PCI_EXP_LNKSTA_NLW_X1
+#define PCI_EXP_LNKSTA_NLW_X1 0x0010
+#endif
+
+#ifndef PCI_EXP_LNKSTA_NLW_X2
+#define PCI_EXP_LNKSTA_NLW_X2 0x0020
+#endif
+
+#ifndef PCI_EXP_LNKSTA_NLW_X4
+#define PCI_EXP_LNKSTA_NLW_X4 0x0040
+#endif
+
+#ifndef PCI_EXP_LNKSTA_NLW_X8
+#define PCI_EXP_LNKSTA_NLW_X8 0x0080
+#endif
+
+#ifndef __GFP_COLD
+#define __GFP_COLD 0
+#endif
+
+#ifndef __GFP_COMP
+#define __GFP_COMP 0
+#endif
+
+#ifndef IP_OFFSET
+#define IP_OFFSET 0x1FFF /* "Fragment Offset" part */
+#endif
+
+/*****************************************************************************/
+/* Installations with ethtool version without eeprom, adapter id, or statistics
+ * support */
+
+#ifndef ETH_GSTRING_LEN
+#define ETH_GSTRING_LEN 32
+#endif
+
+#ifndef ETHTOOL_GSTATS
+#define ETHTOOL_GSTATS 0x1d
+#undef ethtool_drvinfo
+#define ethtool_drvinfo k_ethtool_drvinfo
+struct k_ethtool_drvinfo {
+	u32 cmd;
+	char driver[32];
+	char version[32];
+	char fw_version[32];
+	char bus_info[32];
+	char reserved1[32];
+	char reserved2[16];
+	u32 n_stats;
+	u32 testinfo_len;
+	u32 eedump_len;
+	u32 regdump_len;
+};
+
+struct ethtool_stats {
+	u32 cmd;
+	u32 n_stats;
+	u64 data[0];
+};
+#endif /* ETHTOOL_GSTATS */
+
+#ifndef ETHTOOL_PHYS_ID
+#define ETHTOOL_PHYS_ID 0x1c
+#endif /* ETHTOOL_PHYS_ID */
+
+#ifndef ETHTOOL_GSTRINGS
+#define ETHTOOL_GSTRINGS 0x1b
+enum ethtool_stringset {
+	ETH_SS_TEST             = 0,
+	ETH_SS_STATS,
+};
+struct ethtool_gstrings {
+	u32 cmd;            /* ETHTOOL_GSTRINGS */
+	u32 string_set;     /* string set id e.c. ETH_SS_TEST, etc*/
+	u32 len;            /* number of strings in the string set */
+	u8 data[0];
+};
+#endif /* ETHTOOL_GSTRINGS */
+
+#ifndef ETHTOOL_TEST
+#define ETHTOOL_TEST 0x1a
+enum ethtool_test_flags {
+	ETH_TEST_FL_OFFLINE	= BIT(0),
+	ETH_TEST_FL_FAILED	= BIT(1),
+};
+struct ethtool_test {
+	u32 cmd;
+	u32 flags;
+	u32 reserved;
+	u32 len;
+	u64 data[0];
+};
+#endif /* ETHTOOL_TEST */
+
+#ifndef ETHTOOL_GEEPROM
+#define ETHTOOL_GEEPROM 0xb
+#undef ETHTOOL_GREGS
+struct ethtool_eeprom {
+	u32 cmd;
+	u32 magic;
+	u32 offset;
+	u32 len;
+	u8 data[0];
+};
+
+struct ethtool_value {
+	u32 cmd;
+	u32 data;
+};
+#endif /* ETHTOOL_GEEPROM */
+
+#ifndef ETHTOOL_GLINK
+#define ETHTOOL_GLINK 0xa
+#endif /* ETHTOOL_GLINK */
+
+#ifndef ETHTOOL_GWOL
+#define ETHTOOL_GWOL 0x5
+#define ETHTOOL_SWOL 0x6
+#define SOPASS_MAX      6
+struct ethtool_wolinfo {
+	u32 cmd;
+	u32 supported;
+	u32 wolopts;
+	u8 sopass[SOPASS_MAX]; /* SecureOn(tm) password */
+};
+#endif /* ETHTOOL_GWOL */
+
+#ifndef ETHTOOL_GREGS
+#define ETHTOOL_GREGS		0x00000004 /* Get NIC registers */
+#define ethtool_regs _kc_ethtool_regs
+/* for passing big chunks of data */
+struct _kc_ethtool_regs {
+	u32 cmd;
+	u32 version; /* driver-specific, indicates different chips/revs */
+	u32 len; /* bytes */
+	u8 data[0];
+};
+#endif /* ETHTOOL_GREGS */
+
+#ifndef ETHTOOL_GMSGLVL
+#define ETHTOOL_GMSGLVL		0x00000007 /* Get driver message level */
+#endif
+#ifndef ETHTOOL_SMSGLVL
+#define ETHTOOL_SMSGLVL		0x00000008 /* Set driver msg level, priv. */
+#endif
+#ifndef ETHTOOL_NWAY_RST
+#define ETHTOOL_NWAY_RST	0x00000009 /* Restart autonegotiation, priv */
+#endif
+#ifndef ETHTOOL_GLINK
+#define ETHTOOL_GLINK		0x0000000a /* Get link status */
+#endif
+#ifndef ETHTOOL_GEEPROM
+#define ETHTOOL_GEEPROM		0x0000000b /* Get EEPROM data */
+#endif
+#ifndef ETHTOOL_SEEPROM
+#define ETHTOOL_SEEPROM		0x0000000c /* Set EEPROM data */
+#endif
+#ifndef ETHTOOL_GCOALESCE
+#define ETHTOOL_GCOALESCE	0x0000000e /* Get coalesce config */
+/* for configuring coalescing parameters of chip */
+#define ethtool_coalesce _kc_ethtool_coalesce
+struct _kc_ethtool_coalesce {
+	u32	cmd;	/* ETHTOOL_{G,S}COALESCE */
+
+	/* How many usecs to delay an RX interrupt after
+	 * a packet arrives.  If 0, only rx_max_coalesced_frames
+	 * is used.
+	 */
+	u32	rx_coalesce_usecs;
+
+	/* How many packets to delay an RX interrupt after
+	 * a packet arrives.  If 0, only rx_coalesce_usecs is
+	 * used.  It is illegal to set both usecs and max frames
+	 * to zero as this would cause RX interrupts to never be
+	 * generated.
+	 */
+	u32	rx_max_coalesced_frames;
+
+	/* Same as above two parameters, except that these values
+	 * apply while an IRQ is being serviced by the host.  Not
+	 * all cards support this feature and the values are ignored
+	 * in that case.
+	 */
+	u32	rx_coalesce_usecs_irq;
+	u32	rx_max_coalesced_frames_irq;
+
+	/* How many usecs to delay a TX interrupt after
+	 * a packet is sent.  If 0, only tx_max_coalesced_frames
+	 * is used.
+	 */
+	u32	tx_coalesce_usecs;
+
+	/* How many packets to delay a TX interrupt after
+	 * a packet is sent.  If 0, only tx_coalesce_usecs is
+	 * used.  It is illegal to set both usecs and max frames
+	 * to zero as this would cause TX interrupts to never be
+	 * generated.
+	 */
+	u32	tx_max_coalesced_frames;
+
+	/* Same as above two parameters, except that these values
+	 * apply while an IRQ is being serviced by the host.  Not
+	 * all cards support this feature and the values are ignored
+	 * in that case.
+	 */
+	u32	tx_coalesce_usecs_irq;
+	u32	tx_max_coalesced_frames_irq;
+
+	/* How many usecs to delay in-memory statistics
+	 * block updates.  Some drivers do not have an in-memory
+	 * statistic block, and in such cases this value is ignored.
+	 * This value must not be zero.
+	 */
+	u32	stats_block_coalesce_usecs;
+
+	/* Adaptive RX/TX coalescing is an algorithm implemented by
+	 * some drivers to improve latency under low packet rates and
+	 * improve throughput under high packet rates.  Some drivers
+	 * only implement one of RX or TX adaptive coalescing.  Anything
+	 * not implemented by the driver causes these values to be
+	 * silently ignored.
+	 */
+	u32	use_adaptive_rx_coalesce;
+	u32	use_adaptive_tx_coalesce;
+
+	/* When the packet rate (measured in packets per second)
+	 * is below pkt_rate_low, the {rx,tx}_*_low parameters are
+	 * used.
+	 */
+	u32	pkt_rate_low;
+	u32	rx_coalesce_usecs_low;
+	u32	rx_max_coalesced_frames_low;
+	u32	tx_coalesce_usecs_low;
+	u32	tx_max_coalesced_frames_low;
+
+	/* When the packet rate is below pkt_rate_high but above
+	 * pkt_rate_low (both measured in packets per second) the
+	 * normal {rx,tx}_* coalescing parameters are used.
+	 */
+
+	/* When the packet rate is (measured in packets per second)
+	 * is above pkt_rate_high, the {rx,tx}_*_high parameters are
+	 * used.
+	 */
+	u32	pkt_rate_high;
+	u32	rx_coalesce_usecs_high;
+	u32	rx_max_coalesced_frames_high;
+	u32	tx_coalesce_usecs_high;
+	u32	tx_max_coalesced_frames_high;
+
+	/* How often to do adaptive coalescing packet rate sampling,
+	 * measured in seconds.  Must not be zero.
+	 */
+	u32	rate_sample_interval;
+};
+#endif /* ETHTOOL_GCOALESCE */
+
+#ifndef ETHTOOL_SCOALESCE
+#define ETHTOOL_SCOALESCE	0x0000000f /* Set coalesce config. */
+#endif
+#ifndef ETHTOOL_GRINGPARAM
+#define ETHTOOL_GRINGPARAM	0x00000010 /* Get ring parameters */
+/* for configuring RX/TX ring parameters */
+#define ethtool_ringparam _kc_ethtool_ringparam
+struct _kc_ethtool_ringparam {
+	u32	cmd;	/* ETHTOOL_{G,S}RINGPARAM */
+
+	/* Read only attributes.  These indicate the maximum number
+	 * of pending RX/TX ring entries the driver will allow the
+	 * user to set.
+	 */
+	u32	rx_max_pending;
+	u32	rx_mini_max_pending;
+	u32	rx_jumbo_max_pending;
+	u32	tx_max_pending;
+
+	/* Values changeable by the user.  The valid values are
+	 * in the range 1 to the "*_max_pending" counterpart above.
+	 */
+	u32	rx_pending;
+	u32	rx_mini_pending;
+	u32	rx_jumbo_pending;
+	u32	tx_pending;
+};
+#endif /* ETHTOOL_GRINGPARAM */
+
+#ifndef ETHTOOL_SRINGPARAM
+#define ETHTOOL_SRINGPARAM	0x00000011 /* Set ring parameters, priv. */
+#endif
+#ifndef ETHTOOL_GPAUSEPARAM
+#define ETHTOOL_GPAUSEPARAM	0x00000012 /* Get pause parameters */
+/* for configuring link flow control parameters */
+#define ethtool_pauseparam _kc_ethtool_pauseparam
+struct _kc_ethtool_pauseparam {
+	u32	cmd;	/* ETHTOOL_{G,S}PAUSEPARAM */
+
+	/* If the link is being auto-negotiated (via ethtool_cmd.autoneg
+	 * being true) the user may set 'autoneg' here non-zero to have the
+	 * pause parameters be auto-negotiated too.  In such a case, the
+	 * {rx,tx}_pause values below determine what capabilities are
+	 * advertised.
+	 *
+	 * If 'autoneg' is zero or the link is not being auto-negotiated,
+	 * then {rx,tx}_pause force the driver to use/not-use pause
+	 * flow control.
+	 */
+	u32	autoneg;
+	u32	rx_pause;
+	u32	tx_pause;
+};
+#endif /* ETHTOOL_GPAUSEPARAM */
+
+#ifndef ETHTOOL_SPAUSEPARAM
+#define ETHTOOL_SPAUSEPARAM	0x00000013 /* Set pause parameters. */
+#endif
+#ifndef ETHTOOL_GRXCSUM
+#define ETHTOOL_GRXCSUM		0x00000014 /* Get RX hw csum enable (ethtool_value) */
+#endif
+#ifndef ETHTOOL_SRXCSUM
+#define ETHTOOL_SRXCSUM		0x00000015 /* Set RX hw csum enable (ethtool_value) */
+#endif
+#ifndef ETHTOOL_GTXCSUM
+#define ETHTOOL_GTXCSUM		0x00000016 /* Get TX hw csum enable (ethtool_value) */
+#endif
+#ifndef ETHTOOL_STXCSUM
+#define ETHTOOL_STXCSUM		0x00000017 /* Set TX hw csum enable (ethtool_value) */
+#endif
+#ifndef ETHTOOL_GSG
+#define ETHTOOL_GSG		0x00000018 /* Get scatter-gather enable
+					    * (ethtool_value) */
+#endif
+#ifndef ETHTOOL_SSG
+#define ETHTOOL_SSG		0x00000019 /* Set scatter-gather enable
+					    * (ethtool_value). */
+#endif
+#ifndef ETHTOOL_TEST
+#define ETHTOOL_TEST		0x0000001a /* execute NIC self-test, priv. */
+#endif
+#ifndef ETHTOOL_GSTRINGS
+#define ETHTOOL_GSTRINGS	0x0000001b /* get specified string set */
+#endif
+#ifndef ETHTOOL_PHYS_ID
+#define ETHTOOL_PHYS_ID		0x0000001c /* identify the NIC */
+#endif
+#ifndef ETHTOOL_GSTATS
+#define ETHTOOL_GSTATS		0x0000001d /* get NIC-specific statistics */
+#endif
+#ifndef ETHTOOL_GTSO
+#define ETHTOOL_GTSO		0x0000001e /* Get TSO enable (ethtool_value) */
+#endif
+#ifndef ETHTOOL_STSO
+#define ETHTOOL_STSO		0x0000001f /* Set TSO enable (ethtool_value) */
+#endif
+
+#ifndef ETHTOOL_BUSINFO_LEN
+#define ETHTOOL_BUSINFO_LEN	32
+#endif
+
+#ifndef WAKE_FILTER
+#define WAKE_FILTER	BIT(7)
+#endif
+
+#ifndef SPEED_2500
+#define SPEED_2500 2500
+#endif
+#ifndef SPEED_5000
+#define SPEED_5000 5000
+#endif
+#ifndef SPEED_14000
+#define SPEED_14000 14000
+#endif
+#ifndef SPEED_25000
+#define SPEED_25000 25000
+#endif
+#ifndef SPEED_50000
+#define SPEED_50000 50000
+#endif
+#ifndef SPEED_56000
+#define SPEED_56000 56000
+#endif
+#ifndef SPEED_100000
+#define SPEED_100000 100000
+#endif
+#ifndef SPEED_200000
+#define SPEED_200000 200000
+#endif
+
+#ifndef RHEL_RELEASE_VERSION
+#define RHEL_RELEASE_VERSION(a,b) (((a) << 8) + (b))
+#endif
+#ifndef AX_RELEASE_VERSION
+#define AX_RELEASE_VERSION(a,b) (((a) << 8) + (b))
+#endif
+
+#ifndef AX_RELEASE_CODE
+#define AX_RELEASE_CODE 0
+#endif
+
+#if (AX_RELEASE_CODE && AX_RELEASE_CODE == AX_RELEASE_VERSION(3,0))
+#define RHEL_RELEASE_CODE RHEL_RELEASE_VERSION(5,0)
+#elif (AX_RELEASE_CODE && AX_RELEASE_CODE == AX_RELEASE_VERSION(3,1))
+#define RHEL_RELEASE_CODE RHEL_RELEASE_VERSION(5,1)
+#elif (AX_RELEASE_CODE && AX_RELEASE_CODE == AX_RELEASE_VERSION(3,2))
+#define RHEL_RELEASE_CODE RHEL_RELEASE_VERSION(5,3)
+#endif
+
+#ifndef RHEL_RELEASE_CODE
+/* NOTE: RHEL_RELEASE_* introduced in RHEL4.5 */
+#define RHEL_RELEASE_CODE 0
+#endif
+
+/* RHEL 7 didn't backport the parameter change in
+ * create_singlethread_workqueue.
+ * If/when RH corrects this we will want to tighten up the version check.
+ */
+#if (RHEL_RELEASE_CODE && RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,0))
+#undef create_singlethread_workqueue
+#define create_singlethread_workqueue(name)	\
+	alloc_ordered_workqueue("%s", WQ_MEM_RECLAIM, name)
+#endif
+
+/* Ubuntu Release ABI is the 4th digit of their kernel version. You can find
+ * it in /usr/src/linux/$(uname -r)/include/generated/utsrelease.h for new
+ * enough versions of Ubuntu. Otherwise you can simply see it in the output of
+ * uname as the 4th digit of the kernel. The UTS_UBUNTU_RELEASE_ABI is not in
+ * the linux-source package, but in the linux-headers package. It begins to
+ * appear in later releases of 14.04 and 14.10.
+ *
+ * Ex:
+ * <Ubuntu 14.04.1>
+ *  $uname -r
+ *  3.13.0-45-generic
+ * ABI is 45
+ *
+ * <Ubuntu 14.10>
+ *  $uname -r
+ *  3.16.0-23-generic
+ * ABI is 23
+ */
+#ifndef UTS_UBUNTU_RELEASE_ABI
+#define UTS_UBUNTU_RELEASE_ABI 0
+#define UBUNTU_VERSION_CODE 0
+#else
+/* Ubuntu does not provide actual release version macro, so we use the kernel
+ * version plus the ABI to generate a unique version code specific to Ubuntu.
+ * In addition, we mask the lower 8 bits of LINUX_VERSION_CODE in order to
+ * ignore differences in sublevel which are not important since we have the
+ * ABI value. Otherwise, it becomes impossible to correlate ABI to version for
+ * ordering checks.
+ *
+ * This also lets us store an ABI value up to 65535, since it can take the
+ * space that would use the lower byte of the Linux version code.
+ */
+#define UBUNTU_VERSION_CODE (((~0xFF & LINUX_VERSION_CODE) << 8) + \
+			     UTS_UBUNTU_RELEASE_ABI)
+
+#if UTS_UBUNTU_RELEASE_ABI > 65535
+#error UTS_UBUNTU_RELEASE_ABI is larger than 65535...
+#endif /* UTS_UBUNTU_RELEASE_ABI > 65535 */
+
+#if ( LINUX_VERSION_CODE <= KERNEL_VERSION(3,0,0) )
+/* Our version code scheme does not make sense for non 3.x or newer kernels,
+ * and we have no support in kcompat for this scenario. Thus, treat this as a
+ * non-Ubuntu kernel. Possibly might be better to error here.
+ */
+#define UTS_UBUNTU_RELEASE_ABI 0
+#define UBUNTU_VERSION_CODE 0
+#endif /* <= 3.0.0 */
+#endif /* !UTS_UBUNTU_RELEASE_ABI */
+
+/* We ignore the 3rd digit since we want to give precedence to the additional
+ * ABI value provided by Ubuntu.
+ */
+#define UBUNTU_VERSION(a,b,c,d) (((a) << 24) + ((b) << 16) + (d))
+
+/* SLE_VERSION is used to generate a 3-digit encoding that can order SLE
+ * kernels based on their major release, service pack, and a possible
+ * maintenance release.
+ */
+#define SLE_VERSION(a,b,c)	(((a) << 16) + ((b) << 8) + (c))
+
+/* The SLE_LOCALVERSION_CODE comes from a 3-digit code added as part of the
+ * Linux kernel version. It is extracted by the driver Makefile. This macro is
+ * used to generate codes for making comparisons below.
+ */
+#define SLE_LOCALVERSION(a,b,c)	(((a) << 16) + ((b) << 8) + (c))
+
+#ifdef CONFIG_SUSE_KERNEL
+/* Starting since at least SLE 12sp4 and SLE 15, the SUSE kernels have
+ * provided CONFIG_SUSE_VERSION, CONFIG_SUSE_PATCHLEVEL and
+ * CONFIG_SUSE_AUXRELEASE. Use these to generate SLE_VERSION if available.
+ * Only fall back to the manual table otherwise. We expect all future versions
+ * of SLE kernels to include these values, so the table will remain only for
+ * the older releases.
+ */
+#ifdef CONFIG_SUSE_VERSION
+#ifndef CONFIG_SUSE_PATCHLEVEL
+#error "CONFIG_SUSE_VERSION exists but CONFIG_SUSE_PATCHLEVEL is missing"
+#endif
+#ifndef CONFIG_SUSE_AUXRELEASE
+#error "CONFIG_SUSE_VERSION exists but CONFIG_SUSE_AUXRELEASE is missing"
+#endif
+#define SLE_VERSION_CODE SLE_VERSION(CONFIG_SUSE_VERSION, CONFIG_SUSE_PATCHLEVEL, CONFIG_SUSE_AUXRELEASE)
+#else
+/* If we do not have the CONFIG_SUSE_VERSION configuration values, fall back
+ * to the following table for older releases.
+ */
+#if ( LINUX_VERSION_CODE == KERNEL_VERSION(2,6,27) )
+/* SLES11 GA is 2.6.27 based */
+#define SLE_VERSION_CODE SLE_VERSION(11,0,0)
+#elif ( LINUX_VERSION_CODE == KERNEL_VERSION(2,6,32) )
+/* SLES11 SP1 is 2.6.32 based */
+#define SLE_VERSION_CODE SLE_VERSION(11,1,0)
+#elif ( LINUX_VERSION_CODE == KERNEL_VERSION(3,0,13) )
+/* SLES11 SP2 GA is 3.0.13-0.27 */
+#define SLE_VERSION_CODE SLE_VERSION(11,2,0)
+#elif ((LINUX_VERSION_CODE == KERNEL_VERSION(3,0,76)))
+/* SLES11 SP3 GA is 3.0.76-0.11 */
+#define SLE_VERSION_CODE SLE_VERSION(11,3,0)
+#elif (LINUX_VERSION_CODE == KERNEL_VERSION(3,0,101))
+  #if (SLE_LOCALVERSION_CODE < SLE_LOCALVERSION(0,8,0))
+  /* some SLES11sp2 update kernels up to 3.0.101-0.7.x */
+  #define SLE_VERSION_CODE SLE_VERSION(11,2,0)
+  #elif (SLE_LOCALVERSION_CODE < SLE_LOCALVERSION(63,0,0))
+  /* most SLES11sp3 update kernels */
+  #define SLE_VERSION_CODE SLE_VERSION(11,3,0)
+  #else
+  /* SLES11 SP4 GA (3.0.101-63) and update kernels 3.0.101-63+ */
+  #define SLE_VERSION_CODE SLE_VERSION(11,4,0)
+  #endif
+#elif (LINUX_VERSION_CODE == KERNEL_VERSION(3,12,28))
+/* SLES12 GA is 3.12.28-4
+ * kernel updates 3.12.xx-<33 through 52>[.yy] */
+#define SLE_VERSION_CODE SLE_VERSION(12,0,0)
+#elif (LINUX_VERSION_CODE == KERNEL_VERSION(3,12,49))
+/* SLES12 SP1 GA is 3.12.49-11
+ * updates 3.12.xx-60.yy where xx={51..} */
+#define SLE_VERSION_CODE SLE_VERSION(12,1,0)
+#elif ((LINUX_VERSION_CODE >= KERNEL_VERSION(4,4,21) && \
+       (LINUX_VERSION_CODE <= KERNEL_VERSION(4,4,59))) || \
+       (LINUX_VERSION_CODE >= KERNEL_VERSION(4,4,74) && \
+        LINUX_VERSION_CODE < KERNEL_VERSION(4,5,0) && \
+        SLE_LOCALVERSION_CODE >= KERNEL_VERSION(92,0,0) && \
+        SLE_LOCALVERSION_CODE <  KERNEL_VERSION(93,0,0)))
+/* SLES12 SP2 GA is 4.4.21-69.
+ * SLES12 SP2 updates before SLES12 SP3 are: 4.4.{21,38,49,59}
+ * SLES12 SP2 updates after SLES12 SP3 are: 4.4.{74,90,103,114,120}
+ * but they all use a SLE_LOCALVERSION_CODE matching 92.nn.y */
+#define SLE_VERSION_CODE SLE_VERSION(12,2,0)
+#elif ((LINUX_VERSION_CODE == KERNEL_VERSION(4,4,73) || \
+        LINUX_VERSION_CODE == KERNEL_VERSION(4,4,82) || \
+        LINUX_VERSION_CODE == KERNEL_VERSION(4,4,92)) || \
+       (LINUX_VERSION_CODE == KERNEL_VERSION(4,4,103) && \
+       (SLE_LOCALVERSION_CODE == KERNEL_VERSION(6,33,0) || \
+        SLE_LOCALVERSION_CODE == KERNEL_VERSION(6,38,0))) || \
+       (LINUX_VERSION_CODE >= KERNEL_VERSION(4,4,114) && \
+        LINUX_VERSION_CODE < KERNEL_VERSION(4,5,0) && \
+        SLE_LOCALVERSION_CODE >= KERNEL_VERSION(94,0,0) && \
+        SLE_LOCALVERSION_CODE <  KERNEL_VERSION(95,0,0)) )
+/* SLES12 SP3 GM is 4.4.73-5 and update kernels are 4.4.82-6.3.
+ * SLES12 SP3 updates not conflicting with SP2 are: 4.4.{82,92}
+ * SLES12 SP3 updates conflicting with SP2 are:
+ *   - 4.4.103-6.33.1, 4.4.103-6.38.1
+ *   - 4.4.{114,120}-94.nn.y */
+#define SLE_VERSION_CODE SLE_VERSION(12,3,0)
+#else
+#error "This looks like a SUSE kernel, but it has an unrecognized local version code."
+#endif /* LINUX_VERSION_CODE == KERNEL_VERSION(x,y,z) */
+#endif /* !CONFIG_SUSE_VERSION */
+#endif /* CONFIG_SUSE_KERNEL */
+#ifndef SLE_VERSION_CODE
+#define SLE_VERSION_CODE 0
+#endif /* SLE_VERSION_CODE */
+#ifndef SLE_LOCALVERSION_CODE
+#define SLE_LOCALVERSION_CODE 0
+#endif /* SLE_LOCALVERSION_CODE */
+
+/*
+ * Include the definitions file for HAVE/NEED flags for the standard upstream
+ * kernels.
+ *
+ * Then, based on the distribution we detect, load the distribution specific
+ * definitions file that customizes the definitions for the target
+ * distribution.
+ */
+#include "kcompat_std_defs.h"
+
+#ifdef CONFIG_SUSE_KERNEL
+#include "kcompat_sles_defs.h"
+#elif UBUNTU_VERSION_CODE
+#include "kcompat_ubuntu_defs.h"
+#elif RHEL_RELEASE_CODE
+#include "kcompat_rhel_defs.h"
+#endif
+
+/*
+ * ADQ depends on __TC_MQPRIO_MODE_MAX and related kernel code
+ * added around 4.15. Some distributions (e.g. Oracle Linux 7.7)
+ * have done a partial back-port of that to their kernels based
+ * on older mainline kernels that did not include all the necessary
+ * kernel enablement to support ADQ.
+ * Undefine __TC_MQPRIO_MODE_MAX for all OSV distributions with
+ * kernels based on mainline kernels older than 4.15 except for
+ * RHEL, SLES and Ubuntu which are known to have good back-ports.
+ */
+#if (!RHEL_RELEASE_CODE && !SLE_VERSION_CODE && !UBUNTU_VERSION_CODE)
+  #if (LINUX_VERSION_CODE < KERNEL_VERSION(4,15,0))
+  #undef __TC_MQPRIO_MODE_MAX
+  #endif /*  LINUX_VERSION_CODE == KERNEL_VERSION(4,15,0) */
+#endif /* if (NOT RHEL && NOT SLES && NOT UBUNTU) */
+
+
+#ifdef __KLOCWORK__
+ */
+#ifdef ARRAY_SIZE
+#undef ARRAY_SIZE
+#define ARRAY_SIZE(x) (sizeof(x) / sizeof((x)[0]))
+#endif
+
+#define memcpy(dest, src, len)	memcpy_s(dest, len, src, len)
+#define memset(dest, ch, len)	memset_s(dest, len, ch, len)
+
+static inline int _kc_test_and_clear_bit(int nr, volatile unsigned long *addr)
+{
+	unsigned long mask = BIT_MASK(nr);
+	unsigned long *p = ((unsigned long *)addr) + BIT_WORD(nr);
+	unsigned long old;
+	unsigned long flags = 0;
+
+	_atomic_spin_lock_irqsave(p, flags);
+	old = *p;
+	*p = old & ~mask;
+	_atomic_spin_unlock_irqrestore(p, flags);
+
+	return (old & mask) != 0;
+}
+#define test_and_clear_bit(nr, addr) _kc_test_and_clear_bit(nr, addr)
+
+static inline int _kc_test_and_set_bit(int nr, volatile unsigned long *addr)
+{
+	unsigned long mask = BIT_MASK(nr);
+	unsigned long *p = ((unsigned long *)addr) + BIT_WORD(nr);
+	unsigned long old;
+	unsigned long flags = 0;
+
+	_atomic_spin_lock_irqsave(p, flags);
+	old = *p;
+	*p = old | mask;
+	_atomic_spin_unlock_irqrestore(p, flags);
+
+	return (old & mask) != 0;
+}
+#define test_and_set_bit(nr, addr) _kc_test_and_set_bit(nr, addr)
+
+#ifdef CONFIG_DYNAMIC_DEBUG
+#undef dev_dbg
+#define dev_dbg(dev, format, arg...) dev_printk(KERN_DEBUG, dev, format, ##arg)
+#undef pr_debug
+#define pr_debug(format, arg...) printk(KERN_DEBUG format, ##arg)
+#endif /* CONFIG_DYNAMIC_DEBUG */
+
+#undef hlist_for_each_entry_safe
+#define hlist_for_each_entry_safe(pos, n, head, member)			     \
+	for (n = NULL, pos = hlist_entry_safe((head)->first, typeof(*(pos)), \
+					      member);			     \
+	     pos;							     \
+	     pos = hlist_entry_safe((pos)->member.next, typeof(*(pos)), member))
+
+#ifdef uninitialized_var
+#undef uninitialized_var
+#define uninitialized_var(x) x = *(&(x))
+#endif
+
+#ifdef WRITE_ONCE
+#undef WRITE_ONCE
+#define WRITE_ONCE(x, val)	((x) = (val))
+#endif /* WRITE_ONCE */
+
+#ifdef wait_event_interruptible_timeout
+#undef wait_event_interruptible_timeout
+#define wait_event_interruptible_timeout(wq_head, condition, timeout) ({	\
+	long ret;								\
+	if ((condition))							\
+		ret = timeout;							\
+	else									\
+		ret = 0;							\
+	ret;									\
+})
+#endif /* wait_event_interruptible_timeout */
+
+#ifdef max_t
+#undef max_t
+#define max_t(type, x, y) ({							\
+type __x = (x);								\
+type __y = (y);								\
+__x > __y ? __x : __y;							\
+})
+#endif /* max_t */
+
+#ifdef min_t
+#undef min_t
+#define min_t(type, x, y) ({							\
+type __x = (x);								\
+type __y = (y);								\
+__x < __y ? __x : __y;							\
+})
+#endif /* min_t */
+#endif /* __KLOCWORK__ */
+
+#include "kcompat_vfd.h"
+struct vfd_objects *create_vfd_sysfs(struct pci_dev *pdev, int num_alloc_vfs);
+void destroy_vfd_sysfs(struct pci_dev *pdev, struct vfd_objects *vfd_obj);
+
+/* Older versions of GCC will trigger -Wformat-nonliteral warnings for const
+ * char * strings. Unfortunately, the implementation of do_trace_printk does
+ * this, in order to add a storage attribute to the memory. This was fixed in
+ * GCC 5.1, but we still use older distributions built with GCC 4.x.
+ *
+ * The string pointer is only passed as a const char * to the __trace_bprintk
+ * function. Since that function has the __printf attribute, it will trigger
+ * the warnings. We can't remove the attribute, so instead we'll use the
+ * __diag macro to disable -Wformat-nonliteral around the call to
+ * __trace_bprintk.
+ */
+#if GCC_VERSION < 50100
+#define __trace_bprintk(ip, fmt, args...) ({		\
+	int err;					\
+	__diag_push();					\
+	__diag(ignored "-Wformat-nonliteral");		\
+	err = __trace_bprintk(ip, fmt, ##args);		\
+	__diag_pop();					\
+	err;						\
+})
+#endif /* GCC_VERSION < 5.1.0 */
+
+/* Newer kernels removed <linux/pci-aspm.h> */
+#if ((LINUX_VERSION_CODE < KERNEL_VERSION(5,4,0)) && \
+     (!(RHEL_RELEASE_CODE && RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(8,3)) && \
+     !(SLE_VERSION_CODE && (SLE_VERSION_CODE >= SLE_VERSION(15,3,0)))))
+#define HAVE_PCI_ASPM_H
+#endif
+
+/*****************************************************************************/
+/* 2.4.3 => 2.4.0 */
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,4,3) )
+
+/**************************************/
+/* PCI DRIVER API */
+
+#ifndef pci_set_dma_mask
+#define pci_set_dma_mask _kc_pci_set_dma_mask
+int _kc_pci_set_dma_mask(struct pci_dev *dev, dma_addr_t mask);
+#endif
+
+#ifndef pci_request_regions
+#define pci_request_regions _kc_pci_request_regions
+int _kc_pci_request_regions(struct pci_dev *pdev, char *res_name);
+#endif
+
+#ifndef pci_release_regions
+#define pci_release_regions _kc_pci_release_regions
+void _kc_pci_release_regions(struct pci_dev *pdev);
+#endif
+
+/**************************************/
+/* NETWORK DRIVER API */
+
+#ifndef alloc_etherdev
+#define alloc_etherdev _kc_alloc_etherdev
+struct net_device * _kc_alloc_etherdev(int sizeof_priv);
+#endif
+
+#ifndef is_valid_ether_addr
+#define is_valid_ether_addr _kc_is_valid_ether_addr
+int _kc_is_valid_ether_addr(u8 *addr);
+#endif
+
+/**************************************/
+/* MISCELLANEOUS */
+
+#ifndef INIT_TQUEUE
+#define INIT_TQUEUE(_tq, _routine, _data)		\
+	do {						\
+		INIT_LIST_HEAD(&(_tq)->list);		\
+		(_tq)->sync = 0;			\
+		(_tq)->routine = _routine;		\
+		(_tq)->data = _data;			\
+	} while (0)
+#endif
+
+#endif /* 2.4.3 => 2.4.0 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,4,5) )
+/* Generic MII registers. */
+#define MII_BMCR            0x00        /* Basic mode control register */
+#define MII_BMSR            0x01        /* Basic mode status register  */
+#define MII_PHYSID1         0x02        /* PHYS ID 1                   */
+#define MII_PHYSID2         0x03        /* PHYS ID 2                   */
+#define MII_ADVERTISE       0x04        /* Advertisement control reg   */
+#define MII_LPA             0x05        /* Link partner ability reg    */
+#define MII_EXPANSION       0x06        /* Expansion register          */
+/* Basic mode control register. */
+#define BMCR_FULLDPLX           0x0100  /* Full duplex                 */
+#define BMCR_ANENABLE           0x1000  /* Enable auto negotiation     */
+/* Basic mode status register. */
+#define BMSR_ERCAP              0x0001  /* Ext-reg capability          */
+#define BMSR_ANEGCAPABLE        0x0008  /* Able to do auto-negotiation */
+#define BMSR_10HALF             0x0800  /* Can do 10mbps, half-duplex  */
+#define BMSR_10FULL             0x1000  /* Can do 10mbps, full-duplex  */
+#define BMSR_100HALF            0x2000  /* Can do 100mbps, half-duplex */
+#define BMSR_100FULL            0x4000  /* Can do 100mbps, full-duplex */
+/* Advertisement control register. */
+#define ADVERTISE_CSMA          0x0001  /* Only selector supported     */
+#define ADVERTISE_10HALF        0x0020  /* Try for 10mbps half-duplex  */
+#define ADVERTISE_10FULL        0x0040  /* Try for 10mbps full-duplex  */
+#define ADVERTISE_100HALF       0x0080  /* Try for 100mbps half-duplex */
+#define ADVERTISE_100FULL       0x0100  /* Try for 100mbps full-duplex */
+#define ADVERTISE_ALL (ADVERTISE_10HALF | ADVERTISE_10FULL | \
+                       ADVERTISE_100HALF | ADVERTISE_100FULL)
+/* Expansion register for auto-negotiation. */
+#define EXPANSION_ENABLENPAGE   0x0004  /* This enables npage words    */
+#endif
+
+/*****************************************************************************/
+/* 2.4.6 => 2.4.3 */
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,4,6) )
+
+#ifndef pci_set_power_state
+#define pci_set_power_state _kc_pci_set_power_state
+int _kc_pci_set_power_state(struct pci_dev *dev, int state);
+#endif
+
+#ifndef pci_enable_wake
+#define pci_enable_wake _kc_pci_enable_wake
+int _kc_pci_enable_wake(struct pci_dev *pdev, u32 state, int enable);
+#endif
+
+#ifndef pci_disable_device
+#define pci_disable_device _kc_pci_disable_device
+void _kc_pci_disable_device(struct pci_dev *pdev);
+#endif
+
+/* PCI PM entry point syntax changed, so don't support suspend/resume */
+#undef CONFIG_PM
+
+#endif /* 2.4.6 => 2.4.3 */
+
+#ifndef HAVE_PCI_SET_MWI
+#define pci_set_mwi(X) pci_write_config_word(X, \
+			       PCI_COMMAND, adapter->hw.bus.pci_cmd_word | \
+			       PCI_COMMAND_INVALIDATE);
+#define pci_clear_mwi(X) pci_write_config_word(X, \
+			       PCI_COMMAND, adapter->hw.bus.pci_cmd_word & \
+			       ~PCI_COMMAND_INVALIDATE);
+#endif
+
+/*****************************************************************************/
+/* 2.4.10 => 2.4.9 */
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,4,10) )
+
+/**************************************/
+/* MODULE API */
+
+#ifndef MODULE_LICENSE
+	#define MODULE_LICENSE(X)
+#endif
+
+/**************************************/
+/* OTHER */
+
+#undef min
+#define min(x,y) ({ \
+	const typeof(x) _x = (x);	\
+	const typeof(y) _y = (y);	\
+	(void) (&_x == &_y);		\
+	_x < _y ? _x : _y; })
+
+#undef max
+#define max(x,y) ({ \
+	const typeof(x) _x = (x);	\
+	const typeof(y) _y = (y);	\
+	(void) (&_x == &_y);		\
+	_x > _y ? _x : _y; })
+
+#define min_t(type,x,y) ({ \
+	type _x = (x); \
+	type _y = (y); \
+	_x < _y ? _x : _y; })
+
+#define max_t(type,x,y) ({ \
+	type _x = (x); \
+	type _y = (y); \
+	_x > _y ? _x : _y; })
+
+#ifndef list_for_each_safe
+#define list_for_each_safe(pos, n, head) \
+	for (pos = (head)->next, n = pos->next; pos != (head); \
+		pos = n, n = pos->next)
+#endif
+
+#ifndef ____cacheline_aligned_in_smp
+#ifdef CONFIG_SMP
+#define ____cacheline_aligned_in_smp ____cacheline_aligned
+#else
+#define ____cacheline_aligned_in_smp
+#endif /* CONFIG_SMP */
+#endif
+
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,4,8) )
+int _kc_snprintf(char * buf, size_t size, const char *fmt, ...);
+#define snprintf(buf, size, fmt, args...) _kc_snprintf(buf, size, fmt, ##args)
+int _kc_vsnprintf(char *buf, size_t size, const char *fmt, va_list args);
+#define vsnprintf(buf, size, fmt, args) _kc_vsnprintf(buf, size, fmt, args)
+#else /* 2.4.8 => 2.4.9 */
+int snprintf(char * buf, size_t size, const char *fmt, ...);
+int vsnprintf(char *buf, size_t size, const char *fmt, va_list args);
+#endif
+#endif /* 2.4.10 -> 2.4.6 */
+
+
+/*****************************************************************************/
+/* 2.4.12 => 2.4.10 */
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,4,12) )
+#ifndef HAVE_NETIF_MSG
+#define HAVE_NETIF_MSG 1
+enum {
+	NETIF_MSG_DRV		= 0x0001,
+	NETIF_MSG_PROBE		= 0x0002,
+	NETIF_MSG_LINK		= 0x0004,
+	NETIF_MSG_TIMER		= 0x0008,
+	NETIF_MSG_IFDOWN	= 0x0010,
+	NETIF_MSG_IFUP		= 0x0020,
+	NETIF_MSG_RX_ERR	= 0x0040,
+	NETIF_MSG_TX_ERR	= 0x0080,
+	NETIF_MSG_TX_QUEUED	= 0x0100,
+	NETIF_MSG_INTR		= 0x0200,
+	NETIF_MSG_TX_DONE	= 0x0400,
+	NETIF_MSG_RX_STATUS	= 0x0800,
+	NETIF_MSG_PKTDATA	= 0x1000,
+	NETIF_MSG_HW		= 0x2000,
+	NETIF_MSG_WOL		= 0x4000,
+};
+
+#define netif_msg_drv(p)	((p)->msg_enable & NETIF_MSG_DRV)
+#define netif_msg_probe(p)	((p)->msg_enable & NETIF_MSG_PROBE)
+#define netif_msg_link(p)	((p)->msg_enable & NETIF_MSG_LINK)
+#define netif_msg_timer(p)	((p)->msg_enable & NETIF_MSG_TIMER)
+#define netif_msg_ifdown(p)	((p)->msg_enable & NETIF_MSG_IFDOWN)
+#define netif_msg_ifup(p)	((p)->msg_enable & NETIF_MSG_IFUP)
+#define netif_msg_rx_err(p)	((p)->msg_enable & NETIF_MSG_RX_ERR)
+#define netif_msg_tx_err(p)	((p)->msg_enable & NETIF_MSG_TX_ERR)
+#define netif_msg_tx_queued(p)	((p)->msg_enable & NETIF_MSG_TX_QUEUED)
+#define netif_msg_intr(p)	((p)->msg_enable & NETIF_MSG_INTR)
+#define netif_msg_tx_done(p)	((p)->msg_enable & NETIF_MSG_TX_DONE)
+#define netif_msg_rx_status(p)	((p)->msg_enable & NETIF_MSG_RX_STATUS)
+#define netif_msg_pktdata(p)	((p)->msg_enable & NETIF_MSG_PKTDATA)
+#endif /* !HAVE_NETIF_MSG */
+#endif /* 2.4.12 => 2.4.10 */
+
+/*****************************************************************************/
+/* 2.4.13 => 2.4.12 */
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,4,13) )
+
+/**************************************/
+/* PCI DMA MAPPING */
+
+#ifndef virt_to_page
+	#define virt_to_page(v) (mem_map + (virt_to_phys(v) >> PAGE_SHIFT))
+#endif
+
+#ifndef pci_map_page
+#define pci_map_page _kc_pci_map_page
+u64 _kc_pci_map_page(struct pci_dev *dev, struct page *page, unsigned long offset, size_t size, int direction);
+#endif
+
+#ifndef pci_unmap_page
+#define pci_unmap_page _kc_pci_unmap_page
+void _kc_pci_unmap_page(struct pci_dev *dev, u64 dma_addr, size_t size, int direction);
+#endif
+
+/* pci_set_dma_mask takes dma_addr_t, which is only 32-bits prior to 2.4.13 */
+
+#undef DMA_32BIT_MASK
+#define DMA_32BIT_MASK	0xffffffff
+#undef DMA_64BIT_MASK
+#define DMA_64BIT_MASK	0xffffffff
+
+/**************************************/
+/* OTHER */
+
+#ifndef cpu_relax
+#define cpu_relax()	rep_nop()
+#endif
+
+struct vlan_ethhdr {
+	unsigned char h_dest[ETH_ALEN];
+	unsigned char h_source[ETH_ALEN];
+	unsigned short h_vlan_proto;
+	unsigned short h_vlan_TCI;
+	unsigned short h_vlan_encapsulated_proto;
+};
+#endif /* 2.4.13 => 2.4.12 */
+
+/*****************************************************************************/
+/* 2.4.17 => 2.4.12 */
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,4,17) )
+
+#ifndef __devexit_p
+	#define __devexit_p(x) &(x)
+#endif
+
+#endif /* 2.4.17 => 2.4.13 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,4,18) )
+#define NETIF_MSG_HW	0x2000
+#define NETIF_MSG_WOL	0x4000
+
+#ifndef netif_msg_hw
+#define netif_msg_hw(p)		((p)->msg_enable & NETIF_MSG_HW)
+#endif
+#ifndef netif_msg_wol
+#define netif_msg_wol(p)	((p)->msg_enable & NETIF_MSG_WOL)
+#endif
+#endif /* 2.4.18 */
+
+/*****************************************************************************/
+
+/*****************************************************************************/
+/* 2.4.20 => 2.4.19 */
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,4,20) )
+
+/* we won't support NAPI on less than 2.4.20 */
+#ifdef NAPI
+#undef NAPI
+#endif
+
+#endif /* 2.4.20 => 2.4.19 */
+
+/*****************************************************************************/
+/* 2.4.22 => 2.4.17 */
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,4,22) )
+#define pci_name(x)	((x)->slot_name)
+#define cpu_online(cpuid) test_bit((cpuid), &cpu_online_map)
+
+#ifndef SUPPORTED_10000baseT_Full
+#define SUPPORTED_10000baseT_Full	BIT(12)
+#endif
+#ifndef ADVERTISED_10000baseT_Full
+#define ADVERTISED_10000baseT_Full	BIT(12)
+#endif
+#endif
+
+/*****************************************************************************/
+/*****************************************************************************/
+/* 2.4.23 => 2.4.22 */
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,4,23) )
+/*****************************************************************************/
+#ifdef NAPI
+#ifndef netif_poll_disable
+#define netif_poll_disable(x) _kc_netif_poll_disable(x)
+static inline void _kc_netif_poll_disable(struct net_device *netdev)
+{
+	while (test_and_set_bit(__LINK_STATE_RX_SCHED, &netdev->state)) {
+		/* No hurry */
+		current->state = TASK_INTERRUPTIBLE;
+		schedule_timeout(1);
+	}
+}
+#endif
+#ifndef netif_poll_enable
+#define netif_poll_enable(x) _kc_netif_poll_enable(x)
+static inline void _kc_netif_poll_enable(struct net_device *netdev)
+{
+	clear_bit(__LINK_STATE_RX_SCHED, &netdev->state);
+}
+#endif
+#endif /* NAPI */
+#ifndef netif_tx_disable
+#define netif_tx_disable(x) _kc_netif_tx_disable(x)
+static inline void _kc_netif_tx_disable(struct net_device *dev)
+{
+	spin_lock_bh(&dev->xmit_lock);
+	netif_stop_queue(dev);
+	spin_unlock_bh(&dev->xmit_lock);
+}
+#endif
+#else /* 2.4.23 => 2.4.22 */
+#define HAVE_SCTP
+#endif /* 2.4.23 => 2.4.22 */
+
+/*****************************************************************************/
+/* 2.6.4 => 2.6.0 */
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,4,25) || \
+    ( LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,0) && \
+      LINUX_VERSION_CODE < KERNEL_VERSION(2,6,4) ) )
+#define ETHTOOL_OPS_COMPAT
+#endif /* 2.6.4 => 2.6.0 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,4,27) )
+#define __user
+#endif /* < 2.4.27 */
+
+/*****************************************************************************/
+/* 2.5.71 => 2.4.x */
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,5,71) )
+#define sk_protocol protocol
+#define pci_get_device pci_find_device
+#endif /* 2.5.70 => 2.4.x */
+
+/*****************************************************************************/
+/* < 2.4.27 or 2.6.0 <= 2.6.5 */
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,4,27) || \
+    ( LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,0) && \
+      LINUX_VERSION_CODE < KERNEL_VERSION(2,6,5) ) )
+
+#ifndef netif_msg_init
+#define netif_msg_init _kc_netif_msg_init
+static inline u32 _kc_netif_msg_init(int debug_value, int default_msg_enable_bits)
+{
+	/* use default */
+	if (debug_value < 0 || debug_value >= (sizeof(u32) * 8))
+		return default_msg_enable_bits;
+	if (debug_value == 0) /* no output */
+		return 0;
+	/* set low N bits */
+	return (1 << debug_value) -1;
+}
+#endif
+
+#endif /* < 2.4.27 or 2.6.0 <= 2.6.5 */
+/*****************************************************************************/
+#if (( LINUX_VERSION_CODE < KERNEL_VERSION(2,4,27) ) || \
+     (( LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,0) ) && \
+      ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,3) )))
+#define netdev_priv(x) x->priv
+#endif
+
+/*****************************************************************************/
+/* <= 2.5.0 */
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,5,0) )
+#include <linux/rtnetlink.h>
+#undef pci_register_driver
+#define pci_register_driver pci_module_init
+
+/*
+ * Most of the dma compat code is copied/modifed from the 2.4.37
+ * /include/linux/libata-compat.h header file
+ */
+/* These definitions mirror those in pci.h, so they can be used
+ * interchangeably with their PCI_ counterparts */
+enum dma_data_direction {
+	DMA_BIDIRECTIONAL = 0,
+	DMA_TO_DEVICE = 1,
+	DMA_FROM_DEVICE = 2,
+	DMA_NONE = 3,
+};
+
+struct device {
+	struct pci_dev pdev;
+};
+
+static inline struct pci_dev *to_pci_dev (struct device *dev)
+{
+	return (struct pci_dev *) dev;
+}
+static inline struct device *pci_dev_to_dev(struct pci_dev *pdev)
+{
+	return (struct device *) pdev;
+}
+#define pdev_printk(lvl, pdev, fmt, args...) 	\
+	printk("%s %s: " fmt, lvl, pci_name(pdev), ## args)
+#define dev_err(dev, fmt, args...)            \
+	pdev_printk(KERN_ERR, to_pci_dev(dev), fmt, ## args)
+#define dev_info(dev, fmt, args...)            \
+	pdev_printk(KERN_INFO, to_pci_dev(dev), fmt, ## args)
+#define dev_warn(dev, fmt, args...)            \
+	pdev_printk(KERN_WARNING, to_pci_dev(dev), fmt, ## args)
+#define dev_notice(dev, fmt, args...)            \
+	pdev_printk(KERN_NOTICE, to_pci_dev(dev), fmt, ## args)
+#define dev_dbg(dev, fmt, args...) \
+	pdev_printk(KERN_DEBUG, to_pci_dev(dev), fmt, ## args)
+
+/* NOTE: dangerous! we ignore the 'gfp' argument */
+#define dma_alloc_coherent(dev,sz,dma,gfp) \
+	pci_alloc_consistent(to_pci_dev(dev),(sz),(dma))
+#define dma_free_coherent(dev,sz,addr,dma_addr) \
+	pci_free_consistent(to_pci_dev(dev),(sz),(addr),(dma_addr))
+
+#define dma_map_page(dev,a,b,c,d) \
+	pci_map_page(to_pci_dev(dev),(a),(b),(c),(d))
+#define dma_unmap_page(dev,a,b,c) \
+	pci_unmap_page(to_pci_dev(dev),(a),(b),(c))
+
+#define dma_map_single(dev,a,b,c) \
+	pci_map_single(to_pci_dev(dev),(a),(b),(c))
+#define dma_unmap_single(dev,a,b,c) \
+	pci_unmap_single(to_pci_dev(dev),(a),(b),(c))
+
+#define dma_map_sg(dev, sg, nents, dir) \
+	pci_map_sg(to_pci_dev(dev), (sg), (nents), (dir)
+#define dma_unmap_sg(dev, sg, nents, dir) \
+	pci_unmap_sg(to_pci_dev(dev), (sg), (nents), (dir)
+
+#define dma_sync_single(dev,a,b,c) \
+	pci_dma_sync_single(to_pci_dev(dev),(a),(b),(c))
+
+/* for range just sync everything, that's all the pci API can do */
+#define dma_sync_single_range(dev,addr,off,sz,dir) \
+	pci_dma_sync_single(to_pci_dev(dev),(addr),(off)+(sz),(dir))
+
+#define dma_set_mask(dev,mask) \
+	pci_set_dma_mask(to_pci_dev(dev),(mask))
+
+/* hlist_* code - double linked lists */
+struct hlist_head {
+	struct hlist_node *first;
+};
+
+struct hlist_node {
+	struct hlist_node *next, **pprev;
+};
+
+static inline void __hlist_del(struct hlist_node *n)
+{
+	struct hlist_node *next = n->next;
+	struct hlist_node **pprev = n->pprev;
+	*pprev = next;
+	if (next)
+	next->pprev = pprev;
+}
+
+static inline void hlist_del(struct hlist_node *n)
+{
+	__hlist_del(n);
+	n->next = NULL;
+	n->pprev = NULL;
+}
+
+static inline void hlist_add_head(struct hlist_node *n, struct hlist_head *h)
+{
+	struct hlist_node *first = h->first;
+	n->next = first;
+	if (first)
+		first->pprev = &n->next;
+	h->first = n;
+	n->pprev = &h->first;
+}
+
+static inline int hlist_empty(const struct hlist_head *h)
+{
+	return !h->first;
+}
+#define HLIST_HEAD_INIT { .first = NULL }
+#define HLIST_HEAD(name) struct hlist_head name = {  .first = NULL }
+#define INIT_HLIST_HEAD(ptr) ((ptr)->first = NULL)
+static inline void INIT_HLIST_NODE(struct hlist_node *h)
+{
+	h->next = NULL;
+	h->pprev = NULL;
+}
+
+#ifndef might_sleep
+#define might_sleep()
+#endif
+#else
+static inline struct device *pci_dev_to_dev(struct pci_dev *pdev)
+{
+	return &pdev->dev;
+}
+#endif /* <= 2.5.0 */
+
+/*****************************************************************************/
+/* 2.5.28 => 2.4.23 */
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,5,28) )
+
+#include <linux/tqueue.h>
+#define work_struct tq_struct
+#undef INIT_WORK
+#define INIT_WORK(a,b) INIT_TQUEUE(a,(void (*)(void *))b,a)
+#undef container_of
+#define container_of list_entry
+#define schedule_work schedule_task
+#define flush_scheduled_work flush_scheduled_tasks
+#define cancel_work_sync(x) flush_scheduled_work()
+
+#endif /* 2.5.28 => 2.4.17 */
+
+/*****************************************************************************/
+/* 2.6.0 => 2.5.28 */
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,0) )
+#ifndef read_barrier_depends
+#define read_barrier_depends() rmb()
+#endif
+
+#ifndef rcu_head
+struct __kc_callback_head {
+	struct __kc_callback_head *next;
+	void (*func)(struct callback_head *head);
+};
+#define rcu_head __kc_callback_head
+#endif
+
+#undef get_cpu
+#define get_cpu() smp_processor_id()
+#undef put_cpu
+#define put_cpu() do { } while(0)
+#define MODULE_INFO(version, _version)
+#ifndef CONFIG_E1000_DISABLE_PACKET_SPLIT
+#define CONFIG_E1000_DISABLE_PACKET_SPLIT 1
+#endif
+#ifndef CONFIG_IGB_DISABLE_PACKET_SPLIT
+#define CONFIG_IGB_DISABLE_PACKET_SPLIT 1
+#endif
+#ifndef CONFIG_IGC_DISABLE_PACKET_SPLIT
+#define CONFIG_IGC_DISABLE_PACKET_SPLIT 1
+#endif
+
+#define dma_set_coherent_mask(dev,mask) 1
+
+#undef dev_put
+#define dev_put(dev) __dev_put(dev)
+
+#ifndef skb_fill_page_desc
+#define skb_fill_page_desc _kc_skb_fill_page_desc
+void _kc_skb_fill_page_desc(struct sk_buff *skb, int i, struct page *page, int off, int size);
+#endif
+
+#undef ALIGN
+#define ALIGN(x,a) (((x)+(a)-1)&~((a)-1))
+
+#ifndef page_count
+#define page_count(p) atomic_read(&(p)->count)
+#endif
+
+#ifdef MAX_NUMNODES
+#undef MAX_NUMNODES
+#endif
+#define MAX_NUMNODES 1
+
+/* find_first_bit and find_next bit are not defined for most
+ * 2.4 kernels (except for the redhat 2.4.21 kernels
+ */
+#include <linux/bitops.h>
+#define BITOP_WORD(nr)          ((nr) / BITS_PER_LONG)
+#undef find_next_bit
+#define find_next_bit _kc_find_next_bit
+unsigned long _kc_find_next_bit(const unsigned long *addr, unsigned long size,
+				unsigned long offset);
+#define find_first_bit(addr, size) find_next_bit((addr), (size), 0)
+
+#ifndef netdev_name
+static inline const char *_kc_netdev_name(const struct net_device *dev)
+{
+	if (strchr(dev->name, '%'))
+		return "(unregistered net_device)";
+	return dev->name;
+}
+#define netdev_name(netdev)	_kc_netdev_name(netdev)
+#endif /* netdev_name */
+
+#ifndef strlcpy
+#define strlcpy _kc_strlcpy
+size_t _kc_strlcpy(char *dest, const char *src, size_t size);
+#endif /* strlcpy */
+
+#ifndef do_div
+#if BITS_PER_LONG == 64
+# define do_div(n,base) ({					\
+	uint32_t __base = (base);				\
+	uint32_t __rem;						\
+	__rem = ((uint64_t)(n)) % __base;			\
+	(n) = ((uint64_t)(n)) / __base;				\
+	__rem;							\
+ })
+#elif BITS_PER_LONG == 32
+uint32_t _kc__div64_32(uint64_t *dividend, uint32_t divisor);
+# define do_div(n,base) ({				\
+	uint32_t __base = (base);			\
+	uint32_t __rem;					\
+	if (likely(((n) >> 32) == 0)) {			\
+		__rem = (uint32_t)(n) % __base;		\
+		(n) = (uint32_t)(n) / __base;		\
+	} else 						\
+		__rem = _kc__div64_32(&(n), __base);	\
+	__rem;						\
+ })
+#else /* BITS_PER_LONG == ?? */
+# error do_div() does not yet support the C64
+#endif /* BITS_PER_LONG */
+#endif /* do_div */
+
+#ifndef NSEC_PER_SEC
+#define NSEC_PER_SEC	1000000000L
+#endif
+
+#undef HAVE_I2C_SUPPORT
+#else /* 2.6.0 */
+
+#endif /* 2.6.0 => 2.5.28 */
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,3) )
+#define dma_pool pci_pool
+#define dma_pool_destroy pci_pool_destroy
+#define dma_pool_alloc pci_pool_alloc
+#define dma_pool_free pci_pool_free
+
+#define dma_pool_create(name,dev,size,align,allocation) \
+       pci_pool_create((name),to_pci_dev(dev),(size),(align),(allocation))
+#endif /* < 2.6.3 */
+
+/*****************************************************************************/
+/* 2.6.4 => 2.6.0 */
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,4) )
+#define MODULE_VERSION(_version) MODULE_INFO(version, _version)
+#endif /* 2.6.4 => 2.6.0 */
+
+/*****************************************************************************/
+/* 2.6.5 => 2.6.0 */
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,5) )
+#define dma_sync_single_for_cpu		dma_sync_single
+#define dma_sync_single_for_device	dma_sync_single
+#define dma_sync_single_range_for_cpu		dma_sync_single_range
+#define dma_sync_single_range_for_device	dma_sync_single_range
+#ifndef pci_dma_mapping_error
+#define pci_dma_mapping_error _kc_pci_dma_mapping_error
+static inline int _kc_pci_dma_mapping_error(dma_addr_t dma_addr)
+{
+	return dma_addr == 0;
+}
+#endif
+#endif /* 2.6.5 => 2.6.0 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,4) )
+int _kc_scnprintf(char * buf, size_t size, const char *fmt, ...);
+#define scnprintf(buf, size, fmt, args...) _kc_scnprintf(buf, size, fmt, ##args)
+#endif /* < 2.6.4 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,6) )
+/* taken from 2.6 include/linux/bitmap.h */
+#undef bitmap_zero
+#define bitmap_zero _kc_bitmap_zero
+static inline void _kc_bitmap_zero(unsigned long *dst, int nbits)
+{
+        if (nbits <= BITS_PER_LONG)
+                *dst = 0UL;
+        else {
+                int len = BITS_TO_LONGS(nbits) * sizeof(unsigned long);
+                memset(dst, 0, len);
+        }
+}
+#define page_to_nid(x) 0
+
+#endif /* < 2.6.6 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,7) )
+#undef if_mii
+#define if_mii _kc_if_mii
+static inline struct mii_ioctl_data *_kc_if_mii(struct ifreq *rq)
+{
+	return (struct mii_ioctl_data *) &rq->ifr_ifru;
+}
+
+#ifndef __force
+#define __force
+#endif
+#endif /* < 2.6.7 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,8) )
+#ifndef PCI_EXP_DEVCTL
+#define PCI_EXP_DEVCTL 8
+#endif
+#ifndef PCI_EXP_DEVCTL_CERE
+#define PCI_EXP_DEVCTL_CERE 0x0001
+#endif
+#define PCI_EXP_FLAGS		2	/* Capabilities register */
+#define PCI_EXP_FLAGS_VERS	0x000f	/* Capability version */
+#define PCI_EXP_FLAGS_TYPE	0x00f0	/* Device/Port type */
+#define  PCI_EXP_TYPE_ENDPOINT	0x0	/* Express Endpoint */
+#define  PCI_EXP_TYPE_LEG_END	0x1	/* Legacy Endpoint */
+#define  PCI_EXP_TYPE_ROOT_PORT 0x4	/* Root Port */
+#define  PCI_EXP_TYPE_DOWNSTREAM 0x6	/* Downstream Port */
+#define PCI_EXP_FLAGS_SLOT	0x0100	/* Slot implemented */
+#define PCI_EXP_DEVCAP		4	/* Device capabilities */
+#define PCI_EXP_DEVSTA		10	/* Device Status */
+#define msleep(x)	do { set_current_state(TASK_UNINTERRUPTIBLE); \
+				schedule_timeout((x * HZ)/1000 + 2); \
+			} while (0)
+
+#endif /* < 2.6.8 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,9))
+#include <net/dsfield.h>
+#define __iomem
+
+#ifndef kcalloc
+#define kcalloc(n, size, flags) _kc_kzalloc(((n) * (size)), flags)
+void *_kc_kzalloc(size_t size, int flags);
+#endif
+#define MSEC_PER_SEC    1000L
+static inline unsigned int _kc_jiffies_to_msecs(const unsigned long j)
+{
+#if HZ <= MSEC_PER_SEC && !(MSEC_PER_SEC % HZ)
+	return (MSEC_PER_SEC / HZ) * j;
+#elif HZ > MSEC_PER_SEC && !(HZ % MSEC_PER_SEC)
+	return (j + (HZ / MSEC_PER_SEC) - 1)/(HZ / MSEC_PER_SEC);
+#else
+	return (j * MSEC_PER_SEC) / HZ;
+#endif
+}
+static inline unsigned long _kc_msecs_to_jiffies(const unsigned int m)
+{
+	if (m > _kc_jiffies_to_msecs(MAX_JIFFY_OFFSET))
+		return MAX_JIFFY_OFFSET;
+#if HZ <= MSEC_PER_SEC && !(MSEC_PER_SEC % HZ)
+	return (m + (MSEC_PER_SEC / HZ) - 1) / (MSEC_PER_SEC / HZ);
+#elif HZ > MSEC_PER_SEC && !(HZ % MSEC_PER_SEC)
+	return m * (HZ / MSEC_PER_SEC);
+#else
+	return (m * HZ + MSEC_PER_SEC - 1) / MSEC_PER_SEC;
+#endif
+}
+
+#define msleep_interruptible _kc_msleep_interruptible
+static inline unsigned long _kc_msleep_interruptible(unsigned int msecs)
+{
+	unsigned long timeout = _kc_msecs_to_jiffies(msecs) + 1;
+
+	while (timeout && !signal_pending(current)) {
+		__set_current_state(TASK_INTERRUPTIBLE);
+		timeout = schedule_timeout(timeout);
+	}
+	return _kc_jiffies_to_msecs(timeout);
+}
+
+/* Basic mode control register. */
+#define BMCR_SPEED1000		0x0040  /* MSB of Speed (1000)         */
+
+#ifndef __le16
+#define __le16 u16
+#endif
+#ifndef __le32
+#define __le32 u32
+#endif
+#ifndef __le64
+#define __le64 u64
+#endif
+#ifndef __be16
+#define __be16 u16
+#endif
+#ifndef __be32
+#define __be32 u32
+#endif
+#ifndef __be64
+#define __be64 u64
+#endif
+
+static inline struct vlan_ethhdr *vlan_eth_hdr(const struct sk_buff *skb)
+{
+	return (struct vlan_ethhdr *)skb->mac.raw;
+}
+
+/* Wake-On-Lan options. */
+#define WAKE_PHY		BIT(0)
+#define WAKE_UCAST		BIT(1)
+#define WAKE_MCAST		BIT(2)
+#define WAKE_BCAST		BIT(3)
+#define WAKE_ARP		BIT(4)
+#define WAKE_MAGIC		BIT(5)
+#define WAKE_MAGICSECURE	BIT(6) /* only meaningful if WAKE_MAGIC */
+
+#define skb_header_pointer _kc_skb_header_pointer
+static inline void *_kc_skb_header_pointer(const struct sk_buff *skb,
+					    int offset, int len, void *buffer)
+{
+	int hlen = skb_headlen(skb);
+
+	if (hlen - offset >= len)
+		return skb->data + offset;
+
+#ifdef MAX_SKB_FRAGS
+	if (skb_copy_bits(skb, offset, buffer, len) < 0)
+		return NULL;
+
+	return buffer;
+#else
+	return NULL;
+#endif
+
+#ifndef NETDEV_TX_OK
+#define NETDEV_TX_OK 0
+#endif
+#ifndef NETDEV_TX_BUSY
+#define NETDEV_TX_BUSY 1
+#endif
+#ifndef NETDEV_TX_LOCKED
+#define NETDEV_TX_LOCKED -1
+#endif
+}
+
+#ifndef __bitwise
+#define __bitwise
+#endif
+#endif /* < 2.6.9 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,10) )
+#ifdef module_param_array_named
+#undef module_param_array_named
+#define module_param_array_named(name, array, type, nump, perm)          \
+	static struct kparam_array __param_arr_##name                    \
+	= { ARRAY_SIZE(array), nump, param_set_##type, param_get_##type, \
+	    sizeof(array[0]), array };                                   \
+	module_param_call(name, param_array_set, param_array_get,        \
+			  &__param_arr_##name, perm)
+#endif /* module_param_array_named */
+/*
+ * num_online is broken for all < 2.6.10 kernels.  This is needed to support
+ * Node module parameter of ixgbe.
+ */
+#undef num_online_nodes
+#define num_online_nodes(n) 1
+extern DECLARE_BITMAP(_kcompat_node_online_map, MAX_NUMNODES);
+#undef node_online_map
+#define node_online_map _kcompat_node_online_map
+#define pci_get_class pci_find_class
+#endif /* < 2.6.10 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,11) )
+#define PCI_D0      0
+#define PCI_D1      1
+#define PCI_D2      2
+#define PCI_D3hot   3
+#define PCI_D3cold  4
+typedef int pci_power_t;
+#define pci_choose_state(pdev,state) state
+#define PMSG_SUSPEND 3
+#define PCI_EXP_LNKCTL	16
+
+#undef NETIF_F_LLTX
+
+#ifndef ARCH_HAS_PREFETCH
+#define prefetch(X)
+#endif
+
+#ifndef NET_IP_ALIGN
+#define NET_IP_ALIGN 2
+#endif
+
+#define KC_USEC_PER_SEC	1000000L
+#define usecs_to_jiffies _kc_usecs_to_jiffies
+static inline unsigned int _kc_jiffies_to_usecs(const unsigned long j)
+{
+#if HZ <= KC_USEC_PER_SEC && !(KC_USEC_PER_SEC % HZ)
+	return (KC_USEC_PER_SEC / HZ) * j;
+#elif HZ > KC_USEC_PER_SEC && !(HZ % KC_USEC_PER_SEC)
+	return (j + (HZ / KC_USEC_PER_SEC) - 1)/(HZ / KC_USEC_PER_SEC);
+#else
+	return (j * KC_USEC_PER_SEC) / HZ;
+#endif
+}
+static inline unsigned long _kc_usecs_to_jiffies(const unsigned int m)
+{
+	if (m > _kc_jiffies_to_usecs(MAX_JIFFY_OFFSET))
+		return MAX_JIFFY_OFFSET;
+#if HZ <= KC_USEC_PER_SEC && !(KC_USEC_PER_SEC % HZ)
+	return (m + (KC_USEC_PER_SEC / HZ) - 1) / (KC_USEC_PER_SEC / HZ);
+#elif HZ > KC_USEC_PER_SEC && !(HZ % KC_USEC_PER_SEC)
+	return m * (HZ / KC_USEC_PER_SEC);
+#else
+	return (m * HZ + KC_USEC_PER_SEC - 1) / KC_USEC_PER_SEC;
+#endif
+}
+
+#define PCI_EXP_LNKCAP		12	/* Link Capabilities */
+#define PCI_EXP_LNKSTA		18	/* Link Status */
+#define PCI_EXP_SLTCAP		20	/* Slot Capabilities */
+#define PCI_EXP_SLTCTL		24	/* Slot Control */
+#define PCI_EXP_SLTSTA		26	/* Slot Status */
+#define PCI_EXP_RTCTL		28	/* Root Control */
+#define PCI_EXP_RTCAP		30	/* Root Capabilities */
+#define PCI_EXP_RTSTA		32	/* Root Status */
+#endif /* < 2.6.11 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,12) )
+#include <linux/reboot.h>
+#define USE_REBOOT_NOTIFIER
+
+/* Generic MII registers. */
+#define MII_CTRL1000        0x09        /* 1000BASE-T control          */
+#define MII_STAT1000        0x0a        /* 1000BASE-T status           */
+/* Advertisement control register. */
+#define ADVERTISE_PAUSE_CAP     0x0400  /* Try for pause               */
+#define ADVERTISE_PAUSE_ASYM    0x0800  /* Try for asymmetric pause     */
+/* Link partner ability register. */
+#define LPA_PAUSE_CAP		0x0400	/* Can pause                   */
+#define LPA_PAUSE_ASYM		0x0800	/* Can pause asymetrically     */
+/* 1000BASE-T Control register */
+#define ADVERTISE_1000FULL      0x0200  /* Advertise 1000BASE-T full duplex */
+#define ADVERTISE_1000HALF	0x0100  /* Advertise 1000BASE-T half duplex */
+/* 1000BASE-T Status register */
+#define LPA_1000LOCALRXOK	0x2000	/* Link partner local receiver status */
+#define LPA_1000REMRXOK		0x1000	/* Link partner remote receiver status */
+
+#ifndef is_zero_ether_addr
+#define is_zero_ether_addr _kc_is_zero_ether_addr
+static inline int _kc_is_zero_ether_addr(const u8 *addr)
+{
+	return !(addr[0] | addr[1] | addr[2] | addr[3] | addr[4] | addr[5]);
+}
+#endif /* is_zero_ether_addr */
+#ifndef is_multicast_ether_addr
+#define is_multicast_ether_addr _kc_is_multicast_ether_addr
+static inline int _kc_is_multicast_ether_addr(const u8 *addr)
+{
+	return addr[0] & 0x01;
+}
+#endif /* is_multicast_ether_addr */
+#endif /* < 2.6.12 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,13) )
+#ifndef kstrdup
+#define kstrdup _kc_kstrdup
+char *_kc_kstrdup(const char *s, unsigned int gfp);
+#endif
+#endif /* < 2.6.13 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,14) )
+#define pm_message_t u32
+#ifndef kzalloc
+#define kzalloc _kc_kzalloc
+void *_kc_kzalloc(size_t size, int flags);
+#endif
+
+/* Generic MII registers. */
+#define MII_ESTATUS	    0x0f	/* Extended Status */
+/* Basic mode status register. */
+#define BMSR_ESTATEN		0x0100	/* Extended Status in R15 */
+/* Extended status register. */
+#define ESTATUS_1000_TFULL	0x2000	/* Can do 1000BT Full */
+#define ESTATUS_1000_THALF	0x1000	/* Can do 1000BT Half */
+
+#define SUPPORTED_Pause	        BIT(13)
+#define SUPPORTED_Asym_Pause	BIT(14)
+#define ADVERTISED_Pause	BIT(13)
+#define ADVERTISED_Asym_Pause	BIT(14)
+
+#if (!(RHEL_RELEASE_CODE && \
+       (RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(4,3)) && \
+       (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(5,0))))
+#if ((LINUX_VERSION_CODE == KERNEL_VERSION(2,6,9)) && !defined(gfp_t))
+#define gfp_t unsigned
+#else
+typedef unsigned gfp_t;
+#endif
+#endif /* !RHEL4.3->RHEL5.0 */
+
+#if ( LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,9) )
+#ifdef CONFIG_X86_64
+#define dma_sync_single_range_for_cpu(dev, addr, off, sz, dir)       \
+	dma_sync_single_for_cpu((dev), (addr), (off) + (sz), (dir))
+#define dma_sync_single_range_for_device(dev, addr, off, sz, dir)    \
+	dma_sync_single_for_device((dev), (addr), (off) + (sz), (dir))
+#endif
+#endif
+#endif /* < 2.6.14 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,15) )
+#ifndef kfree_rcu
+/* this is placed here due to a lack of rcu_barrier in previous kernels */
+#define kfree_rcu(_ptr, _offset) kfree(_ptr)
+#endif /* kfree_rcu */
+#ifndef vmalloc_node
+#define vmalloc_node(a,b) vmalloc(a)
+#endif /* vmalloc_node*/
+
+#define setup_timer(_timer, _function, _data) \
+do { \
+	(_timer)->function = _function; \
+	(_timer)->data = _data; \
+	init_timer(_timer); \
+} while (0)
+#ifndef device_can_wakeup
+#define device_can_wakeup(dev)	(1)
+#endif
+#ifndef device_set_wakeup_enable
+#define device_set_wakeup_enable(dev, val)	do{}while(0)
+#endif
+#ifndef device_init_wakeup
+#define device_init_wakeup(dev,val) do {} while (0)
+#endif
+static inline unsigned _kc_compare_ether_addr(const u8 *addr1, const u8 *addr2)
+{
+	const u16 *a = (const u16 *) addr1;
+	const u16 *b = (const u16 *) addr2;
+
+	return ((a[0] ^ b[0]) | (a[1] ^ b[1]) | (a[2] ^ b[2])) != 0;
+}
+#undef compare_ether_addr
+#define compare_ether_addr(addr1, addr2) _kc_compare_ether_addr(addr1, addr2)
+#endif /* < 2.6.15 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,16) )
+#undef DEFINE_MUTEX
+#define DEFINE_MUTEX(x)	DECLARE_MUTEX(x)
+#define mutex_lock(x)	down_interruptible(x)
+#define mutex_unlock(x)	up(x)
+
+#ifndef ____cacheline_internodealigned_in_smp
+#ifdef CONFIG_SMP
+#define ____cacheline_internodealigned_in_smp ____cacheline_aligned_in_smp
+#else
+#define ____cacheline_internodealigned_in_smp
+#endif /* CONFIG_SMP */
+#endif /* ____cacheline_internodealigned_in_smp */
+#undef HAVE_PCI_ERS
+#else /* 2.6.16 and above */
+#undef HAVE_PCI_ERS
+#define HAVE_PCI_ERS
+#if ( SLE_VERSION_CODE && SLE_VERSION_CODE == SLE_VERSION(10,4,0) )
+#ifdef device_can_wakeup
+#undef device_can_wakeup
+#endif /* device_can_wakeup */
+#define device_can_wakeup(dev) 1
+#endif /* SLE_VERSION(10,4,0) */
+#endif /* < 2.6.16 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,17) )
+#ifndef dev_notice
+#define dev_notice(dev, fmt, args...)            \
+	dev_printk(KERN_NOTICE, dev, fmt, ## args)
+#endif
+
+#ifndef first_online_node
+#define first_online_node 0
+#endif
+#ifndef NET_SKB_PAD
+#define NET_SKB_PAD 16
+#endif
+#endif /* < 2.6.17 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,18) )
+
+#ifndef IRQ_HANDLED
+#define irqreturn_t void
+#define IRQ_HANDLED
+#define IRQ_NONE
+#endif
+
+#ifndef IRQF_PROBE_SHARED
+#ifdef SA_PROBEIRQ
+#define IRQF_PROBE_SHARED SA_PROBEIRQ
+#else
+#define IRQF_PROBE_SHARED 0
+#endif
+#endif
+
+#ifndef IRQF_SHARED
+#define IRQF_SHARED SA_SHIRQ
+#endif
+
+#ifndef ARRAY_SIZE
+#define ARRAY_SIZE(x) (sizeof(x) / sizeof((x)[0]))
+#endif
+
+#ifndef skb_is_gso
+#ifdef NETIF_F_TSO
+#define skb_is_gso _kc_skb_is_gso
+static inline int _kc_skb_is_gso(const struct sk_buff *skb)
+{
+	return skb_shinfo(skb)->gso_size;
+}
+#else
+#define skb_is_gso(a) 0
+#endif
+#endif
+
+#ifndef resource_size_t
+#define resource_size_t unsigned long
+#endif
+
+#ifdef skb_pad
+#undef skb_pad
+#endif
+#define skb_pad(x,y) _kc_skb_pad(x, y)
+int _kc_skb_pad(struct sk_buff *skb, int pad);
+#ifdef skb_padto
+#undef skb_padto
+#endif
+#define skb_padto(x,y) _kc_skb_padto(x, y)
+static inline int _kc_skb_padto(struct sk_buff *skb, unsigned int len)
+{
+	unsigned int size = skb->len;
+	if(likely(size >= len))
+		return 0;
+	return _kc_skb_pad(skb, len - size);
+}
+
+#ifndef DECLARE_PCI_UNMAP_ADDR
+#define DECLARE_PCI_UNMAP_ADDR(ADDR_NAME) \
+	dma_addr_t ADDR_NAME
+#define DECLARE_PCI_UNMAP_LEN(LEN_NAME) \
+	u32 LEN_NAME
+#define pci_unmap_addr(PTR, ADDR_NAME) \
+	((PTR)->ADDR_NAME)
+#define pci_unmap_addr_set(PTR, ADDR_NAME, VAL) \
+	(((PTR)->ADDR_NAME) = (VAL))
+#define pci_unmap_len(PTR, LEN_NAME) \
+	((PTR)->LEN_NAME)
+#define pci_unmap_len_set(PTR, LEN_NAME, VAL) \
+	(((PTR)->LEN_NAME) = (VAL))
+#endif /* DECLARE_PCI_UNMAP_ADDR */
+#endif /* < 2.6.18 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,19) )
+enum pcie_link_width {
+	PCIE_LNK_WIDTH_RESRV    = 0x00,
+	PCIE_LNK_X1             = 0x01,
+	PCIE_LNK_X2             = 0x02,
+	PCIE_LNK_X4             = 0x04,
+	PCIE_LNK_X8             = 0x08,
+	PCIE_LNK_X12            = 0x0C,
+	PCIE_LNK_X16            = 0x10,
+	PCIE_LNK_X32            = 0x20,
+	PCIE_LNK_WIDTH_UNKNOWN  = 0xFF,
+};
+
+#if (!(RHEL_RELEASE_CODE && RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(5,0)))
+#define i_private u.generic_ip
+#endif /* >= RHEL 5.0 */
+
+#ifndef DIV_ROUND_UP
+#define DIV_ROUND_UP(n,d) (((n) + (d) - 1) / (d))
+#endif
+#ifndef __ALIGN_MASK
+#define __ALIGN_MASK(x, mask) (((x) + (mask)) & ~(mask))
+#endif
+#if ( LINUX_VERSION_CODE >= KERNEL_VERSION(2,5,0) )
+#if (!((RHEL_RELEASE_CODE && \
+        ((RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(4,4) && \
+          RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(5,0)) || \
+         (RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(5,0))))))
+typedef irqreturn_t (*irq_handler_t)(int, void*, struct pt_regs *);
+#endif
+#if (RHEL_RELEASE_CODE && RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(6,0))
+#undef CONFIG_INET_LRO
+#undef CONFIG_INET_LRO_MODULE
+#endif
+typedef irqreturn_t (*new_handler_t)(int, void*);
+static inline irqreturn_t _kc_request_irq(unsigned int irq, new_handler_t handler, unsigned long flags, const char *devname, void *dev_id)
+#else /* 2.4.x */
+typedef void (*irq_handler_t)(int, void*, struct pt_regs *);
+typedef void (*new_handler_t)(int, void*);
+static inline int _kc_request_irq(unsigned int irq, new_handler_t handler, unsigned long flags, const char *devname, void *dev_id)
+#endif /* >= 2.5.x */
+{
+	irq_handler_t new_handler = (irq_handler_t) handler;
+	return request_irq(irq, new_handler, flags, devname, dev_id);
+}
+
+#undef request_irq
+#define request_irq(irq, handler, flags, devname, dev_id) _kc_request_irq((irq), (handler), (flags), (devname), (dev_id))
+
+#define irq_handler_t new_handler_t
+
+#if ( LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,11) )
+#ifndef skb_checksum_help
+static inline int __kc_skb_checksum_help(struct sk_buff *skb)
+{
+	return skb_checksum_help(skb, 0);
+}
+#define skb_checksum_help(skb) __kc_skb_checksum_help((skb))
+#endif
+#endif /* < 2.6.19 && >= 2.6.11 */
+
+/* pci_restore_state and pci_save_state handles MSI/PCIE from 2.6.19 */
+#if (!(RHEL_RELEASE_CODE && RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(5,4)))
+#define PCIE_CONFIG_SPACE_LEN 256
+#define PCI_CONFIG_SPACE_LEN 64
+#define PCIE_LINK_STATUS 0x12
+#define pci_config_space_ich8lan() do {} while(0)
+#undef pci_save_state
+int _kc_pci_save_state(struct pci_dev *);
+#define pci_save_state(pdev) _kc_pci_save_state(pdev)
+#undef pci_restore_state
+void _kc_pci_restore_state(struct pci_dev *);
+#define pci_restore_state(pdev) _kc_pci_restore_state(pdev)
+#endif /* !(RHEL_RELEASE_CODE >= RHEL 5.4) */
+
+#ifdef HAVE_PCI_ERS
+#undef free_netdev
+void _kc_free_netdev(struct net_device *);
+#define free_netdev(netdev) _kc_free_netdev(netdev)
+#endif
+static inline int pci_enable_pcie_error_reporting(struct pci_dev __always_unused *dev)
+{
+	return 0;
+}
+#define pci_disable_pcie_error_reporting(dev) do {} while (0)
+#define pci_cleanup_aer_uncorrect_error_status(dev) do {} while (0)
+
+void *_kc_kmemdup(const void *src, size_t len, unsigned gfp);
+#define kmemdup(src, len, gfp) _kc_kmemdup(src, len, gfp)
+#ifndef bool
+#define bool _Bool
+#define true 1
+#define false 0
+#endif
+#else /* 2.6.19 */
+#include <linux/aer.h>
+#include <linux/pci_hotplug.h>
+
+#define NEW_SKB_CSUM_HELP
+#endif /* < 2.6.19 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,20) )
+#if ( LINUX_VERSION_CODE >= KERNEL_VERSION(2,5,28) )
+#undef INIT_WORK
+#define INIT_WORK(_work, _func) \
+do { \
+	INIT_LIST_HEAD(&(_work)->entry); \
+	(_work)->pending = 0; \
+	(_work)->func = (void (*)(void *))_func; \
+	(_work)->data = _work; \
+	init_timer(&(_work)->timer); \
+} while (0)
+#endif
+
+#ifndef PCI_VDEVICE
+#define PCI_VDEVICE(ven, dev)        \
+	PCI_VENDOR_ID_##ven, (dev),  \
+	PCI_ANY_ID, PCI_ANY_ID, 0, 0
+#endif
+
+#ifndef PCI_VENDOR_ID_INTEL
+#define PCI_VENDOR_ID_INTEL 0x8086
+#endif
+
+#ifndef round_jiffies
+#define round_jiffies(x) x
+#endif
+
+#define csum_offset csum
+
+#define HAVE_EARLY_VMALLOC_NODE
+#define dev_to_node(dev) -1
+#undef set_dev_node
+/* remove compiler warning with b=b, for unused variable */
+#define set_dev_node(a, b) do { (b) = (b); } while(0)
+
+#if (!(RHEL_RELEASE_CODE && \
+       (((RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(4,7)) && \
+         (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(5,0))) || \
+        (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(5,6)))) && \
+     !(SLE_VERSION_CODE && SLE_VERSION_CODE >= SLE_VERSION(10,2,0)))
+typedef __u16 __bitwise __sum16;
+typedef __u32 __bitwise __wsum;
+#endif
+
+#if (!(RHEL_RELEASE_CODE && \
+       (((RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(4,7)) && \
+         (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(5,0))) || \
+        (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(5,4)))) && \
+     !(SLE_VERSION_CODE && SLE_VERSION_CODE >= SLE_VERSION(10,2,0)))
+static inline __wsum csum_unfold(__sum16 n)
+{
+	return (__force __wsum)n;
+}
+#endif
+
+#else /* < 2.6.20 */
+#define HAVE_DEVICE_NUMA_NODE
+#endif /* < 2.6.20 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,21) )
+#define to_net_dev(class) container_of(class, struct net_device, class_dev)
+#define NETDEV_CLASS_DEV
+#if (!(RHEL_RELEASE_CODE && RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(5,5)))
+#define vlan_group_get_device(vg, id) (vg->vlan_devices[id])
+#define vlan_group_set_device(vg, id, dev)		\
+	do {						\
+		if (vg) vg->vlan_devices[id] = dev;	\
+	} while (0)
+#endif /* !(RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(5,5)) */
+#define pci_channel_offline(pdev) (pdev->error_state && \
+	pdev->error_state != pci_channel_io_normal)
+#define pci_request_selected_regions(pdev, bars, name) \
+        pci_request_regions(pdev, name)
+#define pci_release_selected_regions(pdev, bars) pci_release_regions(pdev);
+
+#ifndef __aligned
+#define __aligned(x)			__attribute__((aligned(x)))
+#endif
+
+struct pci_dev *_kc_netdev_to_pdev(struct net_device *netdev);
+#define netdev_to_dev(netdev)	\
+	pci_dev_to_dev(_kc_netdev_to_pdev(netdev))
+#define devm_kzalloc(dev, size, flags) kzalloc(size, flags)
+#define devm_kfree(dev, p) kfree(p)
+#else /* 2.6.21 */
+static inline struct device *netdev_to_dev(struct net_device *netdev)
+{
+	return &netdev->dev;
+}
+
+#endif /* < 2.6.21 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,22) )
+#define tcp_hdr(skb) (skb->h.th)
+#define tcp_hdrlen(skb) (skb->h.th->doff << 2)
+#define skb_transport_offset(skb) (skb->h.raw - skb->data)
+#define skb_transport_header(skb) (skb->h.raw)
+#define ipv6_hdr(skb) (skb->nh.ipv6h)
+#define ip_hdr(skb) (skb->nh.iph)
+#define skb_network_offset(skb) (skb->nh.raw - skb->data)
+#define skb_network_header(skb) (skb->nh.raw)
+#define skb_tail_pointer(skb) skb->tail
+#define skb_reset_tail_pointer(skb) \
+	do { \
+		skb->tail = skb->data; \
+	} while (0)
+#define skb_set_tail_pointer(skb, offset) \
+	do { \
+		skb->tail = skb->data + offset; \
+	} while (0)
+#define skb_copy_to_linear_data(skb, from, len) \
+				memcpy(skb->data, from, len)
+#define skb_copy_to_linear_data_offset(skb, offset, from, len) \
+				memcpy(skb->data + offset, from, len)
+#define skb_network_header_len(skb) (skb->h.raw - skb->nh.raw)
+#define pci_register_driver pci_module_init
+#define skb_mac_header(skb) skb->mac.raw
+
+#ifdef NETIF_F_MULTI_QUEUE
+#ifndef alloc_etherdev_mq
+#define alloc_etherdev_mq(_a, _b) alloc_etherdev(_a)
+#endif
+#endif /* NETIF_F_MULTI_QUEUE */
+
+#ifndef ETH_FCS_LEN
+#define ETH_FCS_LEN 4
+#endif
+#define cancel_work_sync(x) flush_scheduled_work()
+#ifndef udp_hdr
+#define udp_hdr _udp_hdr
+static inline struct udphdr *_udp_hdr(const struct sk_buff *skb)
+{
+	return (struct udphdr *)skb_transport_header(skb);
+}
+#endif
+
+#ifdef cpu_to_be16
+#undef cpu_to_be16
+#endif
+#define cpu_to_be16(x) __constant_htons(x)
+
+#if (!(RHEL_RELEASE_CODE && RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(5,1)))
+enum {
+	DUMP_PREFIX_NONE,
+	DUMP_PREFIX_ADDRESS,
+	DUMP_PREFIX_OFFSET
+};
+#endif /* !(RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(5,1)) */
+#ifndef hex_asc
+#define hex_asc(x)	"0123456789abcdef"[x]
+#endif
+#include <linux/ctype.h>
+void _kc_print_hex_dump(const char *level, const char *prefix_str,
+			int prefix_type, int rowsize, int groupsize,
+			const void *buf, size_t len, bool ascii);
+#define print_hex_dump(lvl, s, t, r, g, b, l, a) \
+		_kc_print_hex_dump(lvl, s, t, r, g, b, l, a)
+#ifndef ADVERTISED_2500baseX_Full
+#define ADVERTISED_2500baseX_Full BIT(15)
+#endif
+#ifndef SUPPORTED_2500baseX_Full
+#define SUPPORTED_2500baseX_Full BIT(15)
+#endif
+
+#ifndef ETH_P_PAUSE
+#define ETH_P_PAUSE 0x8808
+#endif
+
+static inline int compound_order(struct page *page)
+{
+	return 0;
+}
+
+#define __must_be_array(a) 0
+
+#ifndef SKB_WITH_OVERHEAD
+#define SKB_WITH_OVERHEAD(X) \
+	((X) - SKB_DATA_ALIGN(sizeof(struct skb_shared_info)))
+#endif
+#else /* 2.6.22 */
+#define ETH_TYPE_TRANS_SETS_DEV
+#define HAVE_NETDEV_STATS_IN_NETDEV
+#endif /* < 2.6.22 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE > KERNEL_VERSION(2,6,22) )
+#endif /* > 2.6.22 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,23) )
+#define netif_subqueue_stopped(_a, _b) 0
+#ifndef PTR_ALIGN
+#define PTR_ALIGN(p, a)         ((typeof(p))ALIGN((unsigned long)(p), (a)))
+#endif
+
+#ifndef CONFIG_PM_SLEEP
+#define CONFIG_PM_SLEEP	CONFIG_PM
+#endif
+
+#if ( LINUX_VERSION_CODE > KERNEL_VERSION(2,6,13) )
+#define HAVE_ETHTOOL_GET_PERM_ADDR
+#endif /* 2.6.14 through 2.6.22 */
+
+static inline int __kc_skb_cow_head(struct sk_buff *skb, unsigned int headroom)
+{
+	int delta = 0;
+
+	if (headroom > (skb->data - skb->head))
+		delta = headroom - (skb->data - skb->head);
+
+	if (delta || skb_header_cloned(skb))
+		return pskb_expand_head(skb, ALIGN(delta, NET_SKB_PAD), 0,
+					GFP_ATOMIC);
+	return 0;
+}
+#define skb_cow_head(s, h) __kc_skb_cow_head((s), (h))
+#endif /* < 2.6.23 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,24) )
+#ifndef ETH_FLAG_LRO
+#define ETH_FLAG_LRO NETIF_F_LRO
+#endif
+
+#ifndef ACCESS_ONCE
+#define ACCESS_ONCE(x) (*(volatile typeof(x) *)&(x))
+#endif
+
+/* if GRO is supported then the napi struct must already exist */
+#ifndef NETIF_F_GRO
+/* NAPI API changes in 2.6.24 break everything */
+struct napi_struct {
+	/* used to look up the real NAPI polling routine */
+	int (*poll)(struct napi_struct *, int);
+	struct net_device *dev;
+	int weight;
+};
+#endif
+
+#ifdef NAPI
+int __kc_adapter_clean(struct net_device *, int *);
+/* The following definitions are multi-queue aware, and thus we have a driver
+ * define list which determines which drivers support multiple queues, and
+ * thus need these stronger defines. If a driver does not support multi-queue
+ * functionality, you don't need to add it to this list.
+ */
+struct net_device *napi_to_poll_dev(const struct napi_struct *napi);
+
+static inline void __kc_mq_netif_napi_add(struct net_device *dev, struct napi_struct *napi,
+					  int (*poll)(struct napi_struct *, int), int weight)
+{
+	struct net_device *poll_dev = napi_to_poll_dev(napi);
+	poll_dev->poll = __kc_adapter_clean;
+	poll_dev->priv = napi;
+	poll_dev->weight = weight;
+	set_bit(__LINK_STATE_RX_SCHED, &poll_dev->state);
+	set_bit(__LINK_STATE_START, &poll_dev->state);
+	dev_hold(poll_dev);
+	napi->poll = poll;
+	napi->weight = weight;
+	napi->dev = dev;
+}
+#define netif_napi_add __kc_mq_netif_napi_add
+
+static inline void __kc_mq_netif_napi_del(struct napi_struct *napi)
+{
+	struct net_device *poll_dev = napi_to_poll_dev(napi);
+	WARN_ON(!test_bit(__LINK_STATE_RX_SCHED, &poll_dev->state));
+	dev_put(poll_dev);
+	memset(poll_dev, 0, sizeof(struct net_device));
+}
+
+#define netif_napi_del __kc_mq_netif_napi_del
+
+static inline bool __kc_mq_napi_schedule_prep(struct napi_struct *napi)
+{
+	return netif_running(napi->dev) &&
+		netif_rx_schedule_prep(napi_to_poll_dev(napi));
+}
+#define napi_schedule_prep __kc_mq_napi_schedule_prep
+
+static inline void __kc_mq_napi_schedule(struct napi_struct *napi)
+{
+	if (napi_schedule_prep(napi))
+		__netif_rx_schedule(napi_to_poll_dev(napi));
+}
+#define napi_schedule __kc_mq_napi_schedule
+
+#define napi_enable(_napi) netif_poll_enable(napi_to_poll_dev(_napi))
+#define napi_disable(_napi) netif_poll_disable(napi_to_poll_dev(_napi))
+#ifdef CONFIG_SMP
+static inline void napi_synchronize(const struct napi_struct *n)
+{
+	struct net_device *dev = napi_to_poll_dev(n);
+
+	while (test_bit(__LINK_STATE_RX_SCHED, &dev->state)) {
+		/* No hurry. */
+		msleep(1);
+	}
+}
+#else
+#define napi_synchronize(n)	barrier()
+#endif /* CONFIG_SMP */
+#define __napi_schedule(_napi) __netif_rx_schedule(napi_to_poll_dev(_napi))
+static inline void _kc_napi_complete(struct napi_struct *napi)
+{
+#ifdef NETIF_F_GRO
+	napi_gro_flush(napi);
+#endif
+	netif_rx_complete(napi_to_poll_dev(napi));
+}
+#define napi_complete _kc_napi_complete
+#else /* NAPI */
+
+/* The following definitions are only used if we don't support NAPI at all. */
+
+static inline __kc_netif_napi_add(struct net_device *dev, struct napi_struct *napi,
+				  int (*poll)(struct napi_struct *, int), int weight)
+{
+	dev->poll = poll;
+	dev->weight = weight;
+	napi->poll = poll;
+	napi->weight = weight;
+	napi->dev = dev;
+}
+#define netif_napi_del(_a) do {} while (0)
+#endif /* NAPI */
+
+#undef dev_get_by_name
+#define dev_get_by_name(_a, _b) dev_get_by_name(_b)
+#define __netif_subqueue_stopped(_a, _b) netif_subqueue_stopped(_a, _b)
+#ifndef DMA_BIT_MASK
+#define DMA_BIT_MASK(n)	(((n) == 64) ? DMA_64BIT_MASK : ((1ULL<<(n))-1))
+#endif
+
+#ifdef NETIF_F_TSO6
+#define skb_is_gso_v6 _kc_skb_is_gso_v6
+static inline int _kc_skb_is_gso_v6(const struct sk_buff *skb)
+{
+	return skb_shinfo(skb)->gso_type & SKB_GSO_TCPV6;
+}
+#endif /* NETIF_F_TSO6 */
+
+#ifndef KERN_CONT
+#define KERN_CONT	""
+#endif
+#ifndef pr_err
+#define pr_err(fmt, arg...) \
+	printk(KERN_ERR fmt, ##arg)
+#endif
+
+#ifndef rounddown_pow_of_two
+#define rounddown_pow_of_two(n) \
+	__builtin_constant_p(n) ? ( \
+		(n == 1) ? 0 : \
+		(1UL << ilog2(n))) : \
+		(1UL << (fls_long(n) - 1))
+#endif
+
+#else /* < 2.6.24 */
+#define HAVE_ETHTOOL_GET_SSET_COUNT
+#define HAVE_NETDEV_NAPI_LIST
+#endif /* < 2.6.24 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE > KERNEL_VERSION(2,6,24) )
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,2,0) )
+#define INCLUDE_PM_QOS_PARAMS_H
+#include <linux/pm_qos_params.h>
+#else /* >= 3.2.0 */
+#include <linux/pm_qos.h>
+#endif /* else >= 3.2.0 */
+#endif /* > 2.6.24 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,25) )
+#define PM_QOS_CPU_DMA_LATENCY	1
+
+#if ( LINUX_VERSION_CODE > KERNEL_VERSION(2,6,18) )
+#include <linux/latency.h>
+#define PM_QOS_DEFAULT_VALUE	INFINITE_LATENCY
+#define pm_qos_add_requirement(pm_qos_class, name, value) \
+		set_acceptable_latency(name, value)
+#define pm_qos_remove_requirement(pm_qos_class, name) \
+		remove_acceptable_latency(name)
+#define pm_qos_update_requirement(pm_qos_class, name, value) \
+		modify_acceptable_latency(name, value)
+#else
+#define PM_QOS_DEFAULT_VALUE	-1
+#define pm_qos_add_requirement(pm_qos_class, name, value)
+#define pm_qos_remove_requirement(pm_qos_class, name)
+#define pm_qos_update_requirement(pm_qos_class, name, value) { \
+	if (value != PM_QOS_DEFAULT_VALUE) { \
+		printk(KERN_WARNING "%s: unable to set PM QoS requirement\n", \
+			pci_name(adapter->pdev)); \
+	} \
+}
+
+#endif /* > 2.6.18 */
+
+#define pci_enable_device_mem(pdev) pci_enable_device(pdev)
+
+#ifndef DEFINE_PCI_DEVICE_TABLE
+#define DEFINE_PCI_DEVICE_TABLE(_table) struct pci_device_id _table[]
+#endif /* DEFINE_PCI_DEVICE_TABLE */
+
+#ifndef strict_strtol
+#define strict_strtol(s, b, r) _kc_strict_strtol(s, b, r)
+static inline int _kc_strict_strtol(const char *buf, unsigned int base, long *res)
+{
+	/* adapted from strict_strtoul() in 2.6.25 */
+	char *tail;
+	long val;
+	size_t len;
+
+	*res = 0;
+	len = strlen(buf);
+	if (!len)
+		return -EINVAL;
+	val = simple_strtol(buf, &tail, base);
+	if (tail == buf)
+		return -EINVAL;
+	if ((*tail == '\0') ||
+	    ((len == (size_t)(tail - buf) + 1) && (*tail == '\n'))) {
+		*res = val;
+		return 0;
+	}
+
+	return -EINVAL;
+}
+#endif
+
+#else /* < 2.6.25 */
+
+#endif /* < 2.6.25 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26) )
+#ifndef clamp_t
+#define clamp_t(type, val, min, max) ({		\
+	type __val = (val);			\
+	type __min = (min);			\
+	type __max = (max);			\
+	__val = __val < __min ? __min : __val;	\
+	__val > __max ? __max : __val; })
+#endif /* clamp_t */
+#undef kzalloc_node
+#define kzalloc_node(_size, _flags, _node) kzalloc(_size, _flags)
+
+void _kc_pci_disable_link_state(struct pci_dev *dev, int state);
+#define pci_disable_link_state(p, s) _kc_pci_disable_link_state(p, s)
+#else /* < 2.6.26 */
+#define NETDEV_CAN_SET_GSO_MAX_SIZE
+#ifdef HAVE_PCI_ASPM_H
+#include <linux/pci-aspm.h>
+#endif
+#define HAVE_NETDEV_VLAN_FEATURES
+#ifndef PCI_EXP_LNKCAP_ASPMS
+#define PCI_EXP_LNKCAP_ASPMS 0x00000c00 /* ASPM Support */
+#endif /* PCI_EXP_LNKCAP_ASPMS */
+#endif /* < 2.6.26 */
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,27) )
+static inline void _kc_ethtool_cmd_speed_set(struct ethtool_cmd *ep,
+					     __u32 speed)
+{
+	ep->speed = (__u16)speed;
+	/* ep->speed_hi = (__u16)(speed >> 16); */
+}
+#define ethtool_cmd_speed_set _kc_ethtool_cmd_speed_set
+
+static inline __u32 _kc_ethtool_cmd_speed(struct ethtool_cmd *ep)
+{
+	/* no speed_hi before 2.6.27, and probably no need for it yet */
+	return (__u32)ep->speed;
+}
+#define ethtool_cmd_speed _kc_ethtool_cmd_speed
+
+#if ( LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,15) )
+#if ((LINUX_VERSION_CODE < KERNEL_VERSION(2,6,23)) && defined(CONFIG_PM))
+#define ANCIENT_PM 1
+#elif ((LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,23)) && \
+       (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)) && \
+       defined(CONFIG_PM_SLEEP))
+#define NEWER_PM 1
+#endif
+#if defined(ANCIENT_PM) || defined(NEWER_PM)
+#undef device_set_wakeup_enable
+#define device_set_wakeup_enable(dev, val) \
+	do { \
+		u16 pmc = 0; \
+		int pm = pci_find_capability(adapter->pdev, PCI_CAP_ID_PM); \
+		if (pm) { \
+			pci_read_config_word(adapter->pdev, pm + PCI_PM_PMC, \
+				&pmc); \
+		} \
+		(dev)->power.can_wakeup = !!(pmc >> 11); \
+		(dev)->power.should_wakeup = (val && (pmc >> 11)); \
+	} while (0)
+#endif /* 2.6.15-2.6.22 and CONFIG_PM or 2.6.23-2.6.25 and CONFIG_PM_SLEEP */
+#endif /* 2.6.15 through 2.6.27 */
+#ifndef netif_napi_del
+#define netif_napi_del(_a) do {} while (0)
+#ifdef NAPI
+#ifdef CONFIG_NETPOLL
+#undef netif_napi_del
+#define netif_napi_del(_a) list_del(&(_a)->dev_list);
+#endif
+#endif
+#endif /* netif_napi_del */
+#ifdef dma_mapping_error
+#undef dma_mapping_error
+#endif
+#define dma_mapping_error(dev, dma_addr) pci_dma_mapping_error(dma_addr)
+
+#ifdef CONFIG_NETDEVICES_MULTIQUEUE
+#define HAVE_TX_MQ
+#endif
+
+#ifndef DMA_ATTR_WEAK_ORDERING
+#define DMA_ATTR_WEAK_ORDERING 0
+#endif
+
+#ifdef HAVE_TX_MQ
+void _kc_netif_tx_stop_all_queues(struct net_device *);
+void _kc_netif_tx_wake_all_queues(struct net_device *);
+void _kc_netif_tx_start_all_queues(struct net_device *);
+#define netif_tx_stop_all_queues(a) _kc_netif_tx_stop_all_queues(a)
+#define netif_tx_wake_all_queues(a) _kc_netif_tx_wake_all_queues(a)
+#define netif_tx_start_all_queues(a) _kc_netif_tx_start_all_queues(a)
+#undef netif_stop_subqueue
+#define netif_stop_subqueue(_ndev,_qi) do { \
+	if (netif_is_multiqueue((_ndev))) \
+		netif_stop_subqueue((_ndev), (_qi)); \
+	else \
+		netif_stop_queue((_ndev)); \
+	} while (0)
+#undef netif_start_subqueue
+#define netif_start_subqueue(_ndev,_qi) do { \
+	if (netif_is_multiqueue((_ndev))) \
+		netif_start_subqueue((_ndev), (_qi)); \
+	else \
+		netif_start_queue((_ndev)); \
+	} while (0)
+#else /* HAVE_TX_MQ */
+#define netif_tx_stop_all_queues(a) netif_stop_queue(a)
+#define netif_tx_wake_all_queues(a) netif_wake_queue(a)
+#if ( LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,12) )
+#define netif_tx_start_all_queues(a) netif_start_queue(a)
+#else
+#define netif_tx_start_all_queues(a) do {} while (0)
+#endif
+#define netif_stop_subqueue(_ndev,_qi) netif_stop_queue((_ndev))
+#define netif_start_subqueue(_ndev,_qi) netif_start_queue((_ndev))
+#endif /* HAVE_TX_MQ */
+#ifndef NETIF_F_MULTI_QUEUE
+#define NETIF_F_MULTI_QUEUE 0
+#define netif_is_multiqueue(a) 0
+#define netif_wake_subqueue(a, b)
+#endif /* NETIF_F_MULTI_QUEUE */
+
+#ifndef __WARN_printf
+void __kc_warn_slowpath(const char *file, const int line,
+		const char *fmt, ...) __attribute__((format(printf, 3, 4)));
+#define __WARN_printf(arg...) __kc_warn_slowpath(__FILE__, __LINE__, arg)
+#endif /* __WARN_printf */
+
+#ifndef WARN
+#define WARN(condition, format...) ({						\
+	int __ret_warn_on = !!(condition);				\
+	if (unlikely(__ret_warn_on))					\
+		__WARN_printf(format);					\
+	unlikely(__ret_warn_on);					\
+})
+#endif /* WARN */
+#undef HAVE_IXGBE_DEBUG_FS
+#undef HAVE_IGB_DEBUG_FS
+#define qdisc_reset_all_tx(a)
+#else /* < 2.6.27 */
+#include <net/sch_generic.h>
+#define ethtool_cmd_speed_set _kc_ethtool_cmd_speed_set
+static inline void _kc_ethtool_cmd_speed_set(struct ethtool_cmd *ep,
+					     __u32 speed)
+{
+	ep->speed = (__u16)(speed & 0xFFFF);
+	ep->speed_hi = (__u16)(speed >> 16);
+}
+#define HAVE_TX_MQ
+#define HAVE_NETDEV_SELECT_QUEUE
+#ifdef CONFIG_DEBUG_FS
+#define HAVE_IXGBE_DEBUG_FS
+#define HAVE_IGB_DEBUG_FS
+#endif /* CONFIG_DEBUG_FS */
+#endif /* < 2.6.27 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,28) )
+#define pci_ioremap_bar(pdev, bar)	ioremap(pci_resource_start(pdev, bar), \
+					        pci_resource_len(pdev, bar))
+#define pci_wake_from_d3 _kc_pci_wake_from_d3
+#define pci_prepare_to_sleep _kc_pci_prepare_to_sleep
+int _kc_pci_wake_from_d3(struct pci_dev *dev, bool enable);
+int _kc_pci_prepare_to_sleep(struct pci_dev *dev);
+#define netdev_alloc_page(a) alloc_page(GFP_ATOMIC)
+#ifndef __skb_queue_head_init
+static inline void __kc_skb_queue_head_init(struct sk_buff_head *list)
+{
+	list->prev = list->next = (struct sk_buff *)list;
+	list->qlen = 0;
+}
+#define __skb_queue_head_init(_q) __kc_skb_queue_head_init(_q)
+#endif
+
+#define PCI_EXP_DEVCAP2		36	/* Device Capabilities 2 */
+#define PCI_EXP_DEVCTL2		40	/* Device Control 2 */
+
+#define PCI_EXP_DEVCAP_FLR	0x10000000 /* Function Level Reset */
+#define PCI_EXP_DEVCTL_BCR_FLR	0x8000 /* Bridge Configuration Retry / FLR */
+
+#endif /* < 2.6.28 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,29) )
+#ifndef swap
+#define swap(a, b) \
+	do { typeof(a) __tmp = (a); (a) = (b); (b) = __tmp; } while (0)
+#endif
+#define pci_request_selected_regions_exclusive(pdev, bars, name) \
+		pci_request_selected_regions(pdev, bars, name)
+#ifndef CONFIG_NR_CPUS
+#define CONFIG_NR_CPUS 1
+#endif /* CONFIG_NR_CPUS */
+#ifndef pcie_aspm_enabled
+#define pcie_aspm_enabled()   (1)
+#endif /* pcie_aspm_enabled */
+
+#define  PCI_EXP_SLTSTA_PDS	0x0040	/* Presence Detect State */
+
+#ifndef PCI_EXP_LNKSTA_CLS
+#define  PCI_EXP_LNKSTA_CLS    0x000f  /* Current Link Speed */
+#endif
+#ifndef PCI_EXP_LNKSTA_NLW
+#define  PCI_EXP_LNKSTA_NLW    0x03f0  /* Negotiated Link Width */
+#endif
+
+#ifndef pci_clear_master
+void _kc_pci_clear_master(struct pci_dev *dev);
+#define pci_clear_master(dev)	_kc_pci_clear_master(dev)
+#endif
+
+#ifndef PCI_EXP_LNKCTL_ASPMC
+#define  PCI_EXP_LNKCTL_ASPMC	0x0003	/* ASPM Control */
+#endif
+
+#ifndef PCI_EXP_LNKCAP_MLW
+#define PCI_EXP_LNKCAP_MLW	0x000003f0 /* Maximum Link Width */
+#endif
+
+#else /* < 2.6.29 */
+#ifndef HAVE_NET_DEVICE_OPS
+#define HAVE_NET_DEVICE_OPS
+#endif
+#ifdef CONFIG_DCB
+#define HAVE_PFC_MODE_ENABLE
+#endif /* CONFIG_DCB */
+#endif /* < 2.6.29 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,30) )
+#define NO_PTP_SUPPORT
+#define skb_rx_queue_recorded(a) false
+#define skb_get_rx_queue(a) 0
+#define skb_record_rx_queue(a, b) do {} while (0)
+#define skb_tx_hash(n, s) ___kc_skb_tx_hash((n), (s), (n)->real_num_tx_queues)
+#ifndef CONFIG_PCI_IOV
+#undef pci_enable_sriov
+#define pci_enable_sriov(a, b) -ENOTSUPP
+#undef pci_disable_sriov
+#define pci_disable_sriov(a) do {} while (0)
+#endif /* CONFIG_PCI_IOV */
+#ifndef pr_cont
+#define pr_cont(fmt, ...) \
+	printk(KERN_CONT fmt, ##__VA_ARGS__)
+#endif /* pr_cont */
+static inline void _kc_synchronize_irq(unsigned int a)
+{
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,5,28) )
+	synchronize_irq();
+#else /* < 2.5.28 */
+	synchronize_irq(a);
+#endif /* < 2.5.28 */
+}
+#undef synchronize_irq
+#define synchronize_irq(a) _kc_synchronize_irq(a)
+
+#define PCI_EXP_LNKCTL2		48	/* Link Control 2 */
+
+#ifdef nr_cpus_node
+#undef nr_cpus_node
+#define nr_cpus_node(node) cpumask_weight(cpumask_of_node(node))
+#endif
+
+#if (RHEL_RELEASE_CODE && RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(5,5))
+#define HAVE_PCI_DEV_IS_VIRTFN_BIT
+#endif /* RHEL >= 5.5 */
+
+#if (!(RHEL_RELEASE_CODE && RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(5,5)))
+static inline bool pci_is_root_bus(struct pci_bus *pbus)
+{
+	return !(pbus->parent);
+}
+#endif
+
+#else /* < 2.6.30 */
+#define HAVE_ASPM_QUIRKS
+#define HAVE_PCI_DEV_IS_VIRTFN_BIT
+#endif /* < 2.6.30 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,31) )
+#define ETH_P_1588 0x88F7
+#define ETH_P_FIP  0x8914
+#ifndef netdev_uc_count
+#define netdev_uc_count(dev) ((dev)->uc_count)
+#endif
+#ifndef netdev_for_each_uc_addr
+#define netdev_for_each_uc_addr(uclist, dev) \
+	for (uclist = dev->uc_list; uclist; uclist = uclist->next)
+#endif
+#ifndef PORT_OTHER
+#define PORT_OTHER 0xff
+#endif
+#ifndef MDIO_PHY_ID_PRTAD
+#define MDIO_PHY_ID_PRTAD 0x03e0
+#endif
+#ifndef MDIO_PHY_ID_DEVAD
+#define MDIO_PHY_ID_DEVAD 0x001f
+#endif
+#ifndef skb_dst
+#define skb_dst(s) ((s)->dst)
+#endif
+
+#ifndef SUPPORTED_1000baseKX_Full
+#define SUPPORTED_1000baseKX_Full	BIT(17)
+#endif
+#ifndef SUPPORTED_10000baseKX4_Full
+#define SUPPORTED_10000baseKX4_Full	BIT(18)
+#endif
+#ifndef SUPPORTED_10000baseKR_Full
+#define SUPPORTED_10000baseKR_Full	BIT(19)
+#endif
+
+#ifndef ADVERTISED_1000baseKX_Full
+#define ADVERTISED_1000baseKX_Full	BIT(17)
+#endif
+#ifndef ADVERTISED_10000baseKX4_Full
+#define ADVERTISED_10000baseKX4_Full	BIT(18)
+#endif
+#ifndef ADVERTISED_10000baseKR_Full
+#define ADVERTISED_10000baseKR_Full	BIT(19)
+#endif
+
+static inline unsigned long dev_trans_start(struct net_device *dev)
+{
+	return dev->trans_start;
+}
+#else /* < 2.6.31 */
+#ifndef HAVE_NETDEV_STORAGE_ADDRESS
+#define HAVE_NETDEV_STORAGE_ADDRESS
+#endif
+#ifndef HAVE_NETDEV_HW_ADDR
+#define HAVE_NETDEV_HW_ADDR
+#endif
+#ifndef HAVE_TRANS_START_IN_QUEUE
+#define HAVE_TRANS_START_IN_QUEUE
+#endif
+#ifndef HAVE_INCLUDE_LINUX_MDIO_H
+#define HAVE_INCLUDE_LINUX_MDIO_H
+#endif
+#include <linux/mdio.h>
+#endif /* < 2.6.31 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32) )
+#undef netdev_tx_t
+#define netdev_tx_t int
+#if defined(CONFIG_FCOE) || defined(CONFIG_FCOE_MODULE)
+#ifndef NETIF_F_FCOE_MTU
+#define NETIF_F_FCOE_MTU       BIT(26)
+#endif
+#endif /* CONFIG_FCOE || CONFIG_FCOE_MODULE */
+
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,0) )
+static inline int _kc_pm_runtime_get_sync()
+{
+	return 1;
+}
+#define pm_runtime_get_sync(dev)	_kc_pm_runtime_get_sync()
+#else /* 2.6.0 => 2.6.32 */
+static inline int _kc_pm_runtime_get_sync(struct device __always_unused *dev)
+{
+	return 1;
+}
+#ifndef pm_runtime_get_sync
+#define pm_runtime_get_sync(dev)	_kc_pm_runtime_get_sync(dev)
+#endif
+#endif /* 2.6.0 => 2.6.32 */
+#ifndef pm_runtime_put
+#define pm_runtime_put(dev)		do {} while (0)
+#endif
+#ifndef pm_runtime_put_sync
+#define pm_runtime_put_sync(dev)	do {} while (0)
+#endif
+#ifndef pm_runtime_resume
+#define pm_runtime_resume(dev)		do {} while (0)
+#endif
+#ifndef pm_schedule_suspend
+#define pm_schedule_suspend(dev, t)	do {} while (0)
+#endif
+#ifndef pm_runtime_set_suspended
+#define pm_runtime_set_suspended(dev)	do {} while (0)
+#endif
+#ifndef pm_runtime_disable
+#define pm_runtime_disable(dev)		do {} while (0)
+#endif
+#ifndef pm_runtime_put_noidle
+#define pm_runtime_put_noidle(dev)	do {} while (0)
+#endif
+#ifndef pm_runtime_set_active
+#define pm_runtime_set_active(dev)	do {} while (0)
+#endif
+#ifndef pm_runtime_enable
+#define pm_runtime_enable(dev)	do {} while (0)
+#endif
+#ifndef pm_runtime_get_noresume
+#define pm_runtime_get_noresume(dev)	do {} while (0)
+#endif
+#else /* < 2.6.32 */
+#if (RHEL_RELEASE_CODE && \
+     (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(6,2)) && \
+     (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,0)))
+#define HAVE_RHEL6_NET_DEVICE_EXTENDED
+#endif /* RHEL >= 6.2 && RHEL < 7.0 */
+#if (RHEL_RELEASE_CODE && \
+     (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(6,6)) && \
+     (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,0)))
+#define HAVE_RHEL6_NET_DEVICE_OPS_EXT
+#define HAVE_NDO_SET_FEATURES
+#endif /* RHEL >= 6.6 && RHEL < 7.0 */
+#if defined(CONFIG_FCOE) || defined(CONFIG_FCOE_MODULE)
+#ifndef HAVE_NETDEV_OPS_FCOE_ENABLE
+#define HAVE_NETDEV_OPS_FCOE_ENABLE
+#endif
+#endif /* CONFIG_FCOE || CONFIG_FCOE_MODULE */
+#ifdef CONFIG_DCB
+#ifndef HAVE_DCBNL_OPS_GETAPP
+#define HAVE_DCBNL_OPS_GETAPP
+#endif
+#endif /* CONFIG_DCB */
+#include <linux/pm_runtime.h>
+/* IOV bad DMA target work arounds require at least this kernel rev support */
+#define HAVE_PCIE_TYPE
+#endif /* < 2.6.32 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33) )
+#ifndef pci_pcie_cap
+#define pci_pcie_cap(pdev) pci_find_capability(pdev, PCI_CAP_ID_EXP)
+#endif
+#ifndef IPV4_FLOW
+#define IPV4_FLOW 0x10
+#endif /* IPV4_FLOW */
+#ifndef IPV6_FLOW
+#define IPV6_FLOW 0x11
+#endif /* IPV6_FLOW */
+/* Features back-ported to RHEL6 or SLES11 SP1 after 2.6.32 */
+#if ( (RHEL_RELEASE_CODE && RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(6,0)) || \
+      (SLE_VERSION_CODE && SLE_VERSION_CODE >= SLE_VERSION(11,1,0)) )
+#if defined(CONFIG_FCOE) || defined(CONFIG_FCOE_MODULE)
+#ifndef HAVE_NETDEV_OPS_FCOE_GETWWN
+#define HAVE_NETDEV_OPS_FCOE_GETWWN
+#endif
+#endif /* CONFIG_FCOE || CONFIG_FCOE_MODULE */
+#endif /* RHEL6 or SLES11 SP1 */
+#ifndef __percpu
+#define __percpu
+#endif /* __percpu */
+
+#ifndef PORT_DA
+#define PORT_DA PORT_OTHER
+#endif /* PORT_DA */
+#ifndef PORT_NONE
+#define PORT_NONE PORT_OTHER
+#endif
+
+#if ((RHEL_RELEASE_CODE && \
+     (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(6,3)) && \
+     (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,0))))
+#if !defined(CONFIG_X86_32) && !defined(CONFIG_NEED_DMA_MAP_STATE)
+#undef DEFINE_DMA_UNMAP_ADDR
+#define DEFINE_DMA_UNMAP_ADDR(ADDR_NAME)	dma_addr_t ADDR_NAME
+#undef DEFINE_DMA_UNMAP_LEN
+#define DEFINE_DMA_UNMAP_LEN(LEN_NAME)		__u32 LEN_NAME
+#undef dma_unmap_addr
+#define dma_unmap_addr(PTR, ADDR_NAME)		((PTR)->ADDR_NAME)
+#undef dma_unmap_addr_set
+#define dma_unmap_addr_set(PTR, ADDR_NAME, VAL)	(((PTR)->ADDR_NAME) = (VAL))
+#undef dma_unmap_len
+#define dma_unmap_len(PTR, LEN_NAME)		((PTR)->LEN_NAME)
+#undef dma_unmap_len_set
+#define dma_unmap_len_set(PTR, LEN_NAME, VAL)	(((PTR)->LEN_NAME) = (VAL))
+#endif /* CONFIG_X86_64 && !CONFIG_NEED_DMA_MAP_STATE */
+#endif /* RHEL_RELEASE_CODE */
+
+#if (!(RHEL_RELEASE_CODE && \
+       (((RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(5,8)) && \
+         (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(6,0))) || \
+        ((RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(6,1)) && \
+         (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,0))))))
+static inline bool pci_is_pcie(struct pci_dev *dev)
+{
+	return !!pci_pcie_cap(dev);
+}
+#endif /* RHEL_RELEASE_CODE */
+
+#if (!(RHEL_RELEASE_CODE && \
+      (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(6,2))))
+#define sk_tx_queue_get(_sk) (-1)
+#define sk_tx_queue_set(_sk, _tx_queue) do {} while(0)
+#endif /* !(RHEL >= 6.2) */
+
+#if (RHEL_RELEASE_CODE && \
+     (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(6,4)) && \
+     (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,0)))
+#define HAVE_RHEL6_ETHTOOL_OPS_EXT_STRUCT
+#define HAVE_ETHTOOL_GRXFHINDIR_SIZE
+#define HAVE_ETHTOOL_SET_PHYS_ID
+#define HAVE_ETHTOOL_GET_TS_INFO
+#if (RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(6,5))
+#define HAVE_ETHTOOL_GSRSSH
+#define HAVE_RHEL6_SRIOV_CONFIGURE
+#define HAVE_RXFH_NONCONST
+#endif /* RHEL > 6.5 */
+#endif /* RHEL >= 6.4 && RHEL < 7.0 */
+
+#else /* < 2.6.33 */
+#if defined(CONFIG_FCOE) || defined(CONFIG_FCOE_MODULE)
+#ifndef HAVE_NETDEV_OPS_FCOE_GETWWN
+#define HAVE_NETDEV_OPS_FCOE_GETWWN
+#endif
+#endif /* CONFIG_FCOE || CONFIG_FCOE_MODULE */
+#endif /* < 2.6.33 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,34) )
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(6,0))
+#ifndef pci_num_vf
+#define pci_num_vf(pdev) _kc_pci_num_vf(pdev)
+int _kc_pci_num_vf(struct pci_dev *dev);
+#endif
+#endif /* RHEL_RELEASE_CODE */
+
+#ifndef dev_is_pci
+#define dev_is_pci(d) ((d)->bus == &pci_bus_type)
+#endif
+
+#ifndef ETH_FLAG_NTUPLE
+#define ETH_FLAG_NTUPLE NETIF_F_NTUPLE
+#endif
+
+#ifndef netdev_mc_count
+#define netdev_mc_count(dev) ((dev)->mc_count)
+#endif
+#ifndef netdev_mc_empty
+#define netdev_mc_empty(dev) (netdev_mc_count(dev) == 0)
+#endif
+#ifndef netdev_for_each_mc_addr
+#define netdev_for_each_mc_addr(mclist, dev) \
+	for (mclist = dev->mc_list; mclist; mclist = mclist->next)
+#endif
+#ifndef netdev_uc_count
+#define netdev_uc_count(dev) ((dev)->uc.count)
+#endif
+#ifndef netdev_uc_empty
+#define netdev_uc_empty(dev) (netdev_uc_count(dev) == 0)
+#endif
+#ifndef netdev_for_each_uc_addr
+#define netdev_for_each_uc_addr(ha, dev) \
+	list_for_each_entry(ha, &dev->uc.list, list)
+#endif
+#ifndef dma_set_coherent_mask
+#define dma_set_coherent_mask(dev,mask) \
+	pci_set_consistent_dma_mask(to_pci_dev(dev),(mask))
+#endif
+#ifndef pci_dev_run_wake
+#define pci_dev_run_wake(pdev)	(0)
+#endif
+
+/* netdev logging taken from include/linux/netdevice.h */
+#ifndef netdev_name
+static inline const char *_kc_netdev_name(const struct net_device *dev)
+{
+	if (dev->reg_state != NETREG_REGISTERED)
+		return "(unregistered net_device)";
+	return dev->name;
+}
+#define netdev_name(netdev)	_kc_netdev_name(netdev)
+#endif /* netdev_name */
+
+#undef netdev_printk
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,0) )
+#define netdev_printk(level, netdev, format, args...)		\
+do {								\
+	struct pci_dev *pdev = _kc_netdev_to_pdev(netdev);	\
+	printk(level "%s: " format, pci_name(pdev), ##args);	\
+} while(0)
+#elif ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,21) )
+#define netdev_printk(level, netdev, format, args...)		\
+do {								\
+	struct pci_dev *pdev = _kc_netdev_to_pdev(netdev);	\
+	struct device *dev = pci_dev_to_dev(pdev);		\
+	dev_printk(level, dev, "%s: " format,			\
+		   netdev_name(netdev), ##args);		\
+} while(0)
+#else /* 2.6.21 => 2.6.34 */
+#define netdev_printk(level, netdev, format, args...)		\
+	dev_printk(level, (netdev)->dev.parent,			\
+		   "%s: " format,				\
+		   netdev_name(netdev), ##args)
+#endif /* <2.6.0 <2.6.21 <2.6.34 */
+#undef netdev_emerg
+#define netdev_emerg(dev, format, args...)			\
+	netdev_printk(KERN_EMERG, dev, format, ##args)
+#undef netdev_alert
+#define netdev_alert(dev, format, args...)			\
+	netdev_printk(KERN_ALERT, dev, format, ##args)
+#undef netdev_crit
+#define netdev_crit(dev, format, args...)			\
+	netdev_printk(KERN_CRIT, dev, format, ##args)
+#undef netdev_err
+#define netdev_err(dev, format, args...)			\
+	netdev_printk(KERN_ERR, dev, format, ##args)
+#undef netdev_warn
+#define netdev_warn(dev, format, args...)			\
+	netdev_printk(KERN_WARNING, dev, format, ##args)
+#undef netdev_notice
+#define netdev_notice(dev, format, args...)			\
+	netdev_printk(KERN_NOTICE, dev, format, ##args)
+#undef netdev_info
+#define netdev_info(dev, format, args...)			\
+	netdev_printk(KERN_INFO, dev, format, ##args)
+#undef netdev_dbg
+#if defined(DEBUG)
+#define netdev_dbg(__dev, format, args...)			\
+	netdev_printk(KERN_DEBUG, __dev, format, ##args)
+#elif defined(CONFIG_DYNAMIC_DEBUG)
+#define netdev_dbg(__dev, format, args...)			\
+do {								\
+	dynamic_dev_dbg((__dev)->dev.parent, "%s: " format,	\
+			netdev_name(__dev), ##args);		\
+} while (0)
+#else /* DEBUG */
+#define netdev_dbg(__dev, format, args...)			\
+({								\
+	if (0)							\
+		netdev_printk(KERN_DEBUG, __dev, format, ##args); \
+	0;							\
+})
+#endif /* DEBUG */
+
+#undef netif_printk
+#define netif_printk(priv, type, level, dev, fmt, args...)	\
+do {								\
+	if (netif_msg_##type(priv))				\
+		netdev_printk(level, (dev), fmt, ##args);	\
+} while (0)
+
+#undef netif_emerg
+#define netif_emerg(priv, type, dev, fmt, args...)		\
+	netif_level(emerg, priv, type, dev, fmt, ##args)
+#undef netif_alert
+#define netif_alert(priv, type, dev, fmt, args...)		\
+	netif_level(alert, priv, type, dev, fmt, ##args)
+#undef netif_crit
+#define netif_crit(priv, type, dev, fmt, args...)		\
+	netif_level(crit, priv, type, dev, fmt, ##args)
+#undef netif_err
+#define netif_err(priv, type, dev, fmt, args...)		\
+	netif_level(err, priv, type, dev, fmt, ##args)
+#undef netif_warn
+#define netif_warn(priv, type, dev, fmt, args...)		\
+	netif_level(warn, priv, type, dev, fmt, ##args)
+#undef netif_notice
+#define netif_notice(priv, type, dev, fmt, args...)		\
+	netif_level(notice, priv, type, dev, fmt, ##args)
+#undef netif_info
+#define netif_info(priv, type, dev, fmt, args...)		\
+	netif_level(info, priv, type, dev, fmt, ##args)
+#undef netif_dbg
+#define netif_dbg(priv, type, dev, fmt, args...)		\
+	netif_level(dbg, priv, type, dev, fmt, ##args)
+
+#ifdef SET_SYSTEM_SLEEP_PM_OPS
+#define HAVE_SYSTEM_SLEEP_PM_OPS
+#endif
+
+#ifndef for_each_set_bit
+#define for_each_set_bit(bit, addr, size) \
+	for ((bit) = find_first_bit((addr), (size)); \
+		(bit) < (size); \
+		(bit) = find_next_bit((addr), (size), (bit) + 1))
+#endif /* for_each_set_bit */
+
+#ifndef DEFINE_DMA_UNMAP_ADDR
+#define DEFINE_DMA_UNMAP_ADDR DECLARE_PCI_UNMAP_ADDR
+#define DEFINE_DMA_UNMAP_LEN DECLARE_PCI_UNMAP_LEN
+#define dma_unmap_addr pci_unmap_addr
+#define dma_unmap_addr_set pci_unmap_addr_set
+#define dma_unmap_len pci_unmap_len
+#define dma_unmap_len_set pci_unmap_len_set
+#endif /* DEFINE_DMA_UNMAP_ADDR */
+
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(6,3))
+#ifdef IGB_HWMON
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+#define sysfs_attr_init(attr)				\
+	do {						\
+		static struct lock_class_key __key;	\
+		(attr)->key = &__key;			\
+	} while (0)
+#else
+#define sysfs_attr_init(attr) do {} while (0)
+#endif /* CONFIG_DEBUG_LOCK_ALLOC */
+#endif /* IGB_HWMON */
+#endif /* RHEL_RELEASE_CODE */
+
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,0) )
+static inline bool _kc_pm_runtime_suspended()
+{
+	return false;
+}
+#define pm_runtime_suspended(dev)	_kc_pm_runtime_suspended()
+#else /* 2.6.0 => 2.6.34 */
+static inline bool _kc_pm_runtime_suspended(struct device __always_unused *dev)
+{
+	return false;
+}
+#ifndef pm_runtime_suspended
+#define pm_runtime_suspended(dev)	_kc_pm_runtime_suspended(dev)
+#endif
+#endif /* 2.6.0 => 2.6.34 */
+
+#ifndef pci_bus_speed
+/* override pci_bus_speed introduced in 2.6.19 with an expanded enum type */
+enum _kc_pci_bus_speed {
+	_KC_PCIE_SPEED_2_5GT		= 0x14,
+	_KC_PCIE_SPEED_5_0GT		= 0x15,
+	_KC_PCIE_SPEED_8_0GT		= 0x16,
+	_KC_PCI_SPEED_UNKNOWN		= 0xff,
+};
+#define pci_bus_speed		_kc_pci_bus_speed
+#define PCIE_SPEED_2_5GT	_KC_PCIE_SPEED_2_5GT
+#define PCIE_SPEED_5_0GT	_KC_PCIE_SPEED_5_0GT
+#define PCIE_SPEED_8_0GT	_KC_PCIE_SPEED_8_0GT
+#define PCI_SPEED_UNKNOWN	_KC_PCI_SPEED_UNKNOWN
+#endif /* pci_bus_speed */
+
+#else /* < 2.6.34 */
+#define HAVE_SYSTEM_SLEEP_PM_OPS
+#ifndef HAVE_SET_RX_MODE
+#define HAVE_SET_RX_MODE
+#endif
+
+#endif /* < 2.6.34 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,35) )
+ssize_t _kc_simple_write_to_buffer(void *to, size_t available, loff_t *ppos,
+				   const void __user *from, size_t count);
+#define simple_write_to_buffer _kc_simple_write_to_buffer
+
+#if (!(RHEL_RELEASE_CODE && RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(6,4)))
+static inline struct pci_dev *pci_physfn(struct pci_dev *dev)
+{
+#ifdef HAVE_PCI_DEV_IS_VIRTFN_BIT
+#ifdef CONFIG_PCI_IOV
+	if (dev->is_virtfn)
+		dev = dev->physfn;
+#endif /* CONFIG_PCI_IOV */
+#endif /* HAVE_PCI_DEV_IS_VIRTFN_BIT */
+	return dev;
+}
+#endif /* ! RHEL >= 6.4 */
+
+#ifndef PCI_EXP_LNKSTA_NLW_SHIFT
+#define PCI_EXP_LNKSTA_NLW_SHIFT 4
+#endif
+
+#ifndef numa_node_id
+#define numa_node_id() 0
+#endif
+#ifndef numa_mem_id
+#define numa_mem_id numa_node_id
+#endif
+#if (!(RHEL_RELEASE_CODE && RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(6,0)))
+#ifdef HAVE_TX_MQ
+#include <net/sch_generic.h>
+#ifndef CONFIG_NETDEVICES_MULTIQUEUE
+int _kc_netif_set_real_num_tx_queues(struct net_device *, unsigned int);
+#else /* CONFIG_NETDEVICES_MULTI_QUEUE */
+static inline int _kc_netif_set_real_num_tx_queues(struct net_device *dev,
+						   unsigned int txq)
+{
+	dev->egress_subqueue_count = txq;
+	return 0;
+}
+#endif /* CONFIG_NETDEVICES_MULTI_QUEUE */
+#else /* HAVE_TX_MQ */
+static inline int _kc_netif_set_real_num_tx_queues(struct net_device __always_unused *dev,
+						   unsigned int __always_unused txq)
+{
+	return 0;
+}
+#endif /* HAVE_TX_MQ */
+#define netif_set_real_num_tx_queues(dev, txq) \
+	_kc_netif_set_real_num_tx_queues(dev, txq)
+#endif /* !(RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(6,0)) */
+#ifndef ETH_FLAG_RXHASH
+#define ETH_FLAG_RXHASH (1<<28)
+#endif /* ETH_FLAG_RXHASH */
+#if (RHEL_RELEASE_CODE && RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(6,0))
+#define HAVE_IRQ_AFFINITY_HINT
+#endif
+struct device_node;
+#else /* < 2.6.35 */
+#define HAVE_STRUCT_DEVICE_OF_NODE
+#define HAVE_PM_QOS_REQUEST_LIST
+#define HAVE_IRQ_AFFINITY_HINT
+#include <linux/of.h>
+#endif /* < 2.6.35 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,36) )
+int _kc_ethtool_op_set_flags(struct net_device *, u32, u32);
+#define ethtool_op_set_flags _kc_ethtool_op_set_flags
+u32 _kc_ethtool_op_get_flags(struct net_device *);
+#define ethtool_op_get_flags _kc_ethtool_op_get_flags
+
+enum {
+	WQ_UNBOUND = 0,
+	WQ_RESCUER = 0,
+};
+
+#ifdef CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS
+#ifdef NET_IP_ALIGN
+#undef NET_IP_ALIGN
+#endif
+#define NET_IP_ALIGN 0
+#endif /* CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS */
+
+#ifdef NET_SKB_PAD
+#undef NET_SKB_PAD
+#endif
+
+#if (L1_CACHE_BYTES > 32)
+#define NET_SKB_PAD L1_CACHE_BYTES
+#else
+#define NET_SKB_PAD 32
+#endif
+
+static inline struct sk_buff *_kc_netdev_alloc_skb_ip_align(struct net_device *dev,
+							    unsigned int length)
+{
+	struct sk_buff *skb;
+
+	skb = alloc_skb(length + NET_SKB_PAD + NET_IP_ALIGN, GFP_ATOMIC);
+	if (skb) {
+#if (NET_IP_ALIGN + NET_SKB_PAD)
+		skb_reserve(skb, NET_IP_ALIGN + NET_SKB_PAD);
+#endif
+		skb->dev = dev;
+	}
+	return skb;
+}
+
+#ifdef netdev_alloc_skb_ip_align
+#undef netdev_alloc_skb_ip_align
+#endif
+#define netdev_alloc_skb_ip_align(n, l) _kc_netdev_alloc_skb_ip_align(n, l)
+
+#undef netif_level
+#define netif_level(level, priv, type, dev, fmt, args...)	\
+do {								\
+	if (netif_msg_##type(priv))				\
+		netdev_##level(dev, fmt, ##args);		\
+} while (0)
+
+#if (!(RHEL_RELEASE_CODE && RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(6,3)))
+#undef usleep_range
+#define usleep_range(min, max)	msleep(DIV_ROUND_UP(min, 1000))
+#endif
+
+#define u64_stats_update_begin(a) do { } while(0)
+#define u64_stats_update_end(a) do { } while(0)
+#define u64_stats_fetch_begin(a) do { } while(0)
+#define u64_stats_fetch_retry_bh(a,b) (0)
+#define u64_stats_fetch_begin_bh(a) (0)
+
+#if (RHEL_RELEASE_CODE && RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(6,1))
+#define HAVE_8021P_SUPPORT
+#endif
+
+/* RHEL6.4 and SLES11sp2 backported skb_tx_timestamp */
+#if (!(RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(6,4)) && \
+     !(SLE_VERSION_CODE >= SLE_VERSION(11,2,0)))
+static inline void skb_tx_timestamp(struct sk_buff __always_unused *skb)
+{
+	return;
+}
+#endif
+
+#else /* < 2.6.36 */
+
+#define HAVE_PM_QOS_REQUEST_ACTIVE
+#define HAVE_8021P_SUPPORT
+#define HAVE_NDO_GET_STATS64
+#endif /* < 2.6.36 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,37) )
+#define HAVE_NON_CONST_PCI_DRIVER_NAME
+#ifndef netif_set_real_num_tx_queues
+static inline int _kc_netif_set_real_num_tx_queues(struct net_device *dev,
+						   unsigned int txq)
+{
+	netif_set_real_num_tx_queues(dev, txq);
+	return 0;
+}
+#define netif_set_real_num_tx_queues(dev, txq) \
+	_kc_netif_set_real_num_tx_queues(dev, txq)
+#endif
+#ifndef netif_set_real_num_rx_queues
+static inline int __kc_netif_set_real_num_rx_queues(struct net_device __always_unused *dev,
+						    unsigned int __always_unused rxq)
+{
+	return 0;
+}
+#define netif_set_real_num_rx_queues(dev, rxq) \
+	__kc_netif_set_real_num_rx_queues((dev), (rxq))
+#endif
+#ifndef ETHTOOL_RXNTUPLE_ACTION_CLEAR
+#define ETHTOOL_RXNTUPLE_ACTION_CLEAR (-2)
+#endif
+#ifndef VLAN_N_VID
+#define VLAN_N_VID	VLAN_GROUP_ARRAY_LEN
+#endif /* VLAN_N_VID */
+#ifndef ETH_FLAG_TXVLAN
+#define ETH_FLAG_TXVLAN BIT(7)
+#endif /* ETH_FLAG_TXVLAN */
+#ifndef ETH_FLAG_RXVLAN
+#define ETH_FLAG_RXVLAN BIT(8)
+#endif /* ETH_FLAG_RXVLAN */
+
+#define WQ_MEM_RECLAIM WQ_RESCUER
+
+static inline void _kc_skb_checksum_none_assert(struct sk_buff *skb)
+{
+	WARN_ON(skb->ip_summed != CHECKSUM_NONE);
+}
+#define skb_checksum_none_assert(skb) _kc_skb_checksum_none_assert(skb)
+
+static inline void *_kc_vzalloc_node(unsigned long size, int node)
+{
+	void *addr = vmalloc_node(size, node);
+	if (addr)
+		memset(addr, 0, size);
+	return addr;
+}
+#define vzalloc_node(_size, _node) _kc_vzalloc_node(_size, _node)
+
+static inline void *_kc_vzalloc(unsigned long size)
+{
+	void *addr = vmalloc(size);
+	if (addr)
+		memset(addr, 0, size);
+	return addr;
+}
+#define vzalloc(_size) _kc_vzalloc(_size)
+
+#if (!(RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(5,7)) || \
+     (RHEL_RELEASE_CODE == RHEL_RELEASE_VERSION(6,0)))
+static inline __be16 vlan_get_protocol(const struct sk_buff *skb)
+{
+	if (vlan_tx_tag_present(skb) ||
+	    skb->protocol != cpu_to_be16(ETH_P_8021Q))
+		return skb->protocol;
+
+	if (skb_headlen(skb) < sizeof(struct vlan_ethhdr))
+		return 0;
+
+	return ((struct vlan_ethhdr*)skb->data)->h_vlan_encapsulated_proto;
+}
+#endif /* !RHEL5.7+ || RHEL6.0 */
+
+#ifdef HAVE_HW_TIME_STAMP
+#define SKBTX_HW_TSTAMP BIT(0)
+#define SKBTX_IN_PROGRESS BIT(2)
+#define SKB_SHARED_TX_IS_UNION
+#endif
+
+#ifndef device_wakeup_enable
+#define device_wakeup_enable(dev)	device_set_wakeup_enable(dev, true)
+#endif
+
+#if ( LINUX_VERSION_CODE > KERNEL_VERSION(2,4,18) )
+#ifndef HAVE_VLAN_RX_REGISTER
+#define HAVE_VLAN_RX_REGISTER
+#endif
+#endif /* > 2.4.18 */
+#endif /* < 2.6.37 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,38) )
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,22) )
+#define skb_checksum_start_offset(skb) skb_transport_offset(skb)
+#else /* 2.6.22 -> 2.6.37 */
+static inline int _kc_skb_checksum_start_offset(const struct sk_buff *skb)
+{
+        return skb->csum_start - skb_headroom(skb);
+}
+#define skb_checksum_start_offset(skb) _kc_skb_checksum_start_offset(skb)
+#endif /* 2.6.22 -> 2.6.37 */
+#if IS_ENABLED(CONFIG_DCB)
+#ifndef IEEE_8021QAZ_MAX_TCS
+#define IEEE_8021QAZ_MAX_TCS 8
+#endif
+#ifndef DCB_CAP_DCBX_HOST
+#define DCB_CAP_DCBX_HOST		0x01
+#endif
+#ifndef DCB_CAP_DCBX_LLD_MANAGED
+#define DCB_CAP_DCBX_LLD_MANAGED	0x02
+#endif
+#ifndef DCB_CAP_DCBX_VER_CEE
+#define DCB_CAP_DCBX_VER_CEE		0x04
+#endif
+#ifndef DCB_CAP_DCBX_VER_IEEE
+#define DCB_CAP_DCBX_VER_IEEE		0x08
+#endif
+#ifndef DCB_CAP_DCBX_STATIC
+#define DCB_CAP_DCBX_STATIC		0x10
+#endif
+#endif /* CONFIG_DCB */
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(6,2))
+#define CONFIG_XPS
+#endif /* RHEL_RELEASE_VERSION(6,2) */
+#endif /* < 2.6.38 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,39) )
+#ifndef TC_BITMASK
+#define TC_BITMASK 15
+#endif
+#ifndef NETIF_F_RXCSUM
+#define NETIF_F_RXCSUM		BIT(29)
+#endif
+#ifndef skb_queue_reverse_walk_safe
+#define skb_queue_reverse_walk_safe(queue, skb, tmp)				\
+		for (skb = (queue)->prev, tmp = skb->prev;			\
+		     skb != (struct sk_buff *)(queue);				\
+		     skb = tmp, tmp = skb->prev)
+#endif
+#if defined(CONFIG_FCOE) || defined(CONFIG_FCOE_MODULE)
+#ifndef FCOE_MTU
+#define FCOE_MTU	2158
+#endif
+#endif
+#if IS_ENABLED(CONFIG_DCB)
+#ifndef IEEE_8021QAZ_APP_SEL_ETHERTYPE
+#define IEEE_8021QAZ_APP_SEL_ETHERTYPE	1
+#endif
+#endif
+#if (!(RHEL_RELEASE_CODE && RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(6,4)))
+#define kstrtoul(a, b, c)  ((*(c)) = simple_strtoul((a), NULL, (b)), 0)
+#define kstrtouint(a, b, c)  ((*(c)) = simple_strtoul((a), NULL, (b)), 0)
+#define kstrtou32(a, b, c)  ((*(c)) = simple_strtoul((a), NULL, (b)), 0)
+#endif /* !(RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(6,4)) */
+#if (!(RHEL_RELEASE_CODE && RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(6,0)))
+u16 ___kc_skb_tx_hash(struct net_device *, const struct sk_buff *, u16);
+#define __skb_tx_hash(n, s, q) ___kc_skb_tx_hash((n), (s), (q))
+u8 _kc_netdev_get_num_tc(struct net_device *dev);
+#define netdev_get_num_tc(dev) _kc_netdev_get_num_tc(dev)
+int _kc_netdev_set_num_tc(struct net_device *dev, u8 num_tc);
+#define netdev_set_num_tc(dev, tc) _kc_netdev_set_num_tc((dev), (tc))
+#define netdev_reset_tc(dev) _kc_netdev_set_num_tc((dev), 0)
+#define netdev_set_tc_queue(dev, tc, cnt, off) do {} while (0)
+u8 _kc_netdev_get_prio_tc_map(struct net_device *dev, u8 up);
+#define netdev_get_prio_tc_map(dev, up) _kc_netdev_get_prio_tc_map(dev, up)
+#define netdev_set_prio_tc_map(dev, up, tc) do {} while (0)
+#else /* RHEL6.1 or greater */
+#ifndef HAVE_MQPRIO
+#define HAVE_MQPRIO
+#endif /* HAVE_MQPRIO */
+#if IS_ENABLED(CONFIG_DCB)
+#ifndef HAVE_DCBNL_IEEE
+#define HAVE_DCBNL_IEEE
+#ifndef IEEE_8021QAZ_TSA_STRICT
+#define IEEE_8021QAZ_TSA_STRICT		0
+#endif
+#ifndef IEEE_8021QAZ_TSA_ETS
+#define IEEE_8021QAZ_TSA_ETS		2
+#endif
+#ifndef IEEE_8021QAZ_APP_SEL_ETHERTYPE
+#define IEEE_8021QAZ_APP_SEL_ETHERTYPE	1
+#endif
+#endif
+#endif /* CONFIG_DCB */
+#endif /* !(RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(6,0)) */
+
+#ifndef udp_csum
+#define udp_csum __kc_udp_csum
+static inline __wsum __kc_udp_csum(struct sk_buff *skb)
+{
+	__wsum csum = csum_partial(skb_transport_header(skb),
+				   sizeof(struct udphdr), skb->csum);
+
+	for (skb = skb_shinfo(skb)->frag_list; skb; skb = skb->next) {
+		csum = csum_add(csum, skb->csum);
+	}
+	return csum;
+}
+#endif /* udp_csum */
+#else /* < 2.6.39 */
+#if defined(CONFIG_FCOE) || defined(CONFIG_FCOE_MODULE)
+#ifndef HAVE_NETDEV_OPS_FCOE_DDP_TARGET
+#define HAVE_NETDEV_OPS_FCOE_DDP_TARGET
+#endif
+#endif /* CONFIG_FCOE || CONFIG_FCOE_MODULE */
+#ifndef HAVE_MQPRIO
+#define HAVE_MQPRIO
+#endif
+#ifndef HAVE_SETUP_TC
+#define HAVE_SETUP_TC
+#endif
+#ifdef CONFIG_DCB
+#ifndef HAVE_DCBNL_IEEE
+#define HAVE_DCBNL_IEEE
+#endif
+#endif /* CONFIG_DCB */
+#ifndef HAVE_NDO_SET_FEATURES
+#define HAVE_NDO_SET_FEATURES
+#endif
+#define HAVE_IRQ_AFFINITY_NOTIFY
+#endif /* < 2.6.39 */
+
+/*****************************************************************************/
+/* use < 2.6.40 because of a Fedora 15 kernel update where they
+ * updated the kernel version to 2.6.40.x and they back-ported 3.0 features
+ * like set_phys_id for ethtool.
+ */
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,40) )
+#ifdef ETHTOOL_GRXRINGS
+#ifndef FLOW_EXT
+#define	FLOW_EXT	0x80000000
+union _kc_ethtool_flow_union {
+	struct ethtool_tcpip4_spec		tcp_ip4_spec;
+	struct ethtool_usrip4_spec		usr_ip4_spec;
+	__u8					hdata[60];
+};
+struct _kc_ethtool_flow_ext {
+	__be16	vlan_etype;
+	__be16	vlan_tci;
+	__be32	data[2];
+};
+struct _kc_ethtool_rx_flow_spec {
+	__u32		flow_type;
+	union _kc_ethtool_flow_union h_u;
+	struct _kc_ethtool_flow_ext h_ext;
+	union _kc_ethtool_flow_union m_u;
+	struct _kc_ethtool_flow_ext m_ext;
+	__u64		ring_cookie;
+	__u32		location;
+};
+#define ethtool_rx_flow_spec _kc_ethtool_rx_flow_spec
+#endif /* FLOW_EXT */
+#endif
+
+#define pci_disable_link_state_locked pci_disable_link_state
+
+#ifndef PCI_LTR_VALUE_MASK
+#define  PCI_LTR_VALUE_MASK	0x000003ff
+#endif
+#ifndef PCI_LTR_SCALE_MASK
+#define  PCI_LTR_SCALE_MASK	0x00001c00
+#endif
+#ifndef PCI_LTR_SCALE_SHIFT
+#define  PCI_LTR_SCALE_SHIFT	10
+#endif
+
+#else /* < 2.6.40 */
+#define HAVE_ETHTOOL_SET_PHYS_ID
+#endif /* < 2.6.40 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,0,0) )
+#define USE_LEGACY_PM_SUPPORT
+#ifndef kfree_rcu
+#define kfree_rcu(_ptr, _rcu_head) do { 				\
+	void __kc_kfree_rcu(struct rcu_head *rcu_head)			\
+	{								\
+		void *ptr = container_of(rcu_head,			\
+					 typeof(*_ptr),			\
+					 _rcu_head);			\
+		kfree(ptr);						\
+	}								\
+	call_rcu(&(_ptr)->_rcu_head, __kc_kfree_rcu);			\
+} while (0)
+#define HAVE_KFREE_RCU_BARRIER
+#endif /* kfree_rcu */
+#ifndef kstrtol_from_user
+#define kstrtol_from_user(s, c, b, r) _kc_kstrtol_from_user(s, c, b, r)
+static inline int _kc_kstrtol_from_user(const char __user *s, size_t count,
+					unsigned int base, long *res)
+{
+	/* sign, base 2 representation, newline, terminator */
+	char buf[1 + sizeof(long) * 8 + 1 + 1];
+
+	count = min(count, sizeof(buf) - 1);
+	if (copy_from_user(buf, s, count))
+		return -EFAULT;
+	buf[count] = '\0';
+	return strict_strtol(buf, base, res);
+}
+#endif
+
+#if (RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(7,0) || \
+     RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(5,7)))
+/* 20000base_blah_full Supported and Advertised Registers */
+#define SUPPORTED_20000baseMLD2_Full	BIT(21)
+#define SUPPORTED_20000baseKR2_Full	BIT(22)
+#define ADVERTISED_20000baseMLD2_Full	BIT(21)
+#define ADVERTISED_20000baseKR2_Full	BIT(22)
+#endif /* RHEL_RELEASE_CODE */
+#endif /* < 3.0.0 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,1,0) )
+#ifndef __netdev_alloc_skb_ip_align
+#define __netdev_alloc_skb_ip_align(d,l,_g) netdev_alloc_skb_ip_align(d,l)
+#endif /* __netdev_alloc_skb_ip_align */
+#define dcb_ieee_setapp(dev, app) dcb_setapp(dev, app)
+#define dcb_ieee_delapp(dev, app) 0
+#define dcb_ieee_getapp_mask(dev, app) (1 << app->priority)
+
+/* 1000BASE-T Control register */
+#define CTL1000_AS_MASTER	0x0800
+#define CTL1000_ENABLE_MASTER	0x1000
+
+/* kernels less than 3.0.0 don't have this */
+#ifndef ETH_P_8021AD
+#define ETH_P_8021AD	0x88A8
+#endif
+
+/* Stub definition for !CONFIG_OF is introduced later */
+#ifdef CONFIG_OF
+static inline struct device_node *
+pci_device_to_OF_node(struct pci_dev __maybe_unused *pdev)
+{
+#ifdef HAVE_STRUCT_DEVICE_OF_NODE
+	return pdev ? pdev->dev.of_node : NULL;
+#else
+	return NULL;
+#endif /* !HAVE_STRUCT_DEVICE_OF_NODE */
+}
+#endif /* CONFIG_OF */
+#else /* < 3.1.0 */
+#ifndef HAVE_DCBNL_IEEE_DELAPP
+#define HAVE_DCBNL_IEEE_DELAPP
+#endif
+#endif /* < 3.1.0 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,2,0) )
+#ifndef dma_zalloc_coherent
+#define dma_zalloc_coherent(d, s, h, f) _kc_dma_zalloc_coherent(d, s, h, f)
+static inline void *_kc_dma_zalloc_coherent(struct device *dev, size_t size,
+					    dma_addr_t *dma_handle, gfp_t flag)
+{
+	void *ret = dma_alloc_coherent(dev, size, dma_handle, flag);
+	if (ret)
+		memset(ret, 0, size);
+	return ret;
+}
+#endif
+#ifdef ETHTOOL_GRXRINGS
+#define HAVE_ETHTOOL_GET_RXNFC_VOID_RULE_LOCS
+#endif /* ETHTOOL_GRXRINGS */
+
+#ifndef skb_frag_size
+#define skb_frag_size(frag)	_kc_skb_frag_size(frag)
+static inline unsigned int _kc_skb_frag_size(const skb_frag_t *frag)
+{
+	return frag->size;
+}
+#endif /* skb_frag_size */
+
+#ifndef skb_frag_size_sub
+#define skb_frag_size_sub(frag, delta)	_kc_skb_frag_size_sub(frag, delta)
+static inline void _kc_skb_frag_size_sub(skb_frag_t *frag, int delta)
+{
+	frag->size -= delta;
+}
+#endif /* skb_frag_size_sub */
+
+#ifndef skb_frag_page
+#define skb_frag_page(frag)	_kc_skb_frag_page(frag)
+static inline struct page *_kc_skb_frag_page(const skb_frag_t *frag)
+{
+	return frag->page;
+}
+#endif /* skb_frag_page */
+
+#ifndef skb_frag_address
+#define skb_frag_address(frag)	_kc_skb_frag_address(frag)
+static inline void *_kc_skb_frag_address(const skb_frag_t *frag)
+{
+	return page_address(skb_frag_page(frag)) + frag->page_offset;
+}
+#endif /* skb_frag_address */
+
+#ifndef skb_frag_dma_map
+#if ( LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,0) )
+#include <linux/dma-mapping.h>
+#endif
+#define skb_frag_dma_map(dev,frag,offset,size,dir) \
+		_kc_skb_frag_dma_map(dev,frag,offset,size,dir)
+static inline dma_addr_t _kc_skb_frag_dma_map(struct device *dev,
+					      const skb_frag_t *frag,
+					      size_t offset, size_t size,
+					      enum dma_data_direction dir)
+{
+	return dma_map_page(dev, skb_frag_page(frag),
+			    frag->page_offset + offset, size, dir);
+}
+#endif /* skb_frag_dma_map */
+
+#ifndef __skb_frag_unref
+#define __skb_frag_unref(frag) __kc_skb_frag_unref(frag)
+static inline void __kc_skb_frag_unref(skb_frag_t *frag)
+{
+	put_page(skb_frag_page(frag));
+}
+#endif /* __skb_frag_unref */
+
+#ifndef SPEED_UNKNOWN
+#define SPEED_UNKNOWN	-1
+#endif
+#ifndef DUPLEX_UNKNOWN
+#define DUPLEX_UNKNOWN	0xff
+#endif
+#if ((RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(6,3)) ||\
+     (SLE_VERSION_CODE && SLE_VERSION_CODE >= SLE_VERSION(11,3,0)))
+#ifndef HAVE_PCI_DEV_FLAGS_ASSIGNED
+#define HAVE_PCI_DEV_FLAGS_ASSIGNED
+#endif
+#endif
+#else /* < 3.2.0 */
+#ifndef HAVE_PCI_DEV_FLAGS_ASSIGNED
+#define HAVE_PCI_DEV_FLAGS_ASSIGNED
+#define HAVE_VF_SPOOFCHK_CONFIGURE
+#endif
+#ifndef HAVE_SKB_L4_RXHASH
+#define HAVE_SKB_L4_RXHASH
+#endif
+#define HAVE_IOMMU_PRESENT
+#define HAVE_PM_QOS_REQUEST_LIST_NEW
+#endif /* < 3.2.0 */
+
+#if (RHEL_RELEASE_CODE && RHEL_RELEASE_CODE == RHEL_RELEASE_VERSION(6,2))
+#undef ixgbe_get_netdev_tc_txq
+#define ixgbe_get_netdev_tc_txq(dev, tc) (&netdev_extended(dev)->qos_data.tc_to_txq[tc])
+#endif
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,3,0) )
+/* NOTE: the order of parameters to _kc_alloc_workqueue() is different than
+ * alloc_workqueue() to avoid compiler warning from -Wvarargs
+ */
+static inline struct workqueue_struct * __attribute__ ((format(printf, 3, 4)))
+_kc_alloc_workqueue(__maybe_unused int flags, __maybe_unused int max_active,
+		    const char *fmt, ...)
+{
+	struct workqueue_struct *wq;
+	va_list args, temp;
+	unsigned int len;
+	char *p;
+
+	va_start(args, fmt);
+	va_copy(temp, args);
+	len = vsnprintf(NULL, 0, fmt, temp);
+	va_end(temp);
+
+	p = kmalloc(len + 1, GFP_KERNEL);
+	if (!p) {
+		va_end(args);
+		return NULL;
+	}
+
+	vsnprintf(p, len + 1, fmt, args);
+	va_end(args);
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,36) )
+	wq = create_workqueue(p);
+#else
+	wq = alloc_workqueue(p, flags, max_active);
+#endif
+	kfree(p);
+
+	return wq;
+}
+#ifdef alloc_workqueue
+#undef alloc_workqueue
+#endif
+#define alloc_workqueue(fmt, flags, max_active, args...) \
+	_kc_alloc_workqueue(flags, max_active, fmt, ##args)
+
+#if !(RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(6,5))
+typedef u32 netdev_features_t;
+#endif
+#undef PCI_EXP_TYPE_RC_EC
+#define  PCI_EXP_TYPE_RC_EC	0xa	/* Root Complex Event Collector */
+#ifndef CONFIG_BQL
+#define netdev_tx_completed_queue(_q, _p, _b) do {} while (0)
+#define netdev_completed_queue(_n, _p, _b) do {} while (0)
+#define netdev_tx_sent_queue(_q, _b) do {} while (0)
+#define netdev_sent_queue(_n, _b) do {} while (0)
+#define netdev_tx_reset_queue(_q) do {} while (0)
+#define netdev_reset_queue(_n) do {} while (0)
+#endif
+#if (SLE_VERSION_CODE && SLE_VERSION_CODE >= SLE_VERSION(11,3,0))
+#define HAVE_ETHTOOL_GRXFHINDIR_SIZE
+#endif /* SLE_VERSION(11,3,0) */
+#define netif_xmit_stopped(_q) netif_tx_queue_stopped(_q)
+#if !(SLE_VERSION_CODE && SLE_VERSION_CODE >= SLE_VERSION(11,4,0))
+static inline int __kc_ipv6_skip_exthdr(const struct sk_buff *skb, int start,
+					u8 *nexthdrp,
+					__be16 __always_unused *frag_offp)
+{
+	return ipv6_skip_exthdr(skb, start, nexthdrp);
+}
+#undef ipv6_skip_exthdr
+#define ipv6_skip_exthdr(a,b,c,d) __kc_ipv6_skip_exthdr((a), (b), (c), (d))
+#endif /* !SLES11sp4 or greater */
+
+#if (!(RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(6,4)) && \
+     !(SLE_VERSION_CODE >= SLE_VERSION(11,3,0)))
+static inline u32 ethtool_rxfh_indir_default(u32 index, u32 n_rx_rings)
+{
+	return index % n_rx_rings;
+}
+#endif
+
+#else /* ! < 3.3.0 */
+#define HAVE_ETHTOOL_GRXFHINDIR_SIZE
+#define HAVE_INT_NDO_VLAN_RX_ADD_VID
+#ifdef ETHTOOL_SRXNTUPLE
+#undef ETHTOOL_SRXNTUPLE
+#endif
+#endif /* < 3.3.0 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,4,0) )
+#ifndef NETIF_F_RXFCS
+#define NETIF_F_RXFCS	0
+#endif /* NETIF_F_RXFCS */
+#ifndef NETIF_F_RXALL
+#define NETIF_F_RXALL	0
+#endif /* NETIF_F_RXALL */
+
+#if !(SLE_VERSION_CODE && SLE_VERSION_CODE >= SLE_VERSION(11,3,0))
+#define NUMTCS_RETURNS_U8
+
+int _kc_simple_open(struct inode *inode, struct file *file);
+#define simple_open _kc_simple_open
+#endif /* !(SLE_VERSION_CODE && SLE_VERSION_CODE >= SLE_VERSION(11,3,0)) */
+
+#ifndef skb_add_rx_frag
+#define skb_add_rx_frag _kc_skb_add_rx_frag
+void _kc_skb_add_rx_frag(struct sk_buff * skb, int i, struct page *page,
+			 int off, int size, unsigned int truesize);
+#endif
+#ifdef NET_ADDR_RANDOM
+#define eth_hw_addr_random(N) do { \
+	eth_random_addr(N->dev_addr); \
+	N->addr_assign_type |= NET_ADDR_RANDOM; \
+	} while (0)
+#else /* NET_ADDR_RANDOM */
+#define eth_hw_addr_random(N) eth_random_addr(N->dev_addr)
+#endif /* NET_ADDR_RANDOM */
+
+#ifndef for_each_set_bit_from
+#define for_each_set_bit_from(bit, addr, size) \
+	for ((bit) = find_next_bit((addr), (size), (bit)); \
+			(bit) < (size); \
+			(bit) = find_next_bit((addr), (size), (bit) + 1))
+#endif /* for_each_set_bit_from */
+
+#else /* < 3.4.0 */
+#include <linux/kconfig.h>
+#endif /* >= 3.4.0 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE >= KERNEL_VERSION(3,0,0) ) || \
+    ( RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(6,4) )
+#if !defined(NO_PTP_SUPPORT) && IS_ENABLED(CONFIG_PTP_1588_CLOCK)
+#define HAVE_PTP_1588_CLOCK
+#endif /* !NO_PTP_SUPPORT && IS_ENABLED(CONFIG_PTP_1588_CLOCK) */
+#endif /* >= 3.0.0 || RHEL_RELEASE > 6.4 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,5,0) )
+
+#ifndef SIZE_MAX
+#define SIZE_MAX (~(size_t)0)
+#endif
+
+#ifndef BITS_PER_LONG_LONG
+#define BITS_PER_LONG_LONG 64
+#endif
+
+#ifndef ether_addr_equal
+static inline bool __kc_ether_addr_equal(const u8 *addr1, const u8 *addr2)
+{
+	return !compare_ether_addr(addr1, addr2);
+}
+#define ether_addr_equal(_addr1, _addr2) __kc_ether_addr_equal((_addr1),(_addr2))
+#endif
+
+/* Definitions for !CONFIG_OF_NET are introduced in 3.10 */
+#ifdef CONFIG_OF_NET
+static inline int of_get_phy_mode(struct device_node __always_unused *np)
+{
+	return -ENODEV;
+}
+
+static inline const void *
+of_get_mac_address(struct device_node __always_unused *np)
+{
+	return NULL;
+}
+#endif
+#else
+#include <linux/of_net.h>
+#define HAVE_FDB_OPS
+#define HAVE_ETHTOOL_GET_TS_INFO
+#endif /* < 3.5.0 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,6,0) )
+#define PCI_EXP_LNKCAP2		44	/* Link Capability 2 */
+
+#ifndef MDIO_EEE_100TX
+#define MDIO_EEE_100TX		0x0002	/* 100TX EEE cap */
+#endif
+#ifndef MDIO_EEE_1000T
+#define MDIO_EEE_1000T		0x0004	/* 1000T EEE cap */
+#endif
+#ifndef MDIO_EEE_10GT
+#define MDIO_EEE_10GT		0x0008	/* 10GT EEE cap */
+#endif
+#ifndef MDIO_EEE_1000KX
+#define MDIO_EEE_1000KX		0x0010	/* 1000KX EEE cap */
+#endif
+#ifndef MDIO_EEE_10GKX4
+#define MDIO_EEE_10GKX4		0x0020	/* 10G KX4 EEE cap */
+#endif
+#ifndef MDIO_EEE_10GKR
+#define MDIO_EEE_10GKR		0x0040	/* 10G KR EEE cap */
+#endif
+
+#ifndef __GFP_MEMALLOC
+#define __GFP_MEMALLOC 0
+#endif
+
+#ifndef eth_broadcast_addr
+#define eth_broadcast_addr _kc_eth_broadcast_addr
+static inline void _kc_eth_broadcast_addr(u8 *addr)
+{
+	memset(addr, 0xff, ETH_ALEN);
+}
+#endif
+
+#ifndef eth_random_addr
+#define eth_random_addr _kc_eth_random_addr
+static inline void _kc_eth_random_addr(u8 *addr)
+{
+        get_random_bytes(addr, ETH_ALEN);
+        addr[0] &= 0xfe; /* clear multicast */
+        addr[0] |= 0x02; /* set local assignment */
+}
+#endif /* eth_random_addr */
+
+#ifndef DMA_ATTR_SKIP_CPU_SYNC
+#define DMA_ATTR_SKIP_CPU_SYNC 0
+#endif
+#else /* < 3.6.0 */
+#define HAVE_STRUCT_PAGE_PFMEMALLOC
+#endif /* < 3.6.0 */
+
+/******************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,7,0) )
+#include <linux/workqueue.h>
+#ifndef ADVERTISED_40000baseKR4_Full
+/* these defines were all added in one commit, so should be safe
+ * to trigger activiation on one define
+ */
+#define SUPPORTED_40000baseKR4_Full	BIT(23)
+#define SUPPORTED_40000baseCR4_Full	BIT(24)
+#define SUPPORTED_40000baseSR4_Full	BIT(25)
+#define SUPPORTED_40000baseLR4_Full	BIT(26)
+#define ADVERTISED_40000baseKR4_Full	BIT(23)
+#define ADVERTISED_40000baseCR4_Full	BIT(24)
+#define ADVERTISED_40000baseSR4_Full	BIT(25)
+#define ADVERTISED_40000baseLR4_Full	BIT(26)
+#endif
+
+#ifndef mmd_eee_cap_to_ethtool_sup_t
+/**
+ * mmd_eee_cap_to_ethtool_sup_t
+ * @eee_cap: value of the MMD EEE Capability register
+ *
+ * A small helper function that translates MMD EEE Capability (3.20) bits
+ * to ethtool supported settings.
+ */
+static inline u32 __kc_mmd_eee_cap_to_ethtool_sup_t(u16 eee_cap)
+{
+	u32 supported = 0;
+
+	if (eee_cap & MDIO_EEE_100TX)
+		supported |= SUPPORTED_100baseT_Full;
+	if (eee_cap & MDIO_EEE_1000T)
+		supported |= SUPPORTED_1000baseT_Full;
+	if (eee_cap & MDIO_EEE_10GT)
+		supported |= SUPPORTED_10000baseT_Full;
+	if (eee_cap & MDIO_EEE_1000KX)
+		supported |= SUPPORTED_1000baseKX_Full;
+	if (eee_cap & MDIO_EEE_10GKX4)
+		supported |= SUPPORTED_10000baseKX4_Full;
+	if (eee_cap & MDIO_EEE_10GKR)
+		supported |= SUPPORTED_10000baseKR_Full;
+
+	return supported;
+}
+#define mmd_eee_cap_to_ethtool_sup_t(eee_cap) \
+	__kc_mmd_eee_cap_to_ethtool_sup_t(eee_cap)
+#endif /* mmd_eee_cap_to_ethtool_sup_t */
+
+#ifndef mmd_eee_adv_to_ethtool_adv_t
+/**
+ * mmd_eee_adv_to_ethtool_adv_t
+ * @eee_adv: value of the MMD EEE Advertisement/Link Partner Ability registers
+ *
+ * A small helper function that translates the MMD EEE Advertisement (7.60)
+ * and MMD EEE Link Partner Ability (7.61) bits to ethtool advertisement
+ * settings.
+ */
+static inline u32 __kc_mmd_eee_adv_to_ethtool_adv_t(u16 eee_adv)
+{
+	u32 adv = 0;
+
+	if (eee_adv & MDIO_EEE_100TX)
+		adv |= ADVERTISED_100baseT_Full;
+	if (eee_adv & MDIO_EEE_1000T)
+		adv |= ADVERTISED_1000baseT_Full;
+	if (eee_adv & MDIO_EEE_10GT)
+		adv |= ADVERTISED_10000baseT_Full;
+	if (eee_adv & MDIO_EEE_1000KX)
+		adv |= ADVERTISED_1000baseKX_Full;
+	if (eee_adv & MDIO_EEE_10GKX4)
+		adv |= ADVERTISED_10000baseKX4_Full;
+	if (eee_adv & MDIO_EEE_10GKR)
+		adv |= ADVERTISED_10000baseKR_Full;
+
+	return adv;
+}
+
+#define mmd_eee_adv_to_ethtool_adv_t(eee_adv) \
+	__kc_mmd_eee_adv_to_ethtool_adv_t(eee_adv)
+#endif /* mmd_eee_adv_to_ethtool_adv_t */
+
+#ifndef ethtool_adv_to_mmd_eee_adv_t
+/**
+ * ethtool_adv_to_mmd_eee_adv_t
+ * @adv: the ethtool advertisement settings
+ *
+ * A small helper function that translates ethtool advertisement settings
+ * to EEE advertisements for the MMD EEE Advertisement (7.60) and
+ * MMD EEE Link Partner Ability (7.61) registers.
+ */
+static inline u16 __kc_ethtool_adv_to_mmd_eee_adv_t(u32 adv)
+{
+	u16 reg = 0;
+
+	if (adv & ADVERTISED_100baseT_Full)
+		reg |= MDIO_EEE_100TX;
+	if (adv & ADVERTISED_1000baseT_Full)
+		reg |= MDIO_EEE_1000T;
+	if (adv & ADVERTISED_10000baseT_Full)
+		reg |= MDIO_EEE_10GT;
+	if (adv & ADVERTISED_1000baseKX_Full)
+		reg |= MDIO_EEE_1000KX;
+	if (adv & ADVERTISED_10000baseKX4_Full)
+		reg |= MDIO_EEE_10GKX4;
+	if (adv & ADVERTISED_10000baseKR_Full)
+		reg |= MDIO_EEE_10GKR;
+
+	return reg;
+}
+#define ethtool_adv_to_mmd_eee_adv_t(adv) __kc_ethtool_adv_to_mmd_eee_adv_t(adv)
+#endif /* ethtool_adv_to_mmd_eee_adv_t */
+
+#ifndef pci_pcie_type
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,24) )
+static inline u8 pci_pcie_type(struct pci_dev *pdev)
+{
+	int pos;
+	u16 reg16;
+
+	pos = pci_find_capability(pdev, PCI_CAP_ID_EXP);
+	BUG_ON(!pos);
+	pci_read_config_word(pdev, pos + PCI_EXP_FLAGS, &reg16);
+	return (reg16 & PCI_EXP_FLAGS_TYPE) >> 4;
+}
+#else /* < 2.6.24 */
+#define pci_pcie_type(x)	(x)->pcie_type
+#endif /* < 2.6.24 */
+#endif /* pci_pcie_type */
+
+#if ( ! ( RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(6,4) ) ) && \
+    ( ! ( SLE_VERSION_CODE >= SLE_VERSION(11,3,0) ) ) && \
+    ( LINUX_VERSION_CODE >= KERNEL_VERSION(3,0,0) )
+#define ptp_clock_register(caps, args...) ptp_clock_register(caps)
+#endif
+
+#ifndef pcie_capability_read_word
+int __kc_pcie_capability_read_word(struct pci_dev *dev, int pos, u16 *val);
+#define pcie_capability_read_word(d,p,v) __kc_pcie_capability_read_word(d,p,v)
+#endif /* pcie_capability_read_word */
+
+#ifndef pcie_capability_read_dword
+int __kc_pcie_capability_read_dword(struct pci_dev *dev, int pos, u32 *val);
+#define pcie_capability_read_dword(d,p,v) __kc_pcie_capability_read_dword(d,p,v)
+#endif
+
+#ifndef pcie_capability_write_word
+int __kc_pcie_capability_write_word(struct pci_dev *dev, int pos, u16 val);
+#define pcie_capability_write_word(d,p,v) __kc_pcie_capability_write_word(d,p,v)
+#endif /* pcie_capability_write_word */
+
+#ifndef pcie_capability_clear_and_set_word
+int __kc_pcie_capability_clear_and_set_word(struct pci_dev *dev, int pos,
+					    u16 clear, u16 set);
+#define pcie_capability_clear_and_set_word(d,p,c,s) \
+	__kc_pcie_capability_clear_and_set_word(d,p,c,s)
+#endif /* pcie_capability_clear_and_set_word */
+
+#ifndef pcie_capability_clear_word
+int __kc_pcie_capability_clear_word(struct pci_dev *dev, int pos,
+					     u16 clear);
+#define pcie_capability_clear_word(d, p, c) \
+	__kc_pcie_capability_clear_word(d, p, c)
+#endif /* pcie_capability_clear_word */
+
+#ifndef PCI_EXP_LNKSTA2
+#define PCI_EXP_LNKSTA2		50	/* Link Status 2 */
+#endif
+
+#if (SLE_VERSION_CODE && SLE_VERSION_CODE >= SLE_VERSION(11,3,0))
+#define USE_CONST_DEV_UC_CHAR
+#define HAVE_NDO_FDB_ADD_NLATTR
+#endif
+
+#if !(RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(6,8))
+#define napi_gro_flush(_napi, _flush_old) napi_gro_flush(_napi)
+#endif /* !RHEL6.8+ */
+
+#if (RHEL_RELEASE_CODE && RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(6,6))
+#include <linux/hashtable.h>
+#else
+
+#define DEFINE_HASHTABLE(name, bits)						\
+	struct hlist_head name[1 << (bits)] =					\
+			{ [0 ... ((1 << (bits)) - 1)] = HLIST_HEAD_INIT }
+
+#define DEFINE_READ_MOSTLY_HASHTABLE(name, bits)				\
+	struct hlist_head name[1 << (bits)] __read_mostly =			\
+			{ [0 ... ((1 << (bits)) - 1)] = HLIST_HEAD_INIT }
+
+#define DECLARE_HASHTABLE(name, bits)                                   	\
+	struct hlist_head name[1 << (bits)]
+
+#define HASH_SIZE(name) (ARRAY_SIZE(name))
+#define HASH_BITS(name) ilog2(HASH_SIZE(name))
+
+/* Use hash_32 when possible to allow for fast 32bit hashing in 64bit kernels. */
+#define hash_min(val, bits)							\
+	(sizeof(val) <= 4 ? hash_32(val, bits) : hash_long(val, bits))
+
+static inline void __hash_init(struct hlist_head *ht, unsigned int sz)
+{
+	unsigned int i;
+
+	for (i = 0; i < sz; i++)
+		INIT_HLIST_HEAD(&ht[i]);
+}
+
+#define hash_init(hashtable) __hash_init(hashtable, HASH_SIZE(hashtable))
+
+#define hash_add(hashtable, node, key)						\
+	hlist_add_head(node, &hashtable[hash_min(key, HASH_BITS(hashtable))])
+
+static inline bool hash_hashed(struct hlist_node *node)
+{
+	return !hlist_unhashed(node);
+}
+
+static inline bool __hash_empty(struct hlist_head *ht, unsigned int sz)
+{
+	unsigned int i;
+
+	for (i = 0; i < sz; i++)
+		if (!hlist_empty(&ht[i]))
+			return false;
+
+	return true;
+}
+
+#define hash_empty(hashtable) __hash_empty(hashtable, HASH_SIZE(hashtable))
+
+static inline void hash_del(struct hlist_node *node)
+{
+	hlist_del_init(node);
+}
+#endif /* RHEL >= 6.6 */
+
+/* We don't have @flags support prior to 3.7, so we'll simply ignore the flags
+ * parameter on these older kernels.
+ */
+#define __setup_timer(_timer, _fn, _data, _flags)	\
+	setup_timer((_timer), (_fn), (_data))		\
+
+#if ( ! ( RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(6,7) ) ) && \
+	( ! ( SLE_VERSION_CODE >= SLE_VERSION(12,0,0) ) )
+
+#ifndef mod_delayed_work
+/**
+ * __mod_delayed_work - modify delay or queue delayed work
+ * @wq: workqueue to use
+ * @dwork: delayed work to queue
+ * @delay: number of jiffies to wait before queueing
+ *
+ * Return: %true if @dwork was pending and was rescheduled;
+ *         %false if it wasn't pending
+ *
+ * Note: the dwork parameter was declared as a void*
+ *       to avoid comptibility problems with early 2.6 kernels
+ *       where struct delayed_work is not declared. Unlike the original
+ *       implementation flags are not preserved and it shouldn't be
+ *       used in the interrupt context.
+ */
+static inline bool __mod_delayed_work(struct workqueue_struct *wq,
+				    void *dwork,
+				    unsigned long delay)
+{
+	bool ret = cancel_delayed_work(dwork);
+	queue_delayed_work(wq, dwork, delay);
+	return ret;
+}
+#define mod_delayed_work(wq, dwork, delay) __mod_delayed_work(wq, dwork, delay)
+#endif /* mod_delayed_work */
+
+#endif /* !(RHEL >= 6.7) && !(SLE >= 12.0) */
+#else /* >= 3.7.0 */
+#include <linux/hashtable.h>
+#define HAVE_CONST_STRUCT_PCI_ERROR_HANDLERS
+#define USE_CONST_DEV_UC_CHAR
+#define HAVE_NDO_FDB_ADD_NLATTR
+#endif /* >= 3.7.0 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,8,0) )
+#if (!(RHEL_RELEASE_CODE && RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(6,5)) && \
+     !(SLE_VERSION_CODE && SLE_VERSION_CODE >= SLE_VERSION(11,4,0)))
+#ifndef pci_sriov_set_totalvfs
+static inline int __kc_pci_sriov_set_totalvfs(struct pci_dev __always_unused *dev, u16 __always_unused numvfs)
+{
+	return 0;
+}
+#define pci_sriov_set_totalvfs(a, b) __kc_pci_sriov_set_totalvfs((a), (b))
+#endif
+#endif /* !(RHEL_RELEASE_CODE >= 6.5 && SLE_VERSION_CODE >= 11.4) */
+#ifndef PCI_EXP_LNKCTL_ASPM_L0S
+#define  PCI_EXP_LNKCTL_ASPM_L0S  0x01	/* L0s Enable */
+#endif
+#ifndef PCI_EXP_LNKCTL_ASPM_L1
+#define  PCI_EXP_LNKCTL_ASPM_L1   0x02	/* L1 Enable */
+#endif
+#define HAVE_CONFIG_HOTPLUG
+/* Reserved Ethernet Addresses per IEEE 802.1Q */
+static const u8 eth_reserved_addr_base[ETH_ALEN] __aligned(2) = {
+	0x01, 0x80, 0xc2, 0x00, 0x00, 0x00 };
+
+#ifndef is_link_local_ether_addr
+static inline bool __kc_is_link_local_ether_addr(const u8 *addr)
+{
+	__be16 *a = (__be16 *)addr;
+	static const __be16 *b = (const __be16 *)eth_reserved_addr_base;
+	static const __be16 m = cpu_to_be16(0xfff0);
+
+	return ((a[0] ^ b[0]) | (a[1] ^ b[1]) | ((a[2] ^ b[2]) & m)) == 0;
+}
+#define is_link_local_ether_addr(addr) __kc_is_link_local_ether_addr(addr)
+#endif /* is_link_local_ether_addr */
+
+#ifndef FLOW_MAC_EXT
+#define FLOW_MAC_EXT	0x40000000
+#endif /* FLOW_MAC_EXT */
+
+#if (SLE_VERSION_CODE && SLE_VERSION_CODE >= SLE_VERSION(11,4,0))
+#define HAVE_SRIOV_CONFIGURE
+#endif
+
+#ifndef PCI_EXP_LNKCAP_SLS_2_5GB
+#define PCI_EXP_LNKCAP_SLS_2_5GB 0x00000001 /* LNKCAP2 SLS Vector bit 0 */
+#endif
+
+#ifndef PCI_EXP_LNKCAP_SLS_5_0GB
+#define PCI_EXP_LNKCAP_SLS_5_0GB 0x00000002 /* LNKCAP2 SLS Vector bit 1 */
+#endif
+
+#undef PCI_EXP_LNKCAP2_SLS_2_5GB
+#define PCI_EXP_LNKCAP2_SLS_2_5GB 0x00000002 /* Supported Speed 2.5GT/s */
+
+#undef PCI_EXP_LNKCAP2_SLS_5_0GB
+#define PCI_EXP_LNKCAP2_SLS_5_0GB 0x00000004 /* Supported Speed 5GT/s */
+
+#undef PCI_EXP_LNKCAP2_SLS_8_0GB
+#define PCI_EXP_LNKCAP2_SLS_8_0GB 0x00000008 /* Supported Speed 8GT/s */
+
+#else /* >= 3.8.0 */
+#ifndef __devinit
+#define __devinit
+#endif
+
+#ifndef __devinitdata
+#define __devinitdata
+#endif
+
+#ifndef __devinitconst
+#define __devinitconst
+#endif
+
+#ifndef __devexit
+#define __devexit
+#endif
+
+#ifndef __devexit_p
+#define __devexit_p
+#endif
+
+#ifndef HAVE_ENCAP_CSUM_OFFLOAD
+#define HAVE_ENCAP_CSUM_OFFLOAD
+#endif
+
+#ifndef HAVE_GRE_ENCAP_OFFLOAD
+#define HAVE_GRE_ENCAP_OFFLOAD
+#endif
+
+#ifndef HAVE_SRIOV_CONFIGURE
+#define HAVE_SRIOV_CONFIGURE
+#endif
+
+#define HAVE_BRIDGE_ATTRIBS
+#ifndef BRIDGE_MODE_VEB
+#define BRIDGE_MODE_VEB		0	/* Default loopback mode */
+#endif /* BRIDGE_MODE_VEB */
+#ifndef BRIDGE_MODE_VEPA
+#define BRIDGE_MODE_VEPA	1	/* 802.1Qbg defined VEPA mode */
+#endif /* BRIDGE_MODE_VEPA */
+#endif /* >= 3.8.0 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,9,0) )
+
+#undef BUILD_BUG_ON
+#ifdef __CHECKER__
+#define BUILD_BUG_ON(condition) (0)
+#else /* __CHECKER__ */
+#ifndef __compiletime_warning
+#if defined(__GNUC__) && ((__GNUC__ * 10000 + __GNUC_MINOR__ * 100) >= 40400)
+#define __compiletime_warning(message) __attribute__((warning(message)))
+#else /* __GNUC__ */
+#define __compiletime_warning(message)
+#endif /* __GNUC__ */
+#endif /* __compiletime_warning */
+#ifndef __compiletime_error
+#if defined(__GNUC__) && ((__GNUC__ * 10000 + __GNUC_MINOR__ * 100) >= 40400)
+#define __compiletime_error(message) __attribute__((error(message)))
+#define __compiletime_error_fallback(condition) do { } while (0)
+#else /* __GNUC__ */
+#define __compiletime_error(message)
+#define __compiletime_error_fallback(condition) \
+	do { ((void)sizeof(char[1 - 2 * condition])); } while (0)
+#endif /* __GNUC__ */
+#else /* __compiletime_error */
+#define __compiletime_error_fallback(condition) do { } while (0)
+#endif /* __compiletime_error */
+#define __compiletime_assert(condition, msg, prefix, suffix)		\
+	do {								\
+		bool __cond = !(condition);				\
+		extern void prefix ## suffix(void) __compiletime_error(msg); \
+		if (__cond)						\
+			prefix ## suffix();				\
+		__compiletime_error_fallback(__cond);			\
+	} while (0)
+
+#define _compiletime_assert(condition, msg, prefix, suffix) \
+	__compiletime_assert(condition, msg, prefix, suffix)
+#define compiletime_assert(condition, msg) \
+	_compiletime_assert(condition, msg, __compiletime_assert_, __LINE__)
+#define BUILD_BUG_ON_MSG(cond, msg) compiletime_assert(!(cond), msg)
+#ifndef __OPTIMIZE__
+#define BUILD_BUG_ON(condition) ((void)sizeof(char[1 - 2*!!(condition)]))
+#else /* __OPTIMIZE__ */
+#define BUILD_BUG_ON(condition) \
+	BUILD_BUG_ON_MSG(condition, "BUILD_BUG_ON failed: " #condition)
+#endif /* __OPTIMIZE__ */
+#endif /* __CHECKER__ */
+
+#undef hlist_entry
+#define hlist_entry(ptr, type, member) container_of(ptr,type,member)
+
+#undef hlist_entry_safe
+#define hlist_entry_safe(ptr, type, member) \
+	({ typeof(ptr) ____ptr = (ptr); \
+	   ____ptr ? hlist_entry(____ptr, type, member) : NULL; \
+	})
+
+#undef hlist_for_each_entry
+#define hlist_for_each_entry(pos, head, member)                             \
+	for (pos = hlist_entry_safe((head)->first, typeof(*(pos)), member); \
+	     pos;                                                           \
+	     pos = hlist_entry_safe((pos)->member.next, typeof(*(pos)), member))
+
+#undef hlist_for_each_entry_safe
+#define hlist_for_each_entry_safe(pos, n, head, member) 		    \
+	for (pos = hlist_entry_safe((head)->first, typeof(*pos), member);   \
+	     pos && ({ n = pos->member.next; 1; });			    \
+	     pos = hlist_entry_safe(n, typeof(*pos), member))
+
+#undef hlist_for_each_entry_continue
+#define hlist_for_each_entry_continue(pos, member)			\
+	for (pos = hlist_entry_safe((pos)->member.next, typeof(*(pos)), member);\
+	     pos;							\
+	     pos = hlist_entry_safe((pos)->member.next, typeof(*(pos)), member))
+
+#undef hlist_for_each_entry_from
+#define hlist_for_each_entry_from(pos, member)				\
+	for (; pos;							\
+	     pos = hlist_entry_safe((pos)->member.next, typeof(*(pos)), member))
+
+#undef hash_for_each
+#define hash_for_each(name, bkt, obj, member)				\
+	for ((bkt) = 0, obj = NULL; obj == NULL && (bkt) < HASH_SIZE(name);\
+			(bkt)++)\
+		hlist_for_each_entry(obj, &name[bkt], member)
+
+#undef hash_for_each_safe
+#define hash_for_each_safe(name, bkt, tmp, obj, member)			\
+	for ((bkt) = 0, obj = NULL; obj == NULL && (bkt) < HASH_SIZE(name);\
+			(bkt)++)\
+		hlist_for_each_entry_safe(obj, tmp, &name[bkt], member)
+
+#undef hash_for_each_possible
+#define hash_for_each_possible(name, obj, member, key)			\
+	hlist_for_each_entry(obj, &name[hash_min(key, HASH_BITS(name))], member)
+
+#undef hash_for_each_possible_safe
+#define hash_for_each_possible_safe(name, obj, tmp, member, key)	\
+	hlist_for_each_entry_safe(obj, tmp,\
+		&name[hash_min(key, HASH_BITS(name))], member)
+
+#ifdef CONFIG_XPS
+int __kc_netif_set_xps_queue(struct net_device *, const struct cpumask *, u16);
+#define netif_set_xps_queue(_dev, _mask, _idx) __kc_netif_set_xps_queue((_dev), (_mask), (_idx))
+#else /* CONFIG_XPS */
+#define netif_set_xps_queue(_dev, _mask, _idx) do {} while (0)
+#endif /* CONFIG_XPS */
+
+#ifdef HAVE_NETDEV_SELECT_QUEUE
+#define _kc_hashrnd 0xd631614b /* not so random hash salt */
+u16 __kc_netdev_pick_tx(struct net_device *dev, struct sk_buff *skb);
+#define __netdev_pick_tx __kc_netdev_pick_tx
+#endif /* HAVE_NETDEV_SELECT_QUEUE */
+#else
+#define HAVE_BRIDGE_FILTER
+#define HAVE_FDB_DEL_NLATTR
+#endif /* < 3.9.0 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,10,0) )
+#ifndef NAPI_POLL_WEIGHT
+#define NAPI_POLL_WEIGHT 64
+#endif
+#ifdef CONFIG_PCI_IOV
+int __kc_pci_vfs_assigned(struct pci_dev *dev);
+#else
+static inline int __kc_pci_vfs_assigned(struct pci_dev __always_unused *dev)
+{
+	return 0;
+}
+#endif
+#define pci_vfs_assigned(dev) __kc_pci_vfs_assigned(dev)
+
+#ifndef list_first_entry_or_null
+#define list_first_entry_or_null(ptr, type, member) \
+	(!list_empty(ptr) ? list_first_entry(ptr, type, member) : NULL)
+#endif
+
+#ifndef VLAN_TX_COOKIE_MAGIC
+static inline struct sk_buff *__kc__vlan_hwaccel_put_tag(struct sk_buff *skb,
+							 u16 vlan_tci)
+{
+#ifdef VLAN_TAG_PRESENT
+	vlan_tci |= VLAN_TAG_PRESENT;
+#endif
+	skb->vlan_tci = vlan_tci;
+        return skb;
+}
+#define __vlan_hwaccel_put_tag(skb, vlan_proto, vlan_tci) \
+	__kc__vlan_hwaccel_put_tag(skb, vlan_tci)
+#endif
+
+#ifdef HAVE_FDB_OPS
+#if defined(HAVE_NDO_FDB_ADD_NLATTR)
+int __kc_ndo_dflt_fdb_add(struct ndmsg *ndm, struct nlattr *tb[],
+			  struct net_device *dev,
+			  const unsigned char *addr, u16 flags);
+#elif defined(USE_CONST_DEV_UC_CHAR)
+int __kc_ndo_dflt_fdb_add(struct ndmsg *ndm, struct net_device *dev,
+			  const unsigned char *addr, u16 flags);
+#else
+int __kc_ndo_dflt_fdb_add(struct ndmsg *ndm, struct net_device *dev,
+			  unsigned char *addr, u16 flags);
+#endif /* HAVE_NDO_FDB_ADD_NLATTR */
+#if defined(HAVE_FDB_DEL_NLATTR)
+int __kc_ndo_dflt_fdb_del(struct ndmsg *ndm, struct nlattr *tb[],
+			  struct net_device *dev,
+			  const unsigned char *addr);
+#elif defined(USE_CONST_DEV_UC_CHAR)
+int __kc_ndo_dflt_fdb_del(struct ndmsg *ndm, struct net_device *dev,
+			  const unsigned char *addr);
+#else
+int __kc_ndo_dflt_fdb_del(struct ndmsg *ndm, struct net_device *dev,
+			  unsigned char *addr);
+#endif /* HAVE_FDB_DEL_NLATTR */
+#define ndo_dflt_fdb_add __kc_ndo_dflt_fdb_add
+#define ndo_dflt_fdb_del __kc_ndo_dflt_fdb_del
+#endif /* HAVE_FDB_OPS */
+
+#ifndef PCI_DEVID
+#define PCI_DEVID(bus, devfn)  ((((u16)(bus)) << 8) | (devfn))
+#endif
+
+/* The definitions for these functions when CONFIG_OF_NET is defined are
+ * pulled in from <linux/of_net.h>. For kernels older than 3.5 we already have
+ * backports for when CONFIG_OF_NET is true. These are separated and
+ * duplicated in order to cover all cases so that all kernels get either the
+ * real definitions (when CONFIG_OF_NET is defined) or the stub definitions
+ * (when CONFIG_OF_NET is not defined, or the kernel is too old to have real
+ * definitions).
+ */
+#ifndef CONFIG_OF_NET
+static inline int of_get_phy_mode(struct device_node __always_unused *np)
+{
+	return -ENODEV;
+}
+
+static inline const void *
+of_get_mac_address(struct device_node __always_unused *np)
+{
+	return NULL;
+}
+#endif
+
+#else /* >= 3.10.0 */
+#define HAVE_ENCAP_TSO_OFFLOAD
+#define USE_DEFAULT_FDB_DEL_DUMP
+#define HAVE_SKB_INNER_NETWORK_HEADER
+
+#if (RHEL_RELEASE_CODE && \
+     (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,0)))
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(8,0))
+#define HAVE_RHEL7_PCI_DRIVER_RH
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,2))
+#define HAVE_RHEL7_PCI_RESET_NOTIFY
+#endif /* RHEL >= 7.2 */
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,3))
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,5))
+#define HAVE_GENEVE_RX_OFFLOAD
+#endif /* RHEL < 7.5 */
+#define HAVE_ETHTOOL_FLOW_UNION_IP6_SPEC
+#define HAVE_RHEL7_NET_DEVICE_OPS_EXT
+#if !defined(HAVE_UDP_ENC_TUNNEL) && IS_ENABLED(CONFIG_GENEVE)
+#define HAVE_UDP_ENC_TUNNEL
+#endif /* !HAVE_UDP_ENC_TUNNEL && CONFIG_GENEVE */
+#endif /* RHEL >= 7.3 */
+
+/* new hooks added to net_device_ops_extended in RHEL7.4 */
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,4))
+#define HAVE_RHEL7_NETDEV_OPS_EXT_NDO_SET_VF_VLAN
+#define HAVE_RHEL7_NETDEV_OPS_EXT_NDO_UDP_TUNNEL
+#define HAVE_UDP_ENC_RX_OFFLOAD
+#endif /* RHEL >= 7.4 */
+#else  /* RHEL >= 8.0 */
+#define HAVE_TCF_BLOCK_CB_REGISTER_EXTACK
+#define NO_NETDEV_BPF_PROG_ATTACHED
+#define HAVE_NDO_SELECT_QUEUE_SB_DEV
+#endif /* RHEL >= 8.0 */
+#endif /* RHEL >= 7.0 */
+#endif /* >= 3.10.0 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,11,0) )
+#define netdev_notifier_info_to_dev(ptr) ptr
+#ifndef time_in_range64
+#define time_in_range64(a, b, c) \
+	(time_after_eq64(a, b) && \
+	 time_before_eq64(a, c))
+#endif /* time_in_range64 */
+#if ((RHEL_RELEASE_CODE && RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(6,6)) ||\
+     (SLE_VERSION_CODE && SLE_VERSION_CODE >= SLE_VERSION(11,4,0)))
+#define HAVE_NDO_SET_VF_LINK_STATE
+#endif
+#if RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(7,2))
+#define HAVE_NDO_SELECT_QUEUE_ACCEL_FALLBACK
+#endif
+#else /* >= 3.11.0 */
+#define HAVE_NDO_SET_VF_LINK_STATE
+#define HAVE_SKB_INNER_PROTOCOL
+#define HAVE_MPLS_FEATURES
+#endif /* >= 3.11.0 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,12,0) )
+int __kc_pcie_get_minimum_link(struct pci_dev *dev, enum pci_bus_speed *speed,
+			       enum pcie_link_width *width);
+#ifndef pcie_get_minimum_link
+#define pcie_get_minimum_link(_p, _s, _w) __kc_pcie_get_minimum_link(_p, _s, _w)
+#endif
+
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(6,7))
+int _kc_pci_wait_for_pending_transaction(struct pci_dev *dev);
+#define pci_wait_for_pending_transaction _kc_pci_wait_for_pending_transaction
+#endif /* <RHEL6.7 */
+
+#else /* >= 3.12.0 */
+#if ( SLE_VERSION_CODE && SLE_VERSION_CODE >= SLE_VERSION(12,0,0))
+#define HAVE_NDO_SELECT_QUEUE_ACCEL_FALLBACK
+#endif
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(4,8,0) )
+#define HAVE_VXLAN_RX_OFFLOAD
+#if !defined(HAVE_UDP_ENC_TUNNEL) && IS_ENABLED(CONFIG_VXLAN)
+#define HAVE_UDP_ENC_TUNNEL
+#endif
+#endif /* < 4.8.0 */
+#define HAVE_NDO_GET_PHYS_PORT_ID
+#define HAVE_NETIF_SET_XPS_QUEUE_CONST_MASK
+#endif /* >= 3.12.0 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,13,0) )
+#define dma_set_mask_and_coherent(_p, _m) __kc_dma_set_mask_and_coherent(_p, _m)
+int __kc_dma_set_mask_and_coherent(struct device *dev, u64 mask);
+#ifndef u64_stats_init
+#define u64_stats_init(a) do { } while(0)
+#endif
+#undef BIT_ULL
+#define BIT_ULL(n) (1ULL << (n))
+
+#if (!(SLE_VERSION_CODE && SLE_VERSION_CODE >= SLE_VERSION(12,0,0)) && \
+     !(RHEL_RELEASE_CODE && RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,0)))
+static inline struct pci_dev *pci_upstream_bridge(struct pci_dev *dev)
+{
+	dev = pci_physfn(dev);
+	if (pci_is_root_bus(dev->bus))
+		return NULL;
+
+	return dev->bus->self;
+}
+#endif
+
+#if (SLE_VERSION_CODE && SLE_VERSION_CODE >= SLE_VERSION(12,1,0))
+#undef HAVE_STRUCT_PAGE_PFMEMALLOC
+#define HAVE_DCBNL_OPS_SETAPP_RETURN_INT
+#endif
+#ifndef list_next_entry
+#define list_next_entry(pos, member) \
+	list_entry((pos)->member.next, typeof(*(pos)), member)
+#endif
+#ifndef list_prev_entry
+#define list_prev_entry(pos, member) \
+	list_entry((pos)->member.prev, typeof(*(pos)), member)
+#endif
+
+#if ( LINUX_VERSION_CODE > KERNEL_VERSION(2,6,20) )
+#define devm_kcalloc(dev, cnt, size, flags) \
+	devm_kzalloc(dev, (cnt) * (size), flags)
+#endif /* > 2.6.20 */
+
+#if (!(RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,2)))
+#define list_last_entry(ptr, type, member) list_entry((ptr)->prev, type, member)
+#endif
+
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,0))
+bool _kc_pci_device_is_present(struct pci_dev *pdev);
+#define pci_device_is_present _kc_pci_device_is_present
+#endif /* <RHEL7.0 */
+#else /* >= 3.13.0 */
+#define HAVE_VXLAN_CHECKS
+#if (UBUNTU_VERSION_CODE && UBUNTU_VERSION_CODE >= UBUNTU_VERSION(3,13,0,24))
+#define HAVE_NDO_SELECT_QUEUE_ACCEL_FALLBACK
+#else
+#define HAVE_NDO_SELECT_QUEUE_ACCEL
+#endif
+#define HAVE_HWMON_DEVICE_REGISTER_WITH_GROUPS
+#endif
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,14,0) )
+
+#ifndef U16_MAX
+#define U16_MAX ((u16)~0U)
+#endif
+
+#ifndef U32_MAX
+#define U32_MAX ((u32)~0U)
+#endif
+
+#ifndef U64_MAX
+#define U64_MAX ((u64)~0ULL)
+#endif
+
+#if (!(RHEL_RELEASE_CODE && RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,2)))
+#define dev_consume_skb_any(x) dev_kfree_skb_any(x)
+#define dev_consume_skb_irq(x) dev_kfree_skb_irq(x)
+#endif
+
+#if (!(RHEL_RELEASE_CODE && RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,0)) && \
+     !(SLE_VERSION_CODE && SLE_VERSION_CODE >= SLE_VERSION(12,0,0)))
+
+/* it isn't expected that this would be a #define unless we made it so */
+#ifndef skb_set_hash
+
+#define PKT_HASH_TYPE_NONE	0
+#define PKT_HASH_TYPE_L2	1
+#define PKT_HASH_TYPE_L3	2
+#define PKT_HASH_TYPE_L4	3
+
+enum _kc_pkt_hash_types {
+	_KC_PKT_HASH_TYPE_NONE = PKT_HASH_TYPE_NONE,
+	_KC_PKT_HASH_TYPE_L2 = PKT_HASH_TYPE_L2,
+	_KC_PKT_HASH_TYPE_L3 = PKT_HASH_TYPE_L3,
+	_KC_PKT_HASH_TYPE_L4 = PKT_HASH_TYPE_L4,
+};
+#define pkt_hash_types         _kc_pkt_hash_types
+
+#define skb_set_hash __kc_skb_set_hash
+static inline void __kc_skb_set_hash(struct sk_buff __maybe_unused *skb,
+				     u32 __maybe_unused hash,
+				     int __maybe_unused type)
+{
+#ifdef HAVE_SKB_L4_RXHASH
+	skb->l4_rxhash = (type == PKT_HASH_TYPE_L4);
+#endif
+#ifdef NETIF_F_RXHASH
+	skb->rxhash = hash;
+#endif
+}
+#endif /* !skb_set_hash */
+
+#else	/* RHEL_RELEASE_CODE >= 7.0 || SLE_VERSION_CODE >= 12.0 */
+
+#if ((RHEL_RELEASE_CODE && RHEL_RELEASE_CODE <= RHEL_RELEASE_VERSION(7,0)) ||\
+     (SLE_VERSION_CODE && SLE_VERSION_CODE <= SLE_VERSION(12,1,0)))
+/* GPLv2 code taken from 5.10-rc2 kernel source include/linux/pci.h, Copyright
+ * original authors.
+ */
+static inline int pci_enable_msix_exact(struct pci_dev *dev,
+					struct msix_entry *entries, int nvec)
+{
+	int rc = pci_enable_msix_range(dev, entries, nvec, nvec);
+	if (rc < 0)
+		return rc;
+	return 0;
+}
+#endif /* <=EL7.0 || <=SLES 12.1 */
+#if (!(RHEL_RELEASE_CODE && RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,5)))
+#ifndef HAVE_VXLAN_RX_OFFLOAD
+#define HAVE_VXLAN_RX_OFFLOAD
+#endif /* HAVE_VXLAN_RX_OFFLOAD */
+#endif
+
+#if !defined(HAVE_UDP_ENC_TUNNEL) && IS_ENABLED(CONFIG_VXLAN)
+#define HAVE_UDP_ENC_TUNNEL
+#endif
+
+#ifndef HAVE_VXLAN_CHECKS
+#define HAVE_VXLAN_CHECKS
+#endif /* HAVE_VXLAN_CHECKS */
+#endif /* !(RHEL_RELEASE_CODE >= 7.0 && SLE_VERSION_CODE >= 12.0) */
+
+#if ((RHEL_RELEASE_CODE && RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,3)) ||\
+     (SLE_VERSION_CODE && SLE_VERSION_CODE >= SLE_VERSION(12,0,0)))
+#define HAVE_NDO_DFWD_OPS
+#endif
+
+#ifndef pci_enable_msix_range
+int __kc_pci_enable_msix_range(struct pci_dev *dev, struct msix_entry *entries,
+			       int minvec, int maxvec);
+#define pci_enable_msix_range __kc_pci_enable_msix_range
+#endif
+
+#ifndef ether_addr_copy
+#define ether_addr_copy __kc_ether_addr_copy
+static inline void __kc_ether_addr_copy(u8 *dst, const u8 *src)
+{
+#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS)
+	*(u32 *)dst = *(const u32 *)src;
+	*(u16 *)(dst + 4) = *(const u16 *)(src + 4);
+#else
+	u16 *a = (u16 *)dst;
+	const u16 *b = (const u16 *)src;
+
+	a[0] = b[0];
+	a[1] = b[1];
+	a[2] = b[2];
+#endif
+}
+#endif /* ether_addr_copy */
+int __kc_ipv6_find_hdr(const struct sk_buff *skb, unsigned int *offset,
+		       int target, unsigned short *fragoff, int *flags);
+#define ipv6_find_hdr(a, b, c, d, e) __kc_ipv6_find_hdr((a), (b), (c), (d), (e))
+
+#ifndef OPTIMIZE_HIDE_VAR
+#ifdef __GNUC__
+#define OPTIMIZER_HIDE_VAR(var) __asm__ ("" : "=r" (var) : "0" (var))
+#else
+#include <linux/barrier.h>
+#define OPTIMIZE_HIDE_VAR(var)	barrier()
+#endif
+#endif
+
+#if (!(RHEL_RELEASE_CODE && RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(7,0)) && \
+     !(SLE_VERSION_CODE && SLE_VERSION_CODE >= SLE_VERSION(10,4,0)))
+static inline __u32 skb_get_hash_raw(const struct sk_buff *skb)
+{
+#ifdef NETIF_F_RXHASH
+	return skb->rxhash;
+#else
+	return 0;
+#endif /* NETIF_F_RXHASH */
+}
+#endif /* !RHEL > 5.9 && !SLES >= 10.4 */
+
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,5))
+#define request_firmware_direct	request_firmware
+#endif /* !RHEL || RHEL < 7.5 */
+
+#else /* >= 3.14.0 */
+
+/* for ndo_dfwd_ ops add_station, del_station and _start_xmit */
+#ifndef HAVE_NDO_DFWD_OPS
+#define HAVE_NDO_DFWD_OPS
+#endif
+#define HAVE_NDO_SELECT_QUEUE_ACCEL_FALLBACK
+#endif /* 3.14.0 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,15,0) )
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,35) )
+#define HAVE_SKBUFF_RXHASH
+#endif /* >= 2.6.35 */
+#if (!(RHEL_RELEASE_CODE && RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(7,1)) && \
+     !(UBUNTU_VERSION_CODE && UBUNTU_VERSION_CODE >= UBUNTU_VERSION(3,13,0,30)))
+#define u64_stats_fetch_begin_irq u64_stats_fetch_begin_bh
+#define u64_stats_fetch_retry_irq u64_stats_fetch_retry_bh
+#endif
+
+char *_kc_devm_kstrdup(struct device *dev, const char *s, gfp_t gfp);
+#define devm_kstrdup(dev, s, gfp) _kc_devm_kstrdup(dev, s, gfp)
+
+#else /* >= 3.15.0 */
+#define HAVE_NET_GET_RANDOM_ONCE
+#define HAVE_PTP_1588_CLOCK_PINS
+#define HAVE_NETDEV_PORT
+#endif /* 3.15.0 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,16,0) )
+#ifndef smp_mb__before_atomic
+#define smp_mb__before_atomic() smp_mb()
+#define smp_mb__after_atomic()  smp_mb()
+#endif
+#ifndef __dev_uc_sync
+#ifdef HAVE_SET_RX_MODE
+#ifdef NETDEV_HW_ADDR_T_UNICAST
+int __kc_hw_addr_sync_dev(struct netdev_hw_addr_list *list,
+		struct net_device *dev,
+		int (*sync)(struct net_device *, const unsigned char *),
+		int (*unsync)(struct net_device *, const unsigned char *));
+void __kc_hw_addr_unsync_dev(struct netdev_hw_addr_list *list,
+		struct net_device *dev,
+		int (*unsync)(struct net_device *, const unsigned char *));
+#endif
+#ifndef NETDEV_HW_ADDR_T_MULTICAST
+int __kc_dev_addr_sync_dev(struct dev_addr_list **list, int *count,
+		struct net_device *dev,
+		int (*sync)(struct net_device *, const unsigned char *),
+		int (*unsync)(struct net_device *, const unsigned char *));
+void __kc_dev_addr_unsync_dev(struct dev_addr_list **list, int *count,
+		struct net_device *dev,
+		int (*unsync)(struct net_device *, const unsigned char *));
+#endif
+#endif /* HAVE_SET_RX_MODE */
+
+static inline int __kc_dev_uc_sync(struct net_device __maybe_unused *dev,
+				   int __maybe_unused (*sync)(struct net_device *, const unsigned char *),
+				   int __maybe_unused (*unsync)(struct net_device *, const unsigned char *))
+{
+#ifdef NETDEV_HW_ADDR_T_UNICAST
+	return __kc_hw_addr_sync_dev(&dev->uc, dev, sync, unsync);
+#elif defined(HAVE_SET_RX_MODE)
+	return __kc_dev_addr_sync_dev(&dev->uc_list, &dev->uc_count,
+				      dev, sync, unsync);
+#else
+	return 0;
+#endif
+}
+#define __dev_uc_sync __kc_dev_uc_sync
+
+static inline void __kc_dev_uc_unsync(struct net_device __maybe_unused *dev,
+				      int __maybe_unused (*unsync)(struct net_device *, const unsigned char *))
+{
+#ifdef HAVE_SET_RX_MODE
+#ifdef NETDEV_HW_ADDR_T_UNICAST
+	__kc_hw_addr_unsync_dev(&dev->uc, dev, unsync);
+#else /* NETDEV_HW_ADDR_T_MULTICAST */
+	__kc_dev_addr_unsync_dev(&dev->uc_list, &dev->uc_count, dev, unsync);
+#endif /* NETDEV_HW_ADDR_T_UNICAST */
+#endif /* HAVE_SET_RX_MODE */
+}
+#define __dev_uc_unsync __kc_dev_uc_unsync
+
+static inline int __kc_dev_mc_sync(struct net_device __maybe_unused *dev,
+				   int __maybe_unused (*sync)(struct net_device *, const unsigned char *),
+				   int __maybe_unused (*unsync)(struct net_device *, const unsigned char *))
+{
+#ifdef NETDEV_HW_ADDR_T_MULTICAST
+	return __kc_hw_addr_sync_dev(&dev->mc, dev, sync, unsync);
+#elif defined(HAVE_SET_RX_MODE)
+	return __kc_dev_addr_sync_dev(&dev->mc_list, &dev->mc_count,
+				      dev, sync, unsync);
+#else
+	return 0;
+#endif
+
+}
+#define __dev_mc_sync __kc_dev_mc_sync
+
+static inline void __kc_dev_mc_unsync(struct net_device __maybe_unused *dev,
+				      int __maybe_unused (*unsync)(struct net_device *, const unsigned char *))
+{
+#ifdef HAVE_SET_RX_MODE
+#ifdef NETDEV_HW_ADDR_T_MULTICAST
+	__kc_hw_addr_unsync_dev(&dev->mc, dev, unsync);
+#else /* NETDEV_HW_ADDR_T_MULTICAST */
+	__kc_dev_addr_unsync_dev(&dev->mc_list, &dev->mc_count, dev, unsync);
+#endif /* NETDEV_HW_ADDR_T_MULTICAST */
+#endif /* HAVE_SET_RX_MODE */
+}
+#define __dev_mc_unsync __kc_dev_mc_unsync
+#endif /* __dev_uc_sync */
+
+#if RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(7,1))
+#define HAVE_NDO_SET_VF_MIN_MAX_TX_RATE
+#endif
+
+#ifndef NETIF_F_GSO_UDP_TUNNEL_CSUM
+/* if someone backports this, hopefully they backport as a #define.
+ * declare it as zero on older kernels so that if it get's or'd in
+ * it won't effect anything, therefore preventing core driver changes
+ */
+#define NETIF_F_GSO_UDP_TUNNEL_CSUM 0
+#define SKB_GSO_UDP_TUNNEL_CSUM 0
+#endif
+void *__kc_devm_kmemdup(struct device *dev, const void *src, size_t len,
+			gfp_t gfp);
+#define devm_kmemdup __kc_devm_kmemdup
+
+#else
+#if ( ( LINUX_VERSION_CODE < KERNEL_VERSION(4,13,0) ) && \
+      ! ( SLE_VERSION_CODE && ( SLE_VERSION_CODE >= SLE_VERSION(12,4,0)) ) )
+#define HAVE_PCI_ERROR_HANDLER_RESET_NOTIFY
+#endif /* >= 3.16.0 && < 4.13.0 && !(SLES >= 12sp4) */
+#define HAVE_NDO_SET_VF_MIN_MAX_TX_RATE
+#endif /* 3.16.0 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,17,0) )
+#if !(RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(6,8) && \
+      RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,0)) && \
+    !(RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,2))
+#ifndef timespec64
+#define timespec64 timespec
+static inline struct timespec64 timespec_to_timespec64(const struct timespec ts)
+{
+	return ts;
+}
+static inline struct timespec timespec64_to_timespec(const struct timespec64 ts64)
+{
+	return ts64;
+}
+#define timespec64_equal timespec_equal
+#define timespec64_compare timespec_compare
+#define set_normalized_timespec64 set_normalized_timespec
+#define timespec64_add_safe timespec_add_safe
+#define timespec64_add timespec_add
+#define timespec64_sub timespec_sub
+#define timespec64_valid timespec_valid
+#define timespec64_valid_strict timespec_valid_strict
+#define timespec64_to_ns timespec_to_ns
+#define ns_to_timespec64 ns_to_timespec
+#define ktime_to_timespec64 ktime_to_timespec
+#define ktime_get_ts64 ktime_get_ts
+#define ktime_get_real_ts64 ktime_get_real_ts
+#define timespec64_add_ns timespec_add_ns
+#endif /* timespec64 */
+#endif /* !(RHEL6.8<RHEL7.0) && !RHEL7.2+ */
+
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(6,8) && \
+     RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,0))
+static inline void ktime_get_real_ts64(struct timespec64 *ts)
+{
+	*ts = ktime_to_timespec64(ktime_get_real());
+}
+
+static inline void ktime_get_ts64(struct timespec64 *ts)
+{
+	*ts = ktime_to_timespec64(ktime_get());
+}
+#endif
+
+#if !(RHEL_RELEASE_CODE && RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,4))
+#define hlist_add_behind(_a, _b) hlist_add_after(_b, _a)
+#endif
+
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,5))
+#endif /* RHEL_RELEASE_CODE < RHEL7.5 */
+
+#if RHEL_RELEASE_CODE && \
+	RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(6,3) && \
+	RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,3)
+static inline u64 ktime_get_ns(void)
+{
+	return ktime_to_ns(ktime_get());
+}
+
+static inline u64 ktime_get_real_ns(void)
+{
+	return ktime_to_ns(ktime_get_real());
+}
+
+static inline u64 ktime_get_boot_ns(void)
+{
+	return ktime_to_ns(ktime_get_boottime());
+}
+#endif /* RHEL < 7.3 */
+
+#else
+#define HAVE_DCBNL_OPS_SETAPP_RETURN_INT
+#include <linux/time64.h>
+#define HAVE_RHASHTABLE
+#endif /* 3.17.0 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,18,0) )
+#ifndef NO_PTP_SUPPORT
+#include <linux/errqueue.h>
+struct sk_buff *__kc_skb_clone_sk(struct sk_buff *skb);
+void __kc_skb_complete_tx_timestamp(struct sk_buff *skb,
+				    struct skb_shared_hwtstamps *hwtstamps);
+#define skb_clone_sk __kc_skb_clone_sk
+#define skb_complete_tx_timestamp __kc_skb_complete_tx_timestamp
+#endif
+#if (!(RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(8,2))))
+u32 __kc_eth_get_headlen(const struct net_device *dev, unsigned char *data,
+			 unsigned int max_len);
+#else
+unsigned int __kc_eth_get_headlen(unsigned char *data, unsigned int max_len);
+#endif /* !RHEL >= 8.2 */
+
+#define eth_get_headlen __kc_eth_get_headlen
+#ifndef ETH_P_XDSA
+#define ETH_P_XDSA 0x00F8
+#endif
+/* RHEL 7.1 backported csum_level, but SLES 12 and 12-SP1 did not */
+#if RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,1))
+#define HAVE_SKBUFF_CSUM_LEVEL
+#endif /* >= RH 7.1 */
+
+/* RHEL 7.3 backported xmit_more */
+#if (RHEL_RELEASE_CODE && RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,3))
+#define HAVE_SKB_XMIT_MORE
+#endif /* >= RH 7.3 */
+
+#undef GENMASK
+#define GENMASK(h, l) \
+	(((~0UL) << (l)) & (~0UL >> (BITS_PER_LONG - 1 - (h))))
+#undef GENMASK_ULL
+#define GENMASK_ULL(h, l) \
+	(((~0ULL) << (l)) & (~0ULL >> (BITS_PER_LONG_LONG - 1 - (h))))
+
+#else /*  3.18.0 */
+#define HAVE_SKBUFF_CSUM_LEVEL
+#define HAVE_SKB_XMIT_MORE
+#define HAVE_SKB_INNER_PROTOCOL_TYPE
+#endif /* 3.18.0 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,18,4) )
+#else
+#define HAVE_NDO_FEATURES_CHECK
+#endif /* 3.18.4 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,18,13) )
+#ifndef WRITE_ONCE
+#define WRITE_ONCE(x, val) ({ ACCESS_ONCE(x) = (val); })
+#endif
+#endif /* 3.18.13 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,19,0) )
+/* netdev_phys_port_id renamed to netdev_phys_item_id */
+#define netdev_phys_item_id netdev_phys_port_id
+
+static inline void _kc_napi_complete_done(struct napi_struct *napi,
+					  int __always_unused work_done) {
+	napi_complete(napi);
+}
+/* don't use our backport if the distro kernels already have it */
+#if (SLE_VERSION_CODE && (SLE_VERSION_CODE < SLE_VERSION(12,3,0))) || \
+    (RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,5)))
+#define napi_complete_done _kc_napi_complete_done
+#endif
+
+int _kc_bitmap_print_to_pagebuf(bool list, char *buf,
+				const unsigned long *maskp, int nmaskbits);
+#define bitmap_print_to_pagebuf _kc_bitmap_print_to_pagebuf
+
+#ifndef NETDEV_RSS_KEY_LEN
+#define NETDEV_RSS_KEY_LEN (13 * 4)
+#endif
+#if (!(RHEL_RELEASE_CODE && \
+      ((RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(6,7) && RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,0)) || \
+       (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,2)))))
+#define netdev_rss_key_fill(buffer, len) __kc_netdev_rss_key_fill(buffer, len)
+#endif /* RHEL_RELEASE_CODE */
+void __kc_netdev_rss_key_fill(void *buffer, size_t len);
+#define SPEED_20000 20000
+#define SPEED_40000 40000
+#ifndef dma_rmb
+#define dma_rmb() rmb()
+#endif
+#ifndef dev_alloc_pages
+#ifndef NUMA_NO_NODE
+#define NUMA_NO_NODE -1
+#endif
+#define dev_alloc_pages(_order) alloc_pages_node(NUMA_NO_NODE, (GFP_ATOMIC | __GFP_COLD | __GFP_COMP | __GFP_MEMALLOC), (_order))
+#endif
+#ifndef dev_alloc_page
+#define dev_alloc_page() dev_alloc_pages(0)
+#endif
+#if !defined(eth_skb_pad) && !defined(skb_put_padto)
+/**
+ *     __kc_skb_put_padto - increase size and pad an skbuff up to a minimal size
+ *     @skb: buffer to pad
+ *     @len: minimal length
+ *
+ *     Pads up a buffer to ensure the trailing bytes exist and are
+ *     blanked. If the buffer already contains sufficient data it
+ *     is untouched. Otherwise it is extended. Returns zero on
+ *     success. The skb is freed on error.
+ */
+static inline int __kc_skb_put_padto(struct sk_buff *skb, unsigned int len)
+{
+	unsigned int size = skb->len;
+
+	if (unlikely(size < len)) {
+		len -= size;
+		if (skb_pad(skb, len))
+			return -ENOMEM;
+		__skb_put(skb, len);
+	}
+	return 0;
+}
+#define skb_put_padto(skb, len) __kc_skb_put_padto(skb, len)
+
+static inline int __kc_eth_skb_pad(struct sk_buff *skb)
+{
+	return __kc_skb_put_padto(skb, ETH_ZLEN);
+}
+#define eth_skb_pad(skb) __kc_eth_skb_pad(skb)
+#endif /* eth_skb_pad && skb_put_padto */
+
+#ifndef SKB_ALLOC_NAPI
+/* RHEL 7.2 backported napi_alloc_skb and friends */
+static inline struct sk_buff *__kc_napi_alloc_skb(struct napi_struct *napi, unsigned int length)
+{
+	return netdev_alloc_skb_ip_align(napi->dev, length);
+}
+#define napi_alloc_skb(napi,len) __kc_napi_alloc_skb(napi,len)
+#define __napi_alloc_skb(napi,len,mask) __kc_napi_alloc_skb(napi,len)
+#endif /* SKB_ALLOC_NAPI */
+#define HAVE_CONFIG_PM_RUNTIME
+#if (RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(6,7)) && \
+     (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,0)))
+#define HAVE_RXFH_HASHFUNC
+#endif /* 6.7 < RHEL < 7.0 */
+#if RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(7,1))
+#define HAVE_RXFH_HASHFUNC
+#define NDO_DFLT_BRIDGE_GETLINK_HAS_BRFLAGS
+#endif /* RHEL > 7.1 */
+#ifndef napi_schedule_irqoff
+#define napi_schedule_irqoff	napi_schedule
+#endif
+#ifndef READ_ONCE
+#define READ_ONCE(_x) ACCESS_ONCE(_x)
+#endif
+#if RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(7,2))
+#define HAVE_NDO_FDB_ADD_VID
+#endif
+#ifndef ETH_MODULE_SFF_8636
+#define ETH_MODULE_SFF_8636		0x3
+#endif
+#ifndef ETH_MODULE_SFF_8636_LEN
+#define ETH_MODULE_SFF_8636_LEN		256
+#endif
+#ifndef ETH_MODULE_SFF_8436
+#define ETH_MODULE_SFF_8436		0x4
+#endif
+#ifndef ETH_MODULE_SFF_8436_LEN
+#define ETH_MODULE_SFF_8436_LEN		256
+#endif
+#ifndef writel_relaxed
+#define writel_relaxed	writel
+#endif
+#else /* 3.19.0 */
+#define HAVE_NDO_FDB_ADD_VID
+#define HAVE_RXFH_HASHFUNC
+#define NDO_DFLT_BRIDGE_GETLINK_HAS_BRFLAGS
+#endif /* 3.19.0 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,20,0) )
+/* vlan_tx_xx functions got renamed to skb_vlan */
+#ifndef skb_vlan_tag_get
+#define skb_vlan_tag_get vlan_tx_tag_get
+#endif
+#ifndef skb_vlan_tag_present
+#define skb_vlan_tag_present vlan_tx_tag_present
+#endif
+#if RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(7,1))
+#define HAVE_INCLUDE_LINUX_TIMECOUNTER_H
+#endif
+#if RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(7,2))
+#define HAVE_NDO_BRIDGE_SET_DEL_LINK_FLAGS
+#endif
+#else
+#define HAVE_INCLUDE_LINUX_TIMECOUNTER_H
+#define HAVE_NDO_BRIDGE_SET_DEL_LINK_FLAGS
+#endif /* 3.20.0 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(4,0,0) )
+/* Definition for CONFIG_OF was introduced earlier */
+#if !defined(CONFIG_OF) && \
+    !(RHEL_RELEASE_CODE && RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(7,2))
+static inline struct device_node *
+pci_device_to_OF_node(const struct pci_dev __always_unused *pdev) { return NULL; }
+#else /* !CONFIG_OF && RHEL < 7.3 */
+#define HAVE_DDP_PROFILE_UPLOAD_SUPPORT
+#endif /* !CONFIG_OF && RHEL < 7.3 */
+#else /* < 4.0 */
+#define HAVE_DDP_PROFILE_UPLOAD_SUPPORT
+#endif /* < 4.0 */
+
+/*****************************************************************************/
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(4,1,0) )
+#ifndef NO_PTP_SUPPORT
+#ifdef HAVE_INCLUDE_LINUX_TIMECOUNTER_H
+#include <linux/timecounter.h>
+#else
+#include <linux/clocksource.h>
+#endif
+static inline void __kc_timecounter_adjtime(struct timecounter *tc, s64 delta)
+{
+	tc->nsec += delta;
+}
+
+static inline struct net_device *
+of_find_net_device_by_node(struct device_node __always_unused *np)
+{
+	return NULL;
+}
+
+#define timecounter_adjtime __kc_timecounter_adjtime
+#endif
+#if ((RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,2))) || \
+     (SLE_VERSION_CODE && (SLE_VERSION_CODE >= SLE_VERSION(12,2,0))))
+#define HAVE_NDO_SET_VF_RSS_QUERY_EN
+#endif
+#if RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(7,2))
+#define HAVE_NDO_BRIDGE_GETLINK_NLFLAGS
+#define HAVE_RHEL7_EXTENDED_NDO_SET_TX_MAXRATE
+#define HAVE_NDO_SET_TX_MAXRATE
+#endif
+#if !((RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(6,8) && RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,0)) && \
+      (RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(7,2)) && \
+      (SLE_VERSION_CODE > SLE_VERSION(12,1,0)))
+unsigned int _kc_cpumask_local_spread(unsigned int i, int node);
+#define cpumask_local_spread _kc_cpumask_local_spread
+#endif
+#ifdef HAVE_RHASHTABLE
+#define rhashtable_loopup_fast(ht, key, params)		\
+	do {						\
+		(void)params;				\
+		rhashtable_lookup((ht), (key));		\
+	} while (0)
+
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,19,0) )
+#define rhashtable_insert_fast(ht, obj, params)			\
+	do {							\
+		(void)params;					\
+		rhashtable_insert((ht), (obj), GFP_KERNEL);	\
+	} while (0)
+
+#define rhashtable_remove_fast(ht, obj, params)			\
+	do {							\
+		(void)params;					\
+		rhashtable_remove((ht), (obj), GFP_KERNEL);	\
+	} while (0)
+
+#else /* >= 3,19,0 */
+#define rhashtable_insert_fast(ht, obj, params)			\
+	do {							\
+		(void)params;					\
+		rhashtable_insert((ht), (obj));			\
+	} while (0)
+
+#define rhashtable_remove_fast(ht, obj, params)			\
+	do {							\
+		(void)params;					\
+		rhashtable_remove((ht), (obj));			\
+	} while (0)
+
+#endif /* 3,19,0 */
+#endif /* HAVE_RHASHTABLE */
+#else /* >= 4,1,0 */
+#define HAVE_NDO_GET_PHYS_PORT_NAME
+#define HAVE_PTP_CLOCK_INFO_GETTIME64
+#define HAVE_NDO_BRIDGE_GETLINK_NLFLAGS
+#define HAVE_PASSTHRU_FEATURES_CHECK
+#define HAVE_NDO_SET_VF_RSS_QUERY_EN
+#define HAVE_NDO_SET_TX_MAXRATE
+#endif /* 4,1,0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4,1,9))
+#if (!(RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(7,2)) && \
+     !((SLE_VERSION_CODE == SLE_VERSION(11,3,0)) && \
+       (SLE_LOCALVERSION_CODE >= SLE_LOCALVERSION(0,47,71))) && \
+     !((SLE_VERSION_CODE == SLE_VERSION(11,4,0)) && \
+       (SLE_LOCALVERSION_CODE >= SLE_LOCALVERSION(65,0,0))) && \
+     !(SLE_VERSION_CODE >= SLE_VERSION(12,1,0)))
+static inline bool page_is_pfmemalloc(struct page __maybe_unused *page)
+{
+#ifdef HAVE_STRUCT_PAGE_PFMEMALLOC
+	return page->pfmemalloc;
+#else
+	return false;
+#endif
+}
+#endif /* !RHEL7.2+ && !SLES11sp3(3.0.101-0.47.71+ update) && !SLES11sp4(3.0.101-65+ update) & !SLES12sp1+ */
+#else
+#undef HAVE_STRUCT_PAGE_PFMEMALLOC
+#endif /* 4.1.9 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4,2,0))
+#if (!(RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,2)) && \
+     !(SLE_VERSION_CODE >= SLE_VERSION(12,1,0)))
+#define ETHTOOL_RX_FLOW_SPEC_RING	0x00000000FFFFFFFFULL
+#define ETHTOOL_RX_FLOW_SPEC_RING_VF	0x000000FF00000000ULL
+#define ETHTOOL_RX_FLOW_SPEC_RING_VF_OFF 32
+static inline __u64 ethtool_get_flow_spec_ring(__u64 ring_cookie)
+{
+	return ETHTOOL_RX_FLOW_SPEC_RING & ring_cookie;
+};
+
+static inline __u64 ethtool_get_flow_spec_ring_vf(__u64 ring_cookie)
+{
+	return (ETHTOOL_RX_FLOW_SPEC_RING_VF & ring_cookie) >>
+				ETHTOOL_RX_FLOW_SPEC_RING_VF_OFF;
+};
+#endif /* ! RHEL >= 7.2 && ! SLES >= 12.1 */
+#if (RHEL_RELEASE_CODE && RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,4))
+#define HAVE_NDO_DFLT_BRIDGE_GETLINK_VLAN_SUPPORT
+#endif
+
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2,6,27))
+#if (!((RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(6,8) && \
+	RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,0)) || \
+       RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,2)))
+static inline bool pci_ari_enabled(struct pci_bus *bus)
+{
+	return bus->self && bus->self->ari_enabled;
+}
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,2))
+#define HAVE_VF_STATS
+#endif /* (RHEL7.2+) */
+#endif /* !(RHEL6.8+ || RHEL7.2+) */
+#else
+static inline bool pci_ari_enabled(struct pci_bus *bus)
+{
+	return false;
+}
+#endif /* 2.6.27 */
+#else
+#define HAVE_NDO_DFLT_BRIDGE_GETLINK_VLAN_SUPPORT
+#define HAVE_VF_STATS
+#endif /* 4.2.0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4,3,0))
+#if (!(RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,4)) && \
+     !(SLE_VERSION_CODE >= SLE_VERSION(12,2,0)))
+/**
+ * _kc_flow_dissector_key_ipv4_addrs:
+ * @src: source ip address
+ * @dst: destination ip address
+ */
+struct _kc_flow_dissector_key_ipv4_addrs {
+	__be32 src;
+	__be32 dst;
+};
+
+/**
+ * _kc_flow_dissector_key_ipv6_addrs:
+ * @src: source ip address
+ * @dst: destination ip address
+ */
+struct _kc_flow_dissector_key_ipv6_addrs {
+	struct in6_addr src;
+	struct in6_addr dst;
+};
+
+/**
+ * _kc_flow_dissector_key_addrs:
+ * @v4addrs: IPv4 addresses
+ * @v6addrs: IPv6 addresses
+ */
+struct _kc_flow_dissector_key_addrs {
+	union {
+		struct _kc_flow_dissector_key_ipv4_addrs v4addrs;
+		struct _kc_flow_dissector_key_ipv6_addrs v6addrs;
+	};
+};
+
+/**
+ * _kc_flow_dissector_key_tp_ports:
+ *	@ports: port numbers of Transport header
+ *		src: source port number
+ *		dst: destination port number
+ */
+struct _kc_flow_dissector_key_ports {
+	union {
+		__be32 ports;
+		struct {
+			__be16 src;
+			__be16 dst;
+		};
+	};
+};
+
+/**
+ * _kc_flow_dissector_key_basic:
+ * @n_proto: Network header protocol (eg. IPv4/IPv6)
+ * @ip_proto: Transport header protocol (eg. TCP/UDP)
+ * @padding: padding for alignment
+ */
+struct _kc_flow_dissector_key_basic {
+	__be16	n_proto;
+	u8	ip_proto;
+	u8	padding;
+};
+
+struct _kc_flow_keys {
+	struct _kc_flow_dissector_key_basic basic;
+	struct _kc_flow_dissector_key_ports ports;
+	struct _kc_flow_dissector_key_addrs addrs;
+};
+
+/* These are all the include files for kernels inside this #ifdef block that
+ * have any reference to the in kernel definition of struct flow_keys. The
+ * reason for putting them here is to make 100% sure that these files do not get
+ * included after re-defining flow_keys to _kc_flow_keys. This is done to
+ * prevent any possible ABI issues that this structure re-definition could case.
+ */
+#if ((LINUX_VERSION_CODE >= KERNEL_VERSION(3,3,0) && \
+      LINUX_VERSION_CODE < KERNEL_VERSION(4,2,0)) || \
+      RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(6,7) || \
+      SLE_VERSION_CODE >= SLE_VERSION(11,4,0))
+#include <net/flow_keys.h>
+#endif /* (>= 3.3.0 && < 4.2.0) || >= RHEL 6.7  || >= SLE 11.4 */
+#if (LINUX_VERSION_CODE == KERNEL_VERSION(4,2,0))
+#include <net/flow_dissector.h>
+#endif /* 4.2.0 */
+#include <linux/skbuff.h>
+#include <net/ip.h>
+#include <net/ipv6.h>
+#include <net/flow.h>
+
+#define flow_keys _kc_flow_keys
+bool
+_kc_skb_flow_dissect_flow_keys(const struct sk_buff *skb,
+			       struct flow_keys *flow,
+			       unsigned int __always_unused flags);
+#define skb_flow_dissect_flow_keys	_kc_skb_flow_dissect_flow_keys
+#endif /* ! >= RHEL 7.4 && ! >= SLES 12.2 */
+
+#if ((RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,3)) || \
+     (SLE_VERSION_CODE >= SLE_VERSION(12,2,0)))
+#include <net/dst_metadata.h>
+#endif /* >= RHEL7.3 || >= SLE12sp2 */
+#else /* >= 4.3.0 */
+#include <net/dst_metadata.h>
+#endif /* 4.3.0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4,4,0))
+#if (RHEL_RELEASE_CODE && RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,3))
+#define HAVE_NDO_SET_VF_TRUST
+#endif /* (RHEL_RELEASE >= 7.3) */
+#ifndef CONFIG_64BIT
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3,3,0))
+#include <asm-generic/io-64-nonatomic-lo-hi.h>	/* 32-bit readq/writeq */
+#else /* 3.3.0 => 4.3.x */
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,26))
+#include <asm-generic/int-ll64.h>
+#endif /* 2.6.26 => 3.3.0 */
+#ifndef readq
+static inline __u64 readq(const volatile void __iomem *addr)
+{
+	const volatile u32 __iomem *p = addr;
+	u32 low, high;
+
+	low = readl(p);
+	high = readl(p + 1);
+
+	return low + ((u64)high << 32);
+}
+#define readq readq
+#endif
+
+#ifndef writeq
+static inline void writeq(__u64 val, volatile void __iomem *addr)
+{
+	writel(val, addr);
+	writel(val >> 32, (u8 *)addr + 4);
+}
+#define writeq writeq
+#endif
+#endif /* < 3.3.0 */
+#endif /* !CONFIG_64BIT */
+#else /* < 4.4.0 */
+#define HAVE_NDO_SET_VF_TRUST
+
+#ifndef CONFIG_64BIT
+#include <linux/io-64-nonatomic-lo-hi.h>	/* 32-bit readq/writeq */
+#endif /* !CONFIG_64BIT */
+#endif /* 4.4.0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4,5,0))
+/* protect against a likely backport */
+#ifndef NETIF_F_CSUM_MASK
+#define NETIF_F_CSUM_MASK NETIF_F_ALL_CSUM
+#endif /* NETIF_F_CSUM_MASK */
+#ifndef NETIF_F_SCTP_CRC
+#define NETIF_F_SCTP_CRC NETIF_F_SCTP_CSUM
+#endif /* NETIF_F_SCTP_CRC */
+#if (!(RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,3)))
+#define eth_platform_get_mac_address _kc_eth_platform_get_mac_address
+int _kc_eth_platform_get_mac_address(struct device *dev __maybe_unused,
+				     u8 *mac_addr __maybe_unused);
+#endif /* !(RHEL_RELEASE >= 7.3) */
+#else /* 4.5.0 */
+#if ( LINUX_VERSION_CODE < KERNEL_VERSION(4,8,0) )
+#define HAVE_GENEVE_RX_OFFLOAD
+#if !defined(HAVE_UDP_ENC_TUNNEL) && IS_ENABLED(CONFIG_GENEVE)
+#define HAVE_UDP_ENC_TUNNEL
+#endif
+#endif /* < 4.8.0 */
+#define HAVE_NETIF_NAPI_ADD_CALLS_NAPI_HASH_ADD
+#define HAVE_NETDEV_UPPER_INFO
+#endif /* 4.5.0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4,6,0))
+#if !(RHEL_RELEASE_CODE && RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(7,3))
+static inline unsigned char *skb_checksum_start(const struct sk_buff *skb)
+{
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,22))
+	return skb->head + skb->csum_start;
+#else /* < 2.6.22 */
+	return skb_transport_header(skb);
+#endif
+}
+#endif
+
+#if !(UBUNTU_VERSION_CODE && \
+		UBUNTU_VERSION_CODE >= UBUNTU_VERSION(4,4,0,21)) && \
+	!(RHEL_RELEASE_CODE && \
+		(RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(7,2))) && \
+	!(SLE_VERSION_CODE && (SLE_VERSION_CODE >= SLE_VERSION(12,3,0)))
+static inline void napi_consume_skb(struct sk_buff *skb,
+				    int __always_unused budget)
+{
+	dev_consume_skb_any(skb);
+}
+
+#endif /* UBUNTU 4,4,0,21, RHEL 7.2, SLES12 SP3 */
+#if !(SLE_VERSION_CODE && (SLE_VERSION_CODE >= SLE_VERSION(12,3,0))) && \
+	!(RHEL_RELEASE_CODE && RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,4))
+static inline void csum_replace_by_diff(__sum16 *sum, __wsum diff)
+{
+	* sum = csum_fold(csum_add(diff, ~csum_unfold(*sum)));
+}
+#endif
+#if !(RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(7,2))) && \
+	!(SLE_VERSION_CODE && (SLE_VERSION_CODE > SLE_VERSION(12,3,0)))
+static inline void page_ref_inc(struct page *page)
+{
+	get_page(page);
+}
+#else
+#define HAVE_PAGE_COUNT_BULK_UPDATE
+#endif
+#ifndef IPV4_USER_FLOW
+#define	IPV4_USER_FLOW	0x0d	/* spec only (usr_ip4_spec) */
+#endif
+
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,4))
+#define HAVE_TC_SETUP_CLSFLOWER
+#define HAVE_TC_FLOWER_ENC
+#endif
+
+#if ((RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,7)) || \
+     (SLE_VERSION_CODE >= SLE_VERSION(12,2,0)))
+#define HAVE_TC_SETUP_CLSU32
+#endif
+
+#if (SLE_VERSION_CODE >= SLE_VERSION(12,2,0))
+#define HAVE_TC_SETUP_CLSFLOWER
+#endif
+
+#else /* >= 4.6.0 */
+#define HAVE_PAGE_COUNT_BULK_UPDATE
+#define HAVE_ETHTOOL_FLOW_UNION_IP6_SPEC
+#define HAVE_PTP_CROSSTIMESTAMP
+#define HAVE_TC_SETUP_CLSFLOWER
+#define HAVE_TC_SETUP_CLSU32
+#endif /* 4.6.0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4,7,0))
+#if ((SLE_VERSION_CODE >= SLE_VERSION(12,3,0)) ||\
+     (RHEL_RELEASE_CODE && RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,4)))
+#define HAVE_NETIF_TRANS_UPDATE
+#endif /* SLES12sp3+ || RHEL7.4+ */
+#if ((RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,3)) ||\
+     (SLE_VERSION_CODE >= SLE_VERSION(12,3,0)))
+#define HAVE_ETHTOOL_25G_BITS
+#define HAVE_ETHTOOL_50G_BITS
+#define HAVE_ETHTOOL_100G_BITS
+#endif /* RHEL7.3+ || SLES12sp3+ */
+#else /* 4.7.0 */
+#define HAVE_NETIF_TRANS_UPDATE
+#define HAVE_ETHTOOL_CONVERT_U32_AND_LINK_MODE
+#define HAVE_ETHTOOL_25G_BITS
+#define HAVE_ETHTOOL_50G_BITS
+#define HAVE_ETHTOOL_100G_BITS
+#define HAVE_TCF_MIRRED_REDIRECT
+#endif /* 4.7.0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4,8,0))
+#if !(RHEL_RELEASE_CODE && RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,4))
+enum udp_parsable_tunnel_type {
+	UDP_TUNNEL_TYPE_VXLAN,
+	UDP_TUNNEL_TYPE_GENEVE,
+};
+struct udp_tunnel_info {
+	unsigned short type;
+	sa_family_t sa_family;
+	__be16 port;
+};
+#endif
+
+#if (UBUNTU_VERSION_CODE && UBUNTU_VERSION_CODE < UBUNTU_VERSION(4,8,0,0))
+#define tc_no_actions(_exts) true
+#define tc_for_each_action(_a, _exts) while (0)
+#endif
+#if !(SLE_VERSION_CODE && (SLE_VERSION_CODE >= SLE_VERSION(12,3,0))) &&\
+	!(RHEL_RELEASE_CODE && RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,4))
+static inline int
+#ifdef HAVE_NON_CONST_PCI_DRIVER_NAME
+pci_request_io_regions(struct pci_dev *pdev, char *name)
+#else
+pci_request_io_regions(struct pci_dev *pdev, const char *name)
+#endif
+{
+	return pci_request_selected_regions(pdev,
+			    pci_select_bars(pdev, IORESOURCE_IO), name);
+}
+
+static inline void
+pci_release_io_regions(struct pci_dev *pdev)
+{
+	return pci_release_selected_regions(pdev,
+			    pci_select_bars(pdev, IORESOURCE_IO));
+}
+
+static inline int
+#ifdef HAVE_NON_CONST_PCI_DRIVER_NAME
+pci_request_mem_regions(struct pci_dev *pdev, char *name)
+#else
+pci_request_mem_regions(struct pci_dev *pdev, const char *name)
+#endif
+{
+	return pci_request_selected_regions(pdev,
+			    pci_select_bars(pdev, IORESOURCE_MEM), name);
+}
+
+static inline void
+pci_release_mem_regions(struct pci_dev *pdev)
+{
+	return pci_release_selected_regions(pdev,
+			    pci_select_bars(pdev, IORESOURCE_MEM));
+}
+#endif /* !SLE_VERSION(12,3,0) */
+#if ((RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,4)) ||\
+     (SLE_VERSION_CODE >= SLE_VERSION(12,3,0)))
+#define HAVE_ETHTOOL_NEW_50G_BITS
+#endif /* RHEL7.4+ || SLES12sp3+ */
+#else
+#define HAVE_UDP_ENC_RX_OFFLOAD
+#define HAVE_ETHTOOL_NEW_50G_BITS
+#endif /* 4.8.0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4,9,0))
+#ifdef HAVE_TC_SETUP_CLSFLOWER
+#if (!(RHEL_RELEASE_CODE) && !(SLE_VERSION_CODE) || \
+    (SLE_VERSION_CODE && (SLE_VERSION_CODE < SLE_VERSION(12,3,0))))
+#define HAVE_TC_FLOWER_VLAN_IN_TAGS
+#endif /* !RHEL_RELEASE_CODE && !SLE_VERSION_CODE || <SLE_VERSION(12,3,0) */
+#endif /* HAVE_TC_SETUP_CLSFLOWER */
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,4))
+#define HAVE_ETHTOOL_NEW_1G_BITS
+#define HAVE_ETHTOOL_NEW_10G_BITS
+#endif /* RHEL7.4+ */
+#if (!(SLE_VERSION_CODE) && !(RHEL_RELEASE_CODE)) || \
+     SLE_VERSION_CODE && (SLE_VERSION_CODE <= SLE_VERSION(12,3,0)) || \
+     RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE <= RHEL_RELEASE_VERSION(7,5))
+#define time_is_before_jiffies64(a)	time_after64(get_jiffies_64(), a)
+#endif /* !SLE_VERSION_CODE && !RHEL_RELEASE_CODE || (SLES <= 12.3.0) || (RHEL <= 7.5) */
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,4))
+static inline void bitmap_from_u64(unsigned long *dst, u64 mask)
+{
+	dst[0] = mask & ULONG_MAX;
+
+	if (sizeof(mask) > sizeof(unsigned long))
+		dst[1] = mask >> 32;
+}
+#endif /* <RHEL7.4 */
+#if (!(RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,4)) && \
+     !(SLE_VERSION_CODE >= SLE_VERSION(12,3,0)) && \
+     !(UBUNTU_VERSION_CODE >= UBUNTU_VERSION(4,13,0,16)))
+static inline bool eth_type_vlan(__be16 ethertype)
+{
+	switch (ethertype) {
+	case htons(ETH_P_8021Q):
+#ifdef ETH_P_8021AD
+	case htons(ETH_P_8021AD):
+#endif
+		return true;
+	default:
+		return false;
+	}
+}
+#endif /* Linux < 4.9 || RHEL < 7.4 || SLES < 12.3 || Ubuntu < 4.3.0-16 */
+#else /* >=4.9 */
+#define HAVE_FLOW_DISSECTOR_KEY_VLAN_PRIO
+#define HAVE_ETHTOOL_NEW_1G_BITS
+#define HAVE_ETHTOOL_NEW_10G_BITS
+#endif /* KERNEL_VERSION(4.9.0) */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4,10,0))
+/* SLES 12.3 and RHEL 7.5 backported this interface */
+#if (!SLE_VERSION_CODE && !RHEL_RELEASE_CODE) || \
+    (SLE_VERSION_CODE && (SLE_VERSION_CODE < SLE_VERSION(12,3,0))) || \
+    (RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,5)))
+static inline bool _kc_napi_complete_done2(struct napi_struct *napi,
+					   int __always_unused work_done)
+{
+	/* it was really hard to get napi_complete_done to be safe to call
+	 * recursively without running into our own kcompat, so just use
+	 * napi_complete
+	 */
+	napi_complete(napi);
+
+	/* true means that the stack is telling the driver to go-ahead and
+	 * re-enable interrupts
+	 */
+	return true;
+}
+
+#ifdef napi_complete_done
+#undef napi_complete_done
+#endif
+#define napi_complete_done _kc_napi_complete_done2
+#endif /* sles and rhel exclusion for < 4.10 */
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,4))
+#define HAVE_DEV_WALK_API
+#define HAVE_ETHTOOL_NEW_2500MB_BITS
+#define HAVE_ETHTOOL_5G_BITS
+#endif /* RHEL7.4+ */
+#if (SLE_VERSION_CODE && (SLE_VERSION_CODE == SLE_VERSION(12,3,0)))
+#define HAVE_STRUCT_DMA_ATTRS
+#endif /* (SLES == 12.3.0) */
+#if (SLE_VERSION_CODE && (SLE_VERSION_CODE >= SLE_VERSION(12,3,0)))
+#define HAVE_NETDEVICE_MIN_MAX_MTU
+#endif /* (SLES >= 12.3.0) */
+#if (RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,5)))
+#define HAVE_STRUCT_DMA_ATTRS
+#define HAVE_RHEL7_EXTENDED_MIN_MAX_MTU
+#define HAVE_NETDEVICE_MIN_MAX_MTU
+#endif
+#if (!(SLE_VERSION_CODE && (SLE_VERSION_CODE >= SLE_VERSION(12,3,0))) && \
+     !(RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,5))))
+#ifndef dma_map_page_attrs
+#define dma_map_page_attrs __kc_dma_map_page_attrs
+static inline dma_addr_t __kc_dma_map_page_attrs(struct device *dev,
+						 struct page *page,
+						 size_t offset, size_t size,
+						 enum dma_data_direction dir,
+						 unsigned long __always_unused attrs)
+{
+	return dma_map_page(dev, page, offset, size, dir);
+}
+#endif
+
+#ifndef dma_unmap_page_attrs
+#define dma_unmap_page_attrs __kc_dma_unmap_page_attrs
+static inline void __kc_dma_unmap_page_attrs(struct device *dev,
+					     dma_addr_t addr, size_t size,
+					     enum dma_data_direction dir,
+					     unsigned long __always_unused attrs)
+{
+	dma_unmap_page(dev, addr, size, dir);
+}
+#endif
+
+static inline void __page_frag_cache_drain(struct page *page,
+					   unsigned int count)
+{
+#ifdef HAVE_PAGE_COUNT_BULK_UPDATE
+	if (!page_ref_sub_and_test(page, count))
+		return;
+
+	init_page_count(page);
+#else
+	BUG_ON(count > 1);
+	if (!count)
+		return;
+#endif
+	__free_pages(page, compound_order(page));
+}
+#endif /* !SLE_VERSION(12,3,0) && !RHEL_VERSION(7,5) */
+#if ((SLE_VERSION_CODE && (SLE_VERSION_CODE > SLE_VERSION(12,3,0))) ||\
+     (RHEL_RELEASE_CODE && RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,5)))
+#define HAVE_SWIOTLB_SKIP_CPU_SYNC
+#endif
+
+#if ((SLE_VERSION_CODE && (SLE_VERSION_CODE < SLE_VERSION(15,0,0))) ||\
+     (RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE <= RHEL_RELEASE_VERSION(7,4))))
+#define page_frag_free __free_page_frag
+#endif
+#ifndef ETH_MIN_MTU
+#define ETH_MIN_MTU 68
+#endif /* ETH_MIN_MTU */
+
+/* If kernel is older than 4.10 but distro is RHEL >= 7.5 || SLES > 12SP4,
+ * it does have support for NAPI_STATE
+ */
+#if ((RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,5))) ||\
+     (SLE_VERSION_CODE && (SLE_VERSION_CODE >= SLE_VERSION(12,4,0))))
+#define HAVE_NAPI_STATE_IN_BUSY_POLL
+#endif /* RHEL >= 7.5 || SLES >=12.4 */
+#else /* >= 4.10 */
+#define HAVE_TC_FLOWER_ENC
+#define HAVE_NETDEVICE_MIN_MAX_MTU
+#define HAVE_SWIOTLB_SKIP_CPU_SYNC
+#define HAVE_NETDEV_TC_RESETS_XPS
+#define HAVE_XPS_QOS_SUPPORT
+#define HAVE_DEV_WALK_API
+#define HAVE_ETHTOOL_NEW_2500MB_BITS
+#define HAVE_ETHTOOL_5G_BITS
+/* kernel 4.10 onwards, as part of busy_poll rewrite, new state were added
+ * which is part of NAPI:state. If NAPI:state=NAPI_STATE_IN_BUSY_POLL,
+ * it means napi_poll is invoked in busy_poll context
+ */
+#define HAVE_NAPI_STATE_IN_BUSY_POLL
+#define HAVE_TCF_MIRRED_EGRESS_REDIRECT
+#define HAVE_PTP_CLOCK_INFO_ADJFINE
+#endif /* 4.10.0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4,11,0))
+#ifdef CONFIG_NET_RX_BUSY_POLL
+#define HAVE_NDO_BUSY_POLL
+#endif /* CONFIG_NET_RX_BUSY_POLL */
+#if ((SLE_VERSION_CODE && (SLE_VERSION_CODE >= SLE_VERSION(12,3,0))) || \
+     (RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,5))))
+#define HAVE_VOID_NDO_GET_STATS64
+#endif /* (SLES >= 12.3.0) && (RHEL >= 7.5) */
+
+static inline void _kc_dev_kfree_skb_irq(struct sk_buff *skb)
+{
+	if (!skb)
+		return;
+	dev_kfree_skb_irq(skb);
+}
+
+#undef dev_kfree_skb_irq
+#define dev_kfree_skb_irq _kc_dev_kfree_skb_irq
+
+static inline void _kc_dev_consume_skb_irq(struct sk_buff *skb)
+{
+	if (!skb)
+		return;
+	dev_consume_skb_irq(skb);
+}
+
+#undef dev_consume_skb_irq
+#define dev_consume_skb_irq _kc_dev_consume_skb_irq
+
+static inline void _kc_dev_kfree_skb_any(struct sk_buff *skb)
+{
+	if (!skb)
+		return;
+	dev_kfree_skb_any(skb);
+}
+
+#undef dev_kfree_skb_any
+#define dev_kfree_skb_any _kc_dev_kfree_skb_any
+
+static inline void _kc_dev_consume_skb_any(struct sk_buff *skb)
+{
+	if (!skb)
+		return;
+	dev_consume_skb_any(skb);
+}
+
+#undef dev_consume_skb_any
+#define dev_consume_skb_any _kc_dev_consume_skb_any
+
+#else /* > 4.11 */
+#define HAVE_VOID_NDO_GET_STATS64
+#define HAVE_VM_OPS_FAULT_NO_VMA
+#endif /* 4.11.0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4,12,0))
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,7) && \
+     RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(8,0))
+/* The RHEL 7.7+ NL_SET_ERR_MSG_MOD triggers unused parameter warnings */
+#undef NL_SET_ERR_MSG_MOD
+#endif
+/* If kernel is older than 4.12 but distro is RHEL >= 7.5 || SLES > 12SP4,
+ * it does have support for MIN_NAPI_ID
+ */
+#if ((RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,5))) || \
+     (SLE_VERSION_CODE && (SLE_VERSION_CODE >= SLE_VERSION(12,4,0))))
+#define HAVE_MIN_NAPI_ID
+#endif /* RHEL >= 7.5 || SLES >= 12.4 */
+#ifndef NL_SET_ERR_MSG_MOD
+#define NL_SET_ERR_MSG_MOD(extack, msg)						\
+	do {									\
+		uninitialized_var(extack);					\
+		pr_err(KBUILD_MODNAME ": " msg);				\
+	} while (0)
+#endif /* !NL_SET_ERR_MSG_MOD */
+#else /* >= 4.12 */
+#define HAVE_NAPI_BUSY_LOOP
+#define HAVE_MIN_NAPI_ID
+#endif /* 4.12 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4,13,0))
+#if ((SLE_VERSION_CODE && (SLE_VERSION_CODE > SLE_VERSION(12,3,0))) || \
+     (RHEL_RELEASE_CODE && RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,5)))
+#define HAVE_TCF_EXTS_HAS_ACTION
+#endif
+#define  PCI_EXP_LNKCAP_SLS_8_0GB 0x00000003 /* LNKCAP2 SLS Vector bit 2 */
+#if (SLE_VERSION_CODE && (SLE_VERSION_CODE >= SLE_VERSION(12,4,0)))
+#define HAVE_PCI_ERROR_HANDLER_RESET_PREPARE
+#endif /* SLES >= 12sp4 */
+#if (!(RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,5)) && \
+     !(SLE_VERSION_CODE >= SLE_VERSION(12,4,0)))
+#define UUID_SIZE 16
+typedef struct {
+	__u8 b[UUID_SIZE];
+} uuid_t;
+#define UUID_INIT(a, b, c, d0, d1, d2, d3, d4, d5, d6, d7)		\
+((uuid_t)								\
+{{ ((a) >> 24) & 0xff, ((a) >> 16) & 0xff, ((a) >> 8) & 0xff, (a) & 0xff, \
+   ((b) >> 8) & 0xff, (b) & 0xff,					\
+   ((c) >> 8) & 0xff, (c) & 0xff,					\
+   (d0), (d1), (d2), (d3), (d4), (d5), (d6), (d7) }})
+
+static inline bool uuid_equal(const uuid_t *u1, const uuid_t *u2)
+{
+	return memcmp(u1, u2, sizeof(uuid_t)) == 0;
+}
+#else
+#define HAVE_METADATA_PORT_INFO
+#endif /* !(RHEL >= 7.5) && !(SLES >= 12.4) */
+#else /* > 4.13 */
+#define HAVE_METADATA_PORT_INFO
+#define HAVE_HWTSTAMP_FILTER_NTP_ALL
+#define HAVE_NDO_SETUP_TC_CHAIN_INDEX
+#define HAVE_PCI_ERROR_HANDLER_RESET_PREPARE
+#define HAVE_PTP_CLOCK_DO_AUX_WORK
+#endif /* 4.13.0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4,14,0))
+#ifdef ETHTOOL_GLINKSETTINGS
+#ifndef ethtool_link_ksettings_del_link_mode
+#define ethtool_link_ksettings_del_link_mode(ptr, name, mode)		\
+	__clear_bit(ETHTOOL_LINK_MODE_ ## mode ## _BIT, (ptr)->link_modes.name)
+#endif
+#endif /* ETHTOOL_GLINKSETTINGS */
+#if (SLE_VERSION_CODE && (SLE_VERSION_CODE >= SLE_VERSION(12,4,0)))
+#define HAVE_NDO_SETUP_TC_REMOVE_TC_TO_NETDEV
+#endif
+
+#if (RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,5)))
+#define HAVE_NDO_SETUP_TC_REMOVE_TC_TO_NETDEV
+#define HAVE_RHEL7_NETDEV_OPS_EXT_NDO_SETUP_TC
+#endif
+
+#define TIMER_DATA_TYPE		unsigned long
+#define TIMER_FUNC_TYPE		void (*)(TIMER_DATA_TYPE)
+
+#define timer_setup(timer, callback, flags)				\
+	__setup_timer((timer), (TIMER_FUNC_TYPE)(callback),		\
+		      (TIMER_DATA_TYPE)(timer), (flags))
+
+#define from_timer(var, callback_timer, timer_fieldname) \
+	container_of(callback_timer, typeof(*var), timer_fieldname)
+
+#ifndef xdp_do_flush_map
+#define xdp_do_flush_map() do {} while (0)
+#endif
+struct _kc_xdp_buff {
+	void *data;
+	void *data_end;
+	void *data_hard_start;
+};
+#define xdp_buff _kc_xdp_buff
+struct _kc_bpf_prog {
+};
+#define bpf_prog _kc_bpf_prog
+#ifndef DIV_ROUND_DOWN_ULL
+#define DIV_ROUND_DOWN_ULL(ll, d) \
+	({ unsigned long long _tmp = (ll); do_div(_tmp, d); _tmp; })
+#endif /* DIV_ROUND_DOWN_ULL */
+#else /* > 4.14 */
+#define HAVE_XDP_SUPPORT
+#define HAVE_NDO_SETUP_TC_REMOVE_TC_TO_NETDEV
+#define HAVE_TCF_EXTS_HAS_ACTION
+#endif /* 4.14.0 */
+
+/*****************************************************************************/
+#ifndef ETHTOOL_GLINKSETTINGS
+
+#define __ETHTOOL_LINK_MODE_MASK_NBITS 32
+#define ETHTOOL_LINK_MASK_SIZE BITS_TO_LONGS(__ETHTOOL_LINK_MODE_MASK_NBITS)
+
+/**
+ * struct ethtool_link_ksettings
+ * @link_modes: supported and advertising, single item arrays
+ * @link_modes.supported: bitmask of supported link speeds
+ * @link_modes.advertising: bitmask of currently advertised speeds
+ * @base: base link details
+ * @base.speed: current link speed
+ * @base.port: current port type
+ * @base.duplex: current duplex mode
+ * @base.autoneg: current autonegotiation settings
+ *
+ * This struct and the following macros provide a way to support the old
+ * ethtool get/set_settings API on older kernels, but in the style of the new
+ * GLINKSETTINGS API.  In this way, the same code can be used to support both
+ * APIs as seemlessly as possible.
+ *
+ * It should be noted the old API only has support up to the first 32 bits.
+ */
+struct ethtool_link_ksettings {
+	struct {
+		u32 speed;
+		u8 port;
+		u8 duplex;
+		u8 autoneg;
+	} base;
+	struct {
+		unsigned long supported[ETHTOOL_LINK_MASK_SIZE];
+		unsigned long advertising[ETHTOOL_LINK_MASK_SIZE];
+	} link_modes;
+};
+
+#define ETHTOOL_LINK_NAME_advertising(mode) ADVERTISED_ ## mode
+#define ETHTOOL_LINK_NAME_supported(mode) SUPPORTED_ ## mode
+#define ETHTOOL_LINK_NAME(name) ETHTOOL_LINK_NAME_ ## name
+#define ETHTOOL_LINK_CONVERT(name, mode) ETHTOOL_LINK_NAME(name)(mode)
+
+/**
+ * ethtool_link_ksettings_zero_link_mode
+ * @ptr: ptr to ksettings struct
+ * @name: supported or advertising
+ */
+#define ethtool_link_ksettings_zero_link_mode(ptr, name)\
+	(*((ptr)->link_modes.name) = 0x0)
+
+/**
+ * ethtool_link_ksettings_add_link_mode
+ * @ptr: ptr to ksettings struct
+ * @name: supported or advertising
+ * @mode: link mode to add
+ */
+#define ethtool_link_ksettings_add_link_mode(ptr, name, mode)\
+	(*((ptr)->link_modes.name) |= (typeof(*((ptr)->link_modes.name)))ETHTOOL_LINK_CONVERT(name, mode))
+
+/**
+ * ethtool_link_ksettings_del_link_mode
+ * @ptr: ptr to ksettings struct
+ * @name: supported or advertising
+ * @mode: link mode to delete
+ */
+#define ethtool_link_ksettings_del_link_mode(ptr, name, mode)\
+	(*((ptr)->link_modes.name) &= ~(typeof(*((ptr)->link_modes.name)))ETHTOOL_LINK_CONVERT(name, mode))
+
+/**
+ * ethtool_link_ksettings_test_link_mode
+ * @ptr: ptr to ksettings struct
+ * @name: supported or advertising
+ * @mode: link mode to add
+ */
+#define ethtool_link_ksettings_test_link_mode(ptr, name, mode)\
+	(!!(*((ptr)->link_modes.name) & ETHTOOL_LINK_CONVERT(name, mode)))
+
+/**
+ * _kc_ethtool_ksettings_to_cmd - Convert ethtool_link_ksettings to ethtool_cmd
+ * @ks: ethtool_link_ksettings struct
+ * @cmd: ethtool_cmd struct
+ *
+ * Convert an ethtool_link_ksettings structure into the older ethtool_cmd
+ * structure. We provide this in kcompat.h so that drivers can easily
+ * implement the older .{get|set}_settings as wrappers around the new api.
+ * Hence, we keep it prefixed with _kc_ to make it clear this isn't actually
+ * a real function in the kernel.
+ */
+static inline void
+_kc_ethtool_ksettings_to_cmd(struct ethtool_link_ksettings *ks,
+			     struct ethtool_cmd *cmd)
+{
+	cmd->supported = (u32)ks->link_modes.supported[0];
+	cmd->advertising = (u32)ks->link_modes.advertising[0];
+	ethtool_cmd_speed_set(cmd, ks->base.speed);
+	cmd->duplex = ks->base.duplex;
+	cmd->autoneg = ks->base.autoneg;
+	cmd->port = ks->base.port;
+}
+
+#endif /* !ETHTOOL_GLINKSETTINGS */
+
+/*****************************************************************************/
+#if ((LINUX_VERSION_CODE < KERNEL_VERSION(4,14,0)) || \
+     (SLE_VERSION_CODE && (SLE_VERSION_CODE <= SLE_VERSION(12,3,0))) || \
+     (RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE <= RHEL_RELEASE_VERSION(7,5))))
+#define phy_speed_to_str _kc_phy_speed_to_str
+const char *_kc_phy_speed_to_str(int speed);
+#else /* (LINUX >= 4.14.0) || (SLES > 12.3.0) || (RHEL > 7.5) */
+#include <linux/phy.h>
+#endif /* (LINUX < 4.14.0) || (SLES <= 12.3.0) || (RHEL <= 7.5) */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4,15,0))
+#if ((RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,6))) || \
+     (SLE_VERSION_CODE && (SLE_VERSION_CODE >= SLE_VERSION(15,1,0))))
+#define HAVE_TC_CB_AND_SETUP_QDISC_MQPRIO
+#define HAVE_TCF_BLOCK
+#else /* RHEL >= 7.6 || SLES >= 15.1 */
+#endif /* !(RHEL >= 7.6) && !(SLES >= 15.1) */
+void _kc_ethtool_intersect_link_masks(struct ethtool_link_ksettings *dst,
+				      struct ethtool_link_ksettings *src);
+#define ethtool_intersect_link_masks _kc_ethtool_intersect_link_masks
+#else /* >= 4.15 */
+#define HAVE_NDO_BPF
+#define HAVE_XDP_BUFF_DATA_META
+#define HAVE_TC_CB_AND_SETUP_QDISC_MQPRIO
+#define HAVE_TCF_BLOCK
+#endif /* 4.15.0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4,16,0))
+#if (!(RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,7)) && \
+     !(SLE_VERSION_CODE >= SLE_VERSION(12,4,0) && \
+       SLE_VERSION_CODE < SLE_VERSION(15,0,0)) && \
+     !(SLE_VERSION_CODE >= SLE_VERSION(15,1,0)))
+/* The return value of the strscpy() and strlcpy() functions is different.
+ * This could be potentially hazard for the future.
+ * To avoid this the void result is forced.
+ * So it is not possible use this function with the return value.
+ * Return value is required in kernel 4.3 through 4.15
+ */
+#define strscpy(...) (void)(strlcpy(__VA_ARGS__))
+#endif /* !RHEL >= 7.7 && !SLES12sp4+ && !SLES15sp1+ */
+
+#define pci_printk(level, pdev, fmt, arg...) \
+	dev_printk(level, &(pdev)->dev, fmt, ##arg)
+#define pci_emerg(pdev, fmt, arg...)	dev_emerg(&(pdev)->dev, fmt, ##arg)
+#define pci_alert(pdev, fmt, arg...)	dev_alert(&(pdev)->dev, fmt, ##arg)
+#define pci_crit(pdev, fmt, arg...)	dev_crit(&(pdev)->dev, fmt, ##arg)
+#define pci_err(pdev, fmt, arg...)	dev_err(&(pdev)->dev, fmt, ##arg)
+#define pci_warn(pdev, fmt, arg...)	dev_warn(&(pdev)->dev, fmt, ##arg)
+#define pci_notice(pdev, fmt, arg...)	dev_notice(&(pdev)->dev, fmt, ##arg)
+#define pci_info(pdev, fmt, arg...)	dev_info(&(pdev)->dev, fmt, ##arg)
+#define pci_dbg(pdev, fmt, arg...)	dev_dbg(&(pdev)->dev, fmt, ##arg)
+
+#ifndef array_index_nospec
+static inline unsigned long _kc_array_index_mask_nospec(unsigned long index,
+							unsigned long size)
+{
+	/*
+	 * Always calculate and emit the mask even if the compiler
+	 * thinks the mask is not needed. The compiler does not take
+	 * into account the value of @index under speculation.
+	 */
+	OPTIMIZER_HIDE_VAR(index);
+	return ~(long)(index | (size - 1UL - index)) >> (BITS_PER_LONG - 1);
+}
+
+#define array_index_nospec(index, size)					\
+({									\
+	typeof(index) _i = (index);					\
+	typeof(size) _s = (size);					\
+	unsigned long _mask = _kc_array_index_mask_nospec(_i, _s);	\
+									\
+	BUILD_BUG_ON(sizeof(_i) > sizeof(long));			\
+	BUILD_BUG_ON(sizeof(_s) > sizeof(long));			\
+									\
+	(typeof(_i)) (_i & _mask);					\
+})
+#endif /* array_index_nospec */
+#ifndef sizeof_field
+#define sizeof_field(TYPE, MEMBER) (sizeof((((TYPE *)0)->MEMBER)))
+#endif /* sizeof_field */
+#if !(RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(8,0)) && \
+     !(SLE_VERSION_CODE >= SLE_VERSION(12,5,0) && \
+       SLE_VERSION_CODE < SLE_VERSION(15,0,0) || \
+       SLE_VERSION_CODE >= SLE_VERSION(15,1,0))
+/*
+ * Copy bitmap and clear tail bits in last word.
+ */
+static inline void
+bitmap_copy_clear_tail(unsigned long *dst, const unsigned long *src, unsigned int nbits)
+{
+	bitmap_copy(dst, src, nbits);
+	if (nbits % BITS_PER_LONG)
+		dst[nbits / BITS_PER_LONG] &= BITMAP_LAST_WORD_MASK(nbits);
+}
+
+/*
+ * On 32-bit systems bitmaps are represented as u32 arrays internally, and
+ * therefore conversion is not needed when copying data from/to arrays of u32.
+ */
+#if BITS_PER_LONG == 64
+void bitmap_from_arr32(unsigned long *bitmap, const u32 *buf, unsigned int nbits);
+#else
+#define bitmap_from_arr32(bitmap, buf, nbits)			\
+	bitmap_copy_clear_tail((unsigned long *) (bitmap),	\
+			       (const unsigned long *) (buf), (nbits))
+#endif /* BITS_PER_LONG == 64 */
+#endif /* !(RHEL >= 8.0) && !(SLES >= 12.5 && SLES < 15.0 || SLES >= 15.1) */
+#else /* >= 4.16 */
+#include <linux/nospec.h>
+#define HAVE_XDP_BUFF_RXQ
+#define HAVE_TC_FLOWER_OFFLOAD_COMMON_EXTACK
+#define HAVE_TCF_MIRRED_DEV
+#define HAVE_VF_STATS_DROPPED
+#endif /* 4.16.0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4,17,0))
+#include <linux/pci_regs.h>
+#include <linux/pci.h>
+#define PCIE_SPEED_16_0GT 0x17
+#define PCI_EXP_LNKCAP_SLS_16_0GB 0x00000004 /* LNKCAP2 SLS Vector bit 3 */
+#define PCI_EXP_LNKSTA_CLS_16_0GB 0x0004 /* Current Link Speed 16.0GT/s */
+#define PCI_EXP_LNKCAP2_SLS_16_0GB 0x00000010 /* Supported Speed 16GT/s */
+void _kc_pcie_print_link_status(struct pci_dev *dev);
+#define pcie_print_link_status _kc_pcie_print_link_status
+#else /* >= 4.17.0 */
+#define HAVE_XDP_BUFF_IN_XDP_H
+#endif /* 4.17.0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4,18,0))
+#include "kcompat_overflow.h"
+
+#if (SLE_VERSION_CODE < SLE_VERSION(15,1,0))
+#define firmware_request_nowarn	request_firmware_direct
+#endif /* SLES < 15.1 */
+
+#else
+#include <linux/overflow.h>
+#include <net/xdp_sock.h>
+#define HAVE_XDP_FRAME_STRUCT
+#define HAVE_XDP_SOCK
+#define HAVE_NDO_XDP_XMIT_BULK_AND_FLAGS
+#define NO_NDO_XDP_FLUSH
+#endif /* 4.18.0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4,19,0))
+#if (RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(8,0)) && \
+    (RHEL_RELEASE_CODE <= RHEL_RELEASE_VERSION(8,2)))
+#define HAVE_DEVLINK_REGIONS
+#endif /* RHEL >= 8.0 && RHEL <= 8.2 */
+#define bitmap_alloc(nbits, flags) \
+	kmalloc_array(BITS_TO_LONGS(nbits), sizeof(unsigned long), flags)
+#define bitmap_zalloc(nbits, flags) bitmap_alloc(nbits, ((flags) | __GFP_ZERO))
+#define bitmap_free(bitmap) kfree(bitmap)
+#ifdef ETHTOOL_GLINKSETTINGS
+#define ethtool_ks_clear(ptr, name) \
+	ethtool_link_ksettings_zero_link_mode(ptr, name)
+#define ethtool_ks_add_mode(ptr, name, mode) \
+	ethtool_link_ksettings_add_link_mode(ptr, name, mode)
+#define ethtool_ks_del_mode(ptr, name, mode) \
+	ethtool_link_ksettings_del_link_mode(ptr, name, mode)
+#define ethtool_ks_test(ptr, name, mode) \
+	ethtool_link_ksettings_test_link_mode(ptr, name, mode)
+#endif /* ETHTOOL_GLINKSETTINGS */
+#define HAVE_NETPOLL_CONTROLLER
+#define REQUIRE_PCI_CLEANUP_AER_ERROR_STATUS
+#if (SLE_VERSION_CODE && (SLE_VERSION_CODE >= SLE_VERSION(15,1,0)))
+#define HAVE_TCF_MIRRED_DEV
+#define HAVE_NDO_SELECT_QUEUE_SB_DEV
+#define HAVE_TCF_BLOCK_CB_REGISTER_EXTACK
+#endif
+
+static inline void __kc_metadata_dst_free(void *md_dst)
+{
+	kfree(md_dst);
+}
+
+#define metadata_dst_free(md_dst) __kc_metadata_dst_free(md_dst)
+#else /* >= 4.19.0 */
+#define HAVE_TCF_BLOCK_CB_REGISTER_EXTACK
+#define NO_NETDEV_BPF_PROG_ATTACHED
+#define HAVE_NDO_SELECT_QUEUE_SB_DEV
+#define HAVE_NETDEV_SB_DEV
+#define HAVE_TCF_VLAN_TPID
+#define HAVE_RHASHTABLE_TYPES
+#define HAVE_DEVLINK_REGIONS
+#define HAVE_DEVLINK_PARAMS
+#endif /* 4.19.0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4,20,0))
+#define HAVE_XDP_UMEM_PROPS
+#if (RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(8,0)))
+#define HAVE_DEVLINK_ESWITCH_OPS_EXTACK
+#endif /* RHEL >= 8.0 */
+#if ((SLE_VERSION_CODE >= SLE_VERSION(12,5,0) && \
+      SLE_VERSION_CODE < SLE_VERSION(15,0,0)) || \
+     (SLE_VERSION_CODE >= SLE_VERSION(15,1,0)))
+#define HAVE_DEVLINK_ESWITCH_OPS_EXTACK
+#endif /* SLE == 12sp5 || SLE >= 15sp1 */
+#else /* >= 4.20.0 */
+#define HAVE_DEVLINK_ESWITCH_OPS_EXTACK
+#define HAVE_AF_XDP_ZC_SUPPORT
+#define HAVE_VXLAN_TYPE
+#define HAVE_ETF_SUPPORT /* Earliest TxTime First */
+#endif /* 4.20.0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(5,0,0))
+#if (!(RHEL_RELEASE_CODE && RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(8,0)))
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4,12,0))
+#define NETLINK_MAX_COOKIE_LEN	20
+struct netlink_ext_ack {
+	const char *_msg;
+	const struct nlattr *bad_attr;
+	u8 cookie[NETLINK_MAX_COOKIE_LEN];
+	u8 cookie_len;
+};
+
+#endif /* < 4.12 */
+static inline int _kc_dev_open(struct net_device *netdev,
+			       struct netlink_ext_ack __always_unused *extack)
+{
+	return dev_open(netdev);
+}
+
+#define dev_open _kc_dev_open
+
+static inline int
+_kc_dev_change_flags(struct net_device *netdev, unsigned int flags,
+		     struct netlink_ext_ack __always_unused *extack)
+{
+	return dev_change_flags(netdev, flags);
+}
+
+#define dev_change_flags _kc_dev_change_flags
+#endif /* !(RHEL_RELEASE_CODE && RHEL > RHEL(8,0)) */
+#if (RHEL_RELEASE_CODE && \
+     (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,7) && \
+      RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(8,0)) || \
+     (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(8,1)))
+#define HAVE_PTP_SYS_OFFSET_EXTENDED_IOCTL
+#define HAVE_PTP_CLOCK_INFO_GETTIMEX64
+#else /* RHEL >= 7.7 && RHEL < 8.0 || RHEL >= 8.1 */
+struct ptp_system_timestamp {
+	struct timespec64 pre_ts;
+	struct timespec64 post_ts;
+};
+
+static inline void
+ptp_read_system_prets(struct ptp_system_timestamp __always_unused *sts)
+{
+	;
+}
+
+static inline void
+ptp_read_system_postts(struct ptp_system_timestamp __always_unused *sts)
+{
+	;
+}
+#endif /* !(RHEL >= 7.7 && RHEL != 8.0) */
+#if (RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(8,1)))
+#define HAVE_NDO_BRIDGE_SETLINK_EXTACK
+#endif /* RHEL 8.1 */
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(8,2))
+#define HAVE_TC_INDIR_BLOCK
+#endif /* RHEL 8.2 */
+#define INDIRECT_CALLABLE_DECLARE(x) x
+#else /* >= 5.0.0 */
+#define HAVE_PTP_SYS_OFFSET_EXTENDED_IOCTL
+#define HAVE_PTP_CLOCK_INFO_GETTIMEX64
+#define HAVE_NDO_BRIDGE_SETLINK_EXTACK
+#define HAVE_DMA_ALLOC_COHERENT_ZEROES_MEM
+#define HAVE_GENEVE_TYPE
+#define HAVE_TC_INDIR_BLOCK
+#define HAVE_INDIRECT_CALL_WRAPPER_HEADER
+#endif /* 5.0.0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(5,1,0))
+#if (RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(8,1)))
+#define HAVE_TC_FLOW_RULE_INFRASTRUCTURE
+#define HAVE_NDO_FDB_ADD_EXTACK
+#define HAVE_DEVLINK_INFO_GET
+#define HAVE_DEVLINK_FLASH_UPDATE
+#else /* RHEL < 8.1 */
+#ifdef HAVE_TC_SETUP_CLSFLOWER
+#include <net/pkt_cls.h>
+
+struct flow_match {
+	struct flow_dissector	*dissector;
+	void			*mask;
+	void			*key;
+};
+
+struct flow_match_basic {
+	struct flow_dissector_key_basic *key, *mask;
+};
+
+struct flow_match_control {
+	struct flow_dissector_key_control *key, *mask;
+};
+
+struct flow_match_eth_addrs {
+	struct flow_dissector_key_eth_addrs *key, *mask;
+};
+
+#ifdef HAVE_TC_FLOWER_ENC
+struct flow_match_enc_keyid {
+	struct flow_dissector_key_keyid *key, *mask;
+};
+#endif
+
+#ifndef HAVE_TC_FLOWER_VLAN_IN_TAGS
+struct flow_match_vlan {
+	struct flow_dissector_key_vlan *key, *mask;
+};
+#endif
+
+struct flow_match_ipv4_addrs {
+	struct flow_dissector_key_ipv4_addrs *key, *mask;
+};
+
+struct flow_match_ipv6_addrs {
+	struct flow_dissector_key_ipv6_addrs *key, *mask;
+};
+
+struct flow_match_ports {
+	struct flow_dissector_key_ports *key, *mask;
+};
+
+struct flow_rule {
+	struct flow_match	match;
+#if 0
+	/* In 5.1+ kernels, action is a member of struct flow_rule but is
+	 * not compatible with how we kcompat tc_cls_flower_offload_flow_rule
+	 * below.  By not declaring it here, any driver that attempts to use
+	 * action as an element of struct flow_rule will fail to compile
+	 * instead of silently trying to access memory that shouldn't be.
+	 */
+	struct flow_action	action;
+#endif
+};
+
+void flow_rule_match_basic(const struct flow_rule *rule,
+			   struct flow_match_basic *out);
+void flow_rule_match_control(const struct flow_rule *rule,
+			     struct flow_match_control *out);
+void flow_rule_match_eth_addrs(const struct flow_rule *rule,
+			       struct flow_match_eth_addrs *out);
+#ifndef HAVE_TC_FLOWER_VLAN_IN_TAGS
+void flow_rule_match_vlan(const struct flow_rule *rule,
+			  struct flow_match_vlan *out);
+#endif
+void flow_rule_match_ipv4_addrs(const struct flow_rule *rule,
+				struct flow_match_ipv4_addrs *out);
+void flow_rule_match_ipv6_addrs(const struct flow_rule *rule,
+				struct flow_match_ipv6_addrs *out);
+void flow_rule_match_ports(const struct flow_rule *rule,
+			   struct flow_match_ports *out);
+#ifdef HAVE_TC_FLOWER_ENC
+void flow_rule_match_enc_ports(const struct flow_rule *rule,
+			       struct flow_match_ports *out);
+void flow_rule_match_enc_control(const struct flow_rule *rule,
+				 struct flow_match_control *out);
+void flow_rule_match_enc_ipv4_addrs(const struct flow_rule *rule,
+				    struct flow_match_ipv4_addrs *out);
+void flow_rule_match_enc_ipv6_addrs(const struct flow_rule *rule,
+				    struct flow_match_ipv6_addrs *out);
+void flow_rule_match_enc_keyid(const struct flow_rule *rule,
+			       struct flow_match_enc_keyid *out);
+#endif
+
+static inline struct flow_rule *
+tc_cls_flower_offload_flow_rule(struct tc_cls_flower_offload *tc_flow_cmd)
+{
+	return (struct flow_rule *)&tc_flow_cmd->dissector;
+}
+
+static inline bool flow_rule_match_key(const struct flow_rule *rule,
+				       enum flow_dissector_key_id key)
+{
+	return dissector_uses_key(rule->match.dissector, key);
+}
+#endif /* HAVE_TC_SETUP_CLSFLOWER */
+
+#endif /* RHEL < 8.1 */
+
+#if (!(RHEL_RELEASE_CODE && RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(8,1)))
+#define devlink_params_publish(devlink) do { } while (0)
+#define devlink_params_unpublish(devlink) do { } while (0)
+#endif
+
+#else /* >= 5.1.0 */
+#define HAVE_NDO_FDB_ADD_EXTACK
+#define NO_XDP_QUERY_XSK_UMEM
+#define HAVE_AF_XDP_NETDEV_UMEM
+#define HAVE_TC_FLOW_RULE_INFRASTRUCTURE
+#define HAVE_TC_FLOWER_ENC_IP
+#define HAVE_DEVLINK_INFO_GET
+#define HAVE_DEVLINK_FLASH_UPDATE
+#define HAVE_DEVLINK_PORT_PARAMS
+#endif /* 5.1.0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(5,2,0))
+#if (defined HAVE_SKB_XMIT_MORE) && \
+(!(RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(8,2))))
+#define netdev_xmit_more()	(skb->xmit_more)
+#else
+#define netdev_xmit_more()	(0)
+#endif
+
+#if (!(RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(8,2))))
+#ifndef eth_get_headlen
+static inline u32
+__kc_eth_get_headlen(const struct net_device __always_unused *dev, void *data,
+		     unsigned int len)
+{
+	return eth_get_headlen(data, len);
+}
+
+#define eth_get_headlen(dev, data, len) __kc_eth_get_headlen(dev, data, len)
+#endif /* !eth_get_headlen */
+#endif /* !RHEL >= 8.2 */
+
+#ifndef mmiowb
+#ifdef CONFIG_IA64
+#define mmiowb() asm volatile ("mf.a" ::: "memory")
+#else
+#define mmiowb()
+#endif
+#endif /* mmiowb */
+
+#if (RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(8,1))
+#define HAVE_NDO_GET_DEVLINK_PORT
+#endif /* RHEL > 8.1 */
+
+#else /* >= 5.2.0 */
+#define HAVE_NDO_SELECT_QUEUE_FALLBACK_REMOVED
+#define SPIN_UNLOCK_IMPLIES_MMIOWB
+#define HAVE_NDO_GET_DEVLINK_PORT
+#endif /* 5.2.0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(5,3,0))
+#if (!(RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(8,2)))
+#define flow_block_offload tc_block_offload
+#define flow_block_command tc_block_command
+#define flow_cls_offload tc_cls_flower_offload
+#define flow_block_binder_type tcf_block_binder_type
+#define flow_cls_common_offload tc_cls_common_offload
+#define flow_cls_offload_flow_rule tc_cls_flower_offload_flow_rule
+#define FLOW_CLS_REPLACE TC_CLSFLOWER_REPLACE
+#define FLOW_CLS_DESTROY TC_CLSFLOWER_DESTROY
+#define FLOW_CLS_STATS TC_CLSFLOWER_STATS
+#define FLOW_CLS_TMPLT_CREATE TC_CLSFLOWER_TMPLT_CREATE
+#define FLOW_CLS_TMPLT_DESTROY TC_CLSFLOWER_TMPLT_DESTROY
+#define FLOW_BLOCK_BINDER_TYPE_CLSACT_INGRESS \
+		TCF_BLOCK_BINDER_TYPE_CLSACT_INGRESS
+#define FLOW_BLOCK_BIND TC_BLOCK_BIND
+#define FLOW_BLOCK_UNBIND TC_BLOCK_UNBIND
+
+#ifdef HAVE_TC_CB_AND_SETUP_QDISC_MQPRIO
+#include <net/pkt_cls.h>
+
+int _kc_flow_block_cb_setup_simple(struct flow_block_offload *f,
+				   struct list_head *driver_list,
+				   tc_setup_cb_t *cb,
+				   void *cb_ident, void *cb_priv,
+				   bool ingress_only);
+
+#define flow_block_cb_setup_simple(f, driver_list, cb, cb_ident, cb_priv, \
+				   ingress_only) \
+	_kc_flow_block_cb_setup_simple(f, driver_list, cb, cb_ident, cb_priv, \
+				       ingress_only)
+#endif /* HAVE_TC_CB_AND_SETUP_QDISC_MQPRIO */
+#else /* RHEL >= 8.2 */
+#define HAVE_FLOW_BLOCK_API
+#define HAVE_DEVLINK_PORT_ATTR_PCI_VF
+#endif /* RHEL >= 8.2 */
+
+#ifndef ETH_P_LLDP
+#define ETH_P_LLDP	0x88CC
+#endif /* !ETH_P_LLDP */
+
+#else /* >= 5.3.0 */
+#define XSK_UMEM_RETURNS_XDP_DESC
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(5,8,0))
+#if !(SLE_VERSION_CODE && SLE_VERSION_CODE >= SLE_VERSION(15,3,0))
+#define HAVE_XSK_UMEM_HAS_ADDRS
+#endif /* SLE < 15.3 */
+#endif /* < 5.8.0*/
+#define HAVE_FLOW_BLOCK_API
+#define HAVE_DEVLINK_PORT_ATTR_PCI_VF
+#if IS_ENABLED(CONFIG_DIMLIB)
+#define HAVE_CONFIG_DIMLIB
+#endif
+#endif /* 5.3.0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(5,4,0))
+#if (SLE_VERSION_CODE >= SLE_VERSION(15,2,0))
+#define HAVE_NDO_XSK_WAKEUP
+#endif /* SLES15sp2 */
+#else /* >= 5.4.0 */
+#define HAVE_NDO_XSK_WAKEUP
+#endif /* 5.4.0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(5,5,0))
+static inline unsigned long _kc_bitmap_get_value8(const unsigned long *map,
+						  unsigned long start)
+{
+	const size_t index = BIT_WORD(start);
+	const unsigned long offset = start % BITS_PER_LONG;
+
+	return (map[index] >> offset) & 0xFF;
+}
+#define bitmap_get_value8 _kc_bitmap_get_value8
+
+static inline void _kc_bitmap_set_value8(unsigned long *map,
+					 unsigned long value,
+					 unsigned long start)
+{
+	const size_t index = BIT_WORD(start);
+	const unsigned long offset = start % BITS_PER_LONG;
+
+	map[index] &= ~(0xFFUL << offset);
+	map[index] |= value << offset;
+}
+#define bitmap_set_value8 _kc_bitmap_set_value8
+
+#endif /* 5.5.0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(5,6,0))
+#ifdef HAVE_AF_XDP_ZC_SUPPORT
+#define xsk_umem_release_addr		xsk_umem_discard_addr
+#define xsk_umem_release_addr_rq	xsk_umem_discard_addr_rq
+#endif /* HAVE_AF_XDP_ZC_SUPPORT */
+#if (RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(8,3)) || \
+     (SLE_VERSION_CODE && SLE_VERSION_CODE >= SLE_VERSION(15,3,0)))
+#define HAVE_TX_TIMEOUT_TXQUEUE
+#endif
+#else /* >= 5.6.0 */
+#define HAVE_TX_TIMEOUT_TXQUEUE
+#endif /* 5.6.0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(5,7,0))
+u64 _kc_pci_get_dsn(struct pci_dev *dev);
+#define pci_get_dsn(dev) _kc_pci_get_dsn(dev)
+#if !(SLE_VERSION_CODE > SLE_VERSION(15,2,0)) && \
+    !((LINUX_VERSION_CODE == KERNEL_VERSION(5,3,18)) && \
+      (SLE_LOCALVERSION_CODE >= KERNEL_VERSION(14,0,0))) && \
+    !(RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(8,3)))
+#define pci_aer_clear_nonfatal_status	pci_cleanup_aer_uncorrect_error_status
+#endif
+
+#ifndef DEVLINK_INFO_VERSION_GENERIC_FW_BUNDLE_ID
+#define DEVLINK_INFO_VERSION_GENERIC_FW_BUNDLE_ID "fw.bundle_id"
+#endif
+#else /* >= 5.7.0 */
+#define HAVE_DEVLINK_REGION_OPS_SNAPSHOT
+#define HAVE_ETHTOOL_COALESCE_PARAMS_SUPPORT
+#endif /* 5.7.0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(5,8,0))
+#if !(RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(8,4))) && \
+    !(SLE_VERSION_CODE && SLE_VERSION_CODE >= SLE_VERSION(15,3,0))
+#define xdp_convert_buff_to_frame convert_to_xdp_frame
+#endif /* (RHEL < 8.4) || (SLE < 15.3) */
+#define flex_array_size(p, member, count) \
+	array_size(count, sizeof(*(p)->member) + __must_be_array((p)->member))
+#if (!(SLE_VERSION_CODE && SLE_VERSION_CODE >= SLE_VERSION(15,3,0)))
+#ifdef HAVE_AF_XDP_ZC_SUPPORT
+#ifndef xsk_umem_get_rx_frame_size
+static inline u32 _xsk_umem_get_rx_frame_size(struct xdp_umem *umem)
+{
+	return umem->chunk_size_nohr - XDP_PACKET_HEADROOM;
+}
+
+#define xsk_umem_get_rx_frame_size _xsk_umem_get_rx_frame_size
+#endif /* xsk_umem_get_rx_frame_size */
+#endif /* HAVE_AF_XDP_ZC_SUPPORT */
+#else /* SLE >= 15.3 */
+#define HAVE_XDP_BUFF_FRAME_SZ
+#define HAVE_MEM_TYPE_XSK_BUFF_POOL
+#endif /* SLE >= 15.3 */
+#else /* >= 5.8.0 */
+#define HAVE_TC_FLOW_INDIR_DEV
+#define HAVE_TC_FLOW_INDIR_BLOCK_CLEANUP
+#define HAVE_XDP_BUFF_FRAME_SZ
+#define HAVE_MEM_TYPE_XSK_BUFF_POOL
+#endif /* 5.8.0 */
+#if (RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(8,3)))
+#define HAVE_TC_FLOW_INDIR_DEV
+#endif
+#if (SLE_VERSION_CODE && (SLE_VERSION_CODE >= SLE_VERSION(15,3,0)))
+#define HAVE_TC_FLOW_INDIR_DEV
+#endif /* SLE_VERSION_CODE && SLE_VERSION_CODE >= SLES15SP3 */
+
+/*****************************************************************************/
+#if (RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(8,4)))
+#define HAVE_TC_FLOW_INDIR_BLOCK_CLEANUP
+#endif /* (RHEL >= 8.4) */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(5,9,0))
+#else /* >= 5.9.0 */
+#define HAVE_FLOW_INDIR_BLOCK_QDISC
+#define HAVE_UDP_TUNNEL_NIC_INFO
+#endif /* 5.9.0 */
+#if (RHEL_RELEASE_CODE && (RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(8,3)))
+#define HAVE_FLOW_INDIR_BLOCK_QDISC
+#endif
+#if (SLE_VERSION_CODE && (SLE_VERSION_CODE >= SLE_VERSION(15,3,0)))
+#define HAVE_FLOW_INDIR_BLOCK_QDISC
+#endif /* SLE_VERSION_CODE && SLE_VERSION_CODE >= SLES15SP3 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(5,10,0))
+#if (SLE_VERSION_CODE && SLE_VERSION_CODE >= SLE_VERSION(15,3,0))
+#define HAVE_DEVLINK_REGION_OPS_SNAPSHOT_OPS
+#define HAVE_DEVLINK_FLASH_UPDATE_PARAMS
+#else /* SLE >= 15.3 */
+struct devlink_flash_update_params {
+	const char *file_name;
+	const char *component;
+	u32 overwrite_mask;
+};
+
+#ifndef DEVLINK_FLASH_OVERWRITE_SETTINGS
+#define DEVLINK_FLASH_OVERWRITE_SETTINGS BIT(0)
+#endif
+
+#ifndef DEVLINK_FLASH_OVERWRITE_IDENTIFIERS
+#define DEVLINK_FLASH_OVERWRITE_IDENTIFIERS BIT(1)
+#endif
+#endif /* !(SLE >= 15.3) */
+
+#if (!(SLE_VERSION_CODE && (SLE_VERSION_CODE >= SLE_VERSION(15,3,0))))
+#define XDP_SETUP_XSK_POOL XDP_SETUP_XSK_UMEM
+#define xsk_get_pool_from_qid xdp_get_umem_from_qid
+#define xsk_pool_get_rx_frame_size xsk_umem_get_rx_frame_size
+#define xsk_pool_set_rxq_info xsk_buff_set_rxq_info
+#define xsk_pool_dma_unmap xsk_buff_dma_unmap
+#define xsk_pool_dma_map xsk_buff_dma_map
+#define xsk_tx_peek_desc xsk_umem_consume_tx
+#define xsk_tx_release xsk_umem_consume_tx_done
+#define xsk_tx_completed xsk_umem_complete_tx
+#define xsk_uses_need_wakeup xsk_umem_uses_need_wakeup
+#ifdef HAVE_MEM_TYPE_XSK_BUFF_POOL
+#include <net/xdp_sock_drv.h>
+static inline void
+_kc_xsk_buff_dma_sync_for_cpu(struct xdp_buff *xdp,
+			      void __always_unused *pool)
+{
+	xsk_buff_dma_sync_for_cpu(xdp);
+}
+
+#define xsk_buff_dma_sync_for_cpu(xdp, pool) \
+	_kc_xsk_buff_dma_sync_for_cpu(xdp, pool)
+#endif /* HAVE_MEM_TYPE_XSK_BUFF_POOL */
+#else /* SLE >= 15.3 */
+#define HAVE_NETDEV_BPF_XSK_POOL
+#endif /* SLE >= 15.3 */
+#else /* >= 5.10.0 */
+#define HAVE_DEVLINK_REGION_OPS_SNAPSHOT_OPS
+#define HAVE_DEVLINK_FLASH_UPDATE_PARAMS
+#define HAVE_NETDEV_BPF_XSK_POOL
+#endif /* 5.10.0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(5,11,0))
+#ifdef HAVE_XDP_BUFF_RXQ
+#include <net/xdp.h>
+static inline int
+_kc_xdp_rxq_info_reg(struct xdp_rxq_info *xdp_rxq, struct net_device *dev,
+		     u32 queue_index, unsigned int __always_unused napi_id)
+{
+	return xdp_rxq_info_reg(xdp_rxq, dev, queue_index);
+}
+
+#define xdp_rxq_info_reg(xdp_rxq, dev, queue_index, napi_id) \
+	_kc_xdp_rxq_info_reg(xdp_rxq, dev, queue_index, napi_id)
+#endif /* HAVE_XDP_BUFF_RXQ */
+#ifdef HAVE_NAPI_BUSY_LOOP
+#ifdef CONFIG_NET_RX_BUSY_POLL
+#include <net/busy_poll.h>
+static inline void
+_kc_napi_busy_loop(unsigned int napi_id,
+		   bool (*loop_end)(void *, unsigned long), void *loop_end_arg,
+		   bool __always_unused prefer_busy_poll,
+		   u16 __always_unused budget)
+{
+	napi_busy_loop(napi_id, loop_end, loop_end_arg);
+}
+
+#define napi_busy_loop(napi_id, loop_end, loop_end_arg, prefer_busy_poll, budget) \
+	_kc_napi_busy_loop(napi_id, loop_end, loop_end_arg, prefer_busy_poll, budget)
+#endif /* CONFIG_NET_RX_BUSY_POLL */
+#endif /* HAVE_NAPI_BUSY_LOOP */
+#define HAVE_DEVLINK_FLASH_UPDATE_BEGIN_END_NOTIFY
+#else /* >= 5.11.0 */
+#define HAVE_DEVLINK_FLASH_UPDATE_PARAMS_FW
+#endif /* 5.11.0 */
+
+/*
+ * Load the implementations file which actually defines kcompat backports.
+ * Legacy backports still exist in this file, but all new backports must be
+ * implemented using kcompat_*defs.h and kcompat_impl.h
+ */
+#include "kcompat_impl.h"
+
+#endif /* _KCOMPAT_H_ */
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/kcompat_impl.h linux-5.4.86.new/drivers/net/ethernet/intel/i40e/kcompat_impl.h
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/kcompat_impl.h	1969-12-31 19:00:00.000000000 -0500
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/kcompat_impl.h	2024-05-13 03:58:25.372491940 -0400
@@ -0,0 +1,596 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
+
+#ifndef _KCOMPAT_IMPL_H_
+#define _KCOMPAT_IMPL_H_
+
+/* This file contains implementations of backports from various kernels. It
+ * must rely only on NEED_<FLAG> and HAVE_<FLAG> checks. It must not make any
+ * checks to determine the kernel version when deciding whether to include an
+ * implementation.
+ *
+ * All new implementations must go in this file, and legacy implementations
+ * should be migrated to the new format over time.
+ */
+
+/*
+ * generic network stack functions
+ */
+
+/* NEED_NET_PREFETCH
+ *
+ * net_prefetch was introduced by commit f468f21b7af0 ("net: Take common
+ * prefetch code structure into a function")
+ *
+ * This function is trivial to re-implement in full.
+ */
+#ifdef NEED_NET_PREFETCH
+static inline void net_prefetch(void *p)
+{
+	prefetch(p);
+#if L1_CACHE_BYTES < 128
+	prefetch((u8 *)p + L1_CACHE_BYTES);
+#endif
+}
+#endif /* NEED_NET_PREFETCH */
+
+/* NEED_SKB_FRAG_OFF_ACCESSORS
+ *
+ * skb_frag_off and skb_frag_off_add were added in upstream commit
+ * 7240b60c98d6 ("linux: Add skb_frag_t page_offset accessors")
+ *
+ * Implementing the wrappers directly for older kernels which still have the
+ * old implementation of skb_frag_t is trivial.
+ */
+#ifdef NEED_SKB_FRAG_OFF_ACCESSORS
+static inline unsigned int skb_frag_off(const skb_frag_t *frag)
+{
+	return frag->page_offset;
+}
+
+static inline void skb_frag_off_add(skb_frag_t *frag, int delta)
+{
+	frag->page_offset += delta;
+}
+#endif
+
+/*
+ * NETIF_F_HW_L2FW_DOFFLOAD related functions
+ *
+ * Support for NETIF_F_HW_L2FW_DOFFLOAD was first introduced upstream by
+ * commit a6cc0cfa72e0 ("net: Add layer 2 hardware acceleration operations for
+ * macvlan devices")
+ */
+#ifdef NETIF_F_HW_L2FW_DOFFLOAD
+
+#include <linux/if_macvlan.h>
+
+/* NEED_MACVLAN_ACCEL_PRIV
+ *
+ * macvlan_accel_priv is an accessor function that replaced direct access to
+ * the macvlan->fwd_priv variable. It was introduced in commit 7d775f63470c
+ * ("macvlan: Rename fwd_priv to accel_priv and add accessor function")
+ *
+ * Implement the new wrapper name by simply accessing the older
+ * macvlan->fwd_priv name.
+ */
+#ifdef NEED_MACVLAN_ACCEL_PRIV
+static inline void *macvlan_accel_priv(struct net_device *dev)
+{
+	struct macvlan_dev *macvlan = netdev_priv(dev);
+
+	return macvlan->fwd_priv;
+}
+#endif /* NEED_MACVLAN_ACCEL_PRIV */
+
+/* NEED_MACVLAN_RELEASE_L2FW_OFFLOAD
+ *
+ * macvlan_release_l2fw_offload was introduced upstream by commit 53cd4d8e4dfb
+ * ("macvlan: Provide function for interfaces to release HW offload")
+ *
+ * Implementing this is straight forward, but we must be careful to use
+ * fwd_priv instead of accel_priv. Note that both the change to accel_priv and
+ * introduction of this function happened in the same release.
+ */
+#ifdef NEED_MACVLAN_RELEASE_L2FW_OFFLOAD
+static inline int macvlan_release_l2fw_offload(struct net_device *dev)
+{
+	struct macvlan_dev *macvlan = netdev_priv(dev);
+
+	macvlan->fwd_priv = NULL;
+	return dev_uc_add(macvlan->lowerdev, dev->dev_addr);
+}
+#endif /* NEED_MACVLAN_RELEASE_L2FW_OFFLOAD */
+
+/* NEED_MACVLAN_SUPPORTS_DEST_FILTER
+ *
+ * macvlan_supports_dest_filter was introduced upstream by commit 6cb1937d4eff
+ * ("macvlan: Add function to test for destination filtering support")
+ *
+ * The implementation doesn't rely on anything new and is trivial to backport
+ * for kernels that have NETIF_F_HW_L2FW_DOFFLOAD support.
+ */
+#ifdef NEED_MACVLAN_SUPPORTS_DEST_FILTER
+static inline bool macvlan_supports_dest_filter(struct net_device *dev)
+{
+	struct macvlan_dev *macvlan = netdev_priv(dev);
+
+	return macvlan->mode == MACVLAN_MODE_PRIVATE ||
+	       macvlan->mode == MACVLAN_MODE_VEPA ||
+	       macvlan->mode == MACVLAN_MODE_BRIDGE;
+}
+#endif /* NEED_MACVLAN_SUPPORTS_DEST_FILTER */
+
+#endif /* NETIF_F_HW_L2FW_DOFFLOAD */
+
+/*
+ * tc functions
+ */
+
+/* NEED_FLOW_INDR_BLOCK_CB_REGISTER
+ *
+ * __flow_indr_block_cb_register and __flow_indr_block_cb_unregister were
+ * added in upstream commit 4e481908c51b ("flow_offload: move tc indirect
+ * block to flow offload")
+ *
+ * This was a simple rename so we can just translate from the old
+ * naming scheme with a macro.
+ */
+#ifdef NEED_FLOW_INDR_BLOCK_CB_REGISTER
+#define __flow_indr_block_cb_register __tc_indr_block_cb_register
+#define __flow_indr_block_cb_unregister __tc_indr_block_cb_unregister
+#endif
+
+/*
+ * devlink support
+ */
+#if IS_ENABLED(CONFIG_NET_DEVLINK)
+
+#include <net/devlink.h>
+
+#ifdef HAVE_DEVLINK_REGIONS
+/* NEED_DEVLINK_REGION_CREATE_OPS
+ *
+ * The ops parameter to devlink_region_create was added by commit e8937681797c
+ * ("devlink: prepare to support region operations")
+ *
+ * For older kernels, define _kc_devlink_region_create that takes an ops
+ * parameter, and calls the old implementation function by extracting the name
+ * from the structure.
+ */
+#ifdef NEED_DEVLINK_REGION_CREATE_OPS
+struct devlink_region_ops {
+	const char *name;
+	void (*destructor)(const void *data);
+};
+
+static inline struct devlink_region *
+_kc_devlink_region_create(struct devlink *devlink,
+			  const struct devlink_region_ops *ops,
+			  u32 region_max_snapshots, u64 region_size)
+{
+	return devlink_region_create(devlink, ops->name, region_max_snapshots,
+				     region_size);
+}
+
+#define devlink_region_create _kc_devlink_region_create
+#endif /* NEED_DEVLINK_REGION_CREATE_OPS */
+#endif /* HAVE_DEVLINK_REGIONS */
+
+/* NEED_DEVLINK_FLASH_UPDATE_STATUS_NOTIFY
+ *
+ * devlink_flash_update_status_notify, _begin_notify, and _end_notify were
+ * added by upstream commit 191ed2024de9 ("devlink: allow driver to update
+ * progress of flash update")
+ *
+ * For older kernels that lack the netlink messages, convert the functions
+ * into no-ops.
+ */
+#ifdef NEED_DEVLINK_FLASH_UPDATE_STATUS_NOTIFY
+static inline void
+devlink_flash_update_begin_notify(struct devlink __always_unused *devlink)
+{
+}
+
+static inline void
+devlink_flash_update_end_notify(struct devlink __always_unused *devlink)
+{
+}
+
+static inline void
+devlink_flash_update_status_notify(struct devlink __always_unused *devlink,
+				   const char __always_unused *status_msg,
+				   const char __always_unused *component,
+				   unsigned long __always_unused done,
+				   unsigned long __always_unused total)
+{
+}
+#endif /* NEED_DEVLINK_FLASH_UPDATE_STATUS_NOTIFY */
+
+/* NEED_DEVLINK_FLASH_UPDATE_TIMEOUT_NOTIFY
+ *
+ * devlink_flash_update_timeout_notify was added by upstream commit
+ * f92970c694b3 ("devlink: add timeout information to status_notify").
+ *
+ * For older kernels, just convert timeout notifications into regular status
+ * notification messages without timeout information.
+ */
+#ifdef NEED_DEVLINK_FLASH_UPDATE_TIMEOUT_NOTIFY
+static inline void
+devlink_flash_update_timeout_notify(struct devlink *devlink,
+				    const char *status_msg,
+				    const char *component,
+				    unsigned long __always_unused timeout)
+{
+	devlink_flash_update_status_notify(devlink, status_msg, component, 0, 0);
+}
+#endif /* NEED_DEVLINK_FLASH_UPDATE_TIMEOUT_NOTIFY */
+
+/*
+ * NEED_DEVLINK_PORT_ATTRS_SET_STRUCT
+ *
+ * HAVE_DEVLINK_PORT_ATTRS_SET_PORT_FLAVOUR
+ * HAVE_DEVLINK_PORT_ATTRS_SET_SWITCH_ID
+ *
+ * devlink_port_attrs_set was introduced by commit b9ffcbaf56d3 ("devlink:
+ * introduce devlink_port_attrs_set")
+ *
+ * It's function signature has changed multiple times over several kernel
+ * releases:
+ *
+ * commit 5ec1380a21bb ("devlink: extend attrs_set for setting port
+ * flavours") added the ability to set port flavour. (Note that there is no
+ * official kernel release with devlink_port_attrs_set without the flavour
+ * argument, as they were introduced in the same series.)
+ *
+ * commit bec5267cded2 ("net: devlink: extend port attrs for switch ID") added
+ * the ability to set the switch ID (HAVE_DEVLINK_PORT_ATTRS_SET_SWITCH_ID)
+ *
+ * Finally commit 71ad8d55f8e5 ("devlink: Replace devlink_port_attrs_set
+ * parameters with a struct") refactored to pass devlink_port_attrs struct
+ * instead of individual parameters. (!NEED_DEVLINK_PORT_ATTRS_SET_STRUCT)
+ *
+ * We want core drivers to just use the latest form that takes
+ * a devlink_port_attrs structure. Note that this structure did exist as part
+ * of <net/devlink.h> but was never used directly by driver code prior to the
+ * function parameter change. For this reason, the implementation always
+ * relies on _kc_devlink_port_attrs instead of what was defined in the kernel.
+ */
+#ifdef NEED_DEVLINK_PORT_ATTRS_SET_STRUCT
+
+#ifndef HAVE_DEVLINK_PORT_ATTRS_SET_PORT_FLAVOUR
+enum devlink_port_flavour {
+	DEVLINK_PORT_FLAVOUR_PHYSICAL,
+	DEVLINK_PORT_FLAVOUR_CPU,
+	DEVLINK_PORT_FLAVOUR_DSA,
+	DEVLINK_PORT_FLAVOUR_PCI_PF,
+	DEVLINK_PORT_FLAVOUR_PCI_VF,
+};
+#endif
+
+struct _kc_devlink_port_phys_attrs {
+	u32 port_number;
+	u32 split_subport_number;
+};
+
+struct _kc_devlink_port_pci_pf_attrs {
+	u16 pf;
+};
+
+struct _kc_devlink_port_pci_vf_attrs {
+	u16 pf;
+	u16 vf;
+};
+
+struct _kc_devlink_port_attrs {
+	u8 split:1,
+	   splittable:1;
+	u32 lanes;
+	enum devlink_port_flavour flavour;
+	struct netdev_phys_item_id switch_id;
+	union {
+		struct _kc_devlink_port_phys_attrs phys;
+		struct _kc_devlink_port_pci_pf_attrs pci_pf;
+		struct _kc_devlink_port_pci_vf_attrs pci_vf;
+	};
+};
+
+#define devlink_port_attrs _kc_devlink_port_attrs
+
+static inline void
+_kc_devlink_port_attrs_set(struct devlink_port *devlink_port,
+			   struct _kc_devlink_port_attrs *attrs)
+{
+#if defined(HAVE_DEVLINK_PORT_ATTRS_SET_SWITCH_ID)
+	devlink_port_attrs_set(devlink_port, attrs->flavour, attrs->phys.port_number,
+			       attrs->split, attrs->phys.split_subport_number,
+			       attrs->switch_id.id, attrs->switch_id.id_len);
+#elif defined(HAVE_DEVLINK_PORT_ATTRS_SET_PORT_FLAVOUR)
+	devlink_port_attrs_set(devlink_port, attrs->flavour, attrs->phys.port_number,
+			       attrs->split, attrs->phys.split_subport_number);
+#else
+	if (attrs->split)
+		devlink_port_split_set(devlink_port, attrs->phys.port_number);
+#endif
+}
+
+#define devlink_port_attrs_set _kc_devlink_port_attrs_set
+
+#endif /* NEED_DEVLINK_PORT_ATTRS_SET_STRUCT */
+
+#endif /* CONFIG_NET_DEVLINK */
+
+#ifdef NEED_IDA_ALLOC_MIN_MAX_RANGE_FREE
+/* ida_alloc(), ida_alloc_min(), ida_alloc_max(), ida_alloc_range(), and
+ * ida_free() were added in commit 5ade60dda43c ("ida: add new API").
+ *
+ * Also, using "0" as the "end" argument (3rd argument) to ida_simple_get() is
+ * considered the max value, which is why it's used in ida_alloc() and
+ * ida_alloc_min().
+ */
+static inline int ida_alloc(struct ida *ida, gfp_t gfp)
+{
+	return ida_simple_get(ida, 0, 0, gfp);
+}
+
+static inline int ida_alloc_min(struct ida *ida, unsigned int min, gfp_t gfp)
+{
+	return ida_simple_get(ida, min, 0, gfp);
+}
+
+static inline int ida_alloc_max(struct ida *ida, unsigned int max, gfp_t gfp)
+{
+	return ida_simple_get(ida, 0, max, gfp);
+}
+
+static inline int
+ida_alloc_range(struct ida *ida, unsigned int min, unsigned int max, gfp_t gfp)
+{
+	return ida_simple_get(ida, min, max, gfp);
+}
+
+static inline void ida_free(struct ida *ida, unsigned int id)
+{
+	ida_simple_remove(ida, id);
+}
+#endif /* NEED_IDA_ALLOC_MIN_MAX_RANGE_FREE */
+
+/*
+ * dev_printk implementations
+ */
+
+/* NEED_DEV_PRINTK_ONCE
+ *
+ * The dev_*_once family of printk functions was introduced by commit
+ * e135303bd5be ("device: Add dev_<level>_once variants")
+ *
+ * The implementation is very straight forward so we will just implement them
+ * as-is here.
+ */
+#ifdef NEED_DEV_PRINTK_ONCE
+#ifdef CONFIG_PRINTK
+#define dev_level_once(dev_level, dev, fmt, ...)			\
+do {									\
+	static bool __print_once __read_mostly;				\
+									\
+	if (!__print_once) {						\
+		__print_once = true;					\
+		dev_level(dev, fmt, ##__VA_ARGS__);			\
+	}								\
+} while (0)
+#else
+#define dev_level_once(dev_level, dev, fmt, ...)			\
+do {									\
+	if (0)								\
+		dev_level(dev, fmt, ##__VA_ARGS__);			\
+} while (0)
+#endif
+
+#define dev_emerg_once(dev, fmt, ...)					\
+	dev_level_once(dev_emerg, dev, fmt, ##__VA_ARGS__)
+#define dev_alert_once(dev, fmt, ...)					\
+	dev_level_once(dev_alert, dev, fmt, ##__VA_ARGS__)
+#define dev_crit_once(dev, fmt, ...)					\
+	dev_level_once(dev_crit, dev, fmt, ##__VA_ARGS__)
+#define dev_err_once(dev, fmt, ...)					\
+	dev_level_once(dev_err, dev, fmt, ##__VA_ARGS__)
+#define dev_warn_once(dev, fmt, ...)					\
+	dev_level_once(dev_warn, dev, fmt, ##__VA_ARGS__)
+#define dev_notice_once(dev, fmt, ...)					\
+	dev_level_once(dev_notice, dev, fmt, ##__VA_ARGS__)
+#define dev_info_once(dev, fmt, ...)					\
+	dev_level_once(dev_info, dev, fmt, ##__VA_ARGS__)
+#define dev_dbg_once(dev, fmt, ...)					\
+	dev_level_once(dev_dbg, dev, fmt, ##__VA_ARGS__)
+#endif /* NEED_DEV_PRINTK_ONCE */
+
+#ifdef HAVE_TC_CB_AND_SETUP_QDISC_MQPRIO
+
+/* NEED_TC_CLS_CAN_OFFLOAD_AND_CHAIN0
+ *
+ * tc_cls_can_offload_and_chain0 was added by upstream commit
+ * 878db9f0f26d ("pkt_cls: add new tc cls helper to check offload flag and
+ * chain index").
+ *
+ * This patch backports this function for older kernels by calling
+ * tc_can_offload() directly.
+ */
+#ifdef NEED_TC_CLS_CAN_OFFLOAD_AND_CHAIN0
+#include <net/pkt_cls.h>
+static inline bool
+tc_cls_can_offload_and_chain0(const struct net_device *dev,
+			      struct tc_cls_common_offload *common)
+{
+	if (!tc_can_offload(dev))
+		return false;
+	if (common->chain_index)
+		return false;
+
+	return true;
+}
+#endif /* NEED_TC_CLS_CAN_OFFLOAD_AND_CHAIN0 */
+#endif /* HAVE_TC_CB_AND_SETUP_QDISC_MQPRIO */
+
+/* NEED_TC_SETUP_QDISC_MQPRIO
+ *
+ * TC_SETUP_QDISC_MQPRIO was added by upstream commit
+ * 575ed7d39e2f ("net_sch: mqprio: Change TC_SETUP_MQPRIO to
+ * TC_SETUP_QDISC_MQPRIO").
+ *
+ * For older kernels which are using TC_SETUP_MQPRIO
+ */
+#ifdef NEED_TC_SETUP_QDISC_MQPRIO
+#define TC_SETUP_QDISC_MQPRIO TC_SETUP_MQPRIO
+#endif /* NEED_TC_SETUP_QDISC_MQPRIO */
+
+/*
+ * ART/TSC functions
+ */
+#ifdef HAVE_PTP_CROSSTIMESTAMP
+/* NEED_CONVERT_ART_NS_TO_TSC
+ *
+ * convert_art_ns_to_tsc was added by upstream commit fc804f65d462 ("x86/tsc:
+ * Convert ART in nanoseconds to TSC").
+ *
+ * This function is similar to convert_art_to_tsc, but expects the input in
+ * terms of nanoseconds, rather than ART cycles. We implement this by
+ * accessing the tsc_khz value and performing the proper calculation. In order
+ * to access the correct clock object on returning, we use the function
+ * convert_art_to_tsc, because the art_related_clocksource is inaccessible.
+ */
+#ifdef NEED_CONVERT_ART_NS_TO_TSC
+#ifdef CONFIG_X86
+#include <asm/tsc.h>
+
+static inline struct system_counterval_t convert_art_ns_to_tsc(u64 art_ns)
+{
+	struct system_counterval_t system;
+	u64 tmp, res, rem;
+
+	rem = do_div(art_ns, USEC_PER_SEC);
+
+	res = art_ns * tsc_khz;
+	tmp = rem * tsc_khz;
+
+	do_div(tmp, USEC_PER_SEC);
+	res += tmp;
+
+	system = convert_art_to_tsc(art_ns);
+	system.cycles = res;
+
+	return system;
+}
+#else /* CONFIG_X86 */
+static inline struct system_counterval_t convert_art_ns_to_tsc(u64 art_ns)
+{
+	WARN_ONCE(1, "%s is only supported on X86", __func__);
+	return (struct system_counterval_t){};
+}
+#endif /* !CONFIG_X86 */
+#endif /* NEED_CONVERT_ART_NS_TO_TSC */
+#endif /* HAVE_PTP_CROSSTIMESTAMP */
+
+#ifdef NEED_BUS_FIND_DEVICE_CONST_DATA
+/* NEED_BUS_FIND_DEVICE_CONST_DATA
+ *
+ * bus_find_device() was updated in upstream commit 418e3ea157ef
+ * ("bus_find_device: Unify the match callback with class_find_device")
+ * to take a const void *data parameter and also have the match() function
+ * passed in take a const void *data parameter.
+ *
+ * all of the kcompat below makes it so the caller can always just call
+ * bus_find_device() according to the upstream kernel without having to worry
+ * about const vs. non-const arguments.
+ */
+struct _kc_bus_find_device_custom_data {
+	const void *real_data;
+	int (*real_match)(struct device *dev, const void *data);
+};
+
+static inline int _kc_bus_find_device_wrapped_match(struct device *dev, void *data)
+{
+	struct _kc_bus_find_device_custom_data *custom_data = data;
+
+	return custom_data->real_match(dev, custom_data->real_data);
+}
+
+static inline struct device *
+_kc_bus_find_device(struct bus_type *type, struct device *start,
+		    const void *data,
+		    int (*match)(struct device *dev, const void *data))
+{
+	struct _kc_bus_find_device_custom_data custom_data = {};
+
+	custom_data.real_data = data;
+	custom_data.real_match = match;
+
+	return bus_find_device(type, start, &custom_data,
+			       _kc_bus_find_device_wrapped_match);
+}
+
+/* force callers of bus_find_device() to call _kc_bus_find_device() on kernels
+ * where NEED_BUS_FIND_DEVICE_CONST_DATA is defined
+ */
+#define bus_find_device(type, start, data, match) \
+	_kc_bus_find_device(type, start, data, match)
+#endif /* NEED_BUS_FIND_DEVICE_CONST_DATA */
+
+#ifdef NEED_DEV_PM_DOMAIN_ATTACH_DETACH
+#include <linux/acpi.h>
+/* NEED_DEV_PM_DOMAIN_ATTACH_DETACH
+ *
+ * dev_pm_domain_attach() and dev_pm_domain_detach() were added in upstream
+ * commit 46420dd73b80 ("PM / Domains: Add APIs to attach/detach a PM domain for
+ * a device"). To support older kernels and OSVs that don't have these API, just
+ * implement how older versions worked by directly calling acpi_dev_pm_attach()
+ * and acpi_dev_pm_detach().
+ */
+static inline int dev_pm_domain_attach(struct device *dev, bool power_on)
+{
+	if (dev->pm_domain)
+		return 0;
+
+	if (ACPI_HANDLE(dev))
+		return acpi_dev_pm_attach(dev, true);
+
+	return 0;
+}
+
+static inline void dev_pm_domain_detach(struct device *dev, bool power_off)
+{
+	if (ACPI_HANDLE(dev))
+		acpi_dev_pm_detach(dev, true);
+}
+#endif /* NEED_DEV_PM_DOMAIN_ATTACH_DETACH */
+
+#ifdef NEED_CPU_LATENCY_QOS_RENAME
+/* NEED_CPU_LATENCY_QOS_RENAME
+ *
+ * The PM_QOS_CPU_DMA_LATENCY definition was removed in 67b06ba01857 ("PM:
+ * QoS: Drop PM_QOS_CPU_DMA_LATENCY and rename related functions"). The
+ * related functions were renamed to use "cpu_latency_qos_" prefix.
+ *
+ * Use wrapper functions to map the new API onto the API available in older
+ * kernels.
+ */
+#include <linux/pm_qos.h>
+static inline void
+cpu_latency_qos_add_request(struct pm_qos_request *req, s32 value)
+{
+	pm_qos_add_request(req, PM_QOS_CPU_DMA_LATENCY, value);
+}
+
+static inline void
+cpu_latency_qos_update_request(struct pm_qos_request *req, s32 new_value)
+{
+	pm_qos_update_request(req, new_value);
+}
+
+static inline void
+cpu_latency_qos_remove_request(struct pm_qos_request *req)
+{
+	pm_qos_remove_request(req);
+}
+#endif /* NEED_CPU_LATENCY_QOS_RENAME */
+
+#endif /* _KCOMPAT_IMPL_H_ */
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/kcompat_overflow.h linux-5.4.86.new/drivers/net/ethernet/intel/i40e/kcompat_overflow.h
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/kcompat_overflow.h	1969-12-31 19:00:00.000000000 -0500
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/kcompat_overflow.h	2024-05-13 03:58:25.372491940 -0400
@@ -0,0 +1,319 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
+
+/* SPDX-License-Identifier: GPL-2.0 OR MIT */
+#ifndef __LINUX_OVERFLOW_H
+#define __LINUX_OVERFLOW_H
+
+#include <linux/compiler.h>
+
+/*
+ * In the fallback code below, we need to compute the minimum and
+ * maximum values representable in a given type. These macros may also
+ * be useful elsewhere, so we provide them outside the
+ * COMPILER_HAS_GENERIC_BUILTIN_OVERFLOW block.
+ *
+ * It would seem more obvious to do something like
+ *
+ * #define type_min(T) (T)(is_signed_type(T) ? (T)1 << (8*sizeof(T)-1) : 0)
+ * #define type_max(T) (T)(is_signed_type(T) ? ((T)1 << (8*sizeof(T)-1)) - 1 : ~(T)0)
+ *
+ * Unfortunately, the middle expressions, strictly speaking, have
+ * undefined behaviour, and at least some versions of gcc warn about
+ * the type_max expression (but not if -fsanitize=undefined is in
+ * effect; in that case, the warning is deferred to runtime...).
+ *
+ * The slightly excessive casting in type_min is to make sure the
+ * macros also produce sensible values for the exotic type _Bool. [The
+ * overflow checkers only almost work for _Bool, but that's
+ * a-feature-not-a-bug, since people shouldn't be doing arithmetic on
+ * _Bools. Besides, the gcc builtins don't allow _Bool* as third
+ * argument.]
+ *
+ * Idea stolen from
+ * https://mail-index.netbsd.org/tech-misc/2007/02/05/0000.html -
+ * credit to Christian Biere.
+ */
+/* The is_signed_type macro is redefined in a few places in various kernel
+ * headers. If this header is included at the same time as one of those, we
+ * will generate compilation warnings. Since we can't fix every old kernel,
+ * rename is_signed_type for this file to _kc_is_signed_type. This prevents
+ * the macro name collision, and should be safe since our drivers do not
+ * directly call the macro.
+ */
+#define _kc_is_signed_type(type)       (((type)(-1)) < (type)1)
+#define __type_half_max(type) ((type)1 << (8*sizeof(type) - 1 - _kc_is_signed_type(type)))
+#define type_max(T) ((T)((__type_half_max(T) - 1) + __type_half_max(T)))
+#define type_min(T) ((T)((T)-type_max(T)-(T)1))
+
+
+#ifdef COMPILER_HAS_GENERIC_BUILTIN_OVERFLOW
+/*
+ * For simplicity and code hygiene, the fallback code below insists on
+ * a, b and *d having the same type (similar to the min() and max()
+ * macros), whereas gcc's type-generic overflow checkers accept
+ * different types. Hence we don't just make check_add_overflow an
+ * alias for __builtin_add_overflow, but add type checks similar to
+ * below.
+ */
+#define check_add_overflow(a, b, d) ({		\
+	typeof(a) __a = (a);			\
+	typeof(b) __b = (b);			\
+	typeof(d) __d = (d);			\
+	(void) (&__a == &__b);			\
+	(void) (&__a == __d);			\
+	__builtin_add_overflow(__a, __b, __d);	\
+})
+
+#define check_sub_overflow(a, b, d) ({		\
+	typeof(a) __a = (a);			\
+	typeof(b) __b = (b);			\
+	typeof(d) __d = (d);			\
+	(void) (&__a == &__b);			\
+	(void) (&__a == __d);			\
+	__builtin_sub_overflow(__a, __b, __d);	\
+})
+
+#define check_mul_overflow(a, b, d) ({		\
+	typeof(a) __a = (a);			\
+	typeof(b) __b = (b);			\
+	typeof(d) __d = (d);			\
+	(void) (&__a == &__b);			\
+	(void) (&__a == __d);			\
+	__builtin_mul_overflow(__a, __b, __d);	\
+})
+
+#else
+
+
+/* Checking for unsigned overflow is relatively easy without causing UB. */
+#define __unsigned_add_overflow(a, b, d) ({	\
+	typeof(a) __a = (a);			\
+	typeof(b) __b = (b);			\
+	typeof(d) __d = (d);			\
+	(void) (&__a == &__b);			\
+	(void) (&__a == __d);			\
+	*__d = __a + __b;			\
+	*__d < __a;				\
+})
+#define __unsigned_sub_overflow(a, b, d) ({	\
+	typeof(a) __a = (a);			\
+	typeof(b) __b = (b);			\
+	typeof(d) __d = (d);			\
+	(void) (&__a == &__b);			\
+	(void) (&__a == __d);			\
+	*__d = __a - __b;			\
+	__a < __b;				\
+})
+/*
+ * If one of a or b is a compile-time constant, this avoids a division.
+ */
+#define __unsigned_mul_overflow(a, b, d) ({		\
+	typeof(a) __a = (a);				\
+	typeof(b) __b = (b);				\
+	typeof(d) __d = (d);				\
+	(void) (&__a == &__b);				\
+	(void) (&__a == __d);				\
+	*__d = __a * __b;				\
+	__builtin_constant_p(__b) ?			\
+	  __b > 0 && __a > type_max(typeof(__a)) / __b : \
+	  __a > 0 && __b > type_max(typeof(__b)) / __a;	 \
+})
+
+/*
+ * For signed types, detecting overflow is much harder, especially if
+ * we want to avoid UB. But the interface of these macros is such that
+ * we must provide a result in *d, and in fact we must produce the
+ * result promised by gcc's builtins, which is simply the possibly
+ * wrapped-around value. Fortunately, we can just formally do the
+ * operations in the widest relevant unsigned type (u64) and then
+ * truncate the result - gcc is smart enough to generate the same code
+ * with and without the (u64) casts.
+ */
+
+/*
+ * Adding two signed integers can overflow only if they have the same
+ * sign, and overflow has happened iff the result has the opposite
+ * sign.
+ */
+#define __signed_add_overflow(a, b, d) ({	\
+	typeof(a) __a = (a);			\
+	typeof(b) __b = (b);			\
+	typeof(d) __d = (d);			\
+	(void) (&__a == &__b);			\
+	(void) (&__a == __d);			\
+	*__d = (u64)__a + (u64)__b;		\
+	(((~(__a ^ __b)) & (*__d ^ __a))	\
+		& type_min(typeof(__a))) != 0;	\
+})
+
+/*
+ * Subtraction is similar, except that overflow can now happen only
+ * when the signs are opposite. In this case, overflow has happened if
+ * the result has the opposite sign of a.
+ */
+#define __signed_sub_overflow(a, b, d) ({	\
+	typeof(a) __a = (a);			\
+	typeof(b) __b = (b);			\
+	typeof(d) __d = (d);			\
+	(void) (&__a == &__b);			\
+	(void) (&__a == __d);			\
+	*__d = (u64)__a - (u64)__b;		\
+	((((__a ^ __b)) & (*__d ^ __a))		\
+		& type_min(typeof(__a))) != 0;	\
+})
+
+/*
+ * Signed multiplication is rather hard. gcc always follows C99, so
+ * division is truncated towards 0. This means that we can write the
+ * overflow check like this:
+ *
+ * (a > 0 && (b > MAX/a || b < MIN/a)) ||
+ * (a < -1 && (b > MIN/a || b < MAX/a) ||
+ * (a == -1 && b == MIN)
+ *
+ * The redundant casts of -1 are to silence an annoying -Wtype-limits
+ * (included in -Wextra) warning: When the type is u8 or u16, the
+ * __b_c_e in check_mul_overflow obviously selects
+ * __unsigned_mul_overflow, but unfortunately gcc still parses this
+ * code and warns about the limited range of __b.
+ */
+
+#define __signed_mul_overflow(a, b, d) ({				\
+	typeof(a) __a = (a);						\
+	typeof(b) __b = (b);						\
+	typeof(d) __d = (d);						\
+	typeof(a) __tmax = type_max(typeof(a));				\
+	typeof(a) __tmin = type_min(typeof(a));				\
+	(void) (&__a == &__b);						\
+	(void) (&__a == __d);						\
+	*__d = (u64)__a * (u64)__b;					\
+	(__b > 0   && (__a > __tmax/__b || __a < __tmin/__b)) ||	\
+	(__b < (typeof(__b))-1  && (__a > __tmin/__b || __a < __tmax/__b)) || \
+	(__b == (typeof(__b))-1 && __a == __tmin);			\
+})
+
+
+#define check_add_overflow(a, b, d)					\
+	__builtin_choose_expr(_kc_is_signed_type(typeof(a)),		\
+			__signed_add_overflow(a, b, d),			\
+			__unsigned_add_overflow(a, b, d))
+
+#define check_sub_overflow(a, b, d)					\
+	__builtin_choose_expr(_kc_is_signed_type(typeof(a)),		\
+			__signed_sub_overflow(a, b, d),			\
+			__unsigned_sub_overflow(a, b, d))
+
+#define check_mul_overflow(a, b, d)					\
+	__builtin_choose_expr(_kc_is_signed_type(typeof(a)),		\
+			__signed_mul_overflow(a, b, d),			\
+			__unsigned_mul_overflow(a, b, d))
+
+
+#endif /* COMPILER_HAS_GENERIC_BUILTIN_OVERFLOW */
+
+/** check_shl_overflow() - Calculate a left-shifted value and check overflow
+ *
+ * @a: Value to be shifted
+ * @s: How many bits left to shift
+ * @d: Pointer to where to store the result
+ *
+ * Computes *@d = (@a << @s)
+ *
+ * Returns true if '*d' cannot hold the result or when 'a << s' doesn't
+ * make sense. Example conditions:
+ * - 'a << s' causes bits to be lost when stored in *d.
+ * - 's' is garbage (e.g. negative) or so large that the result of
+ *   'a << s' is guaranteed to be 0.
+ * - 'a' is negative.
+ * - 'a << s' sets the sign bit, if any, in '*d'.
+ *
+ * '*d' will hold the results of the attempted shift, but is not
+ * considered "safe for use" if false is returned.
+ */
+#define check_shl_overflow(a, s, d) ({					\
+	typeof(a) _a = a;						\
+	typeof(s) _s = s;						\
+	typeof(d) _d = d;						\
+	u64 _a_full = _a;						\
+	unsigned int _to_shift =					\
+		_s >= 0 && _s < 8 * sizeof(*d) ? _s : 0;		\
+	*_d = (_a_full << _to_shift);					\
+	(_to_shift != _s || *_d < 0 || _a < 0 ||			\
+		(*_d >> _to_shift) != _a);				\
+})
+
+/**
+ * array_size() - Calculate size of 2-dimensional array.
+ *
+ * @a: dimension one
+ * @b: dimension two
+ *
+ * Calculates size of 2-dimensional array: @a * @b.
+ *
+ * Returns: number of bytes needed to represent the array or SIZE_MAX on
+ * overflow.
+ */
+static inline __must_check size_t array_size(size_t a, size_t b)
+{
+	size_t bytes;
+
+	if (check_mul_overflow(a, b, &bytes))
+		return SIZE_MAX;
+
+	return bytes;
+}
+
+/**
+ * array3_size() - Calculate size of 3-dimensional array.
+ *
+ * @a: dimension one
+ * @b: dimension two
+ * @c: dimension three
+ *
+ * Calculates size of 3-dimensional array: @a * @b * @c.
+ *
+ * Returns: number of bytes needed to represent the array or SIZE_MAX on
+ * overflow.
+ */
+static inline __must_check size_t array3_size(size_t a, size_t b, size_t c)
+{
+	size_t bytes;
+
+	if (check_mul_overflow(a, b, &bytes))
+		return SIZE_MAX;
+	if (check_mul_overflow(bytes, c, &bytes))
+		return SIZE_MAX;
+
+	return bytes;
+}
+
+static inline __must_check size_t __ab_c_size(size_t n, size_t size, size_t c)
+{
+	size_t bytes;
+
+	if (check_mul_overflow(n, size, &bytes))
+		return SIZE_MAX;
+	if (check_add_overflow(bytes, c, &bytes))
+		return SIZE_MAX;
+
+	return bytes;
+}
+
+/**
+ * struct_size() - Calculate size of structure with trailing array.
+ * @p: Pointer to the structure.
+ * @member: Name of the array member.
+ * @n: Number of elements in the array.
+ *
+ * Calculates size of memory needed for structure @p followed by an
+ * array of @n @member elements.
+ *
+ * Return: number of bytes needed or SIZE_MAX on overflow.
+ */
+#define struct_size(p, member, n)					\
+	__ab_c_size(n,							\
+		    sizeof(*(p)->member) + __must_be_array((p)->member),\
+		    sizeof(*(p)))
+
+#endif /* __LINUX_OVERFLOW_H */
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/kcompat_rhel_defs.h linux-5.4.86.new/drivers/net/ethernet/intel/i40e/kcompat_rhel_defs.h
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/kcompat_rhel_defs.h	1969-12-31 19:00:00.000000000 -0500
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/kcompat_rhel_defs.h	2024-05-13 03:58:25.372491940 -0400
@@ -0,0 +1,94 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
+
+#ifndef _KCOMPAT_RHEL_DEFS_H_
+#define _KCOMPAT_RHEL_DEFS_H_
+
+/* This is the RedHat Enterprise Linux distribution specific definitions file.
+ * It defines what features need backports for a given version of the RHEL
+ * kernel.
+ *
+ * It checks the RHEL_RELEASE_CODE and RHEL_RELEASE_VERSION macros to decide
+ * what support the target kernel has.
+ *
+ * It assumes that kcompat_std_defs.h has already been processed, and will
+ * #define or #undef any flags that have changed based on backports done by
+ * RHEL.
+ */
+
+#if !RHEL_RELEASE_CODE
+#error "RHEL_RELEASE_CODE is 0 or undefined"
+#endif
+
+#ifndef RHEL_RELEASE_VERSION
+#error "RHEL_RELEASE_VERSION is undefined"
+#endif
+
+/*****************************************************************************/
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,3))
+#else /* >= 7.3 */
+#undef NEED_DEV_PRINTK_ONCE
+#endif /* 7.3 */
+
+/*****************************************************************************/
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,5))
+#else /* >= 7.5 */
+#define HAVE_TCF_EXTS_TO_LIST
+#endif /* 7.5 */
+
+/*****************************************************************************/
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,6))
+#else /* >= 7.6 */
+#undef NEED_TC_CLS_CAN_OFFLOAD_AND_CHAIN0
+#undef NEED_TC_SETUP_QDISC_MQPRIO
+#endif /* 7.6 */
+
+/*****************************************************************************/
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,7))
+#else /* >= 7.7 */
+#define HAVE_DEVLINK_PORT_ATTRS_SET_PORT_FLAVOUR
+#define HAVE_ETHTOOL_NEW_100G_BITS
+#endif /* 7.7 */
+
+/*****************************************************************************/
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(8,0))
+#else /* >= 8.0 */
+#undef HAVE_TCF_EXTS_TO_LIST
+#undef HAVE_ETHTOOL_NEW_100G_BITS
+#define HAVE_TCF_EXTS_FOR_EACH_ACTION
+#endif /* 7.5 */
+
+/*****************************************************************************/
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(8,1))
+#define NEED_IDA_ALLOC_MIN_MAX_RANGE_FREE
+#else /* >= 8.1 */
+#define HAVE_ETHTOOL_NEW_100G_BITS
+#undef NEED_IDA_ALLOC_MIN_MAX_RANGE_FREE
+#endif /* 8.1 */
+
+/*****************************************************************************/
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(8,2))
+#else /* >= 8.2 */
+#undef NEED_BUS_FIND_DEVICE_CONST_DATA
+#undef NEED_DEVLINK_FLASH_UPDATE_STATUS_NOTIFY
+#undef NEED_SKB_FRAG_OFF_ACCESSORS
+#undef NEED_FLOW_INDR_BLOCK_CB_REGISTER
+#define HAVE_DEVLINK_PORT_ATTRS_SET_SWITCH_ID
+#endif /* 8.2 */
+
+/*****************************************************************************/
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(8,3))
+#else /* >= 8.3 */
+#undef NEED_CPU_LATENCY_QOS_RENAME
+#endif /* 8.3 */
+
+/*****************************************************************************/
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(8,4))
+#else /* >= 8.4 */
+#undef NEED_DEVLINK_PORT_ATTRS_SET_STRUCT
+#undef NEED_NET_PREFETCH
+#undef NEED_DEVLINK_FLASH_UPDATE_TIMEOUT_NOTIFY
+#undef HAVE_XDP_QUERY_PROG
+#endif /* 8.4 */
+
+#endif /* _KCOMPAT_RHEL_DEFS_H_ */
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/kcompat_sles_defs.h linux-5.4.86.new/drivers/net/ethernet/intel/i40e/kcompat_sles_defs.h
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/kcompat_sles_defs.h	1969-12-31 19:00:00.000000000 -0500
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/kcompat_sles_defs.h	2024-05-13 03:58:25.372491940 -0400
@@ -0,0 +1,157 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
+
+#ifndef _KCOMPAT_SLES_DEFS_H_
+#define _KCOMPAT_SLES_DEFS_H_
+
+/* This is the SUSE Linux Enterprise distribution specific definitions file.
+ * It defines what features need backports for a given version of the SUSE
+ * Linux Enterprise kernel.
+ *
+ * It checks a combination of the LINUX_VERSION code and the
+ * SLE_LOCALVERSION_CODE to determine what support the kernel has.
+ *
+ * It assumes that kcompat_std_defs.h has already been processed, and will
+ * #define or #undef any flags that have changed based on backports done by
+ * SUSE.
+ */
+
+#ifndef LINUX_VERSION_CODE
+#error "LINUX_VERSION_CODE is undefined"
+#endif
+
+#ifndef KERNEL_VERSION
+#error "KERNEL_VERSION is undefined"
+#endif
+
+#if !SLE_KERNEL_REVISION
+#error "SLE_KERNEL_REVISION is 0 or undefined"
+#endif
+
+#if SLE_KERNEL_REVISION > 65535
+#error "SLE_KERNEL_REVISION is unexpectedly large"
+#endif
+
+/* SLE kernel versions are a combination of the LINUX_VERSION_CODE along with
+ * an extra digit that indicates the SUSE specific revision of that kernel.
+ * This value is found in the CONFIG_LOCALVERSION of the SUSE kernel, which is
+ * extracted by common.mk and placed into SLE_KERNEL_REVISION_CODE.
+ *
+ * We combine the value of SLE_KERNEL_REVISION along with the LINUX_VERSION code
+ * to generate the useful value that determines what specific kernel we're
+ * dealing with.
+ *
+ * Just in case the SLE_KERNEL_REVISION ever goes above 255, we reserve 16 bits
+ * instead of 8 for this value.
+ */
+#define SLE_KERNEL_CODE ((LINUX_VERSION_CODE << 16) + SLE_KERNEL_REVISION)
+#define SLE_KERNEL_VERSION(a,b,c,d) ((KERNEL_VERSION(a,b,c) << 16) + (d))
+
+/* Unlike RHEL, SUSE kernels are not always tied to a single service pack. For
+ * example, 4.12.14 was used as the base for SLE 15 SP1, SLE 12 SP4, and SLE 12
+ * SP5.
+ *
+ * You can find the patches that SUSE applied to the kernel tree at
+ * https://github.com/SUSE/kernel-source.
+ *
+ * You can find the correct kernel version for a check by using steps similar
+ * to the following
+ *
+ * 1) download the kernel-source repo
+ * 2) checkout the relevant branch, i.e SLE15-SP3
+ * 3) find the relevant backport you're interested in the patches.suse
+ *    directory
+ * 4) git log <patch file> to locate the commit that introduced the backport
+ * 5) git describe --contains to find the relevant tag that includes that
+ *    commit, i.e. rpm-5.3.18-37
+ * 6) those digits represent the SLE kernel that introduced that backport.
+ *
+ * Try to keep the checks in SLE_KERNEL_CODE order and condense where
+ * possible.
+ */
+
+/*****************************************************************************/
+#if (SLE_KERNEL_CODE > SLE_KERNEL_VERSION(4,12,14,23) && \
+     SLE_KERNEL_CODE < SLE_KERNEL_VERSION(4,12,14,94))
+/*
+ * 4.12.14 is used as the base for SLE 12 SP4, SLE 12 SP5, SLE 15, and SLE 15
+ * SP1. Unfortunately the revision codes do not line up cleanly. SLE 15
+ * launched with 4.12.14-23. It appears that SLE 12 SP4 and SLE 15 SP1 both
+ * diverged from this point, with SLE 12 SP4 kernels starting around
+ * 4.12.14-94. A few backports for SLE 15 SP1 landed in some alpha and beta
+ * kernels tagged between 4.12.14-25 up to 4.12.14-32. These changes did not
+ * make it into SLE 12 SP4. This was cleaned up with SLE 12 SP5 by an apparent
+ * merge in 4.12.14-111. The official launch of SLE 15 SP1 ended up with
+ * version 4.12.14-195.
+ *
+ * Because of this inconsistency and because all of these kernels appear to be
+ * alpha or beta kernel releases for SLE 15 SP1, we do not rely on version
+ * checks between this range. Issue a warning to indicate that we do not
+ * support these.
+ */
+#warning "SLE kernel versions between 4.12.14-23 and 4.12.14-94 are not supported"
+#endif
+
+/*****************************************************************************/
+#if (SLE_KERNEL_CODE < SLE_KERNEL_VERSION(4,12,14,100))
+#else /* >= 4.12.14-100 */
+#undef HAVE_TCF_EXTS_TO_LIST
+#define HAVE_TCF_EXTS_FOR_EACH_ACTION
+#endif /* 4.12.14-100 */
+
+/*****************************************************************************/
+#if (SLE_KERNEL_CODE < SLE_KERNEL_VERSION(4,12,14,111))
+#define NEED_IDA_ALLOC_MIN_MAX_RANGE_FREE
+#else /* >= 4.12.14-111 */
+#define HAVE_DEVLINK_PORT_ATTRS_SET_PORT_FLAVOUR
+#undef NEED_MACVLAN_ACCEL_PRIV
+#undef NEED_MACVLAN_RELEASE_L2FW_OFFLOAD
+#undef NEED_MACVLAN_SUPPORTS_DEST_FILTER
+#undef NEED_IDA_ALLOC_MIN_MAX_RANGE_FREE
+#endif /* 4.12.14-111 */
+
+/*****************************************************************************/
+#if (SLE_KERNEL_CODE < SLE_KERNEL_VERSION(4,12,14,120))
+#else /* >= 4.12.14-120 */
+#define HAVE_NDO_SELECT_QUEUE_SB_DEV
+#define HAVE_TCF_MIRRED_DEV
+#define HAVE_TCF_BLOCK
+#define HAVE_TC_CB_AND_SETUP_QDISC_MQPRIO
+#define HAVE_TCF_BLOCK_CB_REGISTER_EXTACK
+#undef NEED_TC_SETUP_QDISC_MQPRIO
+#undef NEED_TC_CLS_CAN_OFFLOAD_AND_CHAIN0
+#endif /* 4.12.14-120 */
+
+/*****************************************************************************/
+#if (SLE_KERNEL_CODE < SLE_KERNEL_VERSION(5,3,8,2))
+#else /* >= 5.3.8-2 */
+#undef NEED_BUS_FIND_DEVICE_CONST_DATA
+#undef NEED_FLOW_INDR_BLOCK_CB_REGISTER
+#undef NEED_SKB_FRAG_OFF_ACCESSORS
+#endif /* 5.3.8-2 */
+
+#if (SLE_KERNEL_CODE < SLE_KERNEL_VERSION(5,3,18,26))
+#else /* >= 5.3.18-26 */
+#undef NEED_CPU_LATENCY_QOS_RENAME
+#endif
+
+/*****************************************************************************/
+#if (SLE_KERNEL_CODE < SLE_KERNEL_VERSION(5,3,18,34))
+#else /* >= 5.3.18-34 */
+#undef NEED_DEVLINK_REGION_CREATE_OPS
+#undef NEED_DEVLINK_PORT_ATTRS_SET_STRUCT
+#endif /* 5.3.18-34 */
+
+/*****************************************************************************/
+#if (SLE_KERNEL_CODE < SLE_KERNEL_VERSION(5,3,18,37))
+#else /* >= 5.3.18-37 */
+#undef NEED_NET_PREFETCH
+#endif /* 5.3.18-37 */
+
+/*****************************************************************************/
+#if (SLE_KERNEL_CODE < SLE_KERNEL_VERSION(5,3,18,38))
+#else /* >= 5.3.18-38 */
+#undef NEED_DEVLINK_FLASH_UPDATE_TIMEOUT_NOTIFY
+#endif /* 5.3.18-38 */
+
+#endif /* _KCOMPAT_SLES_DEFS_H_ */
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/kcompat_std_defs.h linux-5.4.86.new/drivers/net/ethernet/intel/i40e/kcompat_std_defs.h
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/kcompat_std_defs.h	1969-12-31 19:00:00.000000000 -0500
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/kcompat_std_defs.h	2024-05-13 03:58:25.372491940 -0400
@@ -0,0 +1,161 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
+
+#ifndef _KCOMPAT_STD_DEFS_H_
+#define _KCOMPAT_STD_DEFS_H_
+
+/* This file contains the definitions for what kernel features need backports
+ * for a given kernel. It targets only the standard stable kernel releases.
+ * It must check only LINUX_VERSION_CODE and assume the kernel is a standard
+ * release, and not a custom distribution.
+ *
+ * It must define HAVE_<FLAG> and NEED_<FLAG> for features. It must not
+ * implement any backports, instead leaving the implementation to the
+ * kcompat_impl.h header.
+ *
+ * If a feature can be easily implemented as a replacement macro or fully
+ * backported, use a NEED_<FLAG> to indicate that the feature needs
+ * a backport. (If NEED_<FLAG> is undefined, then no backport for that feature
+ * is needed).
+ *
+ * If a feature cannot be easily implemented in kcompat directly, but
+ * requires drivers to make specific changes such as stripping out an entire
+ * feature or modifying a function pointer prototype, use a HAVE_<FLAG>.
+ */
+
+#ifndef LINUX_VERSION_CODE
+#error "LINUX_VERSION_CODE is undefined"
+#endif
+
+#ifndef KERNEL_VERSION
+#error "KERNEL_VERSION is undefined"
+#endif
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,18,0))
+#define NEED_DEV_PM_DOMAIN_ATTACH_DETACH
+#else /* >= 3,18,0 */
+#endif /* 3,18,0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,19,0))
+#define NEED_DEV_PRINTK_ONCE
+#else /* >= 3,19,0 */
+#endif /* 3,19,0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4,8,0))
+#else /* >= 4,8,0 */
+#define HAVE_TCF_EXTS_TO_LIST
+#endif /* 4,8,0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4,9,0))
+#else /* >= 4,9,0 */
+#define HAVE_KTHREAD_DELAYED_API
+#endif /* 4,9,0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4,15,0))
+#define NEED_TC_SETUP_QDISC_MQPRIO
+#else /* >= 4,15,0 */
+#define HAVE_TC_CB_AND_SETUP_QDISC_MQPRIO
+#endif /* 4,15,0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4,16,0))
+#define NEED_TC_CLS_CAN_OFFLOAD_AND_CHAIN0
+#else /* >= 4,16,0 */
+#endif /* 4,16,0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4,17,0))
+#define NEED_CONVERT_ART_NS_TO_TSC
+#else /* >= 4,17,0 */
+#endif /* 4,17,0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4,18,0))
+#define NEED_MACVLAN_ACCEL_PRIV
+#define NEED_MACVLAN_RELEASE_L2FW_OFFLOAD
+#define NEED_MACVLAN_SUPPORTS_DEST_FILTER
+#else /* >= 4,18,0 */
+#define HAVE_DEVLINK_PORT_ATTRS_SET_PORT_FLAVOUR
+#endif /* 4,18,0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4,19,0))
+#define NEED_IDA_ALLOC_MIN_MAX_RANGE_FREE
+#else /* >= 4,19,0 */
+#undef HAVE_TCF_EXTS_TO_LIST
+#define HAVE_TCF_EXTS_FOR_EACH_ACTION
+#endif /* 4,19,0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(5,1,0))
+#else /* >= 5.1.0 */
+#define HAVE_ETHTOOL_200G_BITS
+#define HAVE_ETHTOOL_NEW_100G_BITS
+#endif /* 5.1.0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(5,2,0))
+#else /* >= 5.2.0 */
+#define HAVE_DEVLINK_PORT_ATTRS_SET_SWITCH_ID
+#endif /* 5.2.0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(5,3,0))
+#define NEED_DEVLINK_FLASH_UPDATE_STATUS_NOTIFY
+#else /* >= 5.3.0 */
+#endif /* 5.3.0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(5,3,1))
+#define NEED_BUS_FIND_DEVICE_CONST_DATA
+#else /* >= 5.3.1 */
+#endif /* 5.3.1 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(5,4,0))
+#define NEED_SKB_FRAG_OFF_ACCESSORS
+#define NEED_FLOW_INDR_BLOCK_CB_REGISTER
+#else /* >= 5.4.0 */
+#define HAVE_XSK_UNALIGNED_CHUNK_PLACEMENT
+#endif /* 5.4.0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(5,7,0))
+#define NEED_DEVLINK_REGION_CREATE_OPS
+#define NEED_CPU_LATENCY_QOS_RENAME
+#else /* >= 5.7.0 */
+#endif /* 5.7.0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(5,8,0))
+#else /* >= 5.8.0 */
+#undef HAVE_XSK_UNALIGNED_CHUNK_PLACEMENT
+#endif /* 5.8.0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(5,9,0))
+#define NEED_DEVLINK_PORT_ATTRS_SET_STRUCT
+#define HAVE_XDP_QUERY_PROG
+#else /* >= 5.9.0 */
+#endif /* 5.9.0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(5,10,0))
+#define NEED_NET_PREFETCH
+#define NEED_DEVLINK_FLASH_UPDATE_TIMEOUT_NOTIFY
+#else /* >= 5.10.0 */
+#define HAVE_DEVLINK_RELOAD_ACTION_AND_LIMIT
+#endif /* 5.10.0 */
+
+/*****************************************************************************/
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(5,11,0))
+#else /* >= 5.11.0 */
+#define HAVE_XSK_BATCHED_DESCRIPTOR_INTERFACES
+#endif /* 5.11.0 */
+
+#endif /* _KCOMPAT_STD_DEFS_H_ */
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/kcompat_ubuntu_defs.h linux-5.4.86.new/drivers/net/ethernet/intel/i40e/kcompat_ubuntu_defs.h
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/kcompat_ubuntu_defs.h	1969-12-31 19:00:00.000000000 -0500
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/kcompat_ubuntu_defs.h	2024-05-13 03:58:25.372491940 -0400
@@ -0,0 +1,28 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
+
+#ifndef _KCOMPAT_UBUNTU_DEFS_H_
+#define _KCOMPAT_UBUNTU_DEFS_H_
+
+/* This file contains the definitions for the Ubuntu specific distribution of
+ * the Linux kernel.
+ *
+ * It checks the UBUNTU_VERSION_CODE to decide which features are available in
+ * the target kernel. It assumes that kcompat_std_defs.h has already been
+ * processed, and will #define or #undef the relevant flags based on what
+ * features were backported by Ubuntu.
+ */
+
+#if !UTS_UBUNTU_RELEASE_ABI
+#error "UTS_UBUNTU_RELEASE_ABI is 0 or undefined"
+#endif
+
+#if !UBUNTU_VERSION_CODE
+#error "UBUNTU_VERSION_CODE is 0 or undefined"
+#endif
+
+#ifndef UBUNTU_VERSION
+#error "UBUNTU_VERSION is undefined"
+#endif
+
+#endif /* _KCOMPAT_UBUNTU_DEFS_H_ */
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/kcompat_vfd.c linux-5.4.86.new/drivers/net/ethernet/intel/i40e/kcompat_vfd.c
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/kcompat_vfd.c	1969-12-31 19:00:00.000000000 -0500
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/kcompat_vfd.c	2024-05-13 03:58:25.372491940 -0400
@@ -0,0 +1,3239 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
+
+#include "kcompat.h"
+#include "kcompat_vfd.h"
+
+#define to_dev(obj) container_of(obj, struct device, kobj)
+
+const struct vfd_ops *vfd_ops = NULL;
+
+/**
+ * __get_pf_pdev - helper function to get the pdev
+ * @kobj:	kobject passed
+ * @pdev:	PCI device information struct
+ */
+static int __get_pf_pdev(struct kobject *kobj, struct pci_dev **pdev)
+{
+	struct device *dev;
+
+	if (!kobj->parent)
+		return -EINVAL;
+
+	/* get pdev */
+	dev = to_dev(kobj->parent);
+	*pdev = to_pci_dev(dev);
+
+	return 0;
+}
+
+/**
+ * __get_pdev_and_vfid - helper function to get the pdev and the vf id
+ * @kobj:	kobject passed
+ * @pdev:	PCI device information struct
+ * @vf_id:	VF id of the VF under consideration
+ */
+static int __get_pdev_and_vfid(struct kobject *kobj, struct pci_dev **pdev,
+			       int *vf_id)
+{
+	struct device *dev;
+
+	if (!kobj->parent->parent)
+		return -EINVAL;
+
+	/* get pdev */
+	dev = to_dev(kobj->parent->parent);
+	*pdev = to_pci_dev(dev);
+
+	/* get vf_id */
+	if (kstrtoint(kobj->name, 10, vf_id) != 0) {
+		dev_err(&(*pdev)->dev, "Failed to convert %s to vf_id\n",
+			kobj->name);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+/**
+ * __get_tc - helper function to get the pdev and the vf id
+ * @pdev:	PCI device information struct
+ * @tc_kobj:	kobject passed
+ * @tc:		number of extracted TC
+ */
+static int __get_tc(struct pci_dev *pdev, struct kobject *tc_kobj, int *tc)
+{
+	if (kstrtoint(tc_kobj->name, 10, tc) != 0) {
+		dev_err(&pdev->dev, "Failed to convert %s to tc\n",
+			tc_kobj->name);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+/**
+ * __get_vf_tc_pdev - helper function to get the pdev and the vf id
+ * @kobj:	kobject passed
+ * @pdev:	PCI device information struct
+ * @vf_id:	VF id of the VF under consideration
+ * @tc:		number of extracted TC
+ */
+static int __get_vf_tc_pdev(struct kobject *kobj, struct pci_dev **pdev,
+			    int *vf_id, int *tc)
+{
+	int ret;
+
+	if (!kobj->parent->parent)
+		return -EINVAL;
+
+	ret = __get_pdev_and_vfid(kobj->parent->parent, pdev, vf_id);
+	if (ret)
+		goto err;
+
+	ret = __get_tc(*pdev, kobj, tc);
+err:
+	return ret;
+}
+
+/**
+ * __get_pdev_tc - helper function to get the pdev and the vf id
+ * @kobj:	kobject passed
+ * @pdev:	PCI device information struct
+ * @tc:		number of extracted TC
+ */
+static int __get_pdev_tc(struct kobject *kobj, struct pci_dev **pdev, int *tc)
+{
+	int ret;
+
+	/* check for pci_dev kobject */
+	if (!kobj->parent->parent->parent)
+		return -EINVAL;
+
+	ret = __get_pf_pdev(kobj->parent->parent, pdev);
+	if (ret)
+		goto err;
+
+	ret = __get_tc(*pdev, kobj, tc);
+err:
+	return ret;
+}
+
+/**
+ * __parse_bool_data - helper function to parse boolean data
+ * @pdev:	PCI device information struct
+ * @buff:	buffer with input data
+ * @attr_name:	name of the attribute
+ * @data:	pointer to output data
+ */
+static int __parse_bool_data(struct pci_dev *pdev, const char *buff,
+			     const char *attr_name, bool *data)
+{
+	if (sysfs_streq("on", buff)) {
+		*data = true;
+	} else if (sysfs_streq("off", buff)) {
+		*data = false;
+	} else {
+		dev_err(&pdev->dev, "set %s: invalid input string", attr_name);
+		return -EINVAL;
+	}
+	return 0;
+}
+
+/**
+ * __parse_egress_ingress_input - helper function for ingress/egress_mirror attributes
+ * @pdev:	PCI device information struct
+ * @buff:	buffer with input data
+ * @attr_name:	name of the attribute
+ * @data_new:	pointer to input data merged with the old data
+ * @data_old:	pointer to old data of the attribute
+ *
+ * Get the input data for egress_mirror or ingress_mirror attribute in the form
+ * "add <number>" or "rem <number>".
+ * Set the output data to off if in "rem <number>", <number> matches old data.
+ *
+ */
+static int __parse_egress_ingress_input(struct pci_dev *pdev, const char *buff,
+					const char *attr_name, int *data_new,
+					int *data_old)
+{
+	int ret = 0;
+	char *p;
+
+	if (strstr(buff, "add")) {
+		p = strstr(buff, "add");
+
+		ret = kstrtoint(p + sizeof("add"), 10, data_new);
+		if (ret) {
+			dev_err(&pdev->dev,
+				"add %s: input error %d\n", attr_name, ret);
+			return ret;
+		}
+	} else if (strstr(buff, "rem")) {
+		p = strstr(buff, "rem");
+
+		ret = kstrtoint(p + sizeof("rem"), 10, data_new);
+		if (ret) {
+			dev_err(&pdev->dev,
+				"rem %s: input error %d\n", attr_name, ret);
+			return ret;
+		}
+
+		if (*data_new == *data_old) {
+			if (!strcmp(attr_name, "egress_mirror"))
+				*data_new = VFD_EGRESS_MIRROR_OFF;
+			else if (!strcmp(attr_name, "ingress_mirror"))
+				*data_new = VFD_INGRESS_MIRROR_OFF;
+		} else {
+			dev_err(&pdev->dev,
+				"rem %s: input doesn't match current value",
+				attr_name);
+			return -EINVAL;
+		}
+	} else {
+		dev_err(&pdev->dev, "set %s: invalid input string", attr_name);
+		return -EINVAL;
+	}
+
+	return ret;
+}
+
+/**
+ * __parse_add_rem_bitmap - helper function to parse bitmap data
+ * @pdev:	PCI device information struct
+ * @buff:	buffer with input data
+ * @attr_name:	name of the attribute
+ * @data_new:	pointer to input data merged with the old data
+ * @data_old:	pointer to old data of the attribute
+ *
+ * If passed add: set data_new to "data_old || data_input"
+ * If passed rem: set data_new to "data_old || ~data_input"
+ */
+static int __parse_add_rem_bitmap(struct pci_dev *pdev, const char *buff,
+				  const char *attr_name,
+				  unsigned long *data_new,
+				  unsigned long *data_old)
+{
+	int ret = 0;
+	char *p;
+
+	if (strstr(buff, "add")) {
+		p = strstr(buff, "add");
+		bitmap_zero(data_new, VLAN_N_VID);
+
+		ret = bitmap_parselist(p + sizeof("add"), data_new, VLAN_N_VID);
+		if (ret) {
+			dev_err(&pdev->dev,
+				"add %s: input error %d\n", attr_name, ret);
+			return ret;
+		}
+
+		bitmap_or(data_new, data_new, data_old, VLAN_N_VID);
+	} else if (strstr(buff, "rem")) {
+		p = strstr(buff, "rem");
+		bitmap_zero(data_new, VLAN_N_VID);
+
+		ret = bitmap_parselist(p + sizeof("rem"), data_new, VLAN_N_VID);
+		if (ret) {
+			dev_err(&pdev->dev,
+				"rem %s: input error %d\n", attr_name, ret);
+			return ret;
+		}
+
+		/* new = old & ~rem */
+		bitmap_andnot(data_new, data_old, data_new, VLAN_N_VID);
+	} else {
+		dev_err(&pdev->dev, "set %s: invalid input string", attr_name);
+		return -EINVAL;
+	}
+	return 0;
+}
+
+/**
+ * __parse_promisc_input - helper function for promisc attributes
+ * @buff:	buffer with input data
+ * @count:	size of buff
+ * @cmd:	return pointer to cmd into buff
+ * @subcmd:	return pointer to subcmd into buff
+ *
+ * Get the input data for promisc attributes in the form "add/rem mcast/ucast".
+ */
+static int __parse_promisc_input(const char *buff, size_t count,
+				 const char **cmd, const char **subcmd)
+{
+	size_t idx = 0;
+
+	/* Remove start spaces */
+	while (buff[idx] == ' ' && idx < count)
+		idx++;
+
+	/* Parse cmd */
+	if (strncmp(&buff[idx], "add", strlen("add")) == 0) {
+		*cmd = &buff[idx];
+		idx += strlen("add");
+	} else if (strncmp(&buff[idx], "rem", strlen("rem")) == 0) {
+		*cmd = &buff[idx];
+		idx += strlen("rem");
+	} else {
+		return -EINVAL;
+	}
+
+	if (buff[idx++] != ' ')
+		return -EINVAL;
+
+	/* Remove spaces between cmd */
+	while (buff[idx] == ' ' && idx < count)
+		idx++;
+
+	/* Parse subcmd */
+	if (strncmp(&buff[idx], "ucast", strlen("ucast")) == 0) {
+		*subcmd = &buff[idx];
+		idx += strlen("ucast");
+	} else if (strncmp(&buff[idx], "mcast", strlen("mcast")) == 0) {
+		*subcmd = &buff[idx];
+		idx += strlen("mcast");
+	} else {
+		return -EINVAL;
+	}
+
+	/* Remove spaces after subcmd */
+	while ((buff[idx] == ' ' || buff[idx] == '\n') && idx < count)
+		idx++;
+
+	if (idx != count)
+		return -EINVAL;
+
+	return 0;
+}
+
+/* Handlers for each VFd operation */
+
+/**
+ * vfd_trunk_show - handler for trunk show function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer with input data
+ *
+ * Get current data from driver and copy to buffer
+ **/
+static ssize_t vfd_trunk_show(struct kobject *kobj,
+			      struct kobj_attribute *attr, char *buff)
+{
+	struct pci_dev *pdev;
+	int vf_id, ret = 0;
+
+	DECLARE_BITMAP(data, VLAN_N_VID);
+	bitmap_zero(data, VLAN_N_VID);
+
+	if (!vfd_ops->get_trunk)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_trunk(pdev, vf_id, data);
+	if (ret)
+		ret = bitmap_print_to_pagebuf(1, buff, data, VLAN_N_VID);
+
+	return ret;
+}
+
+/**
+ * vfd_trunk_store - handler for trunk store function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer with input data
+ * @count:	size of buff
+ *
+ * Get current data from driver, compose new data based on input values
+ * depending on "add" or "rem" command, and pass new data to the driver to set.
+ *
+ * On success return count, indicating that we used the whole buffer. On
+ * failure return a negative error condition.
+ **/
+static ssize_t vfd_trunk_store(struct kobject *kobj,
+			       struct kobj_attribute *attr,
+			       const char *buff, size_t count)
+{
+	unsigned long *data_old, *data_new;
+	struct pci_dev *pdev;
+	int vf_id, ret = 0;
+
+	if (!vfd_ops->set_trunk || !vfd_ops->get_trunk)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	data_old = kcalloc(BITS_TO_LONGS(VLAN_N_VID), sizeof(unsigned long),
+			   GFP_KERNEL);
+	if (!data_old)
+		return -ENOMEM;
+	data_new = kcalloc(BITS_TO_LONGS(VLAN_N_VID), sizeof(unsigned long),
+			   GFP_KERNEL);
+	if (!data_new) {
+		kfree(data_old);
+		return -ENOMEM;
+	}
+
+	ret = vfd_ops->get_trunk(pdev, vf_id, data_old);
+	if (ret < 0)
+		goto err_free;
+
+	ret = __parse_add_rem_bitmap(pdev, buff, "trunk", data_new, data_old);
+	if (ret)
+		goto err_free;
+
+	if (!bitmap_equal(data_new, data_old, VLAN_N_VID))
+		ret = vfd_ops->set_trunk(pdev, vf_id, data_new);
+
+err_free:
+	kfree(data_old);
+	kfree(data_new);
+	return ret ? ret : count;
+}
+
+/**
+ * vfd_vlan_mirror_show - handler for vlan_mirror show function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer with input data
+ *
+ * Get current data from driver and copy to buffer
+ **/
+static ssize_t vfd_vlan_mirror_show(struct kobject *kobj,
+				    struct kobj_attribute *attr, char *buff)
+{
+	struct pci_dev *pdev;
+	int vf_id, ret = 0;
+
+	DECLARE_BITMAP(data, VLAN_N_VID);
+	bitmap_zero(data, VLAN_N_VID);
+
+	if (!vfd_ops->get_vlan_mirror)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_vlan_mirror(pdev, vf_id, data);
+	if (ret)
+		ret = bitmap_print_to_pagebuf(1, buff, data, VLAN_N_VID);
+
+	return ret;
+}
+
+/**
+ * vfd_vlan_mirror_store - handler for vlan_mirror store function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer with input data
+ * @count:	size of buff
+ *
+ * Get current data from driver, compose new data based on input values
+ * depending on "add" or "rem" command, and pass new data to the driver to set.
+ *
+ * On success return count, indicating that we used the whole buffer. On
+ * failure return a negative error condition.
+ **/
+static ssize_t vfd_vlan_mirror_store(struct kobject *kobj,
+				     struct kobj_attribute *attr,
+				     const char *buff, size_t count)
+{
+	unsigned long *data_old, *data_new;
+	struct pci_dev *pdev;
+	int vf_id, ret = 0;
+
+	if (!vfd_ops->set_vlan_mirror || !vfd_ops->get_vlan_mirror)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	data_old = kcalloc(BITS_TO_LONGS(VLAN_N_VID), sizeof(unsigned long),
+			   GFP_KERNEL);
+	if (!data_old)
+		return -ENOMEM;
+	data_new = kcalloc(BITS_TO_LONGS(VLAN_N_VID), sizeof(unsigned long),
+			   GFP_KERNEL);
+	if (!data_new) {
+		kfree(data_old);
+		return -ENOMEM;
+	}
+
+	ret = vfd_ops->get_vlan_mirror(pdev, vf_id, data_old);
+	if (ret < 0)
+		goto err_free;
+
+	ret = __parse_add_rem_bitmap(pdev, buff, "vlan_mirror",
+				     data_new, data_old);
+	if (ret)
+		goto err_free;
+
+	if (!bitmap_equal(data_new, data_old, VLAN_N_VID))
+		ret = vfd_ops->set_vlan_mirror(pdev, vf_id, data_new);
+
+err_free:
+	kfree(data_old);
+	kfree(data_new);
+	return ret ? ret : count;
+}
+
+/**
+ * vfd_egress_mirror_show - handler for egress_mirror show function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer for data
+ **/
+static ssize_t vfd_egress_mirror_show(struct kobject *kobj,
+				      struct kobj_attribute *attr, char *buff)
+{
+	struct pci_dev *pdev;
+	int vf_id, ret = 0;
+	int data;
+
+	if (!vfd_ops->get_egress_mirror)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_egress_mirror(pdev, vf_id, &data);
+	if (ret < 0)
+		return ret;
+
+	if (data == VFD_EGRESS_MIRROR_OFF)
+		ret = scnprintf(buff, PAGE_SIZE, "off\n");
+	else
+		ret = scnprintf(buff, PAGE_SIZE, "%u\n", data);
+
+	return ret;
+}
+
+/**
+ * vfd_egress_mirror_store - handler for egress_mirror store function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer with input data
+ * @count:	size of buff
+ **/
+static ssize_t vfd_egress_mirror_store(struct kobject *kobj,
+				       struct kobj_attribute *attr,
+				       const char *buff, size_t count)
+{
+	int data_new, data_old;
+	struct pci_dev *pdev;
+	int vf_id, ret = 0;
+
+	if (!vfd_ops->set_egress_mirror || !vfd_ops->get_egress_mirror)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_egress_mirror(pdev, vf_id, &data_old);
+	if (ret < 0)
+		return ret;
+
+	ret = __parse_egress_ingress_input(pdev, buff, "egress_mirror",
+					   &data_new, &data_old);
+	if (ret)
+		return ret;
+	if(data_new == vf_id) {
+		dev_err(&pdev->dev, "VF %d: Setting egress_mirror to itself is not allowed\n", vf_id);
+		return -EINVAL;
+	}
+
+	if (data_new != data_old)
+		ret = vfd_ops->set_egress_mirror(pdev, vf_id, data_new);
+
+	return ret ? ret : count;
+}
+
+/**
+ * vfd_ingress_mirror_show - handler for ingress_mirror show function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer for data
+ **/
+static ssize_t vfd_ingress_mirror_show(struct kobject *kobj,
+				       struct kobj_attribute *attr,
+				       char *buff)
+{
+	struct pci_dev *pdev;
+	int vf_id, ret = 0;
+	int data;
+
+	if (!vfd_ops->get_ingress_mirror)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_ingress_mirror(pdev, vf_id, &data);
+	if (ret < 0)
+		return ret;
+
+	if (data == VFD_INGRESS_MIRROR_OFF)
+		ret = scnprintf(buff, PAGE_SIZE, "off\n");
+	else
+		ret = scnprintf(buff, PAGE_SIZE, "%u\n", data);
+
+	return ret;
+}
+
+/**
+ * vfd_ingress_mirror_store - handler for ingress_mirror store function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer with input data
+ * @count:	size of buff
+ **/
+static ssize_t vfd_ingress_mirror_store(struct kobject *kobj,
+					struct kobj_attribute *attr,
+					const char *buff, size_t count)
+{
+	int data_new, data_old;
+	struct pci_dev *pdev;
+	int vf_id, ret = 0;
+
+	if (!vfd_ops->set_ingress_mirror || !vfd_ops->get_ingress_mirror)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_ingress_mirror(pdev, vf_id, &data_old);
+	if (ret < 0)
+		return ret;
+
+	ret = __parse_egress_ingress_input(pdev, buff, "ingress_mirror",
+					   &data_new, &data_old);
+	if (ret)
+		return ret;
+	if(data_new == vf_id) {
+		dev_err(&pdev->dev, "VF %d: Setting ingress_mirror to itself is not allowed\n", vf_id);
+		return -EINVAL;
+	}
+
+	if (data_new != data_old)
+		ret = vfd_ops->set_ingress_mirror(pdev, vf_id, data_new);
+
+	return ret ? ret : count;
+}
+
+/**
+ * vfd_mac_anti_spoof_show - handler for mac_anti_spoof show function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer for data
+ **/
+static ssize_t vfd_mac_anti_spoof_show(struct kobject *kobj,
+				       struct kobj_attribute *attr,
+				       char *buff)
+{
+	struct pci_dev *pdev;
+	int vf_id, ret;
+	bool data;
+
+	if (!vfd_ops->get_mac_anti_spoof)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_mac_anti_spoof(pdev, vf_id, &data);
+	if (ret < 0)
+		return ret;
+
+	if (data)
+		ret = scnprintf(buff, PAGE_SIZE, "on\n");
+	else
+		ret = scnprintf(buff, PAGE_SIZE, "off\n");
+
+	return ret;
+}
+
+/**
+ * vfd_mac_anti_spoof_store - handler for mac_anti_spoof store function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer with input data
+ * @count:	size of buff
+ *
+ * On success return count, indicating that we used the whole buffer. On
+ * failure return a negative error condition.
+ **/
+static ssize_t vfd_mac_anti_spoof_store(struct kobject *kobj,
+					struct kobj_attribute *attr,
+					const char *buff, size_t count)
+{
+	bool data_new, data_old;
+	struct pci_dev *pdev;
+	int vf_id, ret;
+
+	if (!vfd_ops->set_mac_anti_spoof || !vfd_ops->get_mac_anti_spoof)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_mac_anti_spoof(pdev, vf_id, &data_old);
+	if (ret < 0)
+		return ret;
+
+	ret = __parse_bool_data(pdev, buff, "mac_anti_spoof", &data_new);
+	if (ret)
+		return ret;
+
+	if (data_new != data_old)
+		ret = vfd_ops->set_mac_anti_spoof(pdev, vf_id, data_new);
+
+	return ret ? ret : count;
+}
+
+/**
+ * vfd_vlan_anti_spoof_show - handler for vlan_anti_spoof show function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer for data
+ **/
+static ssize_t vfd_vlan_anti_spoof_show(struct kobject *kobj,
+					struct kobj_attribute *attr,
+					char *buff)
+{
+	struct pci_dev *pdev;
+	int vf_id, ret;
+	bool data;
+
+	if (!vfd_ops->get_vlan_anti_spoof)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_vlan_anti_spoof(pdev, vf_id, &data);
+	if (ret < 0)
+		return ret;
+
+	if (data)
+		ret = scnprintf(buff, PAGE_SIZE, "on\n");
+	else
+		ret = scnprintf(buff, PAGE_SIZE, "off\n");
+
+	return ret;
+}
+
+/**
+ * vfd_vlan_anti_spoof_store - handler for vlan_anti_spoof store function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer with input data
+ * @count:	size of buff
+ *
+ * On success return count, indicating that we used the whole buffer. On
+ * failure return a negative error condition.
+ **/
+static ssize_t vfd_vlan_anti_spoof_store(struct kobject *kobj,
+					 struct kobj_attribute *attr,
+					 const char *buff, size_t count)
+{
+	bool data_new, data_old;
+	struct pci_dev *pdev;
+	int vf_id, ret;
+
+	if (!vfd_ops->set_vlan_anti_spoof || !vfd_ops->get_vlan_anti_spoof)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_vlan_anti_spoof(pdev, vf_id, &data_old);
+	if (ret < 0)
+		return ret;
+
+	ret = __parse_bool_data(pdev, buff, "vlan_anti_spoof", &data_new);
+	if (ret)
+		return ret;
+
+	if (data_new != data_old)
+		ret = vfd_ops->set_vlan_anti_spoof(pdev, vf_id, data_new);
+
+	return ret ? ret : count;
+}
+
+/**
+ * vfd_allow_untagged_show - handler for allow_untagged show function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer for data
+ **/
+static ssize_t vfd_allow_untagged_show(struct kobject *kobj,
+				       struct kobj_attribute *attr,
+				       char *buff)
+{
+	struct pci_dev *pdev;
+	int vf_id, ret = 0;
+	bool data;
+
+	if (!vfd_ops->get_allow_untagged)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_allow_untagged(pdev, vf_id, &data);
+	if (ret < 0)
+		return ret;
+
+	if (data)
+		ret = scnprintf(buff, PAGE_SIZE, "on\n");
+	else
+		ret = scnprintf(buff, PAGE_SIZE, "off\n");
+
+	return ret;
+}
+
+/**
+ * vfd_allow_untagged_store - handler for allow_untagged store function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer with input data
+ * @count:	size of buff
+ **/
+static ssize_t vfd_allow_untagged_store(struct kobject *kobj,
+				  struct kobj_attribute *attr,
+				  const char *buff, size_t count)
+{
+	struct pci_dev *pdev;
+	int vf_id, ret = 0;
+	bool data_new, data_old;
+
+	if (!vfd_ops->set_allow_untagged || !vfd_ops->get_allow_untagged)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_allow_untagged(pdev, vf_id, &data_old);
+	if (ret < 0)
+		return ret;
+
+	ret = __parse_bool_data(pdev, buff, "allow_untagged", &data_new);
+	if (ret)
+		return ret;
+
+	if (data_new != data_old)
+		ret = vfd_ops->set_allow_untagged(pdev, vf_id, data_new);
+
+	return ret ? ret : count;
+}
+
+/**
+ * vfd_loopback_show - handler for loopback show function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer for data
+ **/
+static ssize_t vfd_loopback_show(struct kobject *kobj,
+				 struct kobj_attribute *attr, char *buff)
+{
+	struct pci_dev *pdev;
+	int vf_id, ret = 0;
+	bool data;
+
+	if (!vfd_ops->get_loopback)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_loopback(pdev, vf_id, &data);
+	if (ret < 0)
+		return ret;
+
+	if (data)
+		ret = scnprintf(buff, PAGE_SIZE, "on\n");
+	else
+		ret = scnprintf(buff, PAGE_SIZE, "off\n");
+
+	return ret;
+}
+
+/**
+ * vfd_loopback_store - handler for loopback store function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer with input data
+ * @count:	size of buff
+ **/
+static ssize_t vfd_loopback_store(struct kobject *kobj,
+				  struct kobj_attribute *attr,
+				  const char *buff, size_t count)
+{
+	struct pci_dev *pdev;
+	int vf_id, ret = 0;
+	bool data_new, data_old;
+
+	if (!vfd_ops->set_loopback || !vfd_ops->get_loopback)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_loopback(pdev, vf_id, &data_old);
+	if (ret < 0)
+		return ret;
+
+	ret = __parse_bool_data(pdev, buff, "loopback", &data_new);
+	if (ret)
+		return ret;
+
+	if (data_new != data_old)
+		ret = vfd_ops->set_loopback(pdev, vf_id, data_new);
+
+	return ret ? ret : count;
+}
+
+/**
+ * vfd_mac_show - handler for mac show function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer for data
+ **/
+static ssize_t vfd_mac_show(struct kobject *kobj, struct kobj_attribute *attr,
+			    char *buff)
+{
+	u8 macaddr[ETH_ALEN];
+	struct pci_dev *pdev;
+	int vf_id, ret = 0;
+
+	if (!vfd_ops->get_mac)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_mac(pdev, vf_id, macaddr);
+	if (ret < 0)
+		return ret;
+
+	ret = scnprintf(buff, PAGE_SIZE, "%pM\n", macaddr);
+
+	return ret;
+}
+
+/**
+ * vfd_mac_store - handler for mac store function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer with input data
+ * @count:	size of buff
+ **/
+static ssize_t vfd_mac_store(struct kobject *kobj,
+			     struct kobj_attribute *attr,
+			     const char *buff, size_t count)
+{
+	u8 macaddr[ETH_ALEN];
+	u8 macaddr_old[ETH_ALEN];
+	struct pci_dev *pdev;
+	int vf_id, ret = 0;
+
+	if (!vfd_ops->set_mac || !vfd_ops->get_mac)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_mac(pdev, vf_id, macaddr_old);
+	if (ret < 0)
+		return ret;
+
+	ret = sscanf(buff, "%hhx:%hhx:%hhx:%hhx:%hhx:%hhx",
+		     &macaddr[0], &macaddr[1], &macaddr[2],
+		     &macaddr[3], &macaddr[4], &macaddr[5]);
+
+	if (ret != 6)
+		return -EINVAL;
+
+	if (!ether_addr_equal(macaddr, macaddr_old))
+		ret = vfd_ops->set_mac(pdev, vf_id, macaddr);
+
+	return ret ? ret : count;
+}
+
+/**
+ * vfd_mac_list_show - handler for mac_list show function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer for data
+ *
+ * This function also frees the memory allocated for mac_list in another function.
+ *
+ **/
+static ssize_t vfd_mac_list_show(struct kobject *kobj,
+				 struct kobj_attribute *attr, char *buff)
+{
+	unsigned int mac_num_allowed, mac_num_list, mac_num_count;
+	const char *overflow_msg = "... and more\n";
+	unsigned int mac_msg_len = 3*ETH_ALEN;
+	struct list_head *pos, *n;
+	struct pci_dev *pdev;
+	int vf_id, ret;
+	char *written;
+	LIST_HEAD(mac_list);
+
+	if (!vfd_ops->get_mac_list)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_mac_list(pdev, vf_id, &mac_list);
+	if (ret < 0)
+		goto err_free;
+
+	mac_num_list = 0;
+	mac_num_count = 0;
+	list_for_each_safe(pos, n, &mac_list)
+		mac_num_list++;
+
+	mac_num_allowed = (PAGE_SIZE - 1) / mac_msg_len;
+	if (mac_num_list > mac_num_allowed)
+		mac_num_allowed = (PAGE_SIZE - 1 - strlen(overflow_msg)) /
+				   mac_msg_len;
+
+	written = buff;
+	list_for_each_safe(pos, n, &mac_list) {
+		struct vfd_macaddr *mac = NULL;
+
+		mac_num_count++;
+		mac = list_entry(pos, struct vfd_macaddr, list);
+		if (mac_num_count > mac_num_allowed) {
+			ret += scnprintf(written, PAGE_SIZE - ret,
+					 "%s", overflow_msg);
+			goto err_free;
+		} else if (list_is_last(pos, &mac_list)) {
+			ret += scnprintf(written, PAGE_SIZE - ret,
+					 "%pM\n", mac->mac);
+		} else {
+			ret += scnprintf(written, PAGE_SIZE - ret,
+					 "%pM,", mac->mac);
+		}
+		written += mac_msg_len;
+	}
+
+err_free:
+	list_for_each_safe(pos, n, &mac_list) {
+		struct vfd_macaddr *mac = NULL;
+
+		mac = list_entry(pos, struct vfd_macaddr, list);
+		list_del(pos);
+		kfree(mac);
+	}
+	return ret;
+}
+
+/**
+ * vfd_mac_list_store - handler for mac_list store function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer with input data
+ * @count:	size of buff
+ *
+ * Get input mac list into the linked list and depending on "add" or "rem" command
+ * pass the input mac list to the driver to either add or remove macs to the list.
+ *
+ * This function also frees the memory allocated for mac_list in another function.
+ *
+ **/
+static ssize_t vfd_mac_list_store(struct kobject *kobj,
+				  struct kobj_attribute *attr,
+				  const char *buff, size_t count)
+{
+	struct list_head *pos, *n;
+	struct pci_dev *pdev;
+	u8 macaddr[ETH_ALEN];
+	int vf_id, ret;
+	size_t shift;
+	bool add;
+	LIST_HEAD(mac_list_inp);
+
+	if (!vfd_ops->add_macs_to_list || !vfd_ops->rem_macs_from_list)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	if (strstr(buff, "add")) {
+		shift = sizeof("add");
+		add = true;
+	} else if (strstr(buff, "rem")) {
+		shift = sizeof("rem");
+		add = false;
+	} else {
+		dev_err(&pdev->dev, "Invalid input string");
+		ret = -EINVAL;
+		goto err_free;
+	}
+
+	/* Get input data */
+	for (;;) {
+		struct vfd_macaddr *mac_new;
+
+		if (*(buff + shift) == ' ' || *(buff + shift) == ',') {
+			shift++;
+			continue;
+		}
+
+		ret = sscanf(buff + shift,
+			     "%hhx:%hhx:%hhx:%hhx:%hhx:%hhx",
+			     &macaddr[0], &macaddr[1], &macaddr[2],
+			     &macaddr[3], &macaddr[4], &macaddr[5]);
+
+		if (ret != 6)
+			break;
+
+		if (!is_valid_ether_addr(macaddr)) {
+			shift += 3*ETH_ALEN;
+			continue;
+		}
+
+		mac_new = kmalloc(sizeof(struct vfd_macaddr), GFP_KERNEL);
+		if (!mac_new) {
+			ret = -ENOMEM;
+			goto err_free;
+		}
+
+		ether_addr_copy(mac_new->mac, macaddr);
+		list_add(&mac_new->list, &mac_list_inp);
+
+		shift += 3*ETH_ALEN;
+	}
+
+	if (add)
+		ret = vfd_ops->add_macs_to_list(pdev, vf_id, &mac_list_inp);
+	else
+		ret = vfd_ops->rem_macs_from_list(pdev, vf_id, &mac_list_inp);
+
+err_free:
+        list_for_each_safe(pos, n, &mac_list_inp) {
+                struct vfd_macaddr *mac = NULL;
+
+                mac = list_entry(pos, struct vfd_macaddr, list);
+                list_del(pos);
+                kfree(mac);
+        }
+	return ret ? ret : count;
+}
+
+/**
+ * vfd_promisc_show - handler for promisc show function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer for data
+ **/
+static ssize_t vfd_promisc_show(struct kobject *kobj,
+				struct kobj_attribute *attr, char *buff)
+{
+	struct pci_dev *pdev;
+	int vf_id, ret = 0;
+	u8 data;
+
+	if (!vfd_ops->get_promisc)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_promisc(pdev, vf_id, &data);
+	if (ret < 0)
+		return ret;
+
+	if (data == VFD_PROMISC_UNICAST)
+		ret = scnprintf(buff, PAGE_SIZE, "ucast\n");
+	else if (data == VFD_PROMISC_MULTICAST)
+		ret = scnprintf(buff, PAGE_SIZE, "mcast\n");
+	else if (data == (VFD_PROMISC_UNICAST | VFD_PROMISC_MULTICAST))
+		ret = scnprintf(buff, PAGE_SIZE, "ucast, mcast\n");
+	else if (data == VFD_PROMISC_OFF)
+		ret = scnprintf(buff, PAGE_SIZE, "off\n");
+
+	return ret;
+}
+
+/**
+ * vfd_promisc_store - handler for promisc store function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer with input data
+ * @count:	size of buff
+ **/
+static ssize_t vfd_promisc_store(struct kobject *kobj,
+				 struct kobj_attribute *attr,
+				 const char *buff, size_t count)
+{
+	u8 data_new, data_old;
+	struct pci_dev *pdev;
+	const char *subcmd;
+	const char *cmd;
+	int vf_id, ret;
+
+	if (!vfd_ops->get_promisc || !vfd_ops->set_promisc)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_promisc(pdev, vf_id, &data_old);
+	if (ret < 0)
+		return ret;
+
+	ret = __parse_promisc_input(buff, count, &cmd, &subcmd);
+	if (ret)
+		goto promisc_err;
+
+	if (strncmp(cmd, "add", strlen("add")) == 0) {
+		if (strncmp(subcmd, "ucast", strlen("ucast")) == 0)
+			data_new = data_old | VFD_PROMISC_UNICAST;
+		else if (strncmp(subcmd, "mcast", strlen("mcast")) == 0)
+			data_new = data_old | VFD_PROMISC_MULTICAST;
+		else
+			goto promisc_err;
+	} else if (strncmp(cmd, "rem", strlen("rem")) == 0) {
+		if (strncmp(subcmd, "ucast", strlen("ucast")) == 0)
+			data_new = data_old & ~VFD_PROMISC_UNICAST;
+		else if (strncmp(subcmd, "mcast", strlen("mcast")) == 0)
+			data_new = data_old & ~VFD_PROMISC_MULTICAST;
+		else
+			goto promisc_err;
+	} else {
+		goto promisc_err;
+	}
+
+	if (data_new != data_old)
+		ret = vfd_ops->set_promisc(pdev, vf_id, data_new);
+
+	return ret ? ret : count;
+
+promisc_err:
+	dev_err(&pdev->dev, "Invalid input string");
+	return -EINVAL;
+}
+
+/**
+ * vfd_vlan_strip_show - handler for vlan_strip show function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer for data
+ **/
+static ssize_t vfd_vlan_strip_show(struct kobject *kobj,
+				   struct kobj_attribute *attr, char *buff)
+{
+	struct pci_dev *pdev;
+	int vf_id, ret = 0;
+	bool data;
+
+	if (!vfd_ops->get_vlan_strip)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_vlan_strip(pdev, vf_id, &data);
+	if (ret < 0)
+		return ret;
+
+	if (data)
+		ret = scnprintf(buff, PAGE_SIZE, "on\n");
+	else
+		ret = scnprintf(buff, PAGE_SIZE, "off\n");
+
+	return ret;
+}
+
+/**
+ * vfd_vlan_strip_store - handler for vlan_strip store function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer with input data
+ * @count:	size of buff
+ **/
+static ssize_t vfd_vlan_strip_store(struct kobject *kobj,
+				    struct kobj_attribute *attr,
+				    const char *buff, size_t count)
+{
+	struct pci_dev *pdev;
+	int vf_id, ret = 0;
+	bool data_new, data_old;
+
+	if (!vfd_ops->set_vlan_strip || !vfd_ops->get_vlan_strip)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_vlan_strip(pdev, vf_id, &data_old);
+	if (ret < 0)
+		return ret;
+
+	ret = __parse_bool_data(pdev, buff, "vlan_strip", &data_new);
+	if (ret)
+		return ret;
+
+	if (data_new != data_old)
+		ret = vfd_ops->set_vlan_strip(pdev, vf_id, data_new);
+
+	return ret ? ret : count;
+}
+
+/**
+ * vfd_link_state_show - handler for link_state show function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer for data
+ **/
+static ssize_t vfd_link_state_show(struct kobject *kobj,
+				   struct kobj_attribute *attr, char *buff)
+{
+	enum vfd_link_speed link_speed;
+	struct pci_dev *pdev;
+	int vf_id, ret = 0;
+	bool enabled;
+
+	if (!vfd_ops->get_link_state)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_link_state(pdev, vf_id, &enabled, &link_speed);
+	if (ret < 0)
+		return ret;
+
+	if (enabled) {
+		const char *speed_str;
+
+		switch (link_speed) {
+		case VFD_LINK_SPEED_100MB:
+			speed_str = "100 Mbps";
+			break;
+		case VFD_LINK_SPEED_1GB:
+			speed_str = "1 Gbps";
+			break;
+		case VFD_LINK_SPEED_2_5GB:
+			speed_str = "2.5 Gbps";
+			break;
+		case VFD_LINK_SPEED_5GB:
+			speed_str = "5 Gbps";
+			break;
+		case VFD_LINK_SPEED_10GB:
+			speed_str = "10 Gbps";
+			break;
+		case VFD_LINK_SPEED_40GB:
+			speed_str = "40 Gbps";
+			break;
+		case VFD_LINK_SPEED_20GB:
+			speed_str = "20 Gbps";
+			break;
+		case VFD_LINK_SPEED_25GB:
+			speed_str = "25 Gbps";
+			break;
+		case VFD_LINK_SPEED_UNKNOWN:
+			speed_str = "unknown speed";
+			break;
+		default:
+			dev_err(&pdev->dev, "Link speed is not supported");
+			return -EOPNOTSUPP;
+		}
+
+		ret = scnprintf(buff, PAGE_SIZE, "%s, %s\n", "up", speed_str);
+	} else {
+		ret = scnprintf(buff, PAGE_SIZE, "down\n");
+	}
+
+	return ret;
+}
+
+/**
+ * vfd_link_state_store - handler for link_state store function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer with input data
+ * @count:	size of buff
+ **/
+static ssize_t vfd_link_state_store(struct kobject *kobj,
+				      struct kobj_attribute *attr,
+				      const char *buff, size_t count)
+{
+	struct pci_dev *pdev;
+	int vf_id, ret = 0;
+	u8 data;
+
+	if (!vfd_ops->set_link_state)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	if (sysfs_streq("enable", buff)) {
+		data = VFD_LINKSTATE_ON;
+	} else if (sysfs_streq("disable", buff)) {
+		data = VFD_LINKSTATE_OFF;
+	} else if (sysfs_streq("auto", buff)) {
+		data = VFD_LINKSTATE_AUTO;
+	} else {
+		dev_err(&pdev->dev, "Invalid input string");
+		return -EINVAL;
+	}
+
+	ret = vfd_ops->set_link_state(pdev, vf_id, data);
+
+	return ret ? ret : count;
+}
+
+/**
+ * vfd_enable_show - handler for VF enable/disable show function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer for data
+ **/
+static ssize_t vfd_enable_show(struct kobject *kobj,
+			       struct kobj_attribute *attr,
+			       char *buff)
+{
+	struct pci_dev *pdev;
+	int vf_id, ret;
+	bool data;
+
+	if (!vfd_ops->get_vf_enable)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_vf_enable(pdev, vf_id, &data);
+	if (ret < 0)
+		return ret;
+
+	if (data)
+		ret = scnprintf(buff, PAGE_SIZE, "on\n");
+	else
+		ret = scnprintf(buff, PAGE_SIZE, "off\n");
+
+	return ret;
+}
+
+/**
+ * vfd_enable_store - handler for VF enable/disable store function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer with input data
+ * @count:	size of buff
+ *
+ * On success return count, indicating that we used the whole buffer. On
+ * failure return a negative error condition.
+ **/
+static ssize_t vfd_enable_store(struct kobject *kobj,
+				struct kobj_attribute *attr,
+				const char *buff, size_t count)
+{
+	bool data_new, data_old;
+	struct pci_dev *pdev;
+	int vf_id, ret;
+
+	if (!vfd_ops->set_vf_enable || !vfd_ops->get_vf_enable)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_vf_enable(pdev, vf_id, &data_old);
+	if (ret < 0)
+		return ret;
+
+	ret = __parse_bool_data(pdev, buff, "enable", &data_new);
+	if (ret)
+		return ret;
+
+	if (data_new != data_old)
+		ret = vfd_ops->set_vf_enable(pdev, vf_id, data_new);
+
+	return ret ? ret : count;
+}
+
+/**
+ * vfd_max_tx_rate_show - handler for mac_tx_rate show function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer for data
+ **/
+static ssize_t vfd_max_tx_rate_show(struct kobject *kobj,
+				    struct kobj_attribute *attr, char *buff)
+{
+	unsigned int max_tx_rate;
+	struct pci_dev *pdev;
+	int vf_id, ret;
+
+	if (!vfd_ops->get_max_tx_rate)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_max_tx_rate(pdev, vf_id, &max_tx_rate);
+	if (ret < 0)
+		return ret;
+
+	ret = scnprintf(buff, PAGE_SIZE, "%u\n", max_tx_rate);
+	return ret;
+}
+
+/**
+ * vfd_max_tx_rate_store - handler for max_tx_rate store function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer with input data
+ * @count:	size of buff
+ **/
+static ssize_t vfd_max_tx_rate_store(struct kobject *kobj,
+				     struct kobj_attribute *attr,
+				     const char *buff, size_t count)
+{
+	unsigned int max_tx_rate;
+	struct pci_dev *pdev;
+	int vf_id, ret;
+
+	if (!vfd_ops->set_max_tx_rate)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = kstrtouint(buff, 10, &max_tx_rate);
+	if (ret) {
+		dev_err(&pdev->dev,
+			"Invalid argument, not a decimal number: %s", buff);
+		return ret;
+	}
+
+	ret = vfd_ops->set_max_tx_rate(pdev, vf_id, &max_tx_rate);
+
+	return ret ? ret : count;
+}
+
+/**
+ * vfd_min_tx_rate_show - handler for min_tx_rate show function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer for data
+ **/
+static ssize_t vfd_min_tx_rate_show(struct kobject *kobj,
+				    struct kobj_attribute *attr, char *buff)
+{
+	if (!vfd_ops->get_min_tx_rate)
+		return -EOPNOTSUPP;
+
+	return vfd_ops->get_min_tx_rate(kobj, attr, buff);
+}
+
+/**
+ * vfd_min_tx_rate_store - handler for min_tx_rate store function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer with input data
+ * @count:	size of buff
+ **/
+static ssize_t vfd_min_tx_rate_store(struct kobject *kobj,
+				     struct kobj_attribute *attr,
+				     const char *buff, size_t count)
+{
+	if (!vfd_ops->set_min_tx_rate)
+		return -EOPNOTSUPP;
+
+	return vfd_ops->set_min_tx_rate(kobj, attr, buff, count);
+}
+
+/**
+ * vfd_trust_show - handler for trust show function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer for data
+ **/
+static ssize_t vfd_trust_show(struct kobject *kobj,
+			      struct kobj_attribute *attr, char *buff)
+{
+	struct pci_dev *pdev;
+	int vf_id, ret;
+	bool data;
+
+	if (!vfd_ops->get_trust_state)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_trust_state(pdev, vf_id, &data);
+	if (ret)
+		return ret;
+
+	if (data)
+		ret = scnprintf(buff, PAGE_SIZE, "on\n");
+	else
+		ret = scnprintf(buff, PAGE_SIZE, "off\n");
+
+	return ret;
+}
+
+/**
+ * vfd_trust_store - handler for trust store function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer with input data
+ * @count:	size of buff
+ **/
+static ssize_t vfd_trust_store(struct kobject *kobj,
+			       struct kobj_attribute *attr,
+			       const char *buff, size_t count)
+{
+	bool data_new, data_old;
+	struct pci_dev *pdev;
+	int vf_id, ret;
+
+	if (!vfd_ops->set_trust_state || !vfd_ops->get_trust_state)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_trust_state(pdev, vf_id, &data_old);
+	if (ret)
+		return ret;
+
+	ret = __parse_bool_data(pdev, buff, "trust", &data_new);
+	if (ret)
+		return ret;
+
+	if (data_new != data_old)
+		ret = vfd_ops->set_trust_state(pdev, vf_id, data_new);
+
+	return ret ? ret : count;
+}
+
+/**
+ * vfd_reset_stats_store - handler for reset stats store function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer with input data
+ * @count:	size of buff
+ **/
+static ssize_t vfd_reset_stats_store(struct kobject *kobj,
+				     struct kobj_attribute *attr,
+				     const char *buff, size_t count)
+{
+	int vf_id, reset, ret;
+	struct pci_dev *pdev;
+
+	if (!vfd_ops->reset_stats)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+	ret = kstrtoint(buff, 10, &reset);
+	if (ret) {
+		dev_err(&pdev->dev, "Invalid input\n");
+		return ret;
+	}
+
+	if (reset != 1)
+		return -EINVAL;
+
+	ret = vfd_ops->reset_stats(pdev, vf_id);
+
+	return ret ? ret : count;
+}
+
+/**
+ * vfd_rx_bytes_show - handler for rx_bytes show function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer for data
+ **/
+static ssize_t vfd_rx_bytes_show(struct kobject *kobj,
+				 struct kobj_attribute *attr, char *buff)
+{
+	struct pci_dev *pdev;
+	int vf_id, ret = 0;
+	u64 data;
+
+	if (!vfd_ops->get_rx_bytes)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_rx_bytes(pdev, vf_id, &data);
+	if (ret < 0)
+		return ret;
+
+	ret = scnprintf(buff, PAGE_SIZE, "%llu\n", data);
+
+	return ret;
+}
+
+/**
+ * vfd_rx_dropped_show - handler for rx_dropped show function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer for data
+ **/
+static ssize_t vfd_rx_dropped_show(struct kobject *kobj,
+				   struct kobj_attribute *attr, char *buff)
+{
+	struct pci_dev *pdev;
+	int vf_id, ret = 0;
+	u64 data;
+
+	if (!vfd_ops->get_rx_dropped)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_rx_dropped(pdev, vf_id, &data);
+	if (ret < 0)
+		return ret;
+
+	ret = scnprintf(buff, PAGE_SIZE, "%llu\n", data);
+
+	return ret;
+}
+
+/**
+ * vfd_rx_packets_show - handler for rx_packets show function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer for data
+ **/
+static ssize_t vfd_rx_packets_show(struct kobject *kobj,
+				   struct kobj_attribute *attr, char *buff)
+{
+	struct pci_dev *pdev;
+	int vf_id, ret = 0;
+	u64 data;
+
+	if (!vfd_ops->get_rx_packets)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_rx_packets(pdev, vf_id, &data);
+	if (ret < 0)
+		return ret;
+
+	ret = scnprintf(buff, PAGE_SIZE, "%llu\n", data);
+
+	return ret;
+}
+
+/**
+ * vfd_tx_bytes_show - handler for tx_bytes show function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer for data
+ **/
+static ssize_t vfd_tx_bytes_show(struct kobject *kobj,
+				 struct kobj_attribute *attr, char *buff)
+{
+	struct pci_dev *pdev;
+	int vf_id, ret = 0;
+	u64 data;
+
+	if (!vfd_ops->get_tx_bytes)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_tx_bytes(pdev, vf_id, &data);
+	if (ret < 0)
+		return ret;
+
+	ret = scnprintf(buff, PAGE_SIZE, "%llu\n", data);
+
+	return ret;
+}
+
+/**
+ * vfd_tx_dropped_show - handler for tx_dropped show function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer for data
+ **/
+static ssize_t vfd_tx_dropped_show(struct kobject *kobj,
+				   struct kobj_attribute *attr, char *buff)
+{
+	struct pci_dev *pdev;
+	int vf_id, ret = 0;
+	u64 data;
+
+	if (!vfd_ops->get_tx_dropped)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_tx_dropped(pdev, vf_id, &data);
+	if (ret < 0)
+		return ret;
+
+	ret = scnprintf(buff, PAGE_SIZE, "%llu\n", data);
+
+	return ret;
+}
+
+/**
+ * vfd_tx_packets_show - handler for tx_packets show function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer for data
+ **/
+static ssize_t vfd_tx_packets_show(struct kobject *kobj,
+				   struct kobj_attribute *attr, char *buff)
+{
+	struct pci_dev *pdev;
+	int vf_id, ret = 0;
+	u64 data;
+
+	if (!vfd_ops->get_tx_packets)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_tx_packets(pdev, vf_id, &data);
+	if (ret < 0)
+		return ret;
+
+	ret = scnprintf(buff, PAGE_SIZE, "%llu\n", data);
+
+	return ret;
+}
+
+/**
+ * vfd_tx_spoofed_show - handler for tx_spoofed show function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer for data
+ **/
+static ssize_t vfd_tx_spoofed_show(struct kobject *kobj,
+				   struct kobj_attribute *attr, char *buff)
+{
+	struct pci_dev *pdev;
+	int vf_id, ret = 0;
+	u64 data;
+
+	if (!vfd_ops->get_tx_spoofed)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_tx_spoofed(pdev, vf_id, &data);
+	if (ret < 0)
+		return ret;
+
+	ret = scnprintf(buff, PAGE_SIZE, "%llu\n", data);
+
+	return ret;
+}
+
+/**
+ * vfd_tx_errors_show - handler for tx_errors show function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer for data
+ **/
+static ssize_t vfd_tx_errors_show(struct kobject *kobj,
+				  struct kobj_attribute *attr, char *buff)
+{
+	struct pci_dev *pdev;
+	int vf_id, ret = 0;
+	u64 data;
+
+	if (!vfd_ops->get_tx_errors)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_tx_errors(pdev, vf_id, &data);
+	if (ret < 0)
+		return ret;
+
+	ret = scnprintf(buff, PAGE_SIZE, "%llu\n", data);
+
+	return ret;
+}
+
+/**
+ * qos_share_show - handler for the bw_share show function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer for data
+ **/
+static ssize_t qos_share_show(struct kobject *kobj,
+			      struct kobj_attribute *attr, char *buff)
+{
+	struct pci_dev *pdev;
+	int vf_id, ret;
+	u8 data = 0;
+
+	if (!vfd_ops->get_vf_bw_share)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj->parent, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_vf_bw_share(pdev, vf_id, &data);
+	if (ret < 0) {
+		dev_err(&pdev->dev, "No bw share applied for VF %d\n", vf_id);
+		return ret;
+	}
+
+	ret = scnprintf(buff, PAGE_SIZE, "%u\n", data);
+
+	return ret;
+}
+
+/**
+ * qos_share_store - handler for the bw_share store function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer with input data
+ * @count:	size of buff
+ **/
+static ssize_t qos_share_store(struct kobject *kobj,
+			       struct kobj_attribute *attr,
+			       const char *buff, size_t count)
+{
+	struct pci_dev *pdev;
+	int vf_id, ret;
+	u8 bw_share;
+
+	if (!vfd_ops->set_vf_bw_share)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj->parent, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	/* parse the bw_share */
+	ret = kstrtou8(buff, 10, &bw_share);
+	if (ret) {
+		dev_err(&pdev->dev, "Invalid input\n");
+		return ret;
+	}
+
+	/* check that the BW is between 1 and 100 */
+	if (bw_share < 1 || bw_share > 100) {
+		dev_err(&pdev->dev, "BW share has to be between 1-100\n");
+		return -EINVAL;
+	}
+	ret = vfd_ops->set_vf_bw_share(pdev, vf_id, bw_share);
+	return ret ? ret : count;
+}
+
+/**
+ * pf_qos_apply_store - handler for pf qos apply store function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer with input data
+ * @count:	size of buff
+ **/
+static ssize_t pf_qos_apply_store(struct kobject *kobj,
+				  struct kobj_attribute *attr,
+				  const char *buff, size_t count)
+{
+	int ret, apply;
+	struct pci_dev *pdev;
+
+	if (!vfd_ops->set_pf_qos_apply)
+		return -EOPNOTSUPP;
+
+	ret = __get_pf_pdev(kobj->parent, &pdev);
+	if (ret)
+		return ret;
+
+	ret = kstrtoint(buff, 10, &apply);
+	if (ret) {
+		dev_err(&pdev->dev,
+			"Invalid input\n");
+		return ret;
+	}
+
+	if (apply != 1)
+		return -EINVAL;
+
+	ret = vfd_ops->set_pf_qos_apply(pdev);
+
+	return ret ? ret : count;
+}
+
+/**
+ * pf_ingress_mirror_show - handler for PF ingress mirror show function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer for data
+ **/
+static ssize_t pf_ingress_mirror_show(struct kobject *kobj,
+				      struct kobj_attribute *attr, char *buff)
+{
+	struct pci_dev *pdev;
+	int ret, data;
+
+	if (!vfd_ops->get_pf_ingress_mirror)
+		return -EOPNOTSUPP;
+
+	ret = __get_pf_pdev(kobj, &pdev);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_pf_ingress_mirror(pdev, &data);
+	if (ret < 0)
+		return ret;
+
+	if (data == VFD_INGRESS_MIRROR_OFF)
+		ret = scnprintf(buff, PAGE_SIZE, "off\n");
+	else
+		ret = scnprintf(buff, PAGE_SIZE, "%u\n", data);
+
+	return ret;
+}
+
+/**
+ * pf_ingress_mirror_store - handler for pf ingress mirror store function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer with input data
+ * @count:	size of buff
+ **/
+static ssize_t pf_ingress_mirror_store(struct kobject *kobj,
+				       struct kobj_attribute *attr,
+				       const char *buff, size_t count)
+{
+	int data_new, data_old;
+	struct pci_dev *pdev;
+	int ret;
+
+	if (!vfd_ops->set_pf_ingress_mirror || !vfd_ops->get_pf_ingress_mirror)
+		return -EOPNOTSUPP;
+
+	ret = __get_pf_pdev(kobj, &pdev);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_pf_ingress_mirror(pdev, &data_old);
+	if (ret < 0)
+		return ret;
+
+	ret = __parse_egress_ingress_input(pdev, buff, "ingress_mirror",
+					   &data_new, &data_old);
+	if (ret)
+		return ret;
+
+	if (data_new != data_old)
+		ret = vfd_ops->set_pf_ingress_mirror(pdev, data_new);
+
+	return ret ? ret : count;
+}
+
+/**
+ * pf_egress_mirror_show - handler for PF egress mirror show function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer for data
+ **/
+static ssize_t pf_egress_mirror_show(struct kobject *kobj,
+				     struct kobj_attribute *attr, char *buff)
+{
+	struct pci_dev *pdev;
+	int ret, data;
+
+	if (!vfd_ops->get_pf_egress_mirror)
+		return -EOPNOTSUPP;
+
+	ret = __get_pf_pdev(kobj, &pdev);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_pf_egress_mirror(pdev, &data);
+	if (ret < 0)
+		return ret;
+
+	if (data == VFD_EGRESS_MIRROR_OFF)
+		ret = scnprintf(buff, PAGE_SIZE, "off\n");
+	else
+		ret = scnprintf(buff, PAGE_SIZE, "%u\n", data);
+
+	return ret;
+}
+
+/**
+ * pf_egress_mirror_store - handler for pf egress mirror store function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer with input data
+ * @count:	size of buff
+ **/
+static ssize_t pf_egress_mirror_store(struct kobject *kobj,
+				      struct kobj_attribute *attr,
+				      const char *buff, size_t count)
+{
+	int data_new, data_old;
+	struct pci_dev *pdev;
+	int ret;
+
+	if (!vfd_ops->set_pf_egress_mirror || !vfd_ops->get_pf_egress_mirror)
+		return -EOPNOTSUPP;
+
+	ret = __get_pf_pdev(kobj, &pdev);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_pf_egress_mirror(pdev, &data_old);
+	if (ret < 0)
+		return ret;
+
+	ret = __parse_egress_ingress_input(pdev, buff, "egress_mirror",
+					   &data_new, &data_old);
+	if (ret)
+		return ret;
+
+	if (data_new != data_old)
+		ret = vfd_ops->set_pf_egress_mirror(pdev, data_new);
+
+	return ret ? ret : count;
+}
+
+/**
+ * pf_tpid_show - handler for pf tpid show function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer for data
+ **/
+static ssize_t pf_tpid_show(struct kobject *kobj,
+			    struct kobj_attribute *attr, char *buff)
+{
+	struct pci_dev *pdev;
+	u16 data;
+	int ret;
+
+	if (!vfd_ops->get_pf_tpid)
+		return -EOPNOTSUPP;
+
+	ret = __get_pf_pdev(kobj, &pdev);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_pf_tpid(pdev, &data);
+	if (ret < 0)
+		return ret;
+
+	ret = scnprintf(buff, PAGE_SIZE, "%x\n", data);
+
+	return ret;
+}
+
+/**
+ * pf_tpid_store - handler for pf tpid store function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer with input data
+ * @count:	size of buff
+ **/
+static ssize_t pf_tpid_store(struct kobject *kobj,
+			     struct kobj_attribute *attr,
+			     const char *buff, size_t count)
+{
+	struct pci_dev *pdev;
+	u16 data;
+	int ret;
+
+	if (!vfd_ops->set_pf_tpid)
+		return -EOPNOTSUPP;
+
+	ret = __get_pf_pdev(kobj, &pdev);
+	if (ret)
+		return ret;
+
+	ret = kstrtou16(buff, 16, &data);
+	if (ret) {
+		dev_err(&pdev->dev, "Invalid input\n");
+		return ret;
+	}
+
+	ret = vfd_ops->set_pf_tpid(pdev, data);
+
+	return ret ? ret : count;
+}
+
+/**
+ * vfd_num_queues_show - handler for num_queues show function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer for data
+ **/
+static ssize_t vfd_num_queues_show(struct kobject *kobj,
+				   struct kobj_attribute *attr, char *buff)
+{
+	struct pci_dev *pdev;
+	int vf_id, ret;
+	int data;
+
+	if (!vfd_ops->get_num_queues)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_num_queues(pdev, vf_id, &data);
+	if (ret)
+		return ret;
+
+	ret = scnprintf(buff, PAGE_SIZE, "%d\n", data);
+
+	return ret;
+}
+
+/**
+ * vfd_num_queues_store - handler for num_queues store function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer with input data
+ * @count:	size of buff
+ **/
+static ssize_t vfd_num_queues_store(struct kobject *kobj,
+				    struct kobj_attribute *attr,
+				    const char *buff, size_t count)
+{
+	int data_new, data_old;
+	struct pci_dev *pdev;
+	int vf_id, ret;
+
+	if (!vfd_ops->set_num_queues)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_num_queues(pdev, vf_id, &data_old);
+	if (ret)
+		return ret;
+
+	ret = kstrtoint(buff, 10, &data_new);
+	if (ret) {
+		dev_err(&pdev->dev, "Invalid input\n");
+		return ret;
+	}
+
+	if (data_new < 1) {
+		dev_err(&pdev->dev, "VF queue count must be at least 1\n");
+		return -EINVAL;
+	}
+
+	if (data_new != data_old)
+		ret = vfd_ops->set_num_queues(pdev, vf_id, data_new);
+
+	return ret ? ret : count;
+}
+
+/**
+ * vfd_queue_type_show - handler for queue_type show function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer for data
+ **/
+static ssize_t vfd_queue_type_show(struct kobject *kobj,
+				   struct kobj_attribute *attr, char *buff)
+{
+	struct pci_dev *pdev;
+	int vf_id, ret = 0;
+	u8 data;
+
+	if (!vfd_ops->get_queue_type)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_queue_type(pdev, vf_id, &data);
+	if (ret)
+		return ret;
+
+	ret = scnprintf(buff, PAGE_SIZE, "%d\n", data);
+
+	return ret;
+}
+
+/**
+ * vfd_queue_type_store - handler for queue_type store function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer with input data
+ * @count:	size of buff
+ **/
+static ssize_t vfd_queue_type_store(struct kobject *kobj,
+				    struct kobj_attribute *attr,
+				    const char *buff, size_t count)
+{
+	// the setting will be updated via different sysfs
+	return -EOPNOTSUPP;
+}
+
+/**
+ * vfd_allow_bcast_show - handler for VF allow broadcast show function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer for data
+ **/
+static ssize_t vfd_allow_bcast_show(struct kobject *kobj,
+				    struct kobj_attribute *attr,
+				    char *buff)
+{
+	struct pci_dev *pdev;
+	int vf_id, ret;
+	bool data;
+
+	if (!vfd_ops->get_allow_bcast)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_allow_bcast(pdev, vf_id, &data);
+	if (ret < 0)
+		return ret;
+
+	if (data)
+		ret = scnprintf(buff, PAGE_SIZE, "on\n");
+	else
+		ret = scnprintf(buff, PAGE_SIZE, "off\n");
+
+	return ret;
+}
+
+/**
+ * vfd_allow_bcast_store - handler for VF allow broadcast store function
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer with input data
+ * @count:	size of buff
+ *
+ * On success return count, indicating that we used the whole buffer. On
+ * failure return a negative error condition.
+ **/
+static ssize_t vfd_allow_bcast_store(struct kobject *kobj,
+				     struct kobj_attribute *attr,
+				     const char *buff, size_t count)
+{
+	bool data_new, data_old;
+	struct pci_dev *pdev;
+	int vf_id, ret;
+
+	if (!vfd_ops->set_allow_bcast || !vfd_ops->get_allow_bcast)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_and_vfid(kobj, &pdev, &vf_id);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_allow_bcast(pdev, vf_id, &data_old);
+	if (ret < 0)
+		return ret;
+
+	ret = __parse_bool_data(pdev, buff, "allow_bcast", &data_new);
+	if (ret)
+		return ret;
+
+	if (data_new != data_old)
+		ret = vfd_ops->set_allow_bcast(pdev, vf_id, data_new);
+
+	return ret ? ret : count;
+}
+
+/**
+ * round_nearest_quanta - helper function for calculating quanta
+ * @num:	Number to be rounded
+ *
+ * Calculates nearest multiple of 50, which is quanta accepted by FW.
+ * For 0 it returns 0, which means unlimitied bandwidth
+ **/
+static int round_nearest_quanta(int num)
+{
+	static const int base = 50;
+
+	if (!(num % base) || !num)
+		return num;
+	else
+		return num + base - (num % base);
+}
+
+/**
+ * pf_qos_tc_priority_show - handler for PF's priority for given TC show
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer for data
+ **/
+static ssize_t pf_qos_tc_priority_show(struct kobject *kobj,
+				       struct kobj_attribute *attr, char *buff)
+{
+	struct pci_dev *pdev;
+	int i, tc, ret;
+	char *written;
+	u8 prio;
+
+	/* check if option is implemented in vfd_ops*/
+	if (!vfd_ops->set_pf_qos_tc_priority ||
+	    !vfd_ops->get_pf_qos_tc_priority)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_tc(kobj, &pdev, &tc);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_pf_qos_tc_priority(pdev, tc, &prio);
+
+	if (!prio)
+		return ret;
+
+	written = buff;
+	/* iterate over prio bits */
+	for (i = 0; i < 8; i++) {
+		if (BIT(i) & prio) {
+			ret += scnprintf(written, PAGE_SIZE, "%d,", i);
+			written += 2;
+		}
+	}
+	ret += scnprintf(written, PAGE_SIZE, "\n");
+	return ret;
+}
+
+/**
+ * pf_qos_tc_priority_store - handler for PF's priority for given TC store
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer with input data
+ * @count:	size of buff
+ *
+ * On success return count, indicating that we used the whole buffer. On
+ * failure return a negative error condition.
+ **/
+static ssize_t pf_qos_tc_priority_store(struct kobject *kobj,
+					struct kobj_attribute *attr,
+					const char *buff, size_t count)
+{
+	struct pci_dev *pdev;
+	int tc, tmp, ret;
+	char *tok, *str;
+	u8 prio = 0;
+
+	/* check if option is implemented in vfd_ops*/
+	if (!vfd_ops->set_pf_qos_tc_priority ||
+	    !vfd_ops->get_pf_qos_tc_priority)
+		return -EOPNOTSUPP;
+
+	str = kzalloc(sizeof(*str) * count + 1, GFP_KERNEL);
+	if (!str)
+		return -ENOMEM;
+
+	strncpy(str, buff, count);
+	ret = __get_pdev_tc(kobj, &pdev, &tc);
+	if (ret)
+		goto err;
+
+	while ((tok = strsep(&str, ",")) != NULL) {
+		tok = strim(tok);
+
+		ret = kstrtoint(tok, 10, &tmp);
+		if (ret) {
+			dev_err(&pdev->dev, "Invalid input\n");
+			goto err;
+		}
+
+		if (tmp < 0 || tmp >= VFD_NUM_TC) {
+			dev_err(&pdev->dev, "Only numbers 0-7 are allowed.\n");
+			ret = -EINVAL;
+			goto err;
+		}
+		prio |= BIT(tmp);
+	}
+	vfd_ops->set_pf_qos_tc_priority(pdev, tc, prio);
+
+	kfree(str);
+	return count;
+
+err:
+	kfree(str);
+	return ret;
+}
+
+/**
+ * pf_qos_tc_lsp_show - handler for PF's link strict priority for given TC show
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer for data
+ **/
+static ssize_t pf_qos_tc_lsp_show(struct kobject *kobj,
+				  struct kobj_attribute *attr, char *buff)
+{
+	struct pci_dev *pdev;
+	int tc, ret;
+	bool lsp;
+
+	/* check if option is implemented in vfd_ops*/
+	if (!vfd_ops->set_pf_qos_tc_lsp || !vfd_ops->get_pf_qos_tc_lsp)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_tc(kobj, &pdev, &tc);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_pf_qos_tc_lsp(pdev, tc, &lsp);
+	if (ret)
+		return ret;
+
+	ret = scnprintf(buff, PAGE_SIZE, lsp ? "on\n" : "off\n");
+
+	return ret;
+}
+
+/**
+ * pf_qos_tc_lsp_store - handler for PF link strict priority for given TC store
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer with input data
+ * @count:	size of buff
+ *
+ * On success return count, indicating that we used the whole buffer. On
+ * failure return a negative error condition.
+ **/
+static ssize_t pf_qos_tc_lsp_store(struct kobject *kobj,
+				   struct kobj_attribute *attr,
+				   const char *buff, size_t count)
+{
+	struct pci_dev *pdev;
+	int tc, ret;
+	bool lsp;
+
+	/* check if option is implemented in vfd_ops*/
+	if (!vfd_ops->set_pf_qos_tc_lsp || !vfd_ops->get_pf_qos_tc_lsp)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_tc(kobj, &pdev, &tc);
+	if (ret)
+		return ret;
+
+	__parse_bool_data(pdev, buff, "lsp", &lsp);
+
+	ret = vfd_ops->set_pf_qos_tc_lsp(pdev, tc, lsp);
+	if (ret) {
+		dev_err(&pdev->dev, "Failed to store PF QoS lsp value.\n");
+		return ret;
+	}
+
+	return count;
+}
+
+/**
+ * pf_qos_tc_max_bw_show - handler for PF's max bandwidth for given TC show
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer for data
+ **/
+static ssize_t pf_qos_tc_max_bw_show(struct kobject *kobj,
+				     struct kobj_attribute *attr, char *buff)
+{
+	struct pci_dev *pdev;
+	int tc, ret;
+	u16 max_bw;
+
+	if (!vfd_ops->set_pf_qos_tc_max_bw || !vfd_ops->get_pf_qos_tc_max_bw)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_tc(kobj, &pdev, &tc);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_pf_qos_tc_max_bw(pdev, tc, &max_bw);
+	if (ret)
+		return ret;
+
+	ret = scnprintf(buff, PAGE_SIZE, "%d\n", max_bw);
+
+	return ret;
+}
+
+/**
+ * pf_qos_tc_max_bw_store - handler for PF's max bandwidth for given TC store
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer with input data
+ * @count:	size of buff
+ *
+ * On success return count, indicating that we used the whole buffer. On
+ * failure return a negative error condition.
+ **/
+static ssize_t pf_qos_tc_max_bw_store(struct kobject *kobj,
+				      struct kobj_attribute *attr,
+				      const char *buff, size_t count)
+{
+	struct pci_dev *pdev;
+	int tc, ret;
+	u16 bw;
+
+	if (!vfd_ops->set_pf_qos_tc_max_bw || !vfd_ops->get_pf_qos_tc_max_bw)
+		return -EOPNOTSUPP;
+
+	ret = __get_pdev_tc(kobj, &pdev, &tc);
+	if (ret)
+		return ret;
+
+	ret = kstrtou16(buff, 10, &bw);
+	if (ret) {
+		dev_err(&pdev->dev, "Invalid input\n");
+		return ret;
+	}
+
+	ret = vfd_ops->set_pf_qos_tc_max_bw(pdev, tc,
+					    round_nearest_quanta(bw));
+	if (ret)
+		return ret;
+
+	return count;
+}
+
+/**
+ * vf_max_tc_tx_rate_show - handler for VF's max per TC tx rate show
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer for data
+ **/
+static ssize_t vf_max_tc_tx_rate_show(struct kobject *kobj,
+				      struct kobj_attribute *attr, char *buff)
+{
+	int tc, vf_id, tc_tx_rate, ret;
+	struct pci_dev *pdev;
+
+	if (!vfd_ops->set_vf_max_tc_tx_rate || !vfd_ops->get_vf_max_tc_tx_rate)
+		return -EOPNOTSUPP;
+
+	ret = __get_vf_tc_pdev(kobj, &pdev, &vf_id, &tc);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_vf_max_tc_tx_rate(pdev, vf_id, tc, &tc_tx_rate);
+	if (ret)
+		return ret;
+
+	ret = scnprintf(buff, PAGE_SIZE, "%d\n", tc_tx_rate);
+	return ret;
+}
+
+/**
+ * vf_max_tc_tx_rate_store - handler for VF's max per TC tx rate store
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer with input data
+ * @count:	size of buff
+ *
+ * On success return count, indicating that we used the whole buffer. On
+ * failure return a negative error condition.
+ **/
+static ssize_t vf_max_tc_tx_rate_store(struct kobject *kobj,
+				       struct kobj_attribute *attr,
+				       const char *buff, size_t count)
+{
+	int tc, vf_id, tc_tx_rate, ret;
+	struct pci_dev *pdev;
+
+	if (!vfd_ops->set_vf_max_tc_tx_rate || !vfd_ops->get_vf_max_tc_tx_rate)
+		return -EOPNOTSUPP;
+
+	ret = __get_vf_tc_pdev(kobj, &pdev, &vf_id, &tc);
+	if (ret)
+		return ret;
+	ret = kstrtoint(buff, 10, &tc_tx_rate);
+	if (ret) {
+		dev_err(&pdev->dev,
+			"Invalid input, provide bandwidth as number.\n");
+		return ret;
+	}
+
+	ret = vfd_ops->set_vf_max_tc_tx_rate(pdev, vf_id, tc,
+					     round_nearest_quanta(tc_tx_rate));
+	if (ret) {
+		dev_err(&pdev->dev,
+			"Failed to assign max TC tx rate.\n");
+		return ret;
+	}
+
+	return count;
+}
+
+/**
+ * vf_qos_tc_share_show - handler for VF bandwidth share per TC show
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer for data
+ **/
+static ssize_t vf_qos_tc_share_show(struct kobject *kobj,
+				    struct kobj_attribute *attr, char *buff)
+{
+	struct pci_dev *pdev;
+	int tc, vf_id, ret;
+	u8 share;
+
+	if (!vfd_ops->set_vf_qos_tc_share || !vfd_ops->get_vf_qos_tc_share)
+		return -EOPNOTSUPP;
+
+	ret = __get_vf_tc_pdev(kobj, &pdev, &vf_id, &tc);
+	if (ret)
+		return ret;
+
+	ret = vfd_ops->get_vf_qos_tc_share(pdev, vf_id, tc, &share);
+	if (ret)
+		return ret;
+
+	ret = scnprintf(buff, PAGE_SIZE, "%d\n", share);
+	return ret;
+}
+
+/**
+ * vf_qos_tc_share_store - handler for VF bandwidth share per TC store
+ * @kobj:	kobject being called
+ * @attr:	struct kobj_attribute
+ * @buff:	buffer with input data
+ * @count:	size of buff
+ *
+ * On success return count, indicating that we used the whole buffer. On
+ * failure return a negative error condition.
+ **/
+static ssize_t vf_qos_tc_share_store(struct kobject *kobj,
+				     struct kobj_attribute *attr,
+				     const char *buff, size_t count)
+{
+	struct pci_dev *pdev;
+	int tc, vf_id, ret;
+	u8 share;
+
+	if (!vfd_ops->set_vf_qos_tc_share || !vfd_ops->get_vf_qos_tc_share)
+		return -EOPNOTSUPP;
+
+	ret = __get_vf_tc_pdev(kobj, &pdev, &vf_id, &tc);
+	if (ret)
+		return ret;
+
+	ret = kstrtou8(buff, 10, &share);
+	if (ret) {
+		dev_err(&pdev->dev, "Invalid input\n");
+		return ret;
+	}
+
+	if (share > 100) {
+		dev_err(&pdev->dev, "Share must be in range 0-100.\n");
+		return -EINVAL;
+	}
+
+	ret = vfd_ops->set_vf_qos_tc_share(pdev, vf_id, tc, share);
+	if (ret)
+		return ret;
+
+	return count;
+}
+
+static struct kobj_attribute trunk_attribute =
+	__ATTR(trunk, 0644, vfd_trunk_show, vfd_trunk_store);
+static struct kobj_attribute vlan_mirror_attribute =
+	__ATTR(vlan_mirror, 0644, vfd_vlan_mirror_show, vfd_vlan_mirror_store);
+static struct kobj_attribute egress_mirror_attribute =
+	__ATTR(egress_mirror, 0644,
+	       vfd_egress_mirror_show, vfd_egress_mirror_store);
+static struct kobj_attribute ingress_mirror_attribute =
+	__ATTR(ingress_mirror, 0644,
+	       vfd_ingress_mirror_show, vfd_ingress_mirror_store);
+static struct kobj_attribute mac_anti_spoof_attribute =
+	__ATTR(mac_anti_spoof, 0644,
+	       vfd_mac_anti_spoof_show, vfd_mac_anti_spoof_store);
+static struct kobj_attribute vlan_anti_spoof_attribute =
+	__ATTR(vlan_anti_spoof, 0644,
+	       vfd_vlan_anti_spoof_show, vfd_vlan_anti_spoof_store);
+static struct kobj_attribute allow_untagged_attribute =
+	__ATTR(allow_untagged, 0644,
+	       vfd_allow_untagged_show, vfd_allow_untagged_store);
+static struct kobj_attribute loopback_attribute =
+	__ATTR(loopback, 0644, vfd_loopback_show, vfd_loopback_store);
+static struct kobj_attribute mac_attribute =
+	__ATTR(mac, 0644, vfd_mac_show, vfd_mac_store);
+static struct kobj_attribute mac_list_attribute =
+	__ATTR(mac_list, 0644, vfd_mac_list_show, vfd_mac_list_store);
+static struct kobj_attribute promisc_attribute =
+	__ATTR(promisc, 0644, vfd_promisc_show, vfd_promisc_store);
+static struct kobj_attribute vlan_strip_attribute =
+	__ATTR(vlan_strip, 0644, vfd_vlan_strip_show, vfd_vlan_strip_store);
+static struct kobj_attribute link_state_attribute =
+	__ATTR(link_state, 0644, vfd_link_state_show, vfd_link_state_store);
+static struct kobj_attribute max_tx_rate_attribute =
+	__ATTR(max_tx_rate, 0644, vfd_max_tx_rate_show, vfd_max_tx_rate_store);
+static struct kobj_attribute min_tx_rate_attribute =
+	__ATTR(min_tx_rate, 0644, vfd_min_tx_rate_show, vfd_min_tx_rate_store);
+static struct kobj_attribute trust_attribute =
+	__ATTR(trust, 0644, vfd_trust_show, vfd_trust_store);
+static struct kobj_attribute reset_stats_attribute =
+	__ATTR(reset_stats, 0200, NULL, vfd_reset_stats_store);
+static struct kobj_attribute enable_attribute =
+	__ATTR(enable, 0644, vfd_enable_show, vfd_enable_store);
+static struct kobj_attribute num_queues_attribute =
+	__ATTR(num_queues, 0644, vfd_num_queues_show, vfd_num_queues_store);
+static struct kobj_attribute queue_type_attribute =
+	__ATTR(queue_type, 0644, vfd_queue_type_show, vfd_queue_type_store);
+static struct kobj_attribute allow_bcast_attribute =
+	__ATTR(allow_bcast, 0644, vfd_allow_bcast_show, vfd_allow_bcast_store);
+
+static struct attribute *s_attrs[] = {
+	&trunk_attribute.attr,
+	&vlan_mirror_attribute.attr,
+	&egress_mirror_attribute.attr,
+	&ingress_mirror_attribute.attr,
+	&mac_anti_spoof_attribute.attr,
+	&vlan_anti_spoof_attribute.attr,
+	&allow_untagged_attribute.attr,
+	&loopback_attribute.attr,
+	&mac_attribute.attr,
+	&mac_list_attribute.attr,
+	&promisc_attribute.attr,
+	&vlan_strip_attribute.attr,
+	&link_state_attribute.attr,
+	&max_tx_rate_attribute.attr,
+	&min_tx_rate_attribute.attr,
+	&trust_attribute.attr,
+	&reset_stats_attribute.attr,
+	&enable_attribute.attr,
+	&num_queues_attribute.attr,
+	&queue_type_attribute.attr,
+	&allow_bcast_attribute.attr,
+	NULL,
+};
+
+static struct attribute_group vfd_group = {
+	.attrs = s_attrs,
+};
+
+static struct kobj_attribute rx_bytes_attribute =
+	__ATTR(rx_bytes, 0444, vfd_rx_bytes_show, NULL);
+static struct kobj_attribute rx_dropped_attribute =
+	__ATTR(rx_dropped, 0444, vfd_rx_dropped_show, NULL);
+static struct kobj_attribute rx_packets_attribute =
+	__ATTR(rx_packets, 0444, vfd_rx_packets_show, NULL);
+static struct kobj_attribute tx_bytes_attribute =
+	__ATTR(tx_bytes, 0444, vfd_tx_bytes_show, NULL);
+static struct kobj_attribute tx_dropped_attribute =
+	__ATTR(tx_dropped, 0444, vfd_tx_dropped_show, NULL);
+static struct kobj_attribute tx_packets_attribute =
+	__ATTR(tx_packets, 0444, vfd_tx_packets_show, NULL);
+static struct kobj_attribute tx_spoofed_attribute =
+	__ATTR(tx_spoofed, 0444, vfd_tx_spoofed_show, NULL);
+static struct kobj_attribute tx_errors_attribute =
+	__ATTR(tx_errors, 0444, vfd_tx_errors_show, NULL);
+
+static struct attribute *stats_attrs[] = {
+	&rx_bytes_attribute.attr,
+	&rx_dropped_attribute.attr,
+	&rx_packets_attribute.attr,
+	&tx_bytes_attribute.attr,
+	&tx_dropped_attribute.attr,
+	&tx_packets_attribute.attr,
+	&tx_spoofed_attribute.attr,
+	&tx_errors_attribute.attr,
+	NULL,
+};
+
+static struct attribute_group stats_group = {
+	.name = "stats",
+	.attrs = stats_attrs,
+};
+
+static struct kobj_attribute share_attribute =
+	__ATTR(share, 0644, qos_share_show, qos_share_store);
+
+static struct attribute *qos_attrs[] = {
+	&share_attribute.attr,
+	NULL,
+};
+
+static struct attribute_group qos_group = {
+	.attrs = qos_attrs,
+};
+
+static struct kobj_attribute apply_attribute =
+	__ATTR(apply, 0200, NULL, pf_qos_apply_store);
+
+static struct attribute *pf_qos_attrs[] = {
+	&apply_attribute.attr,
+	NULL,
+};
+
+static struct attribute_group pf_qos_group = {
+	.attrs = pf_qos_attrs,
+};
+
+static struct kobj_attribute pf_ingress_mirror_attribute =
+	__ATTR(ingress_mirror, 0644, pf_ingress_mirror_show, pf_ingress_mirror_store);
+static struct kobj_attribute pf_egress_mirror_attribute =
+	__ATTR(egress_mirror, 0644, pf_egress_mirror_show, pf_egress_mirror_store);
+static struct kobj_attribute pf_tpid_attribute =
+	__ATTR(tpid, 0644, pf_tpid_show, pf_tpid_store);
+
+static struct attribute *pf_attrs[] = {
+	&pf_ingress_mirror_attribute.attr,
+	&pf_egress_mirror_attribute.attr,
+	&pf_tpid_attribute.attr,
+	NULL,
+};
+
+static struct attribute_group pf_attr_group = {
+	.attrs = pf_attrs,
+};
+
+static struct kobj_attribute vf_qos_tc_max_tc_tx_rate_attribute =
+	__ATTR(max_tc_tx_rate, 0644, vf_max_tc_tx_rate_show,
+	       vf_max_tc_tx_rate_store);
+static struct kobj_attribute vf_qos_tc_share_attribute =
+	__ATTR(share, 0644, vf_qos_tc_share_show,
+	       vf_qos_tc_share_store);
+
+static struct attribute *vf_qos_tc_attrs[] = {
+	&vf_qos_tc_max_tc_tx_rate_attribute.attr,
+	&vf_qos_tc_share_attribute.attr,
+	NULL,
+};
+
+static struct attribute_group vf_qos_tc_group = {
+	.attrs = vf_qos_tc_attrs,
+};
+
+static struct kobj_attribute pf_qos_tc_priority_attribute =
+	__ATTR(priority, 0644, pf_qos_tc_priority_show,
+	       pf_qos_tc_priority_store);
+static struct kobj_attribute pf_qos_tc_lsp_attribute =
+	__ATTR(lsp, 0644, pf_qos_tc_lsp_show, pf_qos_tc_lsp_store);
+static struct kobj_attribute pf_qos_tc_max_bw_attribute =
+	__ATTR(max_bw, 0644, pf_qos_tc_max_bw_show, pf_qos_tc_max_bw_store);
+
+static struct attribute *pf_qos_tc_attrs[] = {
+	&pf_qos_tc_priority_attribute.attr,
+	&pf_qos_tc_lsp_attribute.attr,
+	&pf_qos_tc_max_bw_attribute.attr,
+	NULL,
+};
+
+static struct attribute_group pf_qos_tc_group = {
+	.attrs = pf_qos_tc_attrs,
+};
+
+/**
+ * create_qos_tc_sysfs - create sysfs hierarchy for PF QOS traffic classes.
+ * @pdev:       PCI device information struct
+ * @tc:		Pointer to preallocated array of 8 kobjects.
+ * @parent:     QOS parent
+ * @attr_group:	attribute group to assign to tc kobject
+ *
+ * Creates a kobject for PF QOS traffic classes and assigns attributes to it.
+ * Assumes mem is preallocated
+ **/
+static int create_qos_tc_sysfs(struct pci_dev *pdev, struct kobject **tc,
+			       struct kobject *parent,
+			       struct attribute_group *attr_group)
+{
+	struct kobject *pf_qos_tc;
+	char kname[2];
+	int ret, i;
+
+	for (i = 0; i < VFD_NUM_TC; i++) {
+		int length = snprintf(kname, sizeof(kname), "%d", i);
+
+		if (length >= sizeof(kname)) {
+			dev_err(&pdev->dev,
+				"cannot request %d tcs, try again with smaller number of vfs\n",
+				i);
+			--i;
+			ret = -EINVAL;
+			goto err_qos_tc_sysfs;
+		}
+		pf_qos_tc = kobject_create_and_add(kname, parent);
+
+		if (!pf_qos_tc) {
+			dev_err(&pdev->dev,
+				"failed to create VF kobj: %s\n", kname);
+			i--;
+			ret = -ENOMEM;
+			goto err_qos_tc_sysfs;
+		}
+		dev_info(&pdev->dev, "created VF %s sysfs", parent->name);
+		tc[i] = pf_qos_tc;
+
+		/* create VF sys attr */
+		ret = sysfs_create_group(tc[i], attr_group);
+		if (ret) {
+			dev_err(&pdev->dev, "failed to create PF QOS TC attributes: %d",
+				i);
+			goto err_qos_tc_sysfs;
+		}
+	}
+
+	return 0;
+err_qos_tc_sysfs:
+	for (; i >= 0; i--)
+		kobject_put(tc[i]);
+	return ret;
+}
+
+/**
+ * create_vfs_sysfs - create sysfs hierarchy for VF
+ * @pdev:	PCI device information struct
+ * @vfd_obj:	VF-d kobjects information struct
+ *
+ * Creates a kobject for Virtual Function and assigns attributes to it.
+ **/
+static int create_vfs_sysfs(struct pci_dev *pdev, struct vfd_objects *vfd_obj)
+{
+	struct kobject *vf_kobj;
+	struct vfd_vf_obj *vfs;
+	char kname[4];
+	int ret, i;
+
+	for (i = 0; i < vfd_obj->num_vfs; i++) {
+		int length = snprintf(kname, sizeof(kname), "%d", i);
+
+		if (length >= sizeof(kname)) {
+			dev_err(&pdev->dev,
+				"cannot request %d vfs, try again with smaller number of vfs\n",
+				i);
+			--i;
+			ret = -EINVAL;
+			goto err_vfs_sysfs;
+		}
+
+		vfs = &vfd_obj->vfs[i];
+
+		vf_kobj = kobject_create_and_add(kname, vfd_obj->sriov_kobj);
+		if (!vf_kobj) {
+			dev_err(&pdev->dev,
+				"failed to create VF kobj: %s\n", kname);
+			i--;
+			ret = -ENOMEM;
+			goto err_vfs_sysfs;
+		}
+		dev_info(&pdev->dev, "created VF %s sysfs", vf_kobj->name);
+		vfs->vf_kobj = vf_kobj;
+
+		vfs->vf_qos_kobj = kobject_create_and_add("qos", vfs->vf_kobj);
+		create_qos_tc_sysfs(pdev, vfs->vf_tc_kobjs, vfs->vf_qos_kobj,
+				    &vf_qos_tc_group);
+
+		/* create VF sys attr */
+		ret = sysfs_create_group(vfs->vf_kobj, &vfd_group);
+		if (ret) {
+			dev_err(&pdev->dev, "failed to create VF sys attribute: %d",
+				i);
+			goto err_vfs_sysfs;
+		}
+		/* create VF stats sys attr */
+		ret = sysfs_create_group(vfs->vf_kobj, &stats_group);
+		if (ret) {
+			dev_err(&pdev->dev, "failed to create VF stats attribute: %d",
+				i);
+			goto err_vfs_sysfs;
+		}
+
+		/* create VF qos sys attr */
+		ret = sysfs_create_group(vfs->vf_qos_kobj, &qos_group);
+		if (ret) {
+			dev_err(&pdev->dev, "failed to create VF qos attribute: %d",
+				i);
+			goto err_vfs_sysfs;
+		}
+	}
+
+	return 0;
+
+err_vfs_sysfs:
+	for (; i >= 0; i--)
+		kobject_put(vfd_obj->vfs[i].vf_kobj);
+	return ret;
+}
+
+/**
+ * create_vfd_sysfs - create sysfs hierarchy used by VF-d
+ * @pdev:		PCI device information struct
+ * @num_alloc_vfs:	number of VFs to allocate
+ *
+ * If the kobjects were not able to be created, NULL will be returned.
+ **/
+struct vfd_objects *create_vfd_sysfs(struct pci_dev *pdev, int num_alloc_vfs)
+{
+	struct vfd_qos_objects *qos_objs;
+	struct vfd_objects *vfd_obj;
+	int ret;
+
+	vfd_obj = kzalloc(sizeof(*vfd_obj), GFP_KERNEL);
+	if (!vfd_obj)
+		return NULL;
+
+	qos_objs = kzalloc(sizeof(*qos_objs), GFP_KERNEL);
+	if (!qos_objs)
+		goto err_qos;
+
+	vfd_obj->vfs = kcalloc(num_alloc_vfs, sizeof(*vfd_obj->vfs),
+			       GFP_KERNEL);
+	if (!vfd_obj->vfs)
+		goto err_vfs;
+
+	vfd_obj->qos = qos_objs;
+	vfd_obj->num_vfs = num_alloc_vfs;
+	vfd_obj->sriov_kobj = kobject_create_and_add("sriov", &pdev->dev.kobj);
+	if (!vfd_obj->sriov_kobj)
+		goto err_sysfs;
+	dev_info(&pdev->dev, "created %s sysfs", vfd_obj->sriov_kobj->name);
+
+	qos_objs->qos_kobj = kobject_create_and_add("qos",
+						    vfd_obj->sriov_kobj);
+	if (!qos_objs->qos_kobj) {
+		dev_err(&pdev->dev, "failed to create VF qos pf kobject");
+		goto err_pf_qos;
+	}
+
+	ret = create_vfs_sysfs(pdev, vfd_obj);
+	if (ret)
+		goto err_pf_qos;
+
+	create_qos_tc_sysfs(pdev, qos_objs->pf_qos_kobjs, qos_objs->qos_kobj,
+			    &pf_qos_tc_group);
+	/* create PF qos sys attr */
+	ret = sysfs_create_group(qos_objs->qos_kobj, &pf_qos_group);
+	if (ret) {
+		dev_err(&pdev->dev, "failed to create PF qos sys attribute");
+		goto err_pf_qos;
+	}
+
+	/* create PF attrs */
+	ret = sysfs_create_group(vfd_obj->sriov_kobj, &pf_attr_group);
+	if (ret) {
+		dev_err(&pdev->dev, "failed to create PF attr sys attribute");
+		goto err_pf_qos;
+	}
+
+	return vfd_obj;
+
+err_pf_qos:
+	kobject_put(vfd_obj->sriov_kobj);
+err_sysfs:
+	kfree(vfd_obj->vfs);
+err_vfs:
+	kfree(qos_objs);
+err_qos:
+	kfree(vfd_obj);
+	return NULL;
+}
+
+static void free_vfd_vf(struct pci_dev *pdev, struct vfd_vf_obj *vf)
+{
+	int i;
+
+	for (i = 0; i < VFD_NUM_TC; i++) {
+		dev_dbg(&pdev->dev, "deleting VF %s tc",
+			vf->vf_tc_kobjs[i]->name);
+		kobject_put(vf->vf_tc_kobjs[i]);
+	}
+
+	dev_info(&pdev->dev, "deleting VF %s sysfs", vf->vf_qos_kobj->name);
+	kobject_put(vf->vf_qos_kobj);
+	dev_info(&pdev->dev, "deleting VF %s sysfs", vf->vf_kobj->name);
+	kobject_put(vf->vf_kobj);
+}
+
+/**
+ * destroy_vfd_sysfs - destroy sysfs hierarchy used by VF-d
+ * @pdev:	PCI device information struct
+ * @vfd_obj:	VF-d kobjects information struct
+ **/
+void destroy_vfd_sysfs(struct pci_dev *pdev, struct vfd_objects *vfd_obj)
+{
+	int i;
+
+	for (i = 0; i < vfd_obj->num_vfs; i++)
+		free_vfd_vf(pdev, &vfd_obj->vfs[i]);
+
+	for (i = 0; i < VFD_NUM_TC; i++) {
+		dev_info(&pdev->dev, "deleting sriov qos %s sysfs",
+			 vfd_obj->qos->pf_qos_kobjs[i]->name);
+		kobject_put(vfd_obj->qos->pf_qos_kobjs[i]);
+	}
+
+	dev_info(&pdev->dev, "deleting %s sysfs",
+		 vfd_obj->qos->qos_kobj->name);
+	kobject_put(vfd_obj->qos->qos_kobj);
+
+	dev_info(&pdev->dev, "deleting %s sysfs", vfd_obj->sriov_kobj->name);
+	kobject_put(vfd_obj->sriov_kobj);
+	kfree(vfd_obj->qos);
+	kfree(vfd_obj->vfs);
+	kfree(vfd_obj);
+}
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/kcompat_vfd.h linux-5.4.86.new/drivers/net/ethernet/intel/i40e/kcompat_vfd.h
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/kcompat_vfd.h	1969-12-31 19:00:00.000000000 -0500
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/kcompat_vfd.h	2024-05-13 03:58:25.372491940 -0400
@@ -0,0 +1,189 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
+
+#ifndef _KCOMPAT_VFD_H_
+#define _KCOMPAT_VFD_H_
+
+#define VFD_PROMISC_OFF		0x00
+#define VFD_PROMISC_UNICAST	0x01
+#define VFD_PROMISC_MULTICAST	0x02
+
+#define VFD_LINKSTATE_OFF	0x00
+#define VFD_LINKSTATE_ON	0x01
+#define VFD_LINKSTATE_AUTO	0x02
+
+#define VFD_EGRESS_MIRROR_OFF	-1
+#define VFD_INGRESS_MIRROR_OFF	-1
+
+#define VFD_QUEUE_TYPE_RSS	0x00
+#define VFD_QUEUE_TYPE_QOS	0x01
+
+#define VFD_NUM_TC		0x8
+
+/**
+ * struct vfd_objects - VF-d kobjects information struct
+ * @num_vfs:	number of VFs allocated
+ * @sriov_kobj:	pointer to the top sriov kobject
+ * @vf_kobj:	array of pointer to each VF's kobjects
+ */
+struct vfd_objects {
+	int num_vfs;
+	struct kobject *sriov_kobj;
+	struct vfd_vf_obj *vfs;
+	struct vfd_qos_objects *qos;
+};
+
+/**
+ * struct vfd_vf_obj - VF-d VF kobjects information struct
+ * @vf_kobj:		pointer to VF qos kobject
+ * @vf_qos_kobj:	pointer to VF kobject
+ * @vf_tc_kobj:		pointer to VF TC kobjects
+ */
+struct vfd_vf_obj {
+	struct kobject *vf_qos_kobj;
+	struct kobject *vf_kobj;
+	struct kobject *vf_tc_kobjs[VFD_NUM_TC];
+};
+
+/**
+ * struct vfd_qos_objects - VF-d qos kobjects information struct
+ * @qos_kobj:		pointer to PF qos kobject
+ * @pf_qos_kobj:	pointer to PF TC kobjects
+ */
+struct vfd_qos_objects {
+	struct kobject *qos_kobj;
+	struct kobject *pf_qos_kobjs[VFD_NUM_TC];
+};
+
+struct vfd_macaddr {
+	u8 mac[ETH_ALEN];
+	struct list_head list;
+};
+
+#define VFD_LINK_SPEED_2_5GB_SHIFT		0x0
+#define VFD_LINK_SPEED_100MB_SHIFT		0x1
+#define VFD_LINK_SPEED_1GB_SHIFT		0x2
+#define VFD_LINK_SPEED_10GB_SHIFT		0x3
+#define VFD_LINK_SPEED_40GB_SHIFT		0x4
+#define VFD_LINK_SPEED_20GB_SHIFT		0x5
+#define VFD_LINK_SPEED_25GB_SHIFT		0x6
+#define VFD_LINK_SPEED_5GB_SHIFT		0x7
+
+
+enum vfd_link_speed {
+	VFD_LINK_SPEED_UNKNOWN	= 0,
+	VFD_LINK_SPEED_100MB	= BIT(VFD_LINK_SPEED_100MB_SHIFT),
+	VFD_LINK_SPEED_1GB	= BIT(VFD_LINK_SPEED_1GB_SHIFT),
+	VFD_LINK_SPEED_2_5GB	= BIT(VFD_LINK_SPEED_2_5GB_SHIFT),
+	VFD_LINK_SPEED_5GB	= BIT(VFD_LINK_SPEED_5GB_SHIFT),
+	VFD_LINK_SPEED_10GB	= BIT(VFD_LINK_SPEED_10GB_SHIFT),
+	VFD_LINK_SPEED_40GB	= BIT(VFD_LINK_SPEED_40GB_SHIFT),
+	VFD_LINK_SPEED_20GB	= BIT(VFD_LINK_SPEED_20GB_SHIFT),
+	VFD_LINK_SPEED_25GB	= BIT(VFD_LINK_SPEED_25GB_SHIFT),
+};
+
+struct vfd_ops {
+	int (*get_trunk)(struct pci_dev *pdev, int vf_id, unsigned long *buff);
+	int (*set_trunk)(struct pci_dev *pdev, int vf_id,
+			 const unsigned long *buff);
+	int (*get_vlan_mirror)(struct pci_dev *pdev, int vf_id,
+			       unsigned long *buff);
+	int (*set_vlan_mirror)(struct pci_dev *pdev, int vf_id,
+			       const unsigned long *buff);
+	int (*get_egress_mirror)(struct pci_dev *pdev, int vf_id, int *data);
+	int (*set_egress_mirror)(struct pci_dev *pdev, int vf_id,
+				 const int data);
+	int (*get_ingress_mirror)(struct pci_dev *pdev, int vf_id, int *data);
+	int (*set_ingress_mirror)(struct pci_dev *pdev, int vf_id,
+				  const int data);
+	int (*get_mac_anti_spoof)(struct pci_dev *pdev, int vf_id, bool *data);
+	int (*set_mac_anti_spoof)(struct pci_dev *pdev, int vf_id,
+				  const bool data);
+	int (*get_vlan_anti_spoof)(struct pci_dev *pdev, int vf_id, bool *data);
+	int (*set_vlan_anti_spoof)(struct pci_dev *pdev, int vf_id,
+				   const bool data);
+	int (*get_allow_untagged)(struct pci_dev *pdev, int vf_id, bool *data);
+	int (*set_allow_untagged)(struct pci_dev *pdev, int vf_id,
+				  const bool data);
+	int (*get_loopback)(struct pci_dev *pdev, int vf_id, bool *data);
+	int (*set_loopback)(struct pci_dev *pdev, int vf_id, const bool data);
+	int (*get_mac)(struct pci_dev *pdev, int vf_id, u8 *macaddr);
+	int (*set_mac)(struct pci_dev *pdev, int vf_id, const u8 *macaddr);
+	int (*get_mac_list)(struct pci_dev *pdev, int vf_id,
+			    struct list_head *mac_list);
+	int (*add_macs_to_list)(struct pci_dev *pdev, int vf_id,
+				struct list_head *mac_list);
+	int (*rem_macs_from_list)(struct pci_dev *pdev, int vf_id,
+				  struct list_head *mac_list);
+	int (*get_promisc)(struct pci_dev *pdev, int vf_id, u8 *data);
+	int (*set_promisc)(struct pci_dev *pdev, int vf_id, const u8 data);
+	int (*get_vlan_strip)(struct pci_dev *pdev, int vf_id, bool *data);
+	int (*set_vlan_strip)(struct pci_dev *pdev, int vf_id, const bool data);
+	int (*get_link_state)(struct pci_dev *pdev, int vf_id, bool *enabled,
+			      enum vfd_link_speed *link_speed);
+	int (*set_link_state)(struct pci_dev *pdev, int vf_id, const u8 data);
+	int (*get_max_tx_rate)(struct pci_dev *pdev, int vf_id,
+			       unsigned int *max_tx_rate);
+	int (*set_max_tx_rate)(struct pci_dev *pdev, int vf_id,
+			       unsigned int *max_tx_rate);
+	int (*get_min_tx_rate)(struct kobject *,
+			       struct kobj_attribute *, char *);
+	int (*set_min_tx_rate)(struct kobject *, struct kobj_attribute *,
+			       const char *, size_t);
+	int (*get_spoofcheck)(struct kobject *,
+			      struct kobj_attribute *, char *);
+	int (*set_spoofcheck)(struct kobject *, struct kobj_attribute *,
+			      const char *, size_t);
+	int (*get_trust)(struct kobject *,
+			 struct kobj_attribute *, char *);
+	int (*set_trust)(struct kobject *, struct kobj_attribute *,
+			 const char *, size_t);
+	int (*get_vf_enable)(struct pci_dev *pdev, int vf_id, bool *data);
+	int (*set_vf_enable)(struct pci_dev *pdev, int vf_id, const bool data);
+	int (*get_rx_bytes)  (struct pci_dev *pdev, int vf_id, u64 *data);
+	int (*get_rx_dropped)(struct pci_dev *pdev, int vf_id, u64 *data);
+	int (*get_rx_packets)(struct pci_dev *pdev, int vf_id, u64 *data);
+	int (*get_tx_bytes)  (struct pci_dev *pdev, int vf_id, u64 *data);
+	int (*get_tx_dropped)(struct pci_dev *pdev, int vf_id, u64 *data);
+	int (*get_tx_packets)(struct pci_dev *pdev, int vf_id, u64 *data);
+	int (*get_tx_spoofed)(struct pci_dev *pdev, int vf_id, u64 *data);
+	int (*get_tx_errors)(struct pci_dev *pdev, int vf_id, u64 *data);
+	int (*reset_stats)(struct pci_dev *pdev, int vf_id);
+	int (*set_vf_bw_share)(struct pci_dev *pdev, int vf_id, u8 bw_share);
+	int (*get_vf_bw_share)(struct pci_dev *pdev, int vf_id, u8 *bw_share);
+	int (*set_pf_qos_apply)(struct pci_dev *pdev);
+	int (*get_pf_ingress_mirror)(struct pci_dev *pdev, int *data);
+	int (*set_pf_ingress_mirror)(struct pci_dev *pdev, const int data);
+	int (*get_pf_egress_mirror)(struct pci_dev *pdev, int *data);
+	int (*set_pf_egress_mirror)(struct pci_dev *pdev, const int data);
+	int (*get_pf_tpid)(struct pci_dev *pdev, u16 *data);
+	int (*set_pf_tpid)(struct pci_dev *pdev, const u16 data);
+	int (*get_num_queues)(struct pci_dev *pdev, int vf_id, int *num_queues);
+	int (*set_num_queues)(struct pci_dev *pdev, int vf_id, const int num_queues);
+	int (*get_trust_state)(struct pci_dev *pdev, int vf_id, bool *data);
+	int (*set_trust_state)(struct pci_dev *pdev, int vf_id, bool data);
+	int (*get_queue_type)(struct pci_dev *pdev, int vf_id, u8 *data);
+	int (*set_queue_type)(struct pci_dev *pdev, int vf_id, const u8 data);
+	int (*get_allow_bcast)(struct pci_dev *pdev, int vf_id, bool *data);
+	int (*set_allow_bcast)(struct pci_dev *pdev, int vf_id, const bool data);
+	int (*get_pf_qos_tc_max_bw)(struct pci_dev *pdev, int tc, u16 *req_bw);
+	int (*set_pf_qos_tc_max_bw)(struct pci_dev *pdev, int tc, u16 req_bw);
+	int (*get_pf_qos_tc_lsp)(struct pci_dev *pdev, int tc, bool *on);
+	int (*set_pf_qos_tc_lsp)(struct pci_dev *pdev, int tc, bool on);
+	int (*get_pf_qos_tc_priority)(struct pci_dev *pdev, int tc,
+				      char *tc_bitmap);
+	int (*set_pf_qos_tc_priority)(struct pci_dev *pdev, int tc,
+				      char tc_bitmap);
+	int (*get_vf_qos_tc_share)(struct pci_dev *pdev, int vf_id, int tc,
+				   u8 *share);
+	int (*set_vf_qos_tc_share)(struct pci_dev *pdev, int vf_id, int tc,
+				   u8 share);
+	int (*get_vf_max_tc_tx_rate)(struct pci_dev *pdev, int vf_id, int tc,
+				     int *rate);
+	int (*set_vf_max_tc_tx_rate)(struct pci_dev *pdev, int vf_id, int tc,
+				     int rate);
+};
+
+extern const struct vfd_ops *vfd_ops;
+
+#endif /* _KCOMPAT_VFD_H_ */
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/Makefile linux-5.4.86.new/drivers/net/ethernet/intel/i40e/Makefile
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/Makefile	2024-05-10 01:26:45.309079402 -0400
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/Makefile	2024-05-13 04:05:04.798919590 -0400
@@ -21,9 +21,11 @@
 	i40e_diag.o	\
 	i40e_txrx.o	\
 	i40e_ptp.o	\
+	i40e_filters.o \
 	i40e_ddp.o \
 	i40e_client.o   \
-	i40e_virtchnl_pf.o \
-	i40e_xsk.o
+	i40e_virtchnl_pf.o 
 
 i40e-$(CONFIG_I40E_DCB) += i40e_dcb.o i40e_dcb_nl.o
+i40e-objs += kcompat.o
+i40e-objs += kcompat_vfd.o
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/Module.supported linux-5.4.86.new/drivers/net/ethernet/intel/i40e/Module.supported
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/Module.supported	1969-12-31 19:00:00.000000000 -0500
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/Module.supported	2024-05-13 03:58:25.372491940 -0400
@@ -0,0 +1 @@
+i40e.ko external
diff -Nru linux-5.4.86/drivers/net/ethernet/intel/i40e/virtchnl.h linux-5.4.86.new/drivers/net/ethernet/intel/i40e/virtchnl.h
--- linux-5.4.86/drivers/net/ethernet/intel/i40e/virtchnl.h	1969-12-31 19:00:00.000000000 -0500
+++ linux-5.4.86.new/drivers/net/ethernet/intel/i40e/virtchnl.h	2024-05-13 03:58:25.372491940 -0400
@@ -0,0 +1,2197 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright(c) 2013 - 2021 Intel Corporation. */
+
+#ifndef _VIRTCHNL_H_
+#define _VIRTCHNL_H_
+
+/* Description:
+ * This header file describes the Virtual Function (VF) - Physical Function
+ * (PF) communication protocol used by the drivers for all devices starting
+ * from our 40G product line
+ *
+ * Admin queue buffer usage:
+ * desc->opcode is always aqc_opc_send_msg_to_pf
+ * flags, retval, datalen, and data addr are all used normally.
+ * The Firmware copies the cookie fields when sending messages between the
+ * PF and VF, but uses all other fields internally. Due to this limitation,
+ * we must send all messages as "indirect", i.e. using an external buffer.
+ *
+ * All the VSI indexes are relative to the VF. Each VF can have maximum of
+ * three VSIs. All the queue indexes are relative to the VSI.  Each VF can
+ * have a maximum of sixteen queues for all of its VSIs.
+ *
+ * The PF is required to return a status code in v_retval for all messages
+ * except RESET_VF, which does not require any response. The returned value
+ * is of virtchnl_status_code type, defined in the shared type.h.
+ *
+ * In general, VF driver initialization should roughly follow the order of
+ * these opcodes. The VF driver must first validate the API version of the
+ * PF driver, then request a reset, then get resources, then configure
+ * queues and interrupts. After these operations are complete, the VF
+ * driver may start its queues, optionally add MAC and VLAN filters, and
+ * process traffic.
+ */
+
+/* START GENERIC DEFINES
+ * Need to ensure the following enums and defines hold the same meaning and
+ * value in current and future projects
+ */
+
+/* Error Codes */
+enum virtchnl_status_code {
+	VIRTCHNL_STATUS_SUCCESS				= 0,
+	VIRTCHNL_STATUS_ERR_PARAM			= -5,
+	VIRTCHNL_STATUS_ERR_NO_MEMORY			= -18,
+	VIRTCHNL_STATUS_ERR_OPCODE_MISMATCH		= -38,
+	VIRTCHNL_STATUS_ERR_CQP_COMPL_ERROR		= -39,
+	VIRTCHNL_STATUS_ERR_INVALID_VF_ID		= -40,
+	VIRTCHNL_STATUS_ERR_ADMIN_QUEUE_ERROR		= -53,
+	VIRTCHNL_STATUS_ERR_NOT_SUPPORTED		= -64,
+};
+
+/* Backward compatibility */
+#define VIRTCHNL_ERR_PARAM VIRTCHNL_STATUS_ERR_PARAM
+#define VIRTCHNL_STATUS_NOT_SUPPORTED VIRTCHNL_STATUS_ERR_NOT_SUPPORTED
+
+#define VIRTCHNL_LINK_SPEED_2_5GB_SHIFT		0x0
+#define VIRTCHNL_LINK_SPEED_100MB_SHIFT		0x1
+#define VIRTCHNL_LINK_SPEED_1000MB_SHIFT	0x2
+#define VIRTCHNL_LINK_SPEED_10GB_SHIFT		0x3
+#define VIRTCHNL_LINK_SPEED_40GB_SHIFT		0x4
+#define VIRTCHNL_LINK_SPEED_20GB_SHIFT		0x5
+#define VIRTCHNL_LINK_SPEED_25GB_SHIFT		0x6
+#define VIRTCHNL_LINK_SPEED_5GB_SHIFT		0x7
+
+enum virtchnl_link_speed {
+	VIRTCHNL_LINK_SPEED_UNKNOWN	= 0,
+	VIRTCHNL_LINK_SPEED_100MB	= BIT(VIRTCHNL_LINK_SPEED_100MB_SHIFT),
+	VIRTCHNL_LINK_SPEED_1GB		= BIT(VIRTCHNL_LINK_SPEED_1000MB_SHIFT),
+	VIRTCHNL_LINK_SPEED_10GB	= BIT(VIRTCHNL_LINK_SPEED_10GB_SHIFT),
+	VIRTCHNL_LINK_SPEED_40GB	= BIT(VIRTCHNL_LINK_SPEED_40GB_SHIFT),
+	VIRTCHNL_LINK_SPEED_20GB	= BIT(VIRTCHNL_LINK_SPEED_20GB_SHIFT),
+	VIRTCHNL_LINK_SPEED_25GB	= BIT(VIRTCHNL_LINK_SPEED_25GB_SHIFT),
+	VIRTCHNL_LINK_SPEED_2_5GB	= BIT(VIRTCHNL_LINK_SPEED_2_5GB_SHIFT),
+	VIRTCHNL_LINK_SPEED_5GB		= BIT(VIRTCHNL_LINK_SPEED_5GB_SHIFT),
+};
+
+/* for hsplit_0 field of Rx HMC context */
+/* deprecated with AVF 1.0 */
+enum virtchnl_rx_hsplit {
+	VIRTCHNL_RX_HSPLIT_NO_SPLIT      = 0,
+	VIRTCHNL_RX_HSPLIT_SPLIT_L2      = 1,
+	VIRTCHNL_RX_HSPLIT_SPLIT_IP      = 2,
+	VIRTCHNL_RX_HSPLIT_SPLIT_TCP_UDP = 4,
+	VIRTCHNL_RX_HSPLIT_SPLIT_SCTP    = 8,
+};
+
+enum virtchnl_bw_limit_type {
+	VIRTCHNL_BW_SHAPER = 0,
+};
+
+/* END GENERIC DEFINES */
+
+/* Opcodes for VF-PF communication. These are placed in the v_opcode field
+ * of the virtchnl_msg structure.
+ */
+enum virtchnl_ops {
+/* The PF sends status change events to VFs using
+ * the VIRTCHNL_OP_EVENT opcode.
+ * VFs send requests to the PF using the other ops.
+ * Use of "advanced opcode" features must be negotiated as part of capabilities
+ * exchange and are not considered part of base mode feature set.
+ */
+	VIRTCHNL_OP_UNKNOWN = 0,
+	VIRTCHNL_OP_VERSION = 1, /* must ALWAYS be 1 */
+	VIRTCHNL_OP_RESET_VF = 2,
+	VIRTCHNL_OP_GET_VF_RESOURCES = 3,
+	VIRTCHNL_OP_CONFIG_TX_QUEUE = 4,
+	VIRTCHNL_OP_CONFIG_RX_QUEUE = 5,
+	VIRTCHNL_OP_CONFIG_VSI_QUEUES = 6,
+	VIRTCHNL_OP_CONFIG_IRQ_MAP = 7,
+	VIRTCHNL_OP_ENABLE_QUEUES = 8,
+	VIRTCHNL_OP_DISABLE_QUEUES = 9,
+	VIRTCHNL_OP_ADD_ETH_ADDR = 10,
+	VIRTCHNL_OP_DEL_ETH_ADDR = 11,
+	VIRTCHNL_OP_ADD_VLAN = 12,
+	VIRTCHNL_OP_DEL_VLAN = 13,
+	VIRTCHNL_OP_CONFIG_PROMISCUOUS_MODE = 14,
+	VIRTCHNL_OP_GET_STATS = 15,
+	VIRTCHNL_OP_RSVD = 16,
+	VIRTCHNL_OP_EVENT = 17, /* must ALWAYS be 17 */
+	/* opcode 19 is reserved */
+	VIRTCHNL_OP_IWARP = 20, /* advanced opcode */
+	VIRTCHNL_OP_RDMA = VIRTCHNL_OP_IWARP,
+	VIRTCHNL_OP_CONFIG_IWARP_IRQ_MAP = 21, /* advanced opcode */
+	VIRTCHNL_OP_CONFIG_RDMA_IRQ_MAP = VIRTCHNL_OP_CONFIG_IWARP_IRQ_MAP,
+	VIRTCHNL_OP_RELEASE_IWARP_IRQ_MAP = 22, /* advanced opcode */
+	VIRTCHNL_OP_RELEASE_RDMA_IRQ_MAP = VIRTCHNL_OP_RELEASE_IWARP_IRQ_MAP,
+	VIRTCHNL_OP_CONFIG_RSS_KEY = 23,
+	VIRTCHNL_OP_CONFIG_RSS_LUT = 24,
+	VIRTCHNL_OP_GET_RSS_HENA_CAPS = 25,
+	VIRTCHNL_OP_SET_RSS_HENA = 26,
+	VIRTCHNL_OP_ENABLE_VLAN_STRIPPING = 27,
+	VIRTCHNL_OP_DISABLE_VLAN_STRIPPING = 28,
+	VIRTCHNL_OP_REQUEST_QUEUES = 29,
+	VIRTCHNL_OP_ENABLE_CHANNELS = 30,
+	VIRTCHNL_OP_DISABLE_CHANNELS = 31,
+	VIRTCHNL_OP_ADD_CLOUD_FILTER = 32,
+	VIRTCHNL_OP_DEL_CLOUD_FILTER = 33,
+	/* opcode 34 is reserved */
+	/* opcodes 38, 39, 40, 41, 42 and 43 are reserved */
+	/* opcode 44 is reserved */
+	VIRTCHNL_OP_ADD_RSS_CFG = 45,
+	VIRTCHNL_OP_DEL_RSS_CFG = 46,
+	VIRTCHNL_OP_ADD_FDIR_FILTER = 47,
+	VIRTCHNL_OP_DEL_FDIR_FILTER = 48,
+	VIRTCHNL_OP_GET_MAX_RSS_QREGION = 50,
+	VIRTCHNL_OP_GET_OFFLOAD_VLAN_V2_CAPS = 51,
+	VIRTCHNL_OP_ADD_VLAN_V2 = 52,
+	VIRTCHNL_OP_DEL_VLAN_V2 = 53,
+	VIRTCHNL_OP_ENABLE_VLAN_STRIPPING_V2 = 54,
+	VIRTCHNL_OP_DISABLE_VLAN_STRIPPING_V2 = 55,
+	VIRTCHNL_OP_ENABLE_VLAN_INSERTION_V2 = 56,
+	VIRTCHNL_OP_DISABLE_VLAN_INSERTION_V2 = 57,
+	VIRTCHNL_OP_ENABLE_VLAN_FILTERING_V2 = 58,
+	VIRTCHNL_OP_DISABLE_VLAN_FILTERING_V2 = 59,
+	/* opcodes 60 through 65 are reserved */
+	VIRTCHNL_OP_GET_QOS_CAPS = 66,
+	VIRTCHNL_OP_CONFIG_QUEUE_TC_MAP = 67,
+	/* opcode 68, 69 are reserved */
+	VIRTCHNL_OP_ENABLE_QUEUES_V2 = 107,
+	VIRTCHNL_OP_DISABLE_QUEUES_V2 = 108,
+	VIRTCHNL_OP_MAP_QUEUE_VECTOR = 111,
+	VIRTCHNL_OP_MAX,
+};
+
+static inline const char *virtchnl_op_str(enum virtchnl_ops v_opcode)
+{
+	switch (v_opcode) {
+	case VIRTCHNL_OP_UNKNOWN:
+		return "VIRTCHNL_OP_UNKNOWN";
+	case VIRTCHNL_OP_VERSION:
+		return "VIRTCHNL_OP_VERSION";
+	case VIRTCHNL_OP_RESET_VF:
+		return "VIRTCHNL_OP_RESET_VF";
+	case VIRTCHNL_OP_GET_VF_RESOURCES:
+		return "VIRTCHNL_OP_GET_VF_RESOURCES";
+	case VIRTCHNL_OP_CONFIG_TX_QUEUE:
+		return "VIRTCHNL_OP_CONFIG_TX_QUEUE";
+	case VIRTCHNL_OP_CONFIG_RX_QUEUE:
+		return "VIRTCHNL_OP_CONFIG_RX_QUEUE";
+	case VIRTCHNL_OP_CONFIG_VSI_QUEUES:
+		return "VIRTCHNL_OP_CONFIG_VSI_QUEUES";
+	case VIRTCHNL_OP_CONFIG_IRQ_MAP:
+		return "VIRTCHNL_OP_CONFIG_IRQ_MAP";
+	case VIRTCHNL_OP_ENABLE_QUEUES:
+		return "VIRTCHNL_OP_ENABLE_QUEUES";
+	case VIRTCHNL_OP_DISABLE_QUEUES:
+		return "VIRTCHNL_OP_DISABLE_QUEUES";
+	case VIRTCHNL_OP_ADD_ETH_ADDR:
+		return "VIRTCHNL_OP_ADD_ETH_ADDR";
+	case VIRTCHNL_OP_DEL_ETH_ADDR:
+		return "VIRTCHNL_OP_DEL_ETH_ADDR";
+	case VIRTCHNL_OP_ADD_VLAN:
+		return "VIRTCHNL_OP_ADD_VLAN";
+	case VIRTCHNL_OP_DEL_VLAN:
+		return "VIRTCHNL_OP_DEL_VLAN";
+	case VIRTCHNL_OP_CONFIG_PROMISCUOUS_MODE:
+		return "VIRTCHNL_OP_CONFIG_PROMISCUOUS_MODE";
+	case VIRTCHNL_OP_GET_STATS:
+		return "VIRTCHNL_OP_GET_STATS";
+	case VIRTCHNL_OP_RSVD:
+		return "VIRTCHNL_OP_RSVD";
+	case VIRTCHNL_OP_EVENT:
+		return "VIRTCHNL_OP_EVENT";
+	case VIRTCHNL_OP_RDMA:
+		return "VIRTCHNL_OP_RDMA";
+	case VIRTCHNL_OP_CONFIG_RDMA_IRQ_MAP:
+		return "VIRTCHNL_OP_CONFIG_RDMA_IRQ_MAP:";
+	case VIRTCHNL_OP_RELEASE_RDMA_IRQ_MAP:
+		return "VIRTCHNL_OP_RELEASE_RDMA_IRQ_MAP";
+	case VIRTCHNL_OP_CONFIG_RSS_KEY:
+		return "VIRTCHNL_OP_CONFIG_RSS_KEY";
+	case VIRTCHNL_OP_CONFIG_RSS_LUT:
+		return "VIRTCHNL_OP_CONFIG_RSS_LUT";
+	case VIRTCHNL_OP_GET_RSS_HENA_CAPS:
+		return "VIRTCHNL_OP_GET_RSS_HENA_CAPS";
+	case VIRTCHNL_OP_SET_RSS_HENA:
+		return "VIRTCHNL_OP_SET_RSS_HENA";
+	case VIRTCHNL_OP_ENABLE_VLAN_STRIPPING:
+		return "VIRTCHNL_OP_ENABLE_VLAN_STRIPPING";
+	case VIRTCHNL_OP_DISABLE_VLAN_STRIPPING:
+		return "VIRTCHNL_OP_DISABLE_VLAN_STRIPPING";
+	case VIRTCHNL_OP_REQUEST_QUEUES:
+		return "VIRTCHNL_OP_REQUEST_QUEUES";
+	case VIRTCHNL_OP_ENABLE_CHANNELS:
+		return "VIRTCHNL_OP_ENABLE_CHANNELS";
+	case VIRTCHNL_OP_DISABLE_CHANNELS:
+		return "VIRTCHNL_OP_DISABLE_CHANNELS";
+	case VIRTCHNL_OP_ADD_CLOUD_FILTER:
+		return "VIRTCHNL_OP_ADD_CLOUD_FILTER";
+	case VIRTCHNL_OP_DEL_CLOUD_FILTER:
+		return "VIRTCHNL_OP_DEL_CLOUD_FILTER";
+	case VIRTCHNL_OP_ADD_RSS_CFG:
+		return "VIRTCHNL_OP_ADD_RSS_CFG";
+	case VIRTCHNL_OP_DEL_RSS_CFG:
+		return "VIRTCHNL_OP_DEL_RSS_CFG";
+	case VIRTCHNL_OP_ADD_FDIR_FILTER:
+		return "VIRTCHNL_OP_ADD_FDIR_FILTER";
+	case VIRTCHNL_OP_DEL_FDIR_FILTER:
+		return "VIRTCHNL_OP_DEL_FDIR_FILTER";
+	case VIRTCHNL_OP_GET_MAX_RSS_QREGION:
+		return "VIRTCHNL_OP_GET_MAX_RSS_QREGION";
+	case VIRTCHNL_OP_ENABLE_QUEUES_V2:
+		return "VIRTCHNL_OP_ENABLE_QUEUES_V2";
+	case VIRTCHNL_OP_DISABLE_QUEUES_V2:
+		return "VIRTCHNL_OP_DISABLE_QUEUES_V2";
+	case VIRTCHNL_OP_MAP_QUEUE_VECTOR:
+		return "VIRTCHNL_OP_MAP_QUEUE_VECTOR";
+	case VIRTCHNL_OP_GET_OFFLOAD_VLAN_V2_CAPS:
+		return "VIRTCHNL_OP_GET_OFFLOAD_VLAN_V2_CAPS";
+	case VIRTCHNL_OP_ADD_VLAN_V2:
+		return "VIRTCHNL_OP_ADD_VLAN_V2";
+	case VIRTCHNL_OP_DEL_VLAN_V2:
+		return "VIRTCHNL_OP_DEL_VLAN_V2";
+	case VIRTCHNL_OP_ENABLE_VLAN_STRIPPING_V2:
+		return "VIRTCHNL_OP_ENABLE_VLAN_STRIPPING_V2";
+	case VIRTCHNL_OP_DISABLE_VLAN_STRIPPING_V2:
+		return "VIRTCHNL_OP_DISABLE_VLAN_STRIPPING_V2";
+	case VIRTCHNL_OP_ENABLE_VLAN_INSERTION_V2:
+		return "VIRTCHNL_OP_ENABLE_VLAN_INSERTION_V2";
+	case VIRTCHNL_OP_DISABLE_VLAN_INSERTION_V2:
+		return "VIRTCHNL_OP_DISABLE_VLAN_INSERTION_V2";
+	case VIRTCHNL_OP_ENABLE_VLAN_FILTERING_V2:
+		return "VIRTCHNL_OP_ENABLE_VLAN_FILTERING_V2";
+	case VIRTCHNL_OP_DISABLE_VLAN_FILTERING_V2:
+		return "VIRTCHNL_OP_DISABLE_VLAN_FILTERING_V2";
+	case VIRTCHNL_OP_MAX:
+		return "VIRTCHNL_OP_MAX";
+	default:
+		return "Unsupported (update virtchnl.h)";
+	}
+}
+
+/* These macros are used to generate compilation errors if a structure/union
+ * is not exactly the correct length. It gives a divide by zero error if the
+ * structure/union is not of the correct size, otherwise it creates an enum
+ * that is never used.
+ */
+#define VIRTCHNL_CHECK_STRUCT_LEN(n, X) enum virtchnl_static_assert_enum_##X \
+	{ virtchnl_static_assert_##X = (n)/((sizeof(struct X) == (n)) ? 1 : 0) }
+#define VIRTCHNL_CHECK_UNION_LEN(n, X) enum virtchnl_static_asset_enum_##X \
+	{ virtchnl_static_assert_##X = (n)/((sizeof(union X) == (n)) ? 1 : 0) }
+
+/* Message descriptions and data structures. */
+
+/* VIRTCHNL_OP_VERSION
+ * VF posts its version number to the PF. PF responds with its version number
+ * in the same format, along with a return code.
+ * Reply from PF has its major/minor versions also in param0 and param1.
+ * If there is a major version mismatch, then the VF cannot operate.
+ * If there is a minor version mismatch, then the VF can operate but should
+ * add a warning to the system log.
+ *
+ * This enum element MUST always be specified as == 1, regardless of other
+ * changes in the API. The PF must always respond to this message without
+ * error regardless of version mismatch.
+ */
+#define VIRTCHNL_VERSION_MAJOR		1
+#define VIRTCHNL_VERSION_MINOR		1
+#define VIRTCHNL_VERSION_MAJOR_2	2
+#define VIRTCHNL_VERSION_MINOR_0	0
+#define VIRTCHNL_VERSION_MINOR_NO_VF_CAPS	0
+
+struct virtchnl_version_info {
+	u32 major;
+	u32 minor;
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(8, virtchnl_version_info);
+
+#define VF_IS_V10(_ver) (((_ver)->major == 1) && ((_ver)->minor == 0))
+#define VF_IS_V11(_ver) (((_ver)->major == 1) && ((_ver)->minor == 1))
+#define VF_IS_V20(_ver) (((_ver)->major == 2) && ((_ver)->minor == 0))
+
+/* VIRTCHNL_OP_RESET_VF
+ * VF sends this request to PF with no parameters
+ * PF does NOT respond! VF driver must delay then poll VFGEN_RSTAT register
+ * until reset completion is indicated. The admin queue must be reinitialized
+ * after this operation.
+ *
+ * When reset is complete, PF must ensure that all queues in all VSIs associated
+ * with the VF are stopped, all queue configurations in the HMC are set to 0,
+ * and all MAC and VLAN filters (except the default MAC address) on all VSIs
+ * are cleared.
+ */
+
+/* VSI types that use VIRTCHNL interface for VF-PF communication. VSI_SRIOV
+ * vsi_type should always be 6 for backward compatibility. Add other fields
+ * as needed.
+ */
+enum virtchnl_vsi_type {
+	VIRTCHNL_VSI_TYPE_INVALID = 0,
+	VIRTCHNL_VSI_SRIOV = 6,
+};
+
+/* VIRTCHNL_OP_GET_VF_RESOURCES
+ * Version 1.0 VF sends this request to PF with no parameters
+ * Version 1.1 VF sends this request to PF with u32 bitmap of its capabilities
+ * PF responds with an indirect message containing
+ * virtchnl_vf_resource and one or more
+ * virtchnl_vsi_resource structures.
+ */
+
+struct virtchnl_vsi_resource {
+	u16 vsi_id;
+	u16 num_queue_pairs;
+
+	/* see enum virtchnl_vsi_type */
+	s32 vsi_type;
+	u16 qset_handle;
+	u8 default_mac_addr[ETH_ALEN];
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(16, virtchnl_vsi_resource);
+
+/* VF capability flags
+ * VIRTCHNL_VF_OFFLOAD_L2 flag is inclusive of base mode L2 offloads including
+ * TX/RX Checksum offloading and TSO for non-tunnelled packets.
+ */
+#define VIRTCHNL_VF_OFFLOAD_L2			BIT(0)
+#define VIRTCHNL_VF_OFFLOAD_IWARP		BIT(1)
+#define VIRTCHNL_VF_CAP_RDMA			VIRTCHNL_VF_OFFLOAD_IWARP
+#define VIRTCHNL_VF_OFFLOAD_RSS_AQ		BIT(3)
+#define VIRTCHNL_VF_OFFLOAD_RSS_REG		BIT(4)
+#define VIRTCHNL_VF_OFFLOAD_WB_ON_ITR		BIT(5)
+#define VIRTCHNL_VF_OFFLOAD_REQ_QUEUES		BIT(6)
+/* used to negotiate communicating link speeds in Mbps */
+#define VIRTCHNL_VF_CAP_ADV_LINK_SPEED		BIT(7)
+	/* BIT(8) is reserved */
+#define VIRTCHNL_VF_LARGE_NUM_QPAIRS		BIT(9)
+#define VIRTCHNL_VF_OFFLOAD_CRC			BIT(10)
+#define VIRTCHNL_VF_OFFLOAD_VLAN_V2		BIT(15)
+#define VIRTCHNL_VF_OFFLOAD_VLAN		BIT(16)
+#define VIRTCHNL_VF_OFFLOAD_RX_POLLING		BIT(17)
+#define VIRTCHNL_VF_OFFLOAD_RSS_PCTYPE_V2	BIT(18)
+#define VIRTCHNL_VF_OFFLOAD_RSS_PF		BIT(19)
+#define VIRTCHNL_VF_OFFLOAD_ENCAP		BIT(20)
+#define VIRTCHNL_VF_OFFLOAD_ENCAP_CSUM		BIT(21)
+#define VIRTCHNL_VF_OFFLOAD_RX_ENCAP_CSUM	BIT(22)
+#define VIRTCHNL_VF_OFFLOAD_ADQ			BIT(23)
+#define VIRTCHNL_VF_OFFLOAD_ADQ_V2		BIT(24)
+#define VIRTCHNL_VF_OFFLOAD_USO			BIT(25)
+	/* BIT(26) is reserved */
+#define VIRTCHNL_VF_OFFLOAD_ADV_RSS_PF		BIT(27)
+#define VIRTCHNL_VF_OFFLOAD_FDIR_PF		BIT(28)
+#define VIRTCHNL_VF_OFFLOAD_QOS			BIT(29)
+	/* BIT(30) is reserved */
+	/* BIT(31) is reserved */
+
+#define VF_BASE_MODE_OFFLOADS (VIRTCHNL_VF_OFFLOAD_L2 | \
+			       VIRTCHNL_VF_OFFLOAD_VLAN | \
+			       VIRTCHNL_VF_OFFLOAD_RSS_PF)
+
+struct virtchnl_vf_resource {
+	u16 num_vsis;
+	u16 num_queue_pairs;
+	u16 max_vectors;
+	u16 max_mtu;
+
+	u32 vf_cap_flags;
+	u32 rss_key_size;
+	u32 rss_lut_size;
+
+	struct virtchnl_vsi_resource vsi_res[1];
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(36, virtchnl_vf_resource);
+
+/* VIRTCHNL_OP_CONFIG_TX_QUEUE
+ * VF sends this message to set up parameters for one TX queue.
+ * External data buffer contains one instance of virtchnl_txq_info.
+ * PF configures requested queue and returns a status code.
+ */
+
+/* Tx queue config info */
+struct virtchnl_txq_info {
+	u16 vsi_id;
+	u16 queue_id;
+	u16 ring_len;		/* number of descriptors, multiple of 8 */
+	u16 headwb_enabled; /* deprecated with AVF 1.0 */
+	u64 dma_ring_addr;
+	u64 dma_headwb_addr; /* deprecated with AVF 1.0 */
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(24, virtchnl_txq_info);
+
+/* RX descriptor IDs (range from 0 to 63) */
+enum virtchnl_rx_desc_ids {
+	VIRTCHNL_RXDID_0_16B_BASE		= 0,
+	/* 32B_BASE and FLEX_SPLITQ share desc ids as default descriptors
+	 * because they can be differentiated based on queue model; e.g. single
+	 * queue model can only use 32B_BASE and split queue model can only use
+	 * FLEX_SPLITQ.  Having these as 1 allows them to be used as default
+	 * descriptors without negotiation.
+	 */
+	VIRTCHNL_RXDID_1_32B_BASE		= 1,
+	VIRTCHNL_RXDID_1_FLEX_SPLITQ		= 1,
+	VIRTCHNL_RXDID_2_FLEX_SQ_NIC		= 2,
+	VIRTCHNL_RXDID_3_FLEX_SQ_SW		= 3,
+	VIRTCHNL_RXDID_4_FLEX_SQ_NIC_VEB	= 4,
+	VIRTCHNL_RXDID_5_FLEX_SQ_NIC_ACL	= 5,
+	VIRTCHNL_RXDID_6_FLEX_SQ_NIC_2		= 6,
+	VIRTCHNL_RXDID_7_HW_RSVD		= 7,
+	/* 9 through 15 are reserved */
+	VIRTCHNL_RXDID_16_COMMS_GENERIC 	= 16,
+	VIRTCHNL_RXDID_17_COMMS_AUX_VLAN 	= 17,
+	VIRTCHNL_RXDID_18_COMMS_AUX_IPV4 	= 18,
+	VIRTCHNL_RXDID_19_COMMS_AUX_IPV6 	= 19,
+	VIRTCHNL_RXDID_20_COMMS_AUX_FLOW 	= 20,
+	VIRTCHNL_RXDID_21_COMMS_AUX_TCP 	= 21,
+	/* 22 through 63 are reserved */
+};
+
+/* RX descriptor ID bitmasks */
+enum virtchnl_rx_desc_id_bitmasks {
+	VIRTCHNL_RXDID_0_16B_BASE_M		= BIT(VIRTCHNL_RXDID_0_16B_BASE),
+	VIRTCHNL_RXDID_1_32B_BASE_M		= BIT(VIRTCHNL_RXDID_1_32B_BASE),
+	VIRTCHNL_RXDID_1_FLEX_SPLITQ_M		= BIT(VIRTCHNL_RXDID_1_FLEX_SPLITQ),
+	VIRTCHNL_RXDID_2_FLEX_SQ_NIC_M		= BIT(VIRTCHNL_RXDID_2_FLEX_SQ_NIC),
+	VIRTCHNL_RXDID_3_FLEX_SQ_SW_M		= BIT(VIRTCHNL_RXDID_3_FLEX_SQ_SW),
+	VIRTCHNL_RXDID_4_FLEX_SQ_NIC_VEB_M	= BIT(VIRTCHNL_RXDID_4_FLEX_SQ_NIC_VEB),
+	VIRTCHNL_RXDID_5_FLEX_SQ_NIC_ACL_M	= BIT(VIRTCHNL_RXDID_5_FLEX_SQ_NIC_ACL),
+	VIRTCHNL_RXDID_6_FLEX_SQ_NIC_2_M	= BIT(VIRTCHNL_RXDID_6_FLEX_SQ_NIC_2),
+	VIRTCHNL_RXDID_7_HW_RSVD_M		= BIT(VIRTCHNL_RXDID_7_HW_RSVD),
+	/* 9 through 15 are reserved */
+	VIRTCHNL_RXDID_16_COMMS_GENERIC_M	= BIT(VIRTCHNL_RXDID_16_COMMS_GENERIC),
+	VIRTCHNL_RXDID_17_COMMS_AUX_VLAN_M	= BIT(VIRTCHNL_RXDID_17_COMMS_AUX_VLAN),
+	VIRTCHNL_RXDID_18_COMMS_AUX_IPV4_M	= BIT(VIRTCHNL_RXDID_18_COMMS_AUX_IPV4),
+	VIRTCHNL_RXDID_19_COMMS_AUX_IPV6_M	= BIT(VIRTCHNL_RXDID_19_COMMS_AUX_IPV6),
+	VIRTCHNL_RXDID_20_COMMS_AUX_FLOW_M	= BIT(VIRTCHNL_RXDID_20_COMMS_AUX_FLOW),
+	VIRTCHNL_RXDID_21_COMMS_AUX_TCP_M	= BIT(VIRTCHNL_RXDID_21_COMMS_AUX_TCP),
+	/* 22 through 63 are reserved */
+};
+
+/* VIRTCHNL_OP_CONFIG_RX_QUEUE
+ * VF sends this message to set up parameters for one RX queue.
+ * External data buffer contains one instance of virtchnl_rxq_info.
+ * PF configures requested queue and returns a status code. The
+ * crc_disable flag disables CRC stripping on the VF. Setting
+ * the crc_disable flag to 1 will disable CRC stripping for each
+ * queue in the VF where the flag is set. The VIRTCHNL_VF_OFFLOAD_CRC
+ * offload must have been set prior to sending this info or the PF
+ * will ignore the request. This flag should be set the same for
+ * all of the queues for a VF.
+ */
+
+/* Rx queue config info */
+struct virtchnl_rxq_info {
+	u16 vsi_id;
+	u16 queue_id;
+	u32 ring_len;		/* number of descriptors, multiple of 32 */
+	u16 hdr_size;
+	u16 splithdr_enabled; /* deprecated with AVF 1.0 */
+	u32 databuffer_size;
+	u32 max_pkt_size;
+	u8 crc_disable;
+	u8 pad1[3];
+	u64 dma_ring_addr;
+
+	/* see enum virtchnl_rx_hsplit; deprecated with AVF 1.0 */
+	s32 rx_split_pos;
+	u32 pad2;
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(40, virtchnl_rxq_info);
+
+/* VIRTCHNL_OP_CONFIG_VSI_QUEUES
+ * VF sends this message to set parameters for active TX and RX queues
+ * associated with the specified VSI.
+ * PF configures queues and returns status.
+ * If the number of queues specified is greater than the number of queues
+ * associated with the VSI, an error is returned and no queues are configured.
+ * NOTE: The VF is not required to configure all queues in a single request.
+ * It may send multiple messages. PF drivers must correctly handle all VF
+ * requests.
+ */
+struct virtchnl_queue_pair_info {
+	/* NOTE: vsi_id and queue_id should be identical for both queues. */
+	struct virtchnl_txq_info txq;
+	struct virtchnl_rxq_info rxq;
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(64, virtchnl_queue_pair_info);
+
+struct virtchnl_vsi_queue_config_info {
+	u16 vsi_id;
+	u16 num_queue_pairs;
+	u32 pad;
+	struct virtchnl_queue_pair_info qpair[1];
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(72, virtchnl_vsi_queue_config_info);
+
+/* VIRTCHNL_OP_REQUEST_QUEUES
+ * VF sends this message to request the PF to allocate additional queues to
+ * this VF.  Each VF gets a guaranteed number of queues on init but asking for
+ * additional queues must be negotiated.  This is a best effort request as it
+ * is possible the PF does not have enough queues left to support the request.
+ * If the PF cannot support the number requested it will respond with the
+ * maximum number it is able to support.  If the request is successful, PF will
+ * then reset the VF to institute required changes.
+ */
+
+/* VF resource request */
+struct virtchnl_vf_res_request {
+	u16 num_queue_pairs;
+};
+
+/* VIRTCHNL_OP_CONFIG_IRQ_MAP
+ * VF uses this message to map vectors to queues.
+ * The rxq_map and txq_map fields are bitmaps used to indicate which queues
+ * are to be associated with the specified vector.
+ * The "other" causes are always mapped to vector 0. The VF may not request
+ * that vector 0 be used for traffic.
+ * PF configures interrupt mapping and returns status.
+ * NOTE: due to hardware requirements, all active queues (both TX and RX)
+ * should be mapped to interrupts, even if the driver intends to operate
+ * only in polling mode. In this case the interrupt may be disabled, but
+ * the ITR timer will still run to trigger writebacks.
+ */
+struct virtchnl_vector_map {
+	u16 vsi_id;
+	u16 vector_id;
+	u16 rxq_map;
+	u16 txq_map;
+	u16 rxitr_idx;
+	u16 txitr_idx;
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(12, virtchnl_vector_map);
+
+struct virtchnl_irq_map_info {
+	u16 num_vectors;
+	struct virtchnl_vector_map vecmap[1];
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(14, virtchnl_irq_map_info);
+
+/* VIRTCHNL_OP_ENABLE_QUEUES
+ * VIRTCHNL_OP_DISABLE_QUEUES
+ * VF sends these message to enable or disable TX/RX queue pairs.
+ * The queues fields are bitmaps indicating which queues to act upon.
+ * (Currently, we only support 16 queues per VF, but we make the field
+ * u32 to allow for expansion.)
+ * PF performs requested action and returns status.
+ * NOTE: The VF is not required to enable/disable all queues in a single
+ * request. It may send multiple messages.
+ * PF drivers must correctly handle all VF requests.
+ */
+struct virtchnl_queue_select {
+	u16 vsi_id;
+	u16 pad;
+	u32 rx_queues;
+	u32 tx_queues;
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(12, virtchnl_queue_select);
+
+/* VIRTCHNL_OP_GET_MAX_RSS_QREGION
+ *
+ * if VIRTCHNL_VF_LARGE_NUM_QPAIRS was negotiated in VIRTCHNL_OP_GET_VF_RESOURCES
+ * then this op must be supported.
+ *
+ * VF sends this message in order to query the max RSS queue region
+ * size supported by PF, when VIRTCHNL_VF_LARGE_NUM_QPAIRS is enabled.
+ * This information should be used when configuring the RSS LUT and/or
+ * configuring queue region based filters.
+ *
+ * The maximum RSS queue region is 2^qregion_width. So, a qregion_width
+ * of 6 would inform the VF that the PF supports a maximum RSS queue region
+ * of 64.
+ *
+ * A queue region represents a range of queues that can be used to configure
+ * a RSS LUT. For example, if a VF is given 64 queues, but only a max queue
+ * region size of 16 (i.e. 2^qregion_width = 16) then it will only be able
+ * to configure the RSS LUT with queue indices from 0 to 15. However, other
+ * filters can be used to direct packets to queues >15 via specifying a queue
+ * base/offset and queue region width.
+ */
+struct virtchnl_max_rss_qregion {
+	u16 vport_id;
+	u16 qregion_width;
+	u8 pad[4];
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(8, virtchnl_max_rss_qregion);
+
+/* VIRTCHNL_OP_ADD_ETH_ADDR
+ * VF sends this message in order to add one or more unicast or multicast
+ * address filters for the specified VSI.
+ * PF adds the filters and returns status.
+ */
+
+/* VIRTCHNL_OP_DEL_ETH_ADDR
+ * VF sends this message in order to remove one or more unicast or multicast
+ * filters for the specified VSI.
+ * PF removes the filters and returns status.
+ */
+
+/* VIRTCHNL_ETHER_ADDR_LEGACY
+ * Prior to adding the @type member to virtchnl_ether_addr, there were 2 pad
+ * bytes. Moving forward all VF drivers should not set type to
+ * VIRTCHNL_ETHER_ADDR_LEGACY. This is only here to not break previous/legacy
+ * behavior. The control plane function (i.e. PF) can use a best effort method
+ * of tracking the primary/device unicast in this case, but there is no
+ * guarantee and functionality depends on the implementation of the PF.
+ */
+
+/* VIRTCHNL_ETHER_ADDR_PRIMARY
+ * All VF drivers should set @type to VIRTCHNL_ETHER_ADDR_PRIMARY for the
+ * primary/device unicast MAC address filter for VIRTCHNL_OP_ADD_ETH_ADDR and
+ * VIRTCHNL_OP_DEL_ETH_ADDR. This allows for the underlying control plane
+ * function (i.e. PF) to accurately track and use this MAC address for
+ * displaying on the host and for VM/function reset.
+ */
+
+/* VIRTCHNL_ETHER_ADDR_EXTRA
+ * All VF drivers should set @type to VIRTCHNL_ETHER_ADDR_EXTRA for any extra
+ * unicast and/or multicast filters that are being added/deleted via
+ * VIRTCHNL_OP_DEL_ETH_ADDR/VIRTCHNL_OP_ADD_ETH_ADDR respectively.
+ */
+struct virtchnl_ether_addr {
+	u8 addr[ETH_ALEN];
+	u8 type;
+#define VIRTCHNL_ETHER_ADDR_LEGACY	0
+#define VIRTCHNL_ETHER_ADDR_PRIMARY	1
+#define VIRTCHNL_ETHER_ADDR_EXTRA	2
+#define VIRTCHNL_ETHER_ADDR_TYPE_MASK	3 /* first two bits of type are valid */
+	u8 pad;
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(8, virtchnl_ether_addr);
+
+struct virtchnl_ether_addr_list {
+	u16 vsi_id;
+	u16 num_elements;
+	struct virtchnl_ether_addr list[1];
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(12, virtchnl_ether_addr_list);
+
+/* VIRTCHNL_OP_ADD_VLAN
+ * VF sends this message to add one or more VLAN tag filters for receives.
+ * PF adds the filters and returns status.
+ * If a port VLAN is configured by the PF, this operation will return an
+ * error to the VF.
+ */
+
+/* VIRTCHNL_OP_DEL_VLAN
+ * VF sends this message to remove one or more VLAN tag filters for receives.
+ * PF removes the filters and returns status.
+ * If a port VLAN is configured by the PF, this operation will return an
+ * error to the VF.
+ */
+
+struct virtchnl_vlan_filter_list {
+	u16 vsi_id;
+	u16 num_elements;
+	u16 vlan_id[1];
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(6, virtchnl_vlan_filter_list);
+
+/* This enum is used for all of the VIRTCHNL_VF_OFFLOAD_VLAN_V2_CAPS related
+ * structures and opcodes.
+ *
+ * VIRTCHNL_VLAN_UNSUPPORTED - This field is not supported and if a VF driver
+ * populates it the PF should return VIRTCHNL_STATUS_ERR_NOT_SUPPORTED.
+ *
+ * VIRTCHNL_VLAN_ETHERTYPE_8100 - This field supports 0x8100 ethertype.
+ * VIRTCHNL_VLAN_ETHERTYPE_88A8 - This field supports 0x88A8 ethertype.
+ * VIRTCHNL_VLAN_ETHERTYPE_9100 - This field supports 0x9100 ethertype.
+ *
+ * VIRTCHNL_VLAN_ETHERTYPE_AND - Used when multiple ethertypes can be supported
+ * by the PF concurrently. For example, if the PF can support
+ * VIRTCHNL_VLAN_ETHERTYPE_8100 AND VIRTCHNL_VLAN_ETHERTYPE_88A8 filters it
+ * would OR the following bits:
+ *
+ *	VIRTHCNL_VLAN_ETHERTYPE_8100 |
+ *	VIRTCHNL_VLAN_ETHERTYPE_88A8 |
+ *	VIRTCHNL_VLAN_ETHERTYPE_AND;
+ *
+ * The VF would interpret this as VLAN filtering can be supported on both 0x8100
+ * and 0x88A8 VLAN ethertypes.
+ *
+ * VIRTCHNL_ETHERTYPE_XOR - Used when only a single ethertype can be supported
+ * by the PF concurrently. For example if the PF can support
+ * VIRTCHNL_VLAN_ETHERTYPE_8100 XOR VIRTCHNL_VLAN_ETHERTYPE_88A8 stripping
+ * offload it would OR the following bits:
+ *
+ *	VIRTCHNL_VLAN_ETHERTYPE_8100 |
+ *	VIRTCHNL_VLAN_ETHERTYPE_88A8 |
+ *	VIRTCHNL_VLAN_ETHERTYPE_XOR;
+ *
+ * The VF would interpret this as VLAN stripping can be supported on either
+ * 0x8100 or 0x88a8 VLAN ethertypes. So when requesting VLAN stripping via
+ * VIRTCHNL_OP_ENABLE_VLAN_STRIPPING_V2 the specified ethertype will override
+ * the previously set value.
+ *
+ * VIRTCHNL_VLAN_TAG_LOCATION_L2TAG1 - Used to tell the VF to insert and/or
+ * strip the VLAN tag using the L2TAG1 field of the Tx/Rx descriptors.
+ *
+ * VIRTCHNL_VLAN_TAG_LOCATION_L2TAG2 - Used to tell the VF to insert hardware
+ * offloaded VLAN tags using the L2TAG2 field of the Tx descriptor.
+ *
+ * VIRTCHNL_VLAN_TAG_LOCATION_L2TAG2 - Used to tell the VF to strip hardware
+ * offloaded VLAN tags using the L2TAG2_2 field of the Rx descriptor.
+ *
+ * VIRTCHNL_VLAN_PRIO - This field supports VLAN priority bits. This is used for
+ * VLAN filtering if the underlying PF supports it.
+ *
+ * VIRTCHNL_VLAN_TOGGLE_ALLOWED - This field is used to say whether a
+ * certain VLAN capability can be toggled. For example if the underlying PF/CP
+ * allows the VF to toggle VLAN filtering, stripping, and/or insertion it should
+ * set this bit along with the supported ethertypes.
+ */
+enum virtchnl_vlan_support {
+	VIRTCHNL_VLAN_UNSUPPORTED =		0,
+	VIRTCHNL_VLAN_ETHERTYPE_8100 =		0x00000001,
+	VIRTCHNL_VLAN_ETHERTYPE_88A8 =		0x00000002,
+	VIRTCHNL_VLAN_ETHERTYPE_9100 =		0x00000004,
+	VIRTCHNL_VLAN_TAG_LOCATION_L2TAG1 =	0x00000100,
+	VIRTCHNL_VLAN_TAG_LOCATION_L2TAG2 =	0x00000200,
+	VIRTCHNL_VLAN_TAG_LOCATION_L2TAG2_2 =	0x00000400,
+	VIRTCHNL_VLAN_PRIO =			0x01000000,
+	VIRTCHNL_VLAN_FILTER_MASK =		0x10000000,
+	VIRTCHNL_VLAN_ETHERTYPE_AND =		0x20000000,
+	VIRTCHNL_VLAN_ETHERTYPE_XOR =		0x40000000,
+	VIRTCHNL_VLAN_TOGGLE =			0x80000000
+};
+
+/* This structure is used as part of the VIRTCHNL_OP_GET_OFFLOAD_VLAN_V2_CAPS
+ * for filtering, insertion, and stripping capabilities.
+ *
+ * If only outer capabilities are supported (for filtering, insertion, and/or
+ * stripping) then this refers to the outer most or single VLAN from the VF's
+ * perspective.
+ *
+ * If only inner capabilities are supported (for filtering, insertion, and/or
+ * stripping) then this refers to the outer most or single VLAN from the VF's
+ * perspective. Functionally this is the same as if only outer capabilities are
+ * supported. The VF driver is just forced to use the inner fields when
+ * adding/deleting filters and enabling/disabling offloads (if supported).
+ *
+ * If both outer and inner capabilities are supported (for filtering, insertion,
+ * and/or stripping) then outer refers to the outer most or single VLAN and
+ * inner refers to the second VLAN, if it exists, in the packet.
+ *
+ * There is no support for tunneled VLAN offloads, so outer or inner are never
+ * referring to a tunneled packet from the VF's perspective.
+ */
+struct virtchnl_vlan_supported_caps {
+	u32 outer;
+	u32 inner;
+};
+
+/* The PF populates these fields based on the supported VLAN filtering. If a
+ * field is VIRTCHNL_VLAN_UNSUPPORTED then it's not supported and the PF will
+ * reject any VIRTCHNL_OP_ADD_VLAN_V2 or VIRTCHNL_OP_DEL_VLAN_V2 messages using
+ * the unsupported fields.
+ *
+ * Also, a VF is only allowed to toggle its VLAN filtering setting if the
+ * VIRTCHNL_VLAN_TOGGLE bit is set.
+ *
+ * The ethertype(s) specified in the ethertype_init field are the ethertypes
+ * enabled for VLAN filtering. VLAN filtering in this case refers to the outer
+ * most VLAN from the VF's perspective. If both inner and outer filtering are
+ * allowed then ethertype_init only refers to the outer most VLAN as only
+ * VLAN ethertype supported for inner VLAN filtering is
+ * VIRTCHNL_VLAN_ETHERTYPE_8100. By default, inner VLAN filtering is disabled
+ * when both inner and outer filtering are allowed.
+ *
+ * The max_filters field tells the VF how many VLAN filters it's allowed to have
+ * at any one time. If it exceeds this amount and tries to add another filter,
+ * then the request will be rejected by the PF. To prevent failures, the VF
+ * should keep track of how many VLAN filters it has added and not attempt to
+ * add more than max_filters.
+ */
+struct virtchnl_vlan_filtering_caps {
+	struct virtchnl_vlan_supported_caps filtering_support;
+	u32 ethertype_init;
+	u16 max_filters;
+	u8 pad[2];
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(16, virtchnl_vlan_filtering_caps);
+
+/* This enum is used for the virtchnl_vlan_offload_caps structure to specify
+ * if the PF supports a different ethertype for stripping and insertion.
+ *
+ * VIRTCHNL_ETHERTYPE_STRIPPING_MATCHES_INSERTION - The ethertype(s) specified
+ * for stripping affect the ethertype(s) specified for insertion and visa versa
+ * as well. If the VF tries to configure VLAN stripping via
+ * VIRTCHNL_OP_ENABLE_VLAN_STRIPPING_V2 with VIRTCHNL_VLAN_ETHERTYPE_8100 then
+ * that will be the ethertype for both stripping and insertion.
+ *
+ * VIRTCHNL_ETHERTYPE_MATCH_NOT_REQUIRED - The ethertype(s) specified for
+ * stripping do not affect the ethertype(s) specified for insertion and visa
+ * versa.
+ */
+enum virtchnl_vlan_ethertype_match {
+	VIRTCHNL_ETHERTYPE_STRIPPING_MATCHES_INSERTION = 0,
+	VIRTCHNL_ETHERTYPE_MATCH_NOT_REQUIRED = 1,
+};
+
+/* The PF populates these fields based on the supported VLAN offloads. If a
+ * field is VIRTCHNL_VLAN_UNSUPPORTED then it's not supported and the PF will
+ * reject any VIRTCHNL_OP_ENABLE_VLAN_STRIPPING_V2 or
+ * VIRTCHNL_OP_DISABLE_VLAN_STRIPPING_V2 messages using the unsupported fields.
+ *
+ * Also, a VF is only allowed to toggle its VLAN offload setting if the
+ * VIRTCHNL_VLAN_TOGGLE_ALLOWED bit is set.
+ *
+ * The VF driver needs to be aware of how the tags are stripped by hardware and
+ * inserted by the VF driver based on the level of offload support. The PF will
+ * populate these fields based on where the VLAN tags are expected to be
+ * offloaded via the VIRTHCNL_VLAN_TAG_LOCATION_* bits. The VF will need to
+ * interpret these fields. See the definition of the
+ * VIRTCHNL_VLAN_TAG_LOCATION_* bits above the virtchnl_vlan_support
+ * enumeration.
+ */
+struct virtchnl_vlan_offload_caps {
+	struct virtchnl_vlan_supported_caps stripping_support;
+	struct virtchnl_vlan_supported_caps insertion_support;
+	u32 ethertype_init;
+	u8 ethertype_match;
+	u8 pad[3];
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(24, virtchnl_vlan_offload_caps);
+
+/* VIRTCHNL_OP_GET_OFFLOAD_VLAN_V2_CAPS
+ * VF sends this message to determine its VLAN capabilities.
+ *
+ * PF will mark which capabilities it supports based on hardware support and
+ * current configuration. For example, if a port VLAN is configured the PF will
+ * not allow outer VLAN filtering, stripping, or insertion to be configured so
+ * it will block these features from the VF.
+ *
+ * The VF will need to cross reference its capabilities with the PFs
+ * capabilities in the response message from the PF to determine the VLAN
+ * support.
+ */
+struct virtchnl_vlan_caps {
+	struct virtchnl_vlan_filtering_caps filtering;
+	struct virtchnl_vlan_offload_caps offloads;
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(40, virtchnl_vlan_caps);
+
+struct virtchnl_vlan {
+	u16 tci;	/* tci[15:13] = PCP and tci[11:0] = VID */
+	u16 tci_mask;	/* only valid if VIRTCHNL_VLAN_FILTER_MASK set in
+			 * filtering caps
+			 */
+	u16 tpid;	/* 0x8100, 0x88a8, etc. and only type(s) set in
+			 * filtering caps. Note that tpid here does not refer to
+			 * VIRTCHNL_VLAN_ETHERTYPE_*, but it refers to the
+			 * actual 2-byte VLAN TPID
+			 */
+	u8 pad[2];
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(8, virtchnl_vlan);
+
+struct virtchnl_vlan_filter {
+	struct virtchnl_vlan inner;
+	struct virtchnl_vlan outer;
+	u8 pad[16];
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(32, virtchnl_vlan_filter);
+
+/* VIRTCHNL_OP_ADD_VLAN_V2
+ * VIRTCHNL_OP_DEL_VLAN_V2
+ *
+ * VF sends these messages to add/del one or more VLAN tag filters for Rx
+ * traffic.
+ *
+ * The PF attempts to add the filters and returns status.
+ *
+ * The VF should only ever attempt to add/del virtchnl_vlan_filter(s) using the
+ * supported fields negotiated via VIRTCHNL_OP_GET_OFFLOAD_VLAN_V2_CAPS.
+ */
+struct virtchnl_vlan_filter_list_v2 {
+	u16 vport_id;
+	u16 num_elements;
+	u8 pad[4];
+	struct virtchnl_vlan_filter filters[1];
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(40, virtchnl_vlan_filter_list_v2);
+
+/* VIRTCHNL_OP_ENABLE_VLAN_STRIPPING_V2
+ * VIRTCHNL_OP_DISABLE_VLAN_STRIPPING_V2
+ * VIRTCHNL_OP_ENABLE_VLAN_INSERTION_V2
+ * VIRTCHNL_OP_DISABLE_VLAN_INSERTION_V2
+ *
+ * VF sends this message to enable or disable VLAN stripping or insertion. It
+ * also needs to specify an ethertype. The VF knows which VLAN ethertypes are
+ * allowed and whether or not it's allowed to enable/disable the specific
+ * offload via the VIRTCHNL_OP_GET_OFFLOAD_VLAN_V2_CAPS message. The VF needs to
+ * parse the virtchnl_vlan_caps.offloads fields to determine which offload
+ * messages are allowed.
+ *
+ * For example, if the PF populates the virtchnl_vlan_caps.offloads in the
+ * following manner the VF will be allowed to enable and/or disable 0x8100 inner
+ * VLAN insertion and/or stripping via the opcodes listed above. Inner in this
+ * case means the outer most or single VLAN from the VF's perspective. This is
+ * because no outer offloads are supported. See the comments above the
+ * virtchnl_vlan_supported_caps structure for more details.
+ *
+ * virtchnl_vlan_caps.offloads.stripping_support.inner =
+ *			VIRTCHNL_VLAN_TOGGLE |
+ *			VIRTCHNL_VLAN_ETHERTYPE_8100;
+ *
+ * virtchnl_vlan_caps.offloads.insertion_support.inner =
+ *			VIRTCHNL_VLAN_TOGGLE |
+ *			VIRTCHNL_VLAN_ETHERTYPE_8100;
+ *
+ * In order to enable inner (again note that in this case inner is the outer
+ * most or single VLAN from the VF's perspective) VLAN stripping for 0x8100
+ * VLANs, the VF would populate the virtchnl_vlan_setting structure in the
+ * following manner and send the VIRTCHNL_OP_ENABLE_VLAN_STRIPPING_V2 message.
+ *
+ * virtchnl_vlan_setting.inner_ethertype_setting =
+ *			VIRTCHNL_VLAN_ETHERTYPE_8100;
+ *
+ * virtchnl_vlan_setting.vport_id = vport_id or vsi_id assigned to the VF on
+ * initialization.
+ *
+ * The reason that VLAN TPID(s) are not being used for the
+ * outer_ethertype_setting and inner_ethertype_setting fields is because it's
+ * possible a device could support VLAN insertion and/or stripping offload on
+ * multiple ethertypes concurrently, so this method allows a VF to request
+ * multiple ethertypes in one message using the virtchnl_vlan_support
+ * enumeration.
+ *
+ * For example, if the PF populates the virtchnl_vlan_caps.offloads in the
+ * following manner the VF will be allowed to enable 0x8100 and 0x88a8 outer
+ * VLAN insertion and stripping simultaneously. The
+ * virtchnl_vlan_caps.offloads.ethertype_match field will also have to be
+ * populated based on what the PF can support.
+ *
+ * virtchnl_vlan_caps.offloads.stripping_support.outer =
+ *			VIRTCHNL_VLAN_TOGGLE |
+ *			VIRTCHNL_VLAN_ETHERTYPE_8100 |
+ *			VIRTCHNL_VLAN_ETHERTYPE_88A8 |
+ *			VIRTCHNL_VLAN_ETHERTYPE_AND;
+ *
+ * virtchnl_vlan_caps.offloads.insertion_support.outer =
+ *			VIRTCHNL_VLAN_TOGGLE |
+ *			VIRTCHNL_VLAN_ETHERTYPE_8100 |
+ *			VIRTCHNL_VLAN_ETHERTYPE_88A8 |
+ *			VIRTCHNL_VLAN_ETHERTYPE_AND;
+ *
+ * In order to enable outer VLAN stripping for 0x8100 and 0x88a8 VLANs, the VF
+ * would populate the virthcnl_vlan_offload_structure in the following manner
+ * and send the VIRTCHNL_OP_ENABLE_VLAN_STRIPPING_V2 message.
+ *
+ * virtchnl_vlan_setting.outer_ethertype_setting =
+ *			VIRTHCNL_VLAN_ETHERTYPE_8100 |
+ *			VIRTHCNL_VLAN_ETHERTYPE_88A8;
+ *
+ * virtchnl_vlan_setting.vport_id = vport_id or vsi_id assigned to the VF on
+ * initialization.
+ *
+ * There is also the case where a PF and the underlying hardware can support
+ * VLAN offloads on multiple ethertypes, but not concurrently. For example, if
+ * the PF populates the virtchnl_vlan_caps.offloads in the following manner the
+ * VF will be allowed to enable and/or disable 0x8100 XOR 0x88a8 outer VLAN
+ * offloads. The ethertypes must match for stripping and insertion.
+ *
+ * virtchnl_vlan_caps.offloads.stripping_support.outer =
+ *			VIRTCHNL_VLAN_TOGGLE |
+ *			VIRTCHNL_VLAN_ETHERTYPE_8100 |
+ *			VIRTCHNL_VLAN_ETHERTYPE_88A8 |
+ *			VIRTCHNL_VLAN_ETHERTYPE_XOR;
+ *
+ * virtchnl_vlan_caps.offloads.insertion_support.outer =
+ *			VIRTCHNL_VLAN_TOGGLE |
+ *			VIRTCHNL_VLAN_ETHERTYPE_8100 |
+ *			VIRTCHNL_VLAN_ETHERTYPE_88A8 |
+ *			VIRTCHNL_VLAN_ETHERTYPE_XOR;
+ *
+ * virtchnl_vlan_caps.offloads.ethertype_match =
+ *			VIRTCHNL_ETHERTYPE_STRIPPING_MATCHES_INSERTION;
+ *
+ * In order to enable outer VLAN stripping for 0x88a8 VLANs, the VF would
+ * populate the virtchnl_vlan_setting structure in the following manner and send
+ * the VIRTCHNL_OP_ENABLE_VLAN_STRIPPING_V2. Also, this will change the
+ * ethertype for VLAN insertion if it's enabled. So, for completeness, a
+ * VIRTCHNL_OP_ENABLE_VLAN_INSERTION_V2 with the same ethertype should be sent.
+ *
+ * virtchnl_vlan_setting.outer_ethertype_setting = VIRTHCNL_VLAN_ETHERTYPE_88A8;
+ *
+ * virtchnl_vlan_setting.vport_id = vport_id or vsi_id assigned to the VF on
+ * initialization.
+ *
+ * VIRTCHNL_OP_ENABLE_VLAN_FILTERING_V2
+ * VIRTCHNL_OP_DISABLE_VLAN_FILTERING_V2
+ *
+ * VF sends this message to enable or disable VLAN filtering. It also needs to
+ * specify an ethertype. The VF knows which VLAN ethertypes are allowed and
+ * whether or not it's allowed to enable/disable filtering via the
+ * VIRTCHNL_OP_GET_OFFLOAD_VLAN_V2_CAPS message. The VF needs to
+ * parse the virtchnl_vlan_caps.filtering fields to determine which, if any,
+ * filtering messages are allowed.
+ *
+ * For example, if the PF populates the virtchnl_vlan_caps.filtering in the
+ * following manner the VF will be allowed to enable/disable 0x8100 and 0x88a8
+ * outer VLAN filtering together. Note, that the VIRTCHNL_VLAN_ETHERTYPE_AND
+ * means that all filtering ethertypes will to be enabled and disabled together
+ * regardless of the request from the VF. This means that the underlying
+ * hardware only supports VLAN filtering for all VLAN the specified ethertypes
+ * or none of them.
+ *
+ * virtchnl_vlan_caps.filtering.filtering_support.outer =
+ *			VIRTCHNL_VLAN_TOGGLE |
+ *			VIRTCHNL_VLAN_ETHERTYPE_8100 |
+ *			VIRTHCNL_VLAN_ETHERTYPE_88A8 |
+ *			VIRTCHNL_VLAN_ETHERTYPE_9100 |
+ *			VIRTCHNL_VLAN_ETHERTYPE_AND;
+ *
+ * In order to enable outer VLAN filtering for 0x88a8 and 0x8100 VLANs (0x9100
+ * VLANs aren't supported by the VF driver), the VF would populate the
+ * virtchnl_vlan_setting structure in the following manner and send the
+ * VIRTCHNL_OP_ENABLE_VLAN_FILTERING_V2. The same message format would be used
+ * to disable outer VLAN filtering for 0x88a8 and 0x8100 VLANs, but the
+ * VIRTCHNL_OP_DISABLE_VLAN_FILTERING_V2 opcode is used.
+ *
+ * virtchnl_vlan_setting.outer_ethertype_setting =
+ *			VIRTCHNL_VLAN_ETHERTYPE_8100 |
+ *			VIRTCHNL_VLAN_ETHERTYPE_88A8;
+ *
+ */
+struct virtchnl_vlan_setting {
+	u32 outer_ethertype_setting;
+	u32 inner_ethertype_setting;
+	u16 vport_id;
+	u8 pad[6];
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(16, virtchnl_vlan_setting);
+
+/* VIRTCHNL_OP_CONFIG_PROMISCUOUS_MODE
+ * VF sends VSI id and flags.
+ * PF returns status code in retval.
+ * Note: we assume that broadcast accept mode is always enabled.
+ */
+struct virtchnl_promisc_info {
+	u16 vsi_id;
+	u16 flags;
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(4, virtchnl_promisc_info);
+
+#define FLAG_VF_UNICAST_PROMISC	0x00000001
+#define FLAG_VF_MULTICAST_PROMISC	0x00000002
+
+/* VIRTCHNL_OP_GET_STATS
+ * VF sends this message to request stats for the selected VSI. VF uses
+ * the virtchnl_queue_select struct to specify the VSI. The queue_id
+ * field is ignored by the PF.
+ *
+ * PF replies with struct virtchnl_eth_stats in an external buffer.
+ */
+
+struct virtchnl_eth_stats {
+	u64 rx_bytes;			/* received bytes */
+	u64 rx_unicast;			/* received unicast pkts */
+	u64 rx_multicast;		/* received multicast pkts */
+	u64 rx_broadcast;		/* received broadcast pkts */
+	u64 rx_discards;
+	u64 rx_unknown_protocol;
+	u64 tx_bytes;			/* transmitted bytes */
+	u64 tx_unicast;			/* transmitted unicast pkts */
+	u64 tx_multicast;		/* transmitted multicast pkts */
+	u64 tx_broadcast;		/* transmitted broadcast pkts */
+	u64 tx_discards;
+	u64 tx_errors;
+};
+
+/* VIRTCHNL_OP_CONFIG_RSS_KEY
+ * VIRTCHNL_OP_CONFIG_RSS_LUT
+ * VF sends these messages to configure RSS. Only supported if both PF
+ * and VF drivers set the VIRTCHNL_VF_OFFLOAD_RSS_PF bit during
+ * configuration negotiation. If this is the case, then the RSS fields in
+ * the VF resource struct are valid.
+ * Both the key and LUT are initialized to 0 by the PF, meaning that
+ * RSS is effectively disabled until set up by the VF.
+ */
+struct virtchnl_rss_key {
+	u16 vsi_id;
+	u16 key_len;
+	u8 key[1];         /* RSS hash key, packed bytes */
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(6, virtchnl_rss_key);
+
+struct virtchnl_rss_lut {
+	u16 vsi_id;
+	u16 lut_entries;
+	u8 lut[1];        /* RSS lookup table */
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(6, virtchnl_rss_lut);
+
+/* VIRTCHNL_OP_GET_RSS_HENA_CAPS
+ * VIRTCHNL_OP_SET_RSS_HENA
+ * VF sends these messages to get and set the hash filter enable bits for RSS.
+ * By default, the PF sets these to all possible traffic types that the
+ * hardware supports. The VF can query this value if it wants to change the
+ * traffic types that are hashed by the hardware.
+ */
+struct virtchnl_rss_hena {
+	u64 hena;
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(8, virtchnl_rss_hena);
+
+/* Type of RSS algorithm */
+enum virtchnl_rss_algorithm {
+	VIRTCHNL_RSS_ALG_TOEPLITZ_ASYMMETRIC	= 0,
+	VIRTCHNL_RSS_ALG_R_ASYMMETRIC		= 1,
+	VIRTCHNL_RSS_ALG_TOEPLITZ_SYMMETRIC	= 2,
+	VIRTCHNL_RSS_ALG_XOR_SYMMETRIC		= 3,
+};
+
+/* This is used by PF driver to enforce how many channels can be supported.
+ * When ADQ_V2 capability is negotiated, it will allow 16 channels otherwise
+ * PF driver will allow only max 4 channels
+ */
+#define VIRTCHNL_MAX_ADQ_CHANNELS 4
+#define VIRTCHNL_MAX_ADQ_V2_CHANNELS 16
+
+/* VIRTCHNL_OP_ENABLE_CHANNELS
+ * VIRTCHNL_OP_DISABLE_CHANNELS
+ * VF sends these messages to enable or disable channels based on
+ * the user specified queue count and queue offset for each traffic class.
+ * This struct encompasses all the information that the PF needs from
+ * VF to create a channel.
+ */
+struct virtchnl_channel_info {
+	u16 count; /* number of queues in a channel */
+	u16 offset; /* queues in a channel start from 'offset' */
+	u32 pad;
+	u64 max_tx_rate;
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(16, virtchnl_channel_info);
+
+struct virtchnl_tc_info {
+	u32	num_tc;
+	u32	pad;
+	struct	virtchnl_channel_info list[1];
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(24, virtchnl_tc_info);
+
+/* VIRTCHNL_ADD_CLOUD_FILTER
+ * VIRTCHNL_DEL_CLOUD_FILTER
+ * VF sends these messages to add or delete a cloud filter based on the
+ * user specified match and action filters. These structures encompass
+ * all the information that the PF needs from the VF to add/delete a
+ * cloud filter.
+ */
+
+struct virtchnl_l4_spec {
+	u8	src_mac[ETH_ALEN];
+	u8	dst_mac[ETH_ALEN];
+	/* vlan_prio is part of this 16 bit field even from OS perspective
+	 * vlan_id:12 is actual vlan_id, then vlanid:bit14..12 is vlan_prio
+	 * in future, when decided to offload vlan_prio, pass that information
+	 * as part of the "vlan_id" field, Bit14..12
+	 */
+	__be16	vlan_id;
+	__be16	pad; /* reserved for future use */
+	__be32	src_ip[4];
+	__be32	dst_ip[4];
+	__be16	src_port;
+	__be16	dst_port;
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(52, virtchnl_l4_spec);
+
+union virtchnl_flow_spec {
+	struct	virtchnl_l4_spec tcp_spec;
+	u8	buffer[128]; /* reserved for future use */
+};
+
+VIRTCHNL_CHECK_UNION_LEN(128, virtchnl_flow_spec);
+
+enum virtchnl_action {
+	/* action types */
+	VIRTCHNL_ACTION_DROP = 0,
+	VIRTCHNL_ACTION_TC_REDIRECT,
+	VIRTCHNL_ACTION_PASSTHRU,
+	VIRTCHNL_ACTION_QUEUE,
+	VIRTCHNL_ACTION_Q_REGION,
+	VIRTCHNL_ACTION_MARK,
+	VIRTCHNL_ACTION_COUNT,
+};
+
+enum virtchnl_flow_type {
+	/* flow types */
+	VIRTCHNL_TCP_V4_FLOW = 0,
+	VIRTCHNL_TCP_V6_FLOW,
+	VIRTCHNL_UDP_V4_FLOW,
+	VIRTCHNL_UDP_V6_FLOW,
+};
+
+struct virtchnl_filter {
+	union	virtchnl_flow_spec data;
+	union	virtchnl_flow_spec mask;
+
+	/* see enum virtchnl_flow_type */
+	s32 	flow_type;
+
+	/* see enum virtchnl_action */
+	s32	action;
+	u32	action_meta;
+	u8	field_flags;
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(272, virtchnl_filter);
+
+struct virtchnl_shaper_bw {
+	/* Unit is Kbps */
+	u32 committed;
+	u32 peak;
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(8, virtchnl_shaper_bw);
+
+/* VIRTCHNL_OP_EVENT
+ * PF sends this message to inform the VF driver of events that may affect it.
+ * No direct response is expected from the VF, though it may generate other
+ * messages in response to this one.
+ */
+enum virtchnl_event_codes {
+	VIRTCHNL_EVENT_UNKNOWN = 0,
+	VIRTCHNL_EVENT_LINK_CHANGE,
+	VIRTCHNL_EVENT_RESET_IMPENDING,
+	VIRTCHNL_EVENT_PF_DRIVER_CLOSE,
+};
+
+#define PF_EVENT_SEVERITY_INFO		0
+#define PF_EVENT_SEVERITY_CERTAIN_DOOM	255
+
+struct virtchnl_pf_event {
+	/* see enum virtchnl_event_codes */
+	s32 event;
+	union {
+		/* If the PF driver does not support the new speed reporting
+		 * capabilities then use link_event else use link_event_adv to
+		 * get the speed and link information. The ability to understand
+		 * new speeds is indicated by setting the capability flag
+		 * VIRTCHNL_VF_CAP_ADV_LINK_SPEED in vf_cap_flags parameter
+		 * in virtchnl_vf_resource struct and can be used to determine
+		 * which link event struct to use below.
+		 */
+		struct {
+			enum virtchnl_link_speed link_speed;
+			bool link_status;
+			u8 pad[3];
+		} link_event;
+		struct {
+			/* link_speed provided in Mbps */
+			u32 link_speed;
+			u8 link_status;
+			u8 pad[3];
+		} link_event_adv;
+		struct {
+			/* link_speed provided in Mbps */
+			u32 link_speed;
+			u16 vport_id;
+			u8 link_status;
+			u8 pad;
+		} link_event_adv_vport;
+	} event_data;
+
+	s32 severity;
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(16, virtchnl_pf_event);
+
+/* used to specify if a ceq_idx or aeq_idx is invalid */
+#define VIRTCHNL_RDMA_INVALID_QUEUE_IDX	0xFFFF
+/* VIRTCHNL_OP_CONFIG_RDMA_IRQ_MAP
+ * VF uses this message to request PF to map RDMA vectors to RDMA queues.
+ * The request for this originates from the VF RDMA driver through
+ * a client interface between VF LAN and VF RDMA driver.
+ * A vector could have an AEQ and CEQ attached to it although
+ * there is a single AEQ per VF RDMA instance in which case
+ * most vectors will have an VIRTCHNL_RDMA_INVALID_QUEUE_IDX for aeq and valid
+ * idx for ceqs There will never be a case where there will be multiple CEQs
+ * attached to a single vector.
+ * PF configures interrupt mapping and returns status.
+ */
+#define virtchnl_iwarp_qv_info virtchnl_rdma_qv_info
+struct virtchnl_rdma_qv_info {
+	u32 v_idx; /* msix_vector */
+	u16 ceq_idx; /* set to VIRTCHNL_RDMA_INVALID_QUEUE_IDX if invalid */
+	u16 aeq_idx; /* set to VIRTCHNL_RDMA_INVALID_QUEUE_IDX if invalid */
+	u8 itr_idx;
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(12, virtchnl_rdma_qv_info);
+
+#define virtchnl_iwarp_qvlist_info virtchnl_rdma_qvlist_info
+struct virtchnl_rdma_qvlist_info {
+	u32 num_vectors;
+	struct virtchnl_rdma_qv_info qv_info[1];
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(16, virtchnl_rdma_qvlist_info);
+
+/* VF reset states - these are written into the RSTAT register:
+ * VFGEN_RSTAT on the VF
+ * When the PF initiates a reset, it writes 0
+ * When the reset is complete, it writes 1
+ * When the PF detects that the VF has recovered, it writes 2
+ * VF checks this register periodically to determine if a reset has occurred,
+ * then polls it to know when the reset is complete.
+ * If either the PF or VF reads the register while the hardware
+ * is in a reset state, it will return DEADBEEF, which, when masked
+ * will result in 3.
+ */
+enum virtchnl_vfr_states {
+	VIRTCHNL_VFR_INPROGRESS = 0,
+	VIRTCHNL_VFR_COMPLETED,
+	VIRTCHNL_VFR_VFACTIVE,
+};
+
+#define VIRTCHNL_MAX_NUM_PROTO_HDRS	32
+#define PROTO_HDR_SHIFT			5
+#define PROTO_HDR_FIELD_START(proto_hdr_type) \
+					(proto_hdr_type << PROTO_HDR_SHIFT)
+#define PROTO_HDR_FIELD_MASK ((1UL << PROTO_HDR_SHIFT) - 1)
+
+/* VF use these macros to configure each protocol header.
+ * Specify which protocol headers and protocol header fields base on
+ * virtchnl_proto_hdr_type and virtchnl_proto_hdr_field.
+ * @param hdr: a struct of virtchnl_proto_hdr
+ * @param hdr_type: ETH/IPV4/TCP, etc
+ * @param field: SRC/DST/TEID/SPI, etc
+ */
+#define VIRTCHNL_ADD_PROTO_HDR_FIELD(hdr, field) \
+	((hdr)->field_selector |= BIT((field) & PROTO_HDR_FIELD_MASK))
+#define VIRTCHNL_DEL_PROTO_HDR_FIELD(hdr, field) \
+	((hdr)->field_selector &= ~BIT((field) & PROTO_HDR_FIELD_MASK))
+#define VIRTCHNL_TEST_PROTO_HDR_FIELD(hdr, val) \
+	((hdr)->field_selector & BIT((val) & PROTO_HDR_FIELD_MASK))
+#define VIRTCHNL_GET_PROTO_HDR_FIELD(hdr)	((hdr)->field_selector)
+
+#define VIRTCHNL_ADD_PROTO_HDR_FIELD_BIT(hdr, hdr_type, field) \
+	(VIRTCHNL_ADD_PROTO_HDR_FIELD(hdr, \
+		VIRTCHNL_PROTO_HDR_ ## hdr_type ## _ ## field))
+#define VIRTCHNL_DEL_PROTO_HDR_FIELD_BIT(hdr, hdr_type, field) \
+	(VIRTCHNL_DEL_PROTO_HDR_FIELD(hdr, \
+		VIRTCHNL_PROTO_HDR_ ## hdr_type ## _ ## field))
+
+#define VIRTCHNL_SET_PROTO_HDR_TYPE(hdr, hdr_type) \
+	((hdr)->type = VIRTCHNL_PROTO_HDR_ ## hdr_type)
+#define VIRTCHNL_GET_PROTO_HDR_TYPE(hdr) \
+	(((hdr)->type) >> PROTO_HDR_SHIFT)
+#define VIRTCHNL_TEST_PROTO_HDR_TYPE(hdr, val) \
+	((hdr)->type == ((s32)((val) >> PROTO_HDR_SHIFT)))
+#define VIRTCHNL_TEST_PROTO_HDR(hdr, val) \
+	(VIRTCHNL_TEST_PROTO_HDR_TYPE(hdr, val) && \
+	 VIRTCHNL_TEST_PROTO_HDR_FIELD(hdr, val))
+
+/* Protocol header type within a packet segment. A segment consists of one or
+ * more protocol headers that make up a logical group of protocol headers. Each
+ * logical group of protocol headers encapsulates or is encapsulated using/by
+ * tunneling or encapsulation protocols for network virtualization.
+ */
+enum virtchnl_proto_hdr_type {
+	VIRTCHNL_PROTO_HDR_NONE,
+	VIRTCHNL_PROTO_HDR_ETH,
+	VIRTCHNL_PROTO_HDR_S_VLAN,
+	VIRTCHNL_PROTO_HDR_C_VLAN,
+	VIRTCHNL_PROTO_HDR_IPV4,
+	VIRTCHNL_PROTO_HDR_IPV6,
+	VIRTCHNL_PROTO_HDR_TCP,
+	VIRTCHNL_PROTO_HDR_UDP,
+	VIRTCHNL_PROTO_HDR_SCTP,
+	VIRTCHNL_PROTO_HDR_GTPU_IP,
+	VIRTCHNL_PROTO_HDR_GTPU_EH,
+	VIRTCHNL_PROTO_HDR_GTPU_EH_PDU_DWN,
+	VIRTCHNL_PROTO_HDR_GTPU_EH_PDU_UP,
+	VIRTCHNL_PROTO_HDR_PPPOE,
+	VIRTCHNL_PROTO_HDR_L2TPV3,
+	VIRTCHNL_PROTO_HDR_ESP,
+	VIRTCHNL_PROTO_HDR_AH,
+	VIRTCHNL_PROTO_HDR_PFCP,
+	VIRTCHNL_PROTO_HDR_GTPC,
+	VIRTCHNL_PROTO_HDR_ECPRI,
+	VIRTCHNL_PROTO_HDR_L2TPV2,
+	VIRTCHNL_PROTO_HDR_PPP,
+	/* IPv4 and IPv6 Fragment header types are only associated to
+	 * VIRTCHNL_PROTO_HDR_IPV4 and VIRTCHNL_PROTO_HDR_IPV6 respectively,
+	 * cannot be used independently.
+	 */
+	VIRTCHNL_PROTO_HDR_IPV4_FRAG,
+	VIRTCHNL_PROTO_HDR_IPV6_EH_FRAG,
+	VIRTCHNL_PROTO_HDR_GRE,
+};
+
+/* Protocol header field within a protocol header. */
+enum virtchnl_proto_hdr_field {
+	/* ETHER */
+	VIRTCHNL_PROTO_HDR_ETH_SRC =
+		PROTO_HDR_FIELD_START(VIRTCHNL_PROTO_HDR_ETH),
+	VIRTCHNL_PROTO_HDR_ETH_DST,
+	VIRTCHNL_PROTO_HDR_ETH_ETHERTYPE,
+	/* S-VLAN */
+	VIRTCHNL_PROTO_HDR_S_VLAN_ID =
+		PROTO_HDR_FIELD_START(VIRTCHNL_PROTO_HDR_S_VLAN),
+	/* C-VLAN */
+	VIRTCHNL_PROTO_HDR_C_VLAN_ID =
+		PROTO_HDR_FIELD_START(VIRTCHNL_PROTO_HDR_C_VLAN),
+	/* IPV4 */
+	VIRTCHNL_PROTO_HDR_IPV4_SRC =
+		PROTO_HDR_FIELD_START(VIRTCHNL_PROTO_HDR_IPV4),
+	VIRTCHNL_PROTO_HDR_IPV4_DST,
+	VIRTCHNL_PROTO_HDR_IPV4_DSCP,
+	VIRTCHNL_PROTO_HDR_IPV4_TTL,
+	VIRTCHNL_PROTO_HDR_IPV4_PROT,
+	VIRTCHNL_PROTO_HDR_IPV4_CHKSUM,
+	/* IPV6 */
+	VIRTCHNL_PROTO_HDR_IPV6_SRC =
+		PROTO_HDR_FIELD_START(VIRTCHNL_PROTO_HDR_IPV6),
+	VIRTCHNL_PROTO_HDR_IPV6_DST,
+	VIRTCHNL_PROTO_HDR_IPV6_TC,
+	VIRTCHNL_PROTO_HDR_IPV6_HOP_LIMIT,
+	VIRTCHNL_PROTO_HDR_IPV6_PROT,
+	/* IPV6 Prefix */
+	VIRTCHNL_PROTO_HDR_IPV6_PREFIX32_SRC,
+	VIRTCHNL_PROTO_HDR_IPV6_PREFIX32_DST,
+	VIRTCHNL_PROTO_HDR_IPV6_PREFIX40_SRC,
+	VIRTCHNL_PROTO_HDR_IPV6_PREFIX40_DST,
+	VIRTCHNL_PROTO_HDR_IPV6_PREFIX48_SRC,
+	VIRTCHNL_PROTO_HDR_IPV6_PREFIX48_DST,
+	VIRTCHNL_PROTO_HDR_IPV6_PREFIX56_SRC,
+	VIRTCHNL_PROTO_HDR_IPV6_PREFIX56_DST,
+	VIRTCHNL_PROTO_HDR_IPV6_PREFIX64_SRC,
+	VIRTCHNL_PROTO_HDR_IPV6_PREFIX64_DST,
+	VIRTCHNL_PROTO_HDR_IPV6_PREFIX96_SRC,
+	VIRTCHNL_PROTO_HDR_IPV6_PREFIX96_DST,
+	/* TCP */
+	VIRTCHNL_PROTO_HDR_TCP_SRC_PORT =
+		PROTO_HDR_FIELD_START(VIRTCHNL_PROTO_HDR_TCP),
+	VIRTCHNL_PROTO_HDR_TCP_DST_PORT,
+	VIRTCHNL_PROTO_HDR_TCP_CHKSUM,
+	/* UDP */
+	VIRTCHNL_PROTO_HDR_UDP_SRC_PORT =
+		PROTO_HDR_FIELD_START(VIRTCHNL_PROTO_HDR_UDP),
+	VIRTCHNL_PROTO_HDR_UDP_DST_PORT,
+	VIRTCHNL_PROTO_HDR_UDP_CHKSUM,
+	/* SCTP */
+	VIRTCHNL_PROTO_HDR_SCTP_SRC_PORT =
+		PROTO_HDR_FIELD_START(VIRTCHNL_PROTO_HDR_SCTP),
+	VIRTCHNL_PROTO_HDR_SCTP_DST_PORT,
+	VIRTCHNL_PROTO_HDR_SCTP_CHKSUM,
+	/* GTPU_IP */
+	VIRTCHNL_PROTO_HDR_GTPU_IP_TEID =
+		PROTO_HDR_FIELD_START(VIRTCHNL_PROTO_HDR_GTPU_IP),
+	/* GTPU_EH */
+	VIRTCHNL_PROTO_HDR_GTPU_EH_PDU =
+		PROTO_HDR_FIELD_START(VIRTCHNL_PROTO_HDR_GTPU_EH),
+	VIRTCHNL_PROTO_HDR_GTPU_EH_QFI,
+	/* PPPOE */
+	VIRTCHNL_PROTO_HDR_PPPOE_SESS_ID =
+		PROTO_HDR_FIELD_START(VIRTCHNL_PROTO_HDR_PPPOE),
+	/* L2TPV3 */
+	VIRTCHNL_PROTO_HDR_L2TPV3_SESS_ID =
+		PROTO_HDR_FIELD_START(VIRTCHNL_PROTO_HDR_L2TPV3),
+	/* ESP */
+	VIRTCHNL_PROTO_HDR_ESP_SPI =
+		PROTO_HDR_FIELD_START(VIRTCHNL_PROTO_HDR_ESP),
+	/* AH */
+	VIRTCHNL_PROTO_HDR_AH_SPI =
+		PROTO_HDR_FIELD_START(VIRTCHNL_PROTO_HDR_AH),
+	/* PFCP */
+	VIRTCHNL_PROTO_HDR_PFCP_S_FIELD =
+		PROTO_HDR_FIELD_START(VIRTCHNL_PROTO_HDR_PFCP),
+	VIRTCHNL_PROTO_HDR_PFCP_SEID,
+	/* GTPC */
+	VIRTCHNL_PROTO_HDR_GTPC_TEID =
+		PROTO_HDR_FIELD_START(VIRTCHNL_PROTO_HDR_GTPC),
+	/* ECPRI */
+	VIRTCHNL_PROTO_HDR_ECPRI_MSG_TYPE =
+		PROTO_HDR_FIELD_START(VIRTCHNL_PROTO_HDR_ECPRI),
+	VIRTCHNL_PROTO_HDR_ECPRI_PC_RTC_ID,
+	/* IPv4 Dummy Fragment */
+	VIRTCHNL_PROTO_HDR_IPV4_FRAG_PKID =
+		PROTO_HDR_FIELD_START(VIRTCHNL_PROTO_HDR_IPV4_FRAG),
+	/* IPv6 Extension Fragment */
+	VIRTCHNL_PROTO_HDR_IPV6_EH_FRAG_PKID =
+		PROTO_HDR_FIELD_START(VIRTCHNL_PROTO_HDR_IPV6_EH_FRAG),
+	/* GTPU_DWN/UP */
+	VIRTCHNL_PROTO_HDR_GTPU_DWN_QFI =
+		PROTO_HDR_FIELD_START(VIRTCHNL_PROTO_HDR_GTPU_EH_PDU_DWN),
+	VIRTCHNL_PROTO_HDR_GTPU_UP_QFI =
+		PROTO_HDR_FIELD_START(VIRTCHNL_PROTO_HDR_GTPU_EH_PDU_UP),
+};
+
+struct virtchnl_proto_hdr {
+	/* see enum virtchnl_proto_hdr_type */
+	s32 type;
+	u32 field_selector; /* a bit mask to select field for header type */
+	u8 buffer[64];
+	/**
+	 * binary buffer in network order for specific header type.
+	 * For example, if type = VIRTCHNL_PROTO_HDR_IPV4, a IPv4
+	 * header is expected to be copied into the buffer.
+	 */
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(72, virtchnl_proto_hdr);
+
+struct virtchnl_proto_hdrs {
+	u8 tunnel_level;
+	/**
+	 * specify where protocol header start from.
+	 * 0 - from the outer layer
+	 * 1 - from the first inner layer
+	 * 2 - from the second inner layer
+	 * ....
+	 **/
+	int count; /* the proto layers must < VIRTCHNL_MAX_NUM_PROTO_HDRS */
+	struct virtchnl_proto_hdr proto_hdr[VIRTCHNL_MAX_NUM_PROTO_HDRS];
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(2312, virtchnl_proto_hdrs);
+
+struct virtchnl_rss_cfg {
+	struct virtchnl_proto_hdrs proto_hdrs;	   /* protocol headers */
+
+	/* see enum virtchnl_rss_algorithm; rss algorithm type */
+	s32 rss_algorithm;
+	u8 reserved[128];                          /* reserve for future */
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(2444, virtchnl_rss_cfg);
+
+/* action configuration for FDIR */
+struct virtchnl_filter_action {
+	/* see enum virtchnl_action type */
+	s32 type;
+	union {
+		/* used for queue and qgroup action */
+		struct {
+			u16 index;
+			u8 region;
+		} queue;
+		/* used for count action */
+		struct {
+			/* share counter ID with other flow rules */
+			u8 shared;
+			u32 id; /* counter ID */
+		} count;
+		/* used for mark action */
+		u32 mark_id;
+		u8 reserve[32];
+	} act_conf;
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(36, virtchnl_filter_action);
+
+#define VIRTCHNL_MAX_NUM_ACTIONS  8
+
+struct virtchnl_filter_action_set {
+	/* action number must be less then VIRTCHNL_MAX_NUM_ACTIONS */
+	int count;
+	struct virtchnl_filter_action actions[VIRTCHNL_MAX_NUM_ACTIONS];
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(292, virtchnl_filter_action_set);
+
+/* pattern and action for FDIR rule */
+struct virtchnl_fdir_rule {
+	struct virtchnl_proto_hdrs proto_hdrs;
+	struct virtchnl_filter_action_set action_set;
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(2604, virtchnl_fdir_rule);
+
+/* Status returned to VF after VF requests FDIR commands
+ * VIRTCHNL_FDIR_SUCCESS
+ * VF FDIR related request is successfully done by PF
+ * The request can be OP_ADD/DEL/QUERY_FDIR_FILTER.
+ *
+ * VIRTCHNL_FDIR_FAILURE_RULE_NORESOURCE
+ * OP_ADD_FDIR_FILTER request is failed due to no Hardware resource.
+ *
+ * VIRTCHNL_FDIR_FAILURE_RULE_EXIST
+ * OP_ADD_FDIR_FILTER request is failed due to the rule is already existed.
+ *
+ * VIRTCHNL_FDIR_FAILURE_RULE_CONFLICT
+ * OP_ADD_FDIR_FILTER request is failed due to conflict with existing rule.
+ *
+ * VIRTCHNL_FDIR_FAILURE_RULE_NONEXIST
+ * OP_DEL_FDIR_FILTER request is failed due to this rule doesn't exist.
+ *
+ * VIRTCHNL_FDIR_FAILURE_RULE_INVALID
+ * OP_ADD_FDIR_FILTER request is failed due to parameters validation
+ * or HW doesn't support.
+ *
+ * VIRTCHNL_FDIR_FAILURE_RULE_TIMEOUT
+ * OP_ADD/DEL_FDIR_FILTER request is failed due to timing out
+ * for programming.
+ *
+ * VIRTCHNL_FDIR_FAILURE_QUERY_INVALID
+ * OP_QUERY_FDIR_FILTER request is failed due to parameters validation,
+ * for example, VF query counter of a rule who has no counter action.
+ */
+enum virtchnl_fdir_prgm_status {
+	VIRTCHNL_FDIR_SUCCESS = 0,
+	VIRTCHNL_FDIR_FAILURE_RULE_NORESOURCE,
+	VIRTCHNL_FDIR_FAILURE_RULE_EXIST,
+	VIRTCHNL_FDIR_FAILURE_RULE_CONFLICT,
+	VIRTCHNL_FDIR_FAILURE_RULE_NONEXIST,
+	VIRTCHNL_FDIR_FAILURE_RULE_INVALID,
+	VIRTCHNL_FDIR_FAILURE_RULE_TIMEOUT,
+	VIRTCHNL_FDIR_FAILURE_QUERY_INVALID,
+};
+
+/* VIRTCHNL_OP_ADD_FDIR_FILTER
+ * VF sends this request to PF by filling out vsi_id,
+ * validate_only and rule_cfg. PF will return flow_id
+ * if the request is successfully done and return add_status to VF.
+ */
+struct virtchnl_fdir_add {
+	u16 vsi_id;  /* INPUT */
+	/*
+	 * 1 for validating a fdir rule, 0 for creating a fdir rule.
+	 * Validate and create share one ops: VIRTCHNL_OP_ADD_FDIR_FILTER.
+	 */
+	u16 validate_only; /* INPUT */
+	u32 flow_id;       /* OUTPUT */
+	struct virtchnl_fdir_rule rule_cfg; /* INPUT */
+
+	/* see enum virtchnl_fdir_prgm_status; OUTPUT */
+	s32 status;
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(2616, virtchnl_fdir_add);
+
+/* VIRTCHNL_OP_DEL_FDIR_FILTER
+ * VF sends this request to PF by filling out vsi_id
+ * and flow_id. PF will return del_status to VF.
+ */
+struct virtchnl_fdir_del {
+	u16 vsi_id;  /* INPUT */
+	u16 pad;
+	u32 flow_id; /* INPUT */
+
+	/* see enum virtchnl_fdir_prgm_status; OUTPUT */
+	s32 status;
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(12, virtchnl_fdir_del);
+
+/* VIRTCHNL_OP_GET_QOS_CAPS
+ * VF sends this message to get its QoS Caps, such as
+ * TC number, Arbiter and Bandwidth.
+ */
+struct virtchnl_qos_cap_elem {
+	u8 tc_num;
+	u8 tc_prio;
+#define VIRTCHNL_ABITER_STRICT      0
+#define VIRTCHNL_ABITER_ETS         2
+	u8 arbiter;
+#define VIRTCHNL_STRICT_WEIGHT      1
+	u8 weight;
+	enum virtchnl_bw_limit_type type;
+	union {
+		struct virtchnl_shaper_bw shaper;
+		u8 pad2[32];
+	};
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(40, virtchnl_qos_cap_elem);
+
+struct virtchnl_qos_cap_list {
+	u16 vsi_id;
+	u16 num_elem;
+	struct virtchnl_qos_cap_elem cap[1];
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(44, virtchnl_qos_cap_list);
+
+/* VIRTCHNL_OP_CONFIG_QUEUE_TC_MAP
+ * VF sends message virtchnl_queue_tc_mapping to set queue to tc
+ * mapping for all the Tx and Rx queues with a specified VSI, and
+ * would get response about bitmap of valid user priorities
+ * associated with queues.
+ */
+struct virtchnl_queue_tc_mapping {
+	u16 vsi_id;
+	u16 num_tc;
+	u16 num_queue_pairs;
+	u8 pad[2];
+	union {
+		struct {
+			u16 start_queue_id;
+			u16 queue_count;
+		} req;
+		struct {
+#define VIRTCHNL_USER_PRIO_TYPE_UP	0
+#define VIRTCHNL_USER_PRIO_TYPE_DSCP	1
+			u16 prio_type;
+			u16 valid_prio_bitmap;
+		} resp;
+	} tc[1];
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(12, virtchnl_queue_tc_mapping);
+
+/* TX and RX queue types are valid in legacy as well as split queue models.
+ * With Split Queue model, 2 additional types are introduced - TX_COMPLETION
+ * and RX_BUFFER. In split queue model, RX corresponds to the queue where HW
+ * posts completions.
+ */
+enum virtchnl_queue_type {
+	VIRTCHNL_QUEUE_TYPE_TX			= 0,
+	VIRTCHNL_QUEUE_TYPE_RX			= 1,
+	VIRTCHNL_QUEUE_TYPE_TX_COMPLETION	= 2,
+	VIRTCHNL_QUEUE_TYPE_RX_BUFFER		= 3,
+	VIRTCHNL_QUEUE_TYPE_CONFIG_TX		= 4,
+	VIRTCHNL_QUEUE_TYPE_CONFIG_RX		= 5
+};
+
+/* structure to specify a chunk of contiguous queues */
+struct virtchnl_queue_chunk {
+	/* see enum virtchnl_queue_type */
+	s32 type;
+	u16 start_queue_id;
+	u16 num_queues;
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(8, virtchnl_queue_chunk);
+
+/* structure to specify several chunks of contiguous queues */
+struct virtchnl_queue_chunks {
+	u16 num_chunks;
+	u16 rsvd;
+	struct virtchnl_queue_chunk chunks[1];
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(12, virtchnl_queue_chunks);
+
+/* VIRTCHNL_OP_ENABLE_QUEUES_V2
+ * VIRTCHNL_OP_DISABLE_QUEUES_V2
+ * VIRTCHNL_OP_DEL_QUEUES
+ *
+ * If VIRTCHNL version was negotiated in VIRTCHNL_OP_VERSION as 2.0
+ * then all of these ops are available.
+ *
+ * If VIRTCHNL_VF_LARGE_NUM_QPAIRS was negotiated in VIRTCHNL_OP_GET_VF_RESOURCES
+ * then VIRTCHNL_OP_ENABLE_QUEUES_V2 and VIRTCHNL_OP_DISABLE_QUEUES_V2 are
+ * available.
+ *
+ * PF sends these messages to enable, disable or delete queues specified in
+ * chunks. PF sends virtchnl_del_ena_dis_queues struct to specify the queues
+ * to be enabled/disabled/deleted. Also applicable to single queue RX or
+ * TX. CP performs requested action and returns status.
+ */
+struct virtchnl_del_ena_dis_queues {
+	u16 vport_id;
+	u16 pad;
+	struct virtchnl_queue_chunks chunks;
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(16, virtchnl_del_ena_dis_queues);
+
+/* Virtchannel interrupt throttling rate index */
+enum virtchnl_itr_idx {
+	VIRTCHNL_ITR_IDX_0	= 0,
+	VIRTCHNL_ITR_IDX_1	= 1,
+	VIRTCHNL_ITR_IDX_NO_ITR	= 3,
+};
+
+/* Queue to vector mapping */
+struct virtchnl_queue_vector {
+	u16 queue_id;
+	u16 vector_id;
+	u8 pad[4];
+
+	/* see enum virtchnl_itr_idx */
+	s32 itr_idx;
+
+	/* see enum virtchnl_queue_type */
+	s32 queue_type;
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(16, virtchnl_queue_vector);
+
+/* VIRTCHNL_OP_MAP_QUEUE_VECTOR
+ *
+ * If VIRTCHNL_VF_LARGE_NUM_QPAIRS was negotiated in VIRTCHNL_OP_GET_VF_RESOURCES
+ * then only VIRTCHNL_OP_MAP_QUEUE_VECTOR is available.
+ *
+ * PF sends this message to map or unmap queues to vectors and ITR index
+ * registers. External data buffer contains virtchnl_queue_vector_maps structure
+ * that contains num_qv_maps of virtchnl_queue_vector structures.
+ * CP maps the requested queue vector maps after validating the queue and vector
+ * ids and returns a status code.
+ */
+struct virtchnl_queue_vector_maps {
+	u16 vport_id;
+	u16 num_qv_maps;
+	u8 pad[4];
+	struct virtchnl_queue_vector qv_maps[1];
+};
+
+VIRTCHNL_CHECK_STRUCT_LEN(24, virtchnl_queue_vector_maps);
+
+/* Since VF messages are limited by u16 size, precalculate the maximum possible
+ * values of nested elements in virtchnl structures that virtual channel can
+ * possibly handle in a single message.
+ */
+enum virtchnl_vector_limits {
+	VIRTCHNL_OP_CONFIG_VSI_QUEUES_MAX	=
+		((u16)(~0) - sizeof(struct virtchnl_vsi_queue_config_info)) /
+		sizeof(struct virtchnl_queue_pair_info),
+
+	VIRTCHNL_OP_CONFIG_IRQ_MAP_MAX		=
+		((u16)(~0) - sizeof(struct virtchnl_irq_map_info)) /
+		sizeof(struct virtchnl_vector_map),
+
+	VIRTCHNL_OP_ADD_DEL_ETH_ADDR_MAX	=
+		((u16)(~0) - sizeof(struct virtchnl_ether_addr_list)) /
+		sizeof(struct virtchnl_ether_addr),
+
+	VIRTCHNL_OP_ADD_DEL_VLAN_MAX		=
+		((u16)(~0) - sizeof(struct virtchnl_vlan_filter_list)) /
+		sizeof(u16),
+
+	VIRTCHNL_OP_CONFIG_RDMA_IRQ_MAP_MAX	=
+		((u16)(~0) - sizeof(struct virtchnl_rdma_qvlist_info)) /
+		sizeof(struct virtchnl_rdma_qv_info),
+
+	VIRTCHNL_OP_ENABLE_CHANNELS_MAX		=
+		((u16)(~0) - sizeof(struct virtchnl_tc_info)) /
+		sizeof(struct virtchnl_channel_info),
+
+	VIRTCHNL_OP_ENABLE_DISABLE_DEL_QUEUES_V2_MAX	=
+		((u16)(~0) - sizeof(struct virtchnl_del_ena_dis_queues)) /
+		sizeof(struct virtchnl_queue_chunk),
+
+	VIRTCHNL_OP_MAP_UNMAP_QUEUE_VECTOR_MAX	=
+		((u16)(~0) - sizeof(struct virtchnl_queue_vector_maps)) /
+		sizeof(struct virtchnl_queue_vector),
+
+	VIRTCHNL_OP_ADD_DEL_VLAN_V2_MAX		=
+		((u16)(~0) - sizeof(struct virtchnl_vlan_filter_list_v2)) /
+		sizeof(struct virtchnl_vlan_filter),
+};
+
+/**
+ * virtchnl_vc_validate_vf_msg
+ * @ver: Virtchnl version info
+ * @v_opcode: Opcode for the message
+ * @msg: pointer to the msg buffer
+ * @msglen: msg length
+ *
+ * validate msg format against struct for each opcode
+ */
+static inline int
+virtchnl_vc_validate_vf_msg(struct virtchnl_version_info *ver, u32 v_opcode,
+			    u8 *msg, u16 msglen)
+{
+	bool err_msg_format = false;
+	u32 valid_len = 0;
+
+	/* Validate message length. */
+	switch (v_opcode) {
+	case VIRTCHNL_OP_VERSION:
+		valid_len = sizeof(struct virtchnl_version_info);
+		break;
+	case VIRTCHNL_OP_RESET_VF:
+		break;
+	case VIRTCHNL_OP_GET_VF_RESOURCES:
+		if (VF_IS_V11(ver))
+			valid_len = sizeof(u32);
+		break;
+	case VIRTCHNL_OP_CONFIG_TX_QUEUE:
+		valid_len = sizeof(struct virtchnl_txq_info);
+		break;
+	case VIRTCHNL_OP_CONFIG_RX_QUEUE:
+		valid_len = sizeof(struct virtchnl_rxq_info);
+		break;
+	case VIRTCHNL_OP_CONFIG_VSI_QUEUES:
+		valid_len = sizeof(struct virtchnl_vsi_queue_config_info);
+		if (msglen >= valid_len) {
+			struct virtchnl_vsi_queue_config_info *vqc =
+			    (struct virtchnl_vsi_queue_config_info *)msg;
+
+			if (vqc->num_queue_pairs == 0 || vqc->num_queue_pairs >
+			    VIRTCHNL_OP_CONFIG_VSI_QUEUES_MAX) {
+				err_msg_format = true;
+				break;
+			}
+
+			valid_len += (vqc->num_queue_pairs *
+				      sizeof(struct
+					     virtchnl_queue_pair_info));
+		}
+		break;
+	case VIRTCHNL_OP_CONFIG_IRQ_MAP:
+		valid_len = sizeof(struct virtchnl_irq_map_info);
+		if (msglen >= valid_len) {
+			struct virtchnl_irq_map_info *vimi =
+			    (struct virtchnl_irq_map_info *)msg;
+
+			if (vimi->num_vectors == 0 || vimi->num_vectors >
+			    VIRTCHNL_OP_CONFIG_IRQ_MAP_MAX) {
+				err_msg_format = true;
+				break;
+			}
+
+			valid_len += (vimi->num_vectors *
+				      sizeof(struct virtchnl_vector_map));
+		}
+		break;
+	case VIRTCHNL_OP_ENABLE_QUEUES:
+	case VIRTCHNL_OP_DISABLE_QUEUES:
+		valid_len = sizeof(struct virtchnl_queue_select);
+		break;
+	case VIRTCHNL_OP_GET_MAX_RSS_QREGION:
+		break;
+	case VIRTCHNL_OP_ADD_ETH_ADDR:
+	case VIRTCHNL_OP_DEL_ETH_ADDR:
+		valid_len = sizeof(struct virtchnl_ether_addr_list);
+		if (msglen >= valid_len) {
+			struct virtchnl_ether_addr_list *veal =
+			    (struct virtchnl_ether_addr_list *)msg;
+
+			if (veal->num_elements == 0 || veal->num_elements >
+			    VIRTCHNL_OP_ADD_DEL_ETH_ADDR_MAX) {
+				err_msg_format = true;
+				break;
+			}
+
+			valid_len += veal->num_elements *
+			    sizeof(struct virtchnl_ether_addr);
+		}
+		break;
+	case VIRTCHNL_OP_ADD_VLAN:
+	case VIRTCHNL_OP_DEL_VLAN:
+		valid_len = sizeof(struct virtchnl_vlan_filter_list);
+		if (msglen >= valid_len) {
+			struct virtchnl_vlan_filter_list *vfl =
+			    (struct virtchnl_vlan_filter_list *)msg;
+
+			if (vfl->num_elements == 0 || vfl->num_elements >
+			    VIRTCHNL_OP_ADD_DEL_VLAN_MAX) {
+				err_msg_format = true;
+				break;
+			}
+
+			valid_len += vfl->num_elements * sizeof(u16);
+		}
+		break;
+	case VIRTCHNL_OP_CONFIG_PROMISCUOUS_MODE:
+		valid_len = sizeof(struct virtchnl_promisc_info);
+		break;
+	case VIRTCHNL_OP_GET_STATS:
+		valid_len = sizeof(struct virtchnl_queue_select);
+		break;
+	case VIRTCHNL_OP_RDMA:
+		/* These messages are opaque to us and will be validated in
+		 * the RDMA client code. We just need to check for nonzero
+		 * length. The firmware will enforce max length restrictions.
+		 */
+		if (msglen)
+			valid_len = msglen;
+		else
+			err_msg_format = true;
+		break;
+	case VIRTCHNL_OP_RELEASE_RDMA_IRQ_MAP:
+		break;
+	case VIRTCHNL_OP_CONFIG_RDMA_IRQ_MAP:
+		valid_len = sizeof(struct virtchnl_rdma_qvlist_info);
+		if (msglen >= valid_len) {
+			struct virtchnl_rdma_qvlist_info *qv =
+				(struct virtchnl_rdma_qvlist_info *)msg;
+
+			if (qv->num_vectors == 0 || qv->num_vectors >
+			    VIRTCHNL_OP_CONFIG_RDMA_IRQ_MAP_MAX) {
+				err_msg_format = true;
+				break;
+			}
+
+			valid_len += ((qv->num_vectors - 1) *
+				sizeof(struct virtchnl_rdma_qv_info));
+		}
+		break;
+	case VIRTCHNL_OP_CONFIG_RSS_KEY:
+		valid_len = sizeof(struct virtchnl_rss_key);
+		if (msglen >= valid_len) {
+			struct virtchnl_rss_key *vrk =
+				(struct virtchnl_rss_key *)msg;
+
+			if (vrk->key_len == 0) {
+				/* zero length is allowed as input */
+				break;
+			}
+
+			valid_len += vrk->key_len - 1;
+		}
+		break;
+	case VIRTCHNL_OP_CONFIG_RSS_LUT:
+		valid_len = sizeof(struct virtchnl_rss_lut);
+		if (msglen >= valid_len) {
+			struct virtchnl_rss_lut *vrl =
+				(struct virtchnl_rss_lut *)msg;
+
+			if (vrl->lut_entries == 0) {
+				/* zero entries is allowed as input */
+				break;
+			}
+
+			valid_len += vrl->lut_entries - 1;
+		}
+		break;
+	case VIRTCHNL_OP_GET_RSS_HENA_CAPS:
+		break;
+	case VIRTCHNL_OP_SET_RSS_HENA:
+		valid_len = sizeof(struct virtchnl_rss_hena);
+		break;
+	case VIRTCHNL_OP_ENABLE_VLAN_STRIPPING:
+	case VIRTCHNL_OP_DISABLE_VLAN_STRIPPING:
+		break;
+	case VIRTCHNL_OP_REQUEST_QUEUES:
+		valid_len = sizeof(struct virtchnl_vf_res_request);
+		break;
+	case VIRTCHNL_OP_ENABLE_CHANNELS:
+		valid_len = sizeof(struct virtchnl_tc_info);
+		if (msglen >= valid_len) {
+			struct virtchnl_tc_info *vti =
+				(struct virtchnl_tc_info *)msg;
+
+			if (vti->num_tc == 0 || vti->num_tc >
+			    VIRTCHNL_OP_ENABLE_CHANNELS_MAX) {
+				err_msg_format = true;
+				break;
+			}
+
+			valid_len += (vti->num_tc - 1) *
+				     sizeof(struct virtchnl_channel_info);
+		}
+		break;
+	case VIRTCHNL_OP_DISABLE_CHANNELS:
+		break;
+	case VIRTCHNL_OP_ADD_CLOUD_FILTER:
+	case VIRTCHNL_OP_DEL_CLOUD_FILTER:
+		valid_len = sizeof(struct virtchnl_filter);
+		break;
+	case VIRTCHNL_OP_ADD_RSS_CFG:
+	case VIRTCHNL_OP_DEL_RSS_CFG:
+		valid_len = sizeof(struct virtchnl_rss_cfg);
+		break;
+	case VIRTCHNL_OP_ADD_FDIR_FILTER:
+		valid_len = sizeof(struct virtchnl_fdir_add);
+		break;
+	case VIRTCHNL_OP_DEL_FDIR_FILTER:
+		valid_len = sizeof(struct virtchnl_fdir_del);
+		break;
+	case VIRTCHNL_OP_GET_QOS_CAPS:
+		break;
+	case VIRTCHNL_OP_CONFIG_QUEUE_TC_MAP:
+		valid_len = sizeof(struct virtchnl_queue_tc_mapping);
+		if (msglen >= valid_len) {
+			struct virtchnl_queue_tc_mapping *q_tc =
+				(struct virtchnl_queue_tc_mapping *)msg;
+			if (q_tc->num_tc == 0) {
+				err_msg_format = true;
+				break;
+			}
+			valid_len += (q_tc->num_tc - 1) *
+					 sizeof(q_tc->tc[0]);
+		}
+		break;
+	case VIRTCHNL_OP_GET_OFFLOAD_VLAN_V2_CAPS:
+		break;
+	case VIRTCHNL_OP_ADD_VLAN_V2:
+	case VIRTCHNL_OP_DEL_VLAN_V2:
+		valid_len = sizeof(struct virtchnl_vlan_filter_list_v2);
+		if (msglen >= valid_len) {
+			struct virtchnl_vlan_filter_list_v2 *vfl =
+			    (struct virtchnl_vlan_filter_list_v2 *)msg;
+
+			if (vfl->num_elements == 0 || vfl->num_elements >
+			    VIRTCHNL_OP_ADD_DEL_VLAN_V2_MAX) {
+				err_msg_format = true;
+				break;
+			}
+
+			valid_len += (vfl->num_elements - 1) *
+				sizeof(struct virtchnl_vlan_filter);
+		}
+		break;
+	case VIRTCHNL_OP_ENABLE_VLAN_STRIPPING_V2:
+	case VIRTCHNL_OP_DISABLE_VLAN_STRIPPING_V2:
+	case VIRTCHNL_OP_ENABLE_VLAN_INSERTION_V2:
+	case VIRTCHNL_OP_DISABLE_VLAN_INSERTION_V2:
+	case VIRTCHNL_OP_ENABLE_VLAN_FILTERING_V2:
+	case VIRTCHNL_OP_DISABLE_VLAN_FILTERING_V2:
+		valid_len = sizeof(struct virtchnl_vlan_setting);
+		break;
+	case VIRTCHNL_OP_ENABLE_QUEUES_V2:
+	case VIRTCHNL_OP_DISABLE_QUEUES_V2:
+		valid_len = sizeof(struct virtchnl_del_ena_dis_queues);
+		if (msglen >= valid_len) {
+			struct virtchnl_del_ena_dis_queues *qs =
+				(struct virtchnl_del_ena_dis_queues *)msg;
+			if (qs->chunks.num_chunks == 0 ||
+			    qs->chunks.num_chunks > VIRTCHNL_OP_ENABLE_DISABLE_DEL_QUEUES_V2_MAX) {
+				err_msg_format = true;
+				break;
+			}
+			valid_len += (qs->chunks.num_chunks - 1) *
+				      sizeof(struct virtchnl_queue_chunk);
+		}
+		break;
+	case VIRTCHNL_OP_MAP_QUEUE_VECTOR:
+		valid_len = sizeof(struct virtchnl_queue_vector_maps);
+		if (msglen >= valid_len) {
+			struct virtchnl_queue_vector_maps *v_qp =
+				(struct virtchnl_queue_vector_maps *)msg;
+			if (v_qp->num_qv_maps == 0 ||
+			    v_qp->num_qv_maps > VIRTCHNL_OP_MAP_UNMAP_QUEUE_VECTOR_MAX) {
+				err_msg_format = true;
+				break;
+			}
+			valid_len += (v_qp->num_qv_maps - 1) *
+				      sizeof(struct virtchnl_queue_vector);
+		}
+		break;
+	/* These are always errors coming from the VF. */
+	case VIRTCHNL_OP_EVENT:
+	case VIRTCHNL_OP_UNKNOWN:
+	default:
+		return VIRTCHNL_STATUS_ERR_PARAM;
+	}
+	/* few more checks */
+	if (err_msg_format || valid_len != msglen)
+		return VIRTCHNL_STATUS_ERR_OPCODE_MISMATCH;
+
+	return 0;
+}
+#endif /* _VIRTCHNL_H_ */
